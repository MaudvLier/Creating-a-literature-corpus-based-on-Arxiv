RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models


Introduction


Cloud computing platforms have been increasingly utilized for application and service deployment in recent years [*REF*; *REF*]. Anomalies in cloud computing systems, such as unrecoverable failures and hanged jobs, severely impact customer experience and can potentially violate service level agreements [*REF*; *REF*]. Root Cause Analysis (RCA) [*REF*; *REF*; *REF*], a core component of site reliability engineering, is currently receiving ongoing attention from large cloud computing enterprises such as Amazon, Microsoft, Google, and Alibaba. However, due to the continuous scaling of computation deployments on the cloud, manual workflows for online anomaly RCA, such as creating troubleshooting tools, often overwhelm Site Reliability Engineers (SREs) [*REF*]. To increase the efficiency of cloud service reliability enhancement, a series of Artificial Intelligence for Operations (AIOps) approaches [*REF*; *REF*; *REF*] have been widely adopted in RCA to reduce the MTTR (mean time to resolve).
These approaches grant the ability to handle large volumes of incident-related data in cloud systems and draw conclusions automatically. While these typical AIOps aid in automated processes, their application faces challenges such as poor data quality, shifting data distribution, laborious data annotation, and limited generalization for models [*REF*].


The advancements in Large Language Models (LLMs), especially within the GPT [*REF*; *REF*; *REF*; *REF*] and LLaMA [*REF*; *REF*] families, indicate an intriguing future of solving intricate reasoning tasks. These developments hold significant promise for addressing challenges in AIOps. One notable strength of LLMs is their ability to generalize, allowing them to understand and accomplish unseen reasoning tasks in a zero-shot or few-shot way. This largely results from the extensive and extrapolatable process of pre-training and instruction tuning [*REF*; *REF*]. Such adaptability makes LLMs suitable for specialized and system-specific RCA tasks. Moreover, the performance of LLMs can be amplified by various prompting strategies. Techniques like Chain-of-Thought (CoT) [*REF*; *REF*], Self-Consistency (SC) [*REF*], Reflexion [*REF*], Retrieval-Augmented Generation (RAG) [*REF*], enhance logical capability and Out-of-Domain (OoD) generalization. These techniques further reduce the necessity of high-quality, task-specific annotated training datasets, particularly in RCA applications.
Considering these inherent advantages and the versatility of LLMs, they emerge as new fitting solutions for the RCA task.


Recent works demonstrate the use of LLMs in cloud RCA tasks.
Specifically,  [*REF*] fine-tunes a GPT model to generate text from incident summary to root causes and mitigations without collecting data in the production environment.
Oasis [*REF*] proposes to fine-tune a GPT model using domain-specific data to predict outage summaries for root cause understanding and post hoc maintenance. However, these works rely heavily on the computationally expensive fine-tuning of an LLM to adapt to cloud system tasks and do not fully utilize the generalization and reasoning abilities of LLMs. One possible solution is to use few-shot RAG [*REF*] on LLMs, with representative methods such as RCACopilot [*REF*] and PACE-LM [*REF*].
RCACopilot employs an LLM as a summarizer for information acquired by human-written troubleshooting guides and as a root cause predictor prompted by recent relevant diagnoses, based on the assumption that incidents with similar root causes recur within a short period. PACE-LM enhances the outputs of RAG-based language models through confidence calibration to mitigate hallucinations and reduce false recommendations.
However, these methods are all based on the GPT family and scenarios within Microsoft, not addressing the data privacy concerns associated with using LLMs with cloud system data. Specifically, potential security risks tied to transmitting data from cloud production environments to external APIs, such as ChatGPT [*REF*], could pose problems for many IT enterprises. Furthermore, none of the aforementioned methods fully leverage the autonomous capabilities of LLMs for information collection, decision-making, and environmental interaction [*REF*].


Tool-augmented autonomous agents, as demonstrated in early experiments [*REF*], further unlock the potential of LLMs in interactive environments. By equipping LLMs with defined tools and associated documentation, and by facilitating tool invocation through mechanisms like function calls or command line inputs, and then executing these tools and returning environmental feedback, LLMs can handle extensive tasks. These tasks might require expertise and abilities beyond what LLMs inherently possess, such as web interaction [*REF*], text-based games [*REF*], and operating system and database management [*REF*]. A representative paradigm within the realm of autonomous agents is ReAct [*REF*], a workflow that embodies a thought-action-observation loop. This approach synergizes the reasoning and action capabilities of LLMs and offers flexibility for extensions [*REF*]. Although tool-augmented autonomous agents based on LLM demonstrate significant potential in complex scenarios, their adoption in the AIOps field, especially with noisy and lengthy data, remains limited [*REF*; *REF*]. The primary challenges are action validity and context length, both of which significantly heighten the demands of LLM-as-agent capabilities [*REF*]. Also, to the best of our knowledge, there is no interactive environment built upon realistic production-level RCA problems for LLM agents to operate on.


To this end, we introduce RCAgent, the first practical LLM-based RCA framework within the tool-augmented autonomous agent paradigm. We design an enhanced prompting cycle skeleton and an interactive environment enriched with external knowledge and stabilization techniques, tailored for LLM agents to handle diverse data types. This approach addresses several challenges, including context-length usage, unstable parsing, and insufficient domain knowledge. Additionally, we design aggregation methods for action trajectories and text output, combining suboptimal results from LLMs. Unlike ReAct, our approach to tool invocation operates in a trajectory-level zero-shot way, eliminating the need for manual or auto-generated problem-solving trajectories. Furthermore, to facilitate general and secure industrial usage, we forgo the use of powerful external API models like ChatGPT as the base model for agents and implement this framework on a locally deployed model, further underscoring the efficacy of our stabilization method.


The analysis results from RCAgent are being utilized in the Real-time Compute Platform for Apache Flink of Alibaba Cloud to diagnose anomalous stream processing jobs uncovered by current methods. We have incorporated a feedback mechanism in the company to identify issues in the PaaS and IaaS layers of the cloud system, offering insights for development teams.


We summarize our contributions as follows:


- We propose RCAgent, the first tool-augmented autonomous agent based on LLM for privacy-aware real-world cloud RCA, unleashing the decision-making ability of LLMs in the AIOps field.
- We introduce a bag of methods to enhance the tool agent, including aspects of prompting framework, tool setting, stabilization, and aggregation methods. These make the agent based on locally deployed LLM a valid solution for complex environments like cloud systems.
- We present real-world experiments on the real-time computing jobs in Alibaba Cloud, demonstrating the practical usage of RCAgent.


Motivation and Challenge


Online Job Anomaly RCA with AIOps


In cloud services, job anomalies indicate disruptions in submitted jobs that require human intervention. While big data systems have fault-tolerance mechanisms like failover and failback, they do not guarantee job stability. As the enhancement of reliability, AIOps has been transforming the cloud incident management workflow for years [*REF*]. Equipped with AIOps approaches, a typical semi-automated troubleshooting lifecycle includes: (i) anomaly detection based on data-driven criteria, (ii) RCA using either human insight or AIOps, and (iii) mitigation measures for job recovery and prevention of similar anomalies. The RCA task, crucial for discerning job issues, demands thorough data inspection and up-to-date domain knowledge from either human experts or AIOps methods. Essential for cloud system governance and as a foundation for stability enhancement, RCA is now significantly automated by AIOps.


However, the intricate and dynamic nature of cloud systems combined with ever-growing data presents challenges for AIOps methods. These data-driven methods, relying on historical data, struggle as data and anomaly patterns evolve, prolonging RCA and mitigation processes.


Benifit of Tool augmented LLM agents


The paradigm of integrating tools into LLMs and instructing them to perform actions automatically has demonstrated impressive promise across various tasks [*REF*]. While basic LLM usages are adept at generalizing to OoD data with proper instruction and few-shot samples, they remain dependent on human-designed workflows of data collection and analysis [*REF*] which may periodically be outdated. In contrast, tool-augmented LLM agents can self-direct their data exploration and external ability invocation, making them extendable to heterogeneous tasks. By harnessing LLMs&apos; power of decision-making, autonomous agents possess significant potential for analyzing unprecedented and complex anomalies without laboriously annotated training data.


Challenges


Though tool-augmented LLM agents provide new possibilities for the cloud RCA task, there are several critical challenges.


Privacy


A general LLM method for RCA and other cloud AIOps tasks should be internally hosted for security concerns. Specifically, transmitting production-level confidential data to external API induces privacy risks. This means stronger models like ChatGPT cannot be used except for those close collaboration enterprises.


FIGURE


Context Length


A fundamental problem for an autonomous agent to interact with realistic big data environments is context length because various kinds of data, such as logs, code, and database query results, tend to be enormous. The length distribution from our experiment data is shown in Fig. [1]. Even if the LLMs can extrapolate to larger context length [*REF*], processing unnecessarily excessive tokens is highly inefficient.


Action Validity


Open-ended action generation for LLMs is of great challenge because less sufficiently aligned LLMs have a larger possibility of generating invalid actions [*REF*]. These errors severely damage the performance of autonomous agents on comprehensive tasks. Moreover, the model restriction from the privacy concern, and noisy data in cloud systems make this problem even more arduous.


FIGURE


Methodology


To systematically and reliably prompt the LLM as a tool-augmented autonomous agent for cloud RCA, we propose RCAgent, an enhanced reasoning and acting framework. An overview of our methodology at the decision-loop level and its comparison to the tool agent version of ReAct [*REF*] is shown in Fig. For disambiguousity, the LLM agent with the prompt of thought-action-observation loop is named the controller agent, and RCAgent additionally employs the LLM as tools called the expert agents.


In concordance with the typical ReAct-style tool agent prompt framework [*REF*], the controller agent is injected with three basic prompts: (i) framework rules that describe the thought-action-observation loop, (ii) task requirements that contain instructions for the RCA tasks with basic cloud knowledge, and (iii) tools documentation that describes the description of all invokable tools. Because of its flexibility and readability, JSON is chosen as the data interchange format for all generations in the action step from LLM.
We also define a tool named &apos;finalize&apos; as an exit point that allows the model to freely decide when to report findings in a parsable format.
Note that RCAgent discards the few-shot examples compared to the original ReAct because of limited context length.


Starting from the tool agent version of ReAct, we propose several enhancements to address the challenges of using tool-augmented LLM agents in cloud RCA. We will first introduce an observation management method for compressing context length usage in § [3.1].
Then, we describe the design of tools, including LLM-augmented tools in § [3.2].
The stabilizing methods for action validity are presented in § [3.3]. Lastly, we detail the aggregation method of RCAgent in § [3.4].


FIGURE


Observation Snapshot Key 


One of the basic challenges of building autonomous agents in a comprehensive big data environment is context length [*REF*]. The most inflating part of the agent prompt is the observation content in the action trajectories, containing a large amount of logs, table entries, etc. To overcome the information loss from truncating and summarizing observations, we propose OBservation Snapshot Key (OBSK), a new method to address the context length problem in realistic cloud tasks. As shown in Fig., OBSK only shows the head of observation to the controller agent, leaving a hash ID (snapshot key) for further usage. A key-value store is built for mapping the snapshot key to real observation. Thus, when a snapshot key is found in a parsed action, RCAgent queries through the key-value store and returns the corresponding observation. This ensures necessary information with controlled length is provided for the controller agent as supportive information for decision-making.


An example of using the OBSK mechanism is shown in Fig. [2]. With the runtime_log function giving extensive error log as observation, its tail content is omitted for brevity, and the whole observation is mapped to a snapshot key. The usage of snapshot keys is to pass them as parameters to tools in the form of function calling, using external methods to handle the lengthy data, such as log_agent in Fig. [2]. These rules of OBSK are elaborated in the system prompt, instructing the LLM to use snapshots instead of directly handling long text as parameters if needed.


Tool Preperation 


In this subsection, we provide a general compound paradigm for tool design in root cause analysis tasks, which is extendable to different cloud systems and platforms. We employ data querying functions as information-gathering tools and LLM-based expert agents as analytical tools, similar to the data collection and analysis process done by human SREs.


Information-gathering Tools


Information-gathering tools are designed in an easy-to-use way, hiding all unrelated details in accessing data in cloud systems. For example, instead of giving SQL interface and Log query API to LLM, these tools only accept simple parameters like the ID of entities. This kind of semantically minimalist tool setting will significantly reduce the threshold for LLMs to take valid actions and prevent them from useless exploration in large data warehouses. Adhering to this design principle will result in shorter and less intricate tool documentation, preserving valuable context length for the controller agent. Also, to reduce the computational cost induced by meaningless repetitive data in logs and tables, and make it less likely for LLMs to fall in repetitive degeneration [*REF*], it&apos;s necessary to deduplicate retrieved information. We do so by fuzzy-matching all data entries returned by each tool and eliminating the duplicates.


FIGURE


Analytical Tools


Analytical tools are proposed to extend the domain knowledge and abilities of the controller agent. In cloud system tasks, most domain knowledge-dependent tasks, e.g., log analysis, often require analyzing a large amount of data. This naturally leads to an analyze-aggregate method. Additionally, the analytical tools can be augmented by LLMs with their reasoning ability. We name this kind of analytical tool the expert agent, which is shown in Fig. . We provide two expert agents for RCAgent as complementary knowledge tools, called the code analysis tool and the log analysis tool, which are described in detail below. Both generate analyses and aggregations prompted by the zero-shot CoT [*REF*] and answer extraction instructions.


Code analysis tool. The code analysis tool works in a recursive manner, which is shown in Fig. [3]. Given a class name, the code analysis tool searches the corresponding file in the code repository. After the LLM reading and analyzing the code file, it is prompted to suggest any other classes that would be helpful to analyze as supportive information.
These suggestions from each code-reading round will be stored in a task queue, managing all pending tasks. With this exhaustive search, the code analysis tool stops parsing when no more code files of interest are recommended, or when all remaining recommended files are external dependencies. Then, we utilize an LLM to summarize all the code files, whose result is presented to the controller agent as the observation.
This tool can extend domain knowledge such as the working mechanism of diagnose tools to the LLM control agent.


ALGORITHM


Log analysis tool. The log analysis tool operates in an in-context RAG paradigm with some adaptions to lengthy log data. The complete mechanism is shown in Algorithm . To reduce the context window requirement, we use a semantic partitioning method to split the log into shorter chunks while preserving its relative completeness. Specifically, we split the log *MATH* into lines *MATH* and built edges between lines with the cosine similarity of embeddings exponentially decayed by document distance as weights *MATH*. This yields a weighted undirected dense graph *MATH* regarding lines as vertices. Then the graph is clustered with Louvain community detection [*REF*]. Since the clustering method is unaware of the requirement of log chunking that each cluster must be internally continuous, we remove overlaps by greedily switching the minimum amount of clustering labels. The partitioned log chunks *MATH* are then fed into the log agent one chunk per round. After the above semantic partitioning process, in-context learning with similarity retrieved samples *MATH*  (In-Context Prompt) is performed, generating log analysis with related domain knowledge. Moreover, we instruct the expert agent to output evidence supporting its analysis by directly copying log content.
This is based on our observation that LLM hallucinates occasionally and analyzes in-context examples rather than the partitioned chunk.
Intuitively this phenomenon might result from long prompt content burying the delimiters between the example and target data. If the evidence listed by LLMs cannot be fuzzy-matched to the chunk *MATH*, the analysis result is discarded. Thus, we ensure reliable RAG analysis on lengthy non-natural language.


Stablization 


To overcome the degradation of LLM action validity induced by noisy data and local LLMs with less capability, we introduce two stabilization methods for tool invocation: JSON repairing and error handling. We describe them in detail as follows.


JSON Repairing 


One of the vital problems in real-world applications of tool-augmented LLM autonomous agents is structured inference for parsable data. To our knowledge, there is no pain-free method to guarantee a specific data format (e.g., JSON) for interactions between LLM agents and the environment. Even though there are some toolkits, such as JSONFormer and TypeChat, that help generate the correct JSON string, they either cannot generate free-form JSON with extensive escape characters while not impair generation quality, or solely rely on LLMs&apos; capability of token-level error correction. When the structured output from LLM grows complex, containing information from noisy cloud system data, its insensitivity to token-level errors, including redundant or missing escape slashes and other control symbols, becomes problematic. These erroneous interchange data may lead to failures in both the controller and expert agents who rely on structured data for communication. To solve this issue, we employ an intuitive and effective method to generate structured interchange data named JsonRegen, as shown in Algorithm .


ALGORITHM


Before LLM inference, all sensitive characters that may correspond to control symbols in JSON, and do not belong to real JSON objects such as action history, are replaced with insensitive ones for a clean prompt.
For example, double quotes are reduced to single quotes, and &apos;\[&apos; and &apos;{&apos; are replaced by &apos;`&lt;`:&apos; and &apos;`&lt;`%&apos;. This reduces the risk of LLMs&apos; quoting content as outputs without proper escaping. When trivial cleaning of wrong escape patterns fails to make the JSON-like string from LLM output parsable, which is extracted through curly bracket matching, a regeneration process is performed. To enforce the understanding of JSON structure for the LLM, we instruct it to convert the content to YAML. The LLM is then prompted to regenerate a JSON with the same structure and content. The regeneration proceeds for several rounds until a valid JSON is parsed or the retry count is exceeded.


Error Handling 


The previous work [*REF*] demonstrates that LLMs in tool invocation tend to propagate errors, limiting exploratory actions. These issues are even more pronounced in less capable LLMs. Inspired by [*REF*], we use pre-defined criteria to mark problematic actions or states as erroneous. As shown in Fig., we provide error messages and suggestions to the controller agent, including these circumstances: (i) duplicate invocation of stateless tools with the same arguments, (ii) trivial input to expert agents, and (iii) early finalizing without thorough investigation. These error messages can reduce the frequency of meaningless actions taken by the control agent by alerting it to avoid repeating the same mistakes.


Self-Consistency Aggregation 


Self-Consistency (SC) [*REF*] has proved its efficacy in various close-ended NLP tasks, including multi-choice and numerical problems.
However, aggregating sampled open-ended multi-step generation like RCA with LLM agent, is underexplored. To our knowledge, utilizing SC on ReAct style traces is also not well-defined or rigorously investigated.
Thus, we propose applying the SC paradigm to free-form generation on the topic of LLM autonomous agents, and introduce them in two aspects: SC for text data and SC for tool using trajectories.


Self-Consistency for Text Data.


SC is initially proposed to ensemble close-ended results with CoT and cannot aggregate open-ended text results from ReAct trajectories. To apply SC to text data, we utilize two methods in our experiments:


- Vote with embedding. We directly generalize the idea of unweighted SC (majority vote), which performs best across all tasks [*REF*]. The voting can be rewritten as *MATH* *MATH*, *MATH* where *MATH* is sample count, and *MATH* is a one-hot vector representing sampled result *MATH* with each position as a candidate choice or numerical result. We simply replace *MATH* as semantic embeddings for text output. This intuitively means the text result closest to the majority is chosen as an aggregated result.
- Aggregate with LLMs. Considering the possible diversity of generated content, we prompt LLM to aggregate the candidates and output in similar form and length.


FIGURE


Self-Consistency for Tool Using Trajectories


SC has been comprehensively tested on CoT reasoning paths, and can naturally be utilized on ReAct style trajectories. However, directly sampling multiple cycles of thought-action-observation can be computationally expensive. This is even more costly while some actions, such as activating expert agents, have substantial consumption.
Moreover, random sampling from the first step without history or few-shot examples leads to flooding erroneous actions, e.g. consecutive calling of non-existent tools.


Therefore, we propose a mid-way sampling method named Trajectory-level Self-Consistency (TSC) as shown in Fig. . Specifically, only when the controller agent is stepping into finalization does the sampling start from the second last step. This sampling strategy shares most preliminary steps between trajectory samples and reduces unnecessary consumption. Besides, the more stable action history from greedy decoding provides exemplification without additional context-length consumption from few-shot examples, thereby suppressing the validity drop from sampling.
We don&apos;t constrain the steps of further actions, allowing for multiple or even zero steps of action sampling until either finalization is reached or a global upper bound is met. This method strikes a balance between full-process SC on agent trajectories and one-step CoT SC. It preserves stability at the initial phases of information collection while also promoting diversity in reasoning paths.


Experiment


We develop and evaluate RCAgent on the Real-time Compute Platform in Alibaba Cloud, which is an enterprise-level, high-performance system capable of real-time stream data computation based on Apache Flink. This system is largely optimized on the community version and achieves a throughput of *MATH* million data records per second during peak hours.
Hence, our experiment is based on complex, real-world, and crucial cloud systems.


As the key contribution of our work, we build a tool-augmented autonomous agent based on locally hosted LLM to accomplish the task of cloud RCA and demonstrate its efficacy on real-world cloud systems. We should ensure the stability of agent actions and the quality of the analysis result, enabling it to work in the production environment. To achieve and verify this, we answer the following questions with experiments.


- RQ1: How effective is RCAgent as a comprehensive RCA agent on the real-world cloud system?
- RQ2: To what extent does each design component in RCAgent contribute?
- RQ3: Is RCAgent superior at generating efficient and stable decision trajectories?
- RQ4: Is RCAgent useful for real production services and does it cover anomalies beyond the scope of current troubleshooting rules?
- RQ5: How much does Self-Consistency improve agents of different methods and sampling scales?


In the following subsections, we discuss the configurations of our experiment.


Model Configuration


The base model in our implementation is Vicuna-13B-V1.5-16K [*REF*], one of the LLaMA 2 [*REF*] fine-tuned models with extended context length. We do model inference with vLLM [*REF*] backend on a single NVIDIA A100 SXM4 GPU (80 GB). To make most results reproducible and stable, we use the greedy decoding strategy by default. To reduce repetitive degeneration [*REF*] and prevent quality downgrade introduced by decoding penalty and sampling [*REF*], we use an adaptive penalty strategy to suppress repetition: whenever a generation exceeds a token threshold (e.g., *MATH*, suggesting a looping pattern), the generation is restarted with *MATH* repetition and frequency penalty. This penalty adjustment can be applied iteratively if repetition persists. During self-consistency where random sampling is required, we use the default configuration of Vicuna: *MATH* temperature and *MATH* nucleus sampling.


The embedding model we use is GTE-LARGE [*REF*], for its slightly better results on MTEB [*REF*] than text-embedding-ada-002 from OpenAI, providing an internally deployable substitute.


For Self-Consistency results, we use *MATH* output samples by default. The experiment about Self-Consistency sample count with *MATH* different runs is also provided. We employ a step-wise Self-Consistency denoted as SC in tables and figures, which only accepts samples that finalize synchronously with the greedy geocoding trajectory. This trajectory solely samples the thinking process which typically performs in CoT style before the finalizing function, without the chance of additional action steps.


Dataset Preperation


Anomaly Selection


For root cause analysis, we collect a dataset of *MATH* anomalous jobs, either unrecoverable fail, or fail to start in *MATH* minutes. We filter the data and obtain about *MATH* non-trivial anomalous jobs with substantial log content. We use the Flink Advisor knowledge base, which is a large rule set distilled from experienced SREs&apos; domain knowledge, to create analysis results for these jobs. *MATH* of these jobs are successfully analyzed by Flink Advisor, while the rest are mostly long-tailed problems or unresolved problems.


FIGURE


Due to the imbalance of anomalies, which means a large proportion of anomalies have the same root cause, we reduce the successfully analyzed jobs to an offline dataset of *MATH* jobs. The reduction is done with the class-balance constraint that no more than two jobs have identical root causes, making sure there are no prevalent easy RCA tasks. The required annotation of these jobs contains four items:


- root cause, the fundamental cause of the anomaly
- solution, the mitigation method to deal with the problem
- evidence, the direct supportive information for determining the result
- responsibility, who should care about the problem among users or platform according to rules in Fig. [4]


Examples of the label are shown in Fig. [5]. To align with the annotation format, the controller agent is instructed in its documentation to return all the above items as arguments in their finalization function tool.


FIGURE


The annotation process for these jobs is LLM-assisted. We first use LLM to summarize the analysis result from Flink Advisor and output the above four items. Then human proofreading and correction are done by the SRE team.


The remaining jobs that are neither trivial nor analyzable by Flink Advisor are semantically clustered, resulting in *MATH* representative online cases. These potential OoD cases are beyond the capability of the current rules and are manually labeled by experienced SREs with two items: responsibility and root cause.


We guarantee that all of our annotations do not show similar but uninformative patterns like &quot;The root cause of this anomaly is...&quot; that cause some of the semantic scores untrustworthy, as is discovered in [*REF*; *REF*].


Data Sources


The available data sources, on which we build information-gathering tools for the LLM agent, of the above anomalous jobs include


- Log data at three levels: platform, runtime, and infrastructure, stored and queried in SLS (Simple Log Service) of Alibaba Cloud.
- Database containing the history of advisor services
- Repositories containing the code of advisor services


For log and database entries, only data before the detection time of the anomaly can be retrieved, preventing the analysis of future information and adhering to real-world usage.


The retrieval log database for the log analysis agent is a history subset of Flink Advisor. We guarantee that no analysis rules for labeling are present in the log database by strict filtering.


Evaluation Metrics


Besides semantic metric scores including METEOR [*REF*], BERTScore [*REF*] (deberta-large-mnli), NUBIA [*REF*] (6-dim), BLEURT [*REF*], and BARTScore [*REF*] (F-Score, CNNDM), we use additional embedding Score (EmbScore), the cosine similarity from the default embedding model in our experiment: *MATH* *MATH*, *MATH* where *MATH* is the predicted result and *MATH* is the ground truth. Due to the narrow numerical range impairing the readability of some metrics when evaluating RCA results, also shown in [*REF*], we add a normalized score for BERTScore and EmbScore: *MATH* *MATH*, *MATH* where *MATH* is baseline content, being &apos;Unclear&apos; in our experiments. The baseline content is also automatically filled for trajectories that either fail or return incomplete items in the results.


To evaluate the validness of action trajectories and stability of the autonomous agent, we add Pass Rate and Invalid Rate indicating how much proportion of trajectories successfully exit by calling the finalizing function within *MATH* steps, and how much proportion of actions are invalid, averaged across trajectories, respectively.


We also follow the common practice of using stronger models to estimate model prediction [*REF*; *REF*; *REF*]. We use greedy decoding gpt-4-0613, a frozen version of GPT4 [*REF*] for better reproducibility. We prompt the model to judge the accuracy and helpfulness of root cause and solution predictions, marked as G-Correctness and G-Helpfulness, respectively, and give a score within *MATH*. The prompts we use are:


Judge the correctness of the prediction, *MATH* is completely wrong and *MATH* is well-matched


Judge the helpfulness of the prediction, *MATH* is completely misleading and *MATH* is very helpful


A Win Rate metric is calculated with GPT evaluation by judging if the result from each method is better than the one from ReAct.


For human evaluation, we present our results from trajectories to *MATH* members of the SRE team, instructing each of them to assign a *MATH* H-Helpfulness score to every result. A thorough guideline is provided on each score for justified scoring, e.g., *MATH* for misleading, *MATH* for moderately helpful with related content, and *MATH* for exceptionally helpful and well-aligned. Each participant is required to evaluate the entirety of the presented results, ensuring comprehensive coverage.


Based on our observation, certain online jobs require investigation beyond the current agent&apos;s environment reachabilities, e.g., knowledge about underlying connections between different cloud systems.
Accordingly, we identify these jobs based on human feedback and exempt their human evaluation scores.


TABLE


TABLE


Result


RQ1: How effective is RCAgent as a comprehensive RCA agent on the real-world cloud system?


We present the effectiveness of RCAgent on the offline dataset in TABLE . RCAgent outperforms the original ReAct in all aspects of comprehensive RCA encompassing root cause, solution, and evidence prediction. The performance superiority is evident and consistent across all LLM evaluation metrics, including a *MATH* and *MATH* Win Rate against ReAct in the root cause and solution prediction subtasks, respectively. Moreover, when it comes to semantic metrics, RCAgent generally leads by a significant margin. Especially in the task of predicting evidence supporting the RCA, RCAgent achieves a remarkable *MATH* METEOR score over ReAct, highlighting its ability of data collection and evidence synthesis.


Employing TSC aggregation using LLM summarization, the overall performance of RCAgent gains further enhancements, especially on solution prediction witnessing gains of *MATH* METEOR, *MATH* BLEURT, *MATH* NUBIA and *MATH* Win Rate. This boost can be explained by the broader diversity of solution sampling compared to other subtasks. Note that there is a slight dip in BERTScore after LLM aggregation, which might be influenced by LLM&apos;s paraphrasing tendencies affecting token-level pairing.


In summary, these results show RCAgent&apos;s superiority against vanilla ReAct.


RQ2: To what extent does each design component in RCAgent contribute? 


To gauge the contribution of each component of RCAgent, we conduct an ablation study by successively removing enhancements introduced by RCAgent, including LLM expert agents, JsonRegen, and OBSK. The ablative result is shown in TABLE.


w/o LLM Expert Agents


When experts are removed, we see a drastic drop in all metrics, such as *MATH* to *MATH* METEOR on root cause prediction, with only marginal improvement over ReAct. This shows the power of building analytical tools for the LLM. Indeed, relieving the burden of the controller agent directly analyzing complex data greatly enhances the LLM agent.


w/o JsonRegen


When JsonRegen is gone, meaning the controller and expert agents generate more malformed output, RCAgent also loses a large proportion of its performance, primarily due to erroneous decisions.


w/o OBSK


After removing OBSK, marked as ReAct+JsonRegen+LLM experts in tables, the controller agent cannot use snapshots anymore and has to operate on truncated data. The absence of snapshots impacts the overall metrics, including *MATH* BLEURT and *MATH* G-Correctness on root cause prediction, though not as dramatically as excluding the LLM experts.
This indicates that the controller can still put analysis on the log with the expert. However, a large part of the environmental observation is lost.


Additionally, We experiment by removing the observation head and only showing a snapshot to the controller agent in the root cause prediction subtask. This removal incurs the least performance degradation, like *MATH* BLEURT and *MATH* G-Correctness, implying that the snapshot mechanism outweighs the benefit of the observation itself.


RQ3: Is RCAgent superior at generating efficient and stable decision trajectories? 


We study the action trajectories of different settings in TABLE . With all enhancements, the RCAgent achieves a *MATH* Pass Rate and a *MATH* Invalid Rate, meaning nearly perfect stability and a significant edge over ReAct. With such a minuscule chance of generating problematic actions, RCAgent consistently delivers more accurate and helpful RCA results with shorter trajectories.


When the LLM expert or OBSK is removed, the controller agent still maintains a high Pass Rate exceeding *MATH*, thanks to the stabilization strategies. However, both absence leads to an error-prone exploration, diverting some of its actions toward redundant tool invocations, possibly due to a less efficient analytical process. The removal of JsonRegen significantly damages the stability due to the invalidness of the data interchange between agents and the environment. An interesting phenomenon is that ReAct exhibits a marginally lower Pass Rate than RCAgent without JsonRegen. This discrepancy can be attributed to an oversimplified analytical environment, making the agent opt for quitting with a finalization tool.


Moreover, when the default decoding strategy for the controller agent is changed to nucleus sampling, marked as RCAgent+Sampling, the stability collapses to *MATH* Pass Rate and *MATH* Invalid Rate. The trajectory with this condition contains many erroneous actions and hallucinations about the tool documentation. This is likely due to the absence of exemplifying action histories. Such results highlight the vitality of optimal decoding during initial steps and lead to the consideration of our mid-way TSC rather than a pure full-process SC.


TABLE


RQ4: Is RCAgent useful for real production services and does it cover anomalies beyond the scope of current troubleshooting rules? 


We thoroughly test RCAgent&apos;s performance on the online dataset with human evaluation shown in TABLE . Consistent with all LLM and semantic metrics, the human evaluators in the SRE team express a positive estimation of *MATH* H-Helpfulness. The score indicates our best RCAgent result generally offers moderate support for RCA. This result substantially outperforms ReAct&apos;s *MATH* H-Helpfulness, the evaluation guideline of which means vague and lacking related content.


Notably, results on these potential OoD jobs are from an LLM agent neither fine-tuned on any AIOps field data nor taught by human-crafted diagnostic procedures, underscoring the capability of LLM&apos;s autonomous decision-making.


Furthermore, equipped with TSC (LLM), RCAgent demonstrates a precision of *MATH* in determining the responsibility. This facilitates RCAgent as a feedback mechanism in our company, aiding in the detection of potential platform-side errors and bugs.


FIGURE


RQ5: How much does Self-Consistency improve agents of different methods and sampling scales? 


We study combinations of SC methods and sample counts on the online dataset, each for *MATH* different runs. The results are detailed in Fig. . Note that we join the predicted root causes and solutions to compare with the ground truth for more readable differences in scores. There exists a negligible variance between LLM aggregation and embedding voting with only one sampled path, resulting from LLM&apos;s inclination towards paraphrasing.


The statistics show that every SC method consistently augments the performance of RCAgent in terms of BLEURT, BARTScore, and NUBIA. This enhancement seems to plateau when the number of samples reaches *MATH*.
Interestingly, when limited to a single sample, the performance difference of all SC methods compared to greedy decoding is marginal rather than a substantial degradation as shown in [*REF*]. Among different methods, TSC brings superiority due to its diverse action sampling. In all metrics, LLM aggregation outperforms embedding voting, and this gap broadens with an increasing number of samples, illustrating LLM aggregation&apos;s ability to offer more comprehensive results as the candidate pool grows.


Discussion And Limination


Sensitivity to the choice of embedding models


We test RCAgent with different embedding models including MPNet [*REF*], SentenceT5 [*REF*], and MiniLM [*REF*] with the same experiment setting as § [5.5]. We select checkpoints of these models based on their performance on the more general MTEB [*REF*] benchmark, to create a diversified spectrum of model capabilities. As presented in TABLE, there is a marginal correlation between RCAgent&apos;s performance and the capability of the embedding models. While this observation indicates that embedding models can enhance RAG-style tools grasped by LLM autonomous agents, it&apos;s noteworthy that their impact appears secondary to strategies like SC. Given this understanding, one of the directions of further optimization is training a domain-specific embedding model tuned on realistic AIOps datasets, offering additional enhancements to RCAgent.


Future Works


We further discuss the limitations of the current autonomous agents based on non-API models that remain open problems.


Trajectory Evaluation


Backtracking and path-searching methods for agent action trajectories, such as Tree-of-Thought [*REF*], DFSDT [*REF*], and Reflexion [*REF*] are typical strategies to enhance agent performance without external intervention. However, we opt not to use these methods in our current approach, primarily because these techniques demand either well-defined heuristics or LLMs&apos; evaluation of states and trajectories. As detailed in § [3.3.2], our approach merely integrates error handling messages for only the most obvious subset of erroneous states.


When addressing complex real-world tasks, well-defined heuristics can be challenging, making LLM an intuitive solution for a flexible and comprehensive trajectory evaluation method. Yet, our empirical observation indicates a capability gap between local-hosted and API-based models when it comes to self-assessment. We conduct experiments to prompt our base model and other similar-sized models to gauge the confidence of a trajectory it generates. When asked to score, the model can only offer *MATH* and *MATH*, exhibiting a lack of diversity in its assessments. When provided with descriptive criteria spanning different confidence levels, the model engages in flawed reasoning, conflating nuanced differences and delivering wholly unjustifiable results. Based on this observation, there seems a prevailing reliance on GPT&apos;s unique power at self-evaluating trajectories to perform backtracking and path-searching. Yet, how this specialized competence can be gained by other models remains underexplored.


Constrained Inference


LLMs are deployed as agents with a free-form generation approach.
However, this unconstrained generation impairs the validity of actions such as tool invocation when the model is not perfectly aligned.


As discussed in § [3.3.1], LLMs struggle to maintain consistent structured output. This issue is evident even when the base model is ChatGPT, on which we observe the same wrong structures when the complexity of content increases. Our solution, although effective, is admittedly simplistic and may not be adaptable to models with lesser capabilities.


Another challenge we encounter revolves around documentation adjustments. A significant portion of our efforts to achieve an acceptable pass rate is spent grinding documentation to ensure utmost clarity. This often means simplifying intricate functionalities and logic, making them less extendable. Without such meticulous adjustments, the agent risks invoking non-existent functions or incorrectly parameterizing function calls. Although GPT demonstrates proficiency in grasping a wide array of APIs [*REF*] and can even generate code as actions, other models stumble in understanding the correct formats.


Given these observations, we underscore the urgency for model-agnostic constrained inference with semantic constraints such as those in formal grammar. Methods like [*REF*; *REF*] offer context-free grammar approaches by integrating the finite state machine during decoding. However, they work in a completely auto-regressively masking way, rendering them incapable of rectifying former token-level violations such as wrong escaping, and occasionally collapsing the generation due to enforcing prefixes. Addressing this limitation is crucial for practical applications of LLM agents and is a significant direction for future exploration.


Related Work


LLM as Autonomous Agents


As LLMs demonstrate impressive capabilities [*REF*; *REF*; *REF*], some literatures [*REF*; *REF*; *REF*] leverage these models to construct LLM-based agents. Specifically, they use LLMs as the main controllers for these agents, generatively utilizing multimodal perception and exploring action space [*REF*; *REF*; *REF*]. A popular paradigm is autonomous agents [*REF*; *REF*; *REF*; *REF*; *REF*], in which LLM agents explore self-directedly without human intervention and step-by-step instructions. They perform tasks with an action trajectory, adapting their intentions and outputs according to the environment [*REF*]. While autonomous agents have been implemented and tested on a variety of toy tasks [*REF*] including database RCA in limited scenerios [*REF*], our RCAgent is the first work to introduce autonomous LLM agents to the field of realistic cloud RCA tasks.


LLM Augmented by Tools


Recent studies [*REF*; *REF*; *REF*; *REF*] have showcased the proficiency of LLMs to invoke tools and make decisions across a wide range of tasks. The tools, in the form of simple functions or external APIs, extend LLM&apos;s knowledge and capability evidently [*REF*; *REF*; *REF*]. While stronger LLM can easily grasp tools and accomplish tasks with documentation, others can be taught by generated and filtered trajectories [*REF*; *REF*]. In this paper, we aim to augment agents with a comprehensive toolset, extending the tool-using paradigm to the real-world cloud RCA field.


Cloud RCA with LLMs


RCA in large cloud services is a prominent subject of study within software engineering communities [*REF*; *REF*]. A large part of RCA is coupled with NLP due to subtasks like log analysis [*REF*; *REF*; *REF*]. As LLMs advance, they are leveraged for cloud RCA tasks with fine-tuning [*REF*; *REF*] or in-context learning [*REF*]. Besides, researchers calibrate confidences for few-shot LLM-based RCA [*REF*]. However, these models are not aware of the data collection and analysis workflow of cloud RCA, leaving them analytical tools in a sealed environment. We thus investigate tool-augmented LLM as autonomous agents for the complex and ever-changing environment of cloud RCA.


Conclusion


In this work, we introduce RCAgent, a tool-augmented LLM autonomous agent tailored for cloud root cause analysis. RCAgent ensures secure industrial usage of LLM agents in cloud systems by utilizing internally deployed models instead of powerful external ones like ChatGPT. Our methodology encompasses a spectrum of enhancements including unique Self-Consistency for action trajectories, a comprehensive prompting framework, expert agents, and stabilization methods. Furthermore, RCAgent&apos;s efficacy is demonstrated by its practical application in the Real-time Compute Platform for Apache Flink of Alibaba. In general, this work pioneers the real-world application of LLM agents in the cloud RCA field.