CACTUS: Chemistry Agent Connecting Tool-Usage to Science


Introduction


Large Language Models (LLMs) are foundation models that are combined
under a single paradigm to support various tasks or services. Despite
being trained on vast corpora of data, these transformer-based LLMs have
a limited understanding of the curated or parsed text.[*REF*].
Current research has revealed the possibility of augmenting LLMs with
tools that aid in efficiently solving various problems and tasks
[*REF*; *REF*; *REF*]. Previous work has also
shown that providing specific prompts, curated towards a specific task,
can enhance the time and quality of the text generated by the models
[*REF*]. Combining these two approaches is the Tool Augmented
Language Model (TALM) framework, detailed in *REF*, which
outperforms existing models on the tasks it is configured for. However
with any of these approaches, although the generated answers may appear
correct, LLMs fail to reason or demonstrate subject knowledge as is
typically demonstrated by humans [*REF*; *REF*].
Mistakes made by the model due to the statistical relationships it
learned from data might appear in a similar way across different
applications [*REF*]. If foundation models become integrated with
important systems that leverage the foundation model&apos;s ability to
quickly adapt to many different tasks and situations, failures could
result in significantly unwanted outcomes.


The resourceful LLMs like GPT4 [*REF*], LLaMA [*REF*], Gemma
[*REF*], MPT [*REF*], Falcon [*REF*], and Mistral [*REF*] show improved
performance over a range of activities [*REF*; *REF*; *REF*].
Despite these strides, the inherent limitations of such models become
apparent when faced with challenges that require access to dynamic,
real-time, or confidential data, which remain inaccessible within their
static training datasets. This gap underscores a critical need for LLMs
to evolve beyond their current capacities, leveraging external APIs to
fetch or interact with live data, thereby extending their utility in
real-world applications [*REF*]. In the domain-specific
applications, particularly within the chemical, biological and material
sciences, the limitations of LLMs are even more pronounced. The
intricate nature of chemical data coupled with the dynamic landscape of
drug discovery and development, presents a complex challenge that pure
computational models alone cannot address effectively. Recognizing this,
the integration of cheminformatics tools with the cognitive and
analytical ability of LLMs offers a promising pathway.


At the forefront of this transformation are Intelligent Agents,
autonomous entities capable of designing, planning, and executing
complex chemistry-related tasks with exceptional efficiency and
precision [*REF*]. These systems are not only capable of utilizing a
variety of LLMs for specific tasks but also adept at employing APIs and
internet search tools to gather relevant material and data. For example,
integrating an Agent into large, tool-based platforms such as KNIME
[*REF*] or Galaxy [*REF*] could form a natural language interface
between the user and their analysis. By acting as intermediaries, these
Agents could significantly streamline the process of scientific
discovery and autonomous experimentation with or without human in the
loop. Towards that end and taking inspiration from ChemCrow[*REF*],
an LLM-assisted chemistry synthesis planner, we have developed an
Intelligent Cheminformatics Agent focused on assisting scientists with
de novo drug design and molecular discovery. Cheminformatics focuses
on storing, retrieving, analyzing, and manipulating chemical data. It
provides the framework and methodologies to connect computational
linguistics with chemical science. This synergistic approach aims to
leverage the strengths of both domains by facilitating a more
comprehensive and effective exploration of therapeutic compounds,
streamlining the drug development process, and ultimately accelerating
the discovery from conceptualization to clinical application. In this
work, we developed CACTUS (Chemistry Agent Connecting Tool Usage to
Science) an LLM-powered agent that possesses the ability to
intelligently determine the most suitable tools for a given task and the
optimal sequence in which they should be applied, effectively optimizing
workflows for chemical research and development.


The implications of these intelligent agents are far-reaching. They
enable the autonomous operation of complex tasks from data analysis to
experimental planning, hypothesis generation, testing, and push the
boundaries of what can be achieved through computational chemistry. The
synergistic relationship between human intelligence, artificial
intelligence, and specialized software tools holds the potential to
transform the landscape of drug discovery, catalysis, material science,
and beyond. This relationship and combination of domains makes the
molecular discovery process more efficient, accurate, and innovative. As
we stand on the precipice of this new era in cheminformatics, the
integration of LLMs and computational tools through intelligent agents
like CACTUS promises to unlock a future where the limits of scientific
discovery are bound only by the depths of our imagination.


Methods


Tool-augmented language models consist of two major components: external
tools and language models. This section will discuss the approaches used
to implement the language model agent and provide a focused look at the
tools used. We will also go into great detail about the strategies used
when prompting our agent and how we performed benchmarking. Each of
these steps is a critical component of forming a complete intelligent
agent able to solve a wide range of problems with the added ability of
quick model swapping.


The Agent


An important consideration when building a TALM is the framework in
which it will be implemented. We have selected the commonly used
open-source platform, LangChain [*REF*], for this
purpose. This framework simplifies the integration of prompts with LLMs
through a comprehensive set of pre-built Python modules known as
\&quot;chains\&quot;. It also provides convenient integration with popular LLM
hosting/inference platforms such as the OpenAI API and HuggingFace
Transformers [*REF*]. CACTUS utilizes LangChain&apos;s implementation of a
custom MRKL agent [*REF*] which can be broken into 3 parts: tools, LLMChain, 
and agent class. The tools in this instance are a
collection of cheminformatics helper functions that wrap well-known
Python libraries into well-described tools for an agent to use. These
tools are explained in much more detail in Section
[2.2]. The LLMChain is a LangChain specific feature that helps chain the tools and
the agent together. This is the prompt provided to the LLM when running
any inference and helps to instantiate the model and parse the user
input. In CACTUS, we provide a prompt that guides the agent to answer
cheminformatics questions by describing the typical steps involved in
answering such questions. The last requirement for CACTUS is the agent
class. These are also LangChain implemented functions that are used to
interpret the user input after the initial prompt and make decisions on
which actions to take to best solve the question. CACTUS sticks with a
general purpose implementation of the zero-shot agent class that uses
the ReAct [*REF*] framework to determine which tool to use from the
tool&apos;s description. This combination of tools, LLMChain, and zero-shot
agent makes CACTUS an extensible LLM tool that can quickly integrate new
tools to solve a range of cheminformatics questions.


Here, we introduce mathematical formulation to describe the key
components and processes of the CACTUS framework: Let&apos;s consider *MATH* the set of cheminformatics
tools available to CACTUS as discussed above, where each tool *MATH* is a
function that takes an input *MATH* and produces an output *MATH*: *MATH*. 
The LLMChain is represented as a function *MATH* that
takes a user input *MATH* and a set of tools *MATH* as input, and outputs a
sequence of actions *MATH*: *MATH*. Each
action *MATH* in the sequence *MATH* corresponds to the application of a
specific tool *MATH* on an input *MATH*, resulting in an output *MATH*: *MATH*. 
The zero-shot agent class is modeled as a
function *MATH* that takes the user input *MATH*, the set of tools *MATH*, and
the LLMChain output *MATH* as input, and produces a final output *MATH*: *MATH*. 
The final output *MATH* is the result of executing the
sequence of actions *MATH* determined by the LLMChain, given the user input
*MATH* and the available tools *MATH*. Here, The ReAct framework used by the
zero-shot agent class was represented as a function *MATH* that takes the
user input *MATH*, the set of tools *MATH*, and the tool descriptions
*MATH* as input, and outputs the most appropriate
tool *MATH* to use: *MATH*.


This combination of cheminformatics tools, LLMChain, and zero-shot agent
makes CACTUS an extensible LLM tool that can quickly integrate new tools
to solve a range of cheminformatics questions.


FIGURE


Cheminformatics Tools 


For the purpose of creating a robust LLM agent able to answer a variety
of cheminformatics questions, CACTUS includes a wide range of tools
integrating common functions found in Python libraries such as RDKit
[*REF*] and SciPy [*REF*], along with
interfaces to databases such as PubChem [*REF*], ChEMBL
[*REF*], and ZINC [*REF*]. These tools allow for
a chat-based analysis of molecules starting with a SMILES string and
ending with information such as molecular descriptors, similarity, or
absorption, distribution, metabolism, and excretion (ADME) attributes.
The model consists of ten different tools providing information on
various descriptors for any given chemical compound used as input. Table contains 
a list of currently available
tools that can assist in obtaining different physio-chemical properties
and molecular descriptors of the input chemical compounds. This includes
molecular weight, log of the partition coefficient (LogP), topological
polar surface area (TPSA), quantitative estimate of drug-likeness (QED),
and synthetic accessibility (SA) of the input chemical compounds.
Moreover, using the BOILED-Egg method, CACTUS can also estimate the
pharmacokinetic properties like blood-brain barrier permeability and
gastrointestinal absorption of any given chemical compound
[*REF*]. Our model also implements drug-likeness, PAINS, and
Brenk filters to identify structural and toxicity alerts. All these
tools in our model will assist in identifying and screening both
currently available and new lead compounds. Currently restricted to
using a simple SMILES as input, future releases will allow for varied
user input (compound name, molecular formula, InChI key, CAS number,
SMILES, ChEMBL ID, or ZINC ID) where the agent will first convert it to
SMILES notation, and then used as input for the available tools.


TABLE


Prompting Strategy


One important aspect investigated was the significance of the prompt for
the agent. Through the LangChain implementation of LLM agents, there is
a default prompt that provides a generic instruction of what tools are
available and what the task of the LLM is. However, this is not
necessarily primed for understanding domain-specific information. To
test the hypothesis we ran 2 scenarios: one where we left the default
prompt unchanged and only included tool descriptions (Minimal Prompt),
and one where we modified the prompt to align the agent more with the
domain of chemistry (Domain Prompt). The belief is that a domain aligned
prompt will steer the LLM towards better interpretation of the questions
being asked, and therefore be more effective in answering user queries.
Since we were using a wide range of LLMs for testing, the minimal prompt
also included model-specific tokens so that we weren&apos;t unfairly
evaluating models against the domain prompt.


Benchmarking 


Evaluation of domain-specific TALMs can be a difficult task but we can
follow the examples set by general benchmarking suites
[*REF*; *REF*; *REF*; *REF*]. Therefore, we rely on
sets of questions that replicate the typical questions the agent would
see and score how many the agent is able to answer correctly without
requiring extra prompting effort from the user (i.e. having to rephrase
the typed question to get a correct answer). To evaluate CACTUS we
created sets of cheminformatics questions that test 3 sets of questions
depending on the output of the tool. The first set is of qualitative
questions, and is represented by questions that return answers like
Yes/No, or True/False. The second is quantitative, which represents
tools that return numerical values to be interpreted by the agent. The
third is a combination of both qualitative and quantitative which we
call full or combined set. Table highlights examples of questions passed as
user-input to the CACTUS agent. The qualitative and quantitative
datasets each contain 500 questions, and the combined dataset contains
1000. Most tests will be done on the combined dataset as we want to test
the LLM agent&apos;s ability to perform a diverse set of tasks.


TABLE


Results and Discussion


The implementation of CACTUS represents a significant step forward in
the field of cheminformatics, offering a powerful and flexible tool for
researchers and chemists engaged in molecular discovery and drug design.
The benchmarking studies conducted on various 7b parameter models
demonstrate the robustness and efficiency of the CACTUS framework,
highlighting its potential to streamline and accelerate the drug
discovery process as an example.


Benchmarking and Performance Evaluation 


The performance of CACTUS was evaluated using a comprehensive set of
1000 questions, covering 10 different tools (Table, with and without the domain prompt on
each 7b parameter model as shown in the Figure [4]). Correct
answers were scored as correct, while wrong answers, inability to
converge on an answer, or inability to use the provided tool correctly
were marked as incorrect. In this paper, we did not differentiate
between incorrect tool usage and simply providing a wrong answer. Any
answers that did not coherently address the question were considered
incorrect. We accepted correct answers that contained additional
formatted text after the correct answer, although this is not the
preferred format. This additional information can be programmatically
removed before returning the response to the user, or further prompts
can be engineered to reduce additional text. Each type of question in
the full question set was asked 100 times, resulting in 10 types of
questions corresponding to the 10 tools provided in Table. This approach allowed us to identify
which tools posed a greater challenge for the model, and where
improvements to either the tool description or model prompt could be
made.


The results shown in Figure [4] highlight the importance of domain-specific
prompting in improving the accuracy of the model&apos;s responses;
particularly for qualitative questions. This finding aligns with recent
research emphasizing the role of prompt engineering in enhancing the
performance of language models [*REF*].


FIGURE


In the progression of AI and its applications in scientific inquiry, it
is crucial to analyze the comparative effectiveness of various models in
handling domain-specific tasks. The benchmarking analysis presented in
Figure [5] offers significant insights into the
performance of different language models when prompted with both minimal
and domain-specific information. A comprehensive review of the
performance data across the full spectrum of question types reveals that
Gemma-7b and Mistral-7b models showcase robustness and versatility,
performing admirably regardless of the nature of the prompt. Their
consistent accuracy across different types of questions ranging from
physiochemical properties like druglikeness and blood-brain barrier
permeability to more complex metrics like quantitative estimate of
drug-likeness (QED) highlight their reliability for a broad range of
inquiries within the domain of molecular science. In contrast, models
like Falcon-7b exhibit a noticeable disparity between performances with
minimal and domain prompts. This variability suggests that Falcon-7b,
while capable, may require more fine-tuned prompting to leverage its
full potential effectively. The substantial difference in performance
based on the prompt type points to an intrinsic model sensitivity to
input structure and content, which can be pivotal in crafting effective
inquiry strategies. Furthermore, the successful deployment of smaller
models, such as Phi2 and OLMo-1b, on consumer-grade hardware (Figure
[6]) highlights the potential for democratizing access to powerful
cheminformatics tools, enabling researchers with limited computational
resources to harness the capabilities of CACTUS.


Open Source Models in Varied Settings


This comprehensive model comparison and analysis has broader
implications for the employment of open-source models in scientific
environments. The ability of models to perform well with domain-specific
prompts is particularly encouraging, as it implies that with proper
configuration, open-source models can be highly effective tools. The
adaptability demonstrated by the Gemma-7b and Mistral-7b models
indicates their potential for widespread applicability across various
computational settings, from high-performance clusters to more modest
research setups. Moreover, the ability to effectively prompt open-source
models opens the door to their use in a variety of scientific contexts.
It allows researchers to customize models to their specific domain,
potentially bridging the gap between generalized AI capabilities and
specialized knowledge areas.


FIGURE


The flexibility and performance of these models have significant
implications for scientific research, particularly in fields like
synthetic organic chemistry and drug discovery. For researchers in these
domains, the ability to utilize open-source models effectively can
accelerate the discovery process, enhance predictive accuracy, and
optimize computational resources. The insights from this benchmarking
study provide a roadmap for selecting and tailoring models to specific
research needs, thereby maximizing their utility in advancing scientific
goals. The benchmarking study of the selected 7b parameter models serves
as a testament to the progress in AI-driven research tools. It
highlights the necessity of prompt optimization and the promise of
open-source models in diverse scientific inquiries. The analysis
underscores the potential of these models to become integral components
in the computational chemist&apos;s toolkit, paving the way for innovative
breakthroughs in molecular design and drug discovery.


Hardware Performance and Model Efficacy


The deployment of CACTUS models through vLLM offers a significant
advantage by optimizing performance across a variety of GPUs used for
LLM inference. In our benchmarking studies we utilized three types of
NVIDIA GPUs: the A100 80GB, V100, and RTX 2080 Ti. Our objective was to
evaluate the performance of models under different combinations of model
size, GPU type, and prompting strategy (minimal or domain-specific). The
performance metric was determined by the inference speed in relation to
the model&apos;s accuracy. Figure [6] shows the summary of LLMs deployed under
different conditions (GPU hardware used, prompt, and benchmark set used)
and how well they performed. The efficiency of these models across
diverse hardware highlights their potential for widespread
implementation in a range of research settings.


The models evaluated include Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b,
Mistral-7b, as well as two smaller models, Phi2 and OLMo-1b. The
inclusion of these smaller models highlights the potential for
successfully deploying models on local resources with limited
computational power (e.g., consumer-grade GPUs like the RTX 2080 Ti)
while still achieving accurate results. Overall, the model performance
was found to be relatively quick on both the 500-question sets
(Qualitative/Quantitative) and the 1000-question combined set (Full).
One notable outlier was the Llama2-7b model with domain prompting, which
took 185 minutes to complete the inference on the full dataset; however,
its accuracy was similar to the minimally prompted version. This model
is considered to be an outlier and therefor not included in Figure
[6]. A full list of the data used to plot these summary figures can be found in
the Appendix.


FIGURE


The most interesting outcome is that smaller models deployed on consumer
grade hardware do not perform drastically worse than their larger
parameter model counterparts. Looking at the performance of the Phi2
model (2.7B parameters), it quickly and accurately tackles the 500
question quantitative benchmark with similar performance regardless of
the GPU used with the A100 80GB version unsurprisingly as the fastest.
Another interesting outcome is the performance of the OLMo-1b parameter
model on the combined question set and the RTX 2080 Ti GPU. While unable
to obtain any correct answers for the minimal prompt, it jumps up to a
surprising 52.2% accuracy when provided a domain prompt. These results
are promising that these smaller models can be deployed locally by users
and still be able to interpret questions, possibly by providing more
specialized prompts.


In general, inference time increased as question set size increased
(e.g., from quantitative/qualitative to full), while accuracy tended to
decrease with longer inference times. Domain prompts achieved faster
inference and accuracy than minimal prompts for models like Falcon-7b,
MPT-7b, and Mistral-7b. However, there was an exception in the case of
the Phi2 model on the full question set, where the minimal prompt
resulted in faster inference but lower accuracy.


The hardware performance analysis highlights the importance of
considering the interplay between model size, GPU capabilities, and
prompting strategies when deploying CACTUS models for molecular property
prediction and drug discovery. The ability to achieve accurate results
with smaller models on consumer-grade hardware opens up the possibility
of wider adoption and accessibility of CACTUS for researchers with
limited computational resources. Furthermore, the impact of
domain-specific prompting on both inference speed and accuracy
emphasizes the need for carefully designed prompts tailored to the
specific application domain. As CACTUS continues to evolve and integrate
with other computational tools and autonomous discovery platforms,
optimizing hardware performance will remain a critical consideration.
Future research should explore the development of more efficient
algorithms and architectures (energy efficiency) for deploying CACTUS
models on a variety of hardware configurations, ensuring that the
benefits of this powerful tool can be realized across a wide range of
research settings and computational resources.


Issues Encountered and Resolutions


During the development and benchmarking of CACTUS agent using
open-source models and the LangChain framework, several key challenges
were identified. These issues, along with the solutions implemented,
provide valuable insights for researchers and developers working on
similar workflows.


One of the primary issues encountered was the slow inference speed when
hosting open-source language models locally on machines utilizing CPUs.
Most APIs quickly provide inference results when making calls and this
is not something locally hosted models typically replicate well,
especially when running on CPUs over GPUs. For this work, we initially
used models from HuggingFace and deployed through the HuggingFace
Pipelines python package. This allowed us to serve models, but the
inference time was quite slow when wrapped in the LangChain agent. To
address this, we began utilizing vLLM to host HuggingFace models
instead. This substantially decreased our inference time, and allowed
for API-like response times from models, even those hosted on less
powerful consumer grade GPU hardware.


The second major challenge was related to prompt engineering. Our
results shown previously highlight that for some models the prompt has a
great effect on not only the model accuracy, but the inference time. We
spent a good amount of time trying to hone our prompting strategy to
yield consistently accurate and efficient results with mixed effect. We
ended up needing specialized prompts for each open-source LLM we used,
as some were fine-tuned much differently than others and required a very
specific prompt style to return usable results.


These challenges highlight the need for continued research and
development in the areas of model deployment and prompt engineering.
Future work will be focused on optimizing the deployment of open-source
models on various hardware configurations, including CPUs and GPUs, to
ensure that CACTUS can be efficiently utilized across a wide range of
computational resources. This may involve the development of novel
algorithms and architectures that can better leverage the capabilities
of different hardware setups, as well as the creation of more
user-friendly tools and frameworks for model deployment and management.
In terms of prompt engineering, the development of standardized prompt
templates and best practices for prompt engineering in the context of
molecular property prediction and drug discovery could help streamline
the development process and improve the consistency of results across
different models and datasets.


Future Outlook - Molecular Design


CACTUS has already demonstrated its potential in estimating basic
metrics for input chemical compounds, but its future lies in its
evolution into a comprehensive, open-source tool specifically designed
for chemists and researchers working on therapeutic drug design and
discovery. This will be achieved by the integration of physics-based
molecular AI/ML models, such as 3D-scaffold [*REF*],
reinforcement learning [*REF*], and graph neural networks
(GNNs) [*REF*] accompanied with molecular dynamics
simulations, quantum chemistry calculations, and high-throughput virtual
screening [*REF*; *REF*; *REF*; *REF*; *REF*].
Such capabilities are essential for accurately modeling molecular
interactions and predicting the efficacy and safety of potential
therapeutic agents [*REF*].


The development plan also includes implementing advanced functionalities
for identifying compounds that exhibit structural and chemical
similarities, as well as pinpointing key fragments crucial for
biological activity. This feature will allow researchers to explore a
vast chemical space more efficiently, identifying lead compounds with
higher precision. These additions are expected to significantly
accelerate and deepen the agent&apos;s ability to understand compound
behaviors in 3D spaces and allow researchers to develop more
comprehensive and effective workflows for drug discovery and materials
design. Additionally, we plan to include tools that identify key
fragments and compounds with similar structural and chemical features
from the vast available chemical databases. Tools which can calculate
physio-chemical, pharmacokinetic properties, and about sixty other
descriptors will be added to the agent to identify quantitative
structure-activity relationship (QSAR) and quantitative
structure-property relationship (QSPR) to help us with screening the
compounds and identifying toxic groups.


Beyond these technical enhancements, there&apos;s a focus on making CACTUS
more explainable and capable of symbolic reasoning. The aim is to
address common criticisms of LLMs, particularly their struggle with
reasoning and providing explainable outputs. By integrating more
advanced symbolic reasoning capabilities, CACTUS will not only become
more powerful in its predictive and analytical functions but also
provide users with understandable, logical explanations for its
recommendations and predictions. This feature would automate the process
of predicting how small molecules, such as drug candidates, interact
with targets like proteins, thereby providing invaluable insights into
the potential efficacy of new compounds.


The applications of CACTUS extend beyond drug discovery and can be
leveraged in other domains such as chemistry, catalysis, and materials
science. In the field of catalysis, CACTUS could aid in the discovery
and optimization of novel catalysts by predicting their properties and
performance based on their structural and chemical features
[*REF*]. Similarly, in materials science, CACTUS could
assist in the design of new materials with desired properties by
exploring the vast chemical space and identifying promising candidates
for further experimental validation [*REF*].


The future development of CACTUS is geared towards creating an
intelligent, comprehensive cheminformatics tool for molecular discovery
that not only aids in the identification and design of therapeutic drugs
but also ensures a high degree of safety and efficacy. Through the
integration of advanced computational techniques and models, alongside
improvements in usability and explainability, CACTUS is set to become an
indispensable resource in the quest for novel, effective, and safe
therapeutic agents, as well as in the discovery and optimization of
catalysts and materials.


Conclusions


In this paper, we have introduced CACTUS, an innovative open-source
agent that leverages the power of large language models and
cheminformatics tools to revolutionize the field of drug discovery and
molecular property prediction. By integrating a wide range of
computational tools and models, CACTUS provides a comprehensive and
user-friendly platform for researchers and chemists to explore the vast
chemical space for molecular discovery and identify promising compounds
for therapeutic applications.


We assessed CACTUS performance using various open-source LLMs, including
Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, across a set of
one thousand chemistry questions. Our findings indicate that CACTUS
outperforms baseline LLMs significantly, with the Gemma-7b and
Mistral-7b models achieving the highest accuracy regardless of the
prompting strategy employed. Additionally, we investigated the impact of
domain-specific prompting and hardware configurations on model
performance, highlight the importance of prompt engineering and the
potential for deploying smaller models on consumer-grade hardware
without significant loss in accuracy. The ability to achieve accurate
results with smaller models such Phi on consumer-grade hardware opens up
the possibility of wider adoption and accessibility of CACTUS, even for
researchers with limited computational resources.


One of the key takeaways from the development and benchmarking of CACTUS
is the importance of addressing the challenges associated with model
deployment and prompt engineering. The solutions implemented in this
work, such as the use of vLLM for hosting models and the development of
tailored prompts for each open-source LLM, serve as a valuable
foundation for future efforts in this field. As the field of AI
continues to evolve rapidly, it is essential to keep abreast of new
developments in language modeling and related technologies to further
enhance the capabilities and performance of CACTUS. The development and
benchmarking of CACTUS also highlighted key challenges in integrating
open-source LLMs with domain-specific tools, such as optimizing
inference speed and developing effective prompting strategies. We
discussed the solutions implemented to address these challenges,
including the use of vLLM for model hosting and the creation of tailored
prompts for each LLM.


Looking ahead, the future of CACTUS is incredibly promising, with the
potential to transform not only drug discovery but also various other
domains such as chemistry, catalysis, and materials science. The
integration of advanced physics-based AI/ML models, such as 3D-scaffold,
reinforcement learning and graph neural networks, will enable a deeper
understanding of compound behaviors in 3D spaces, leading to more
accurate predictions of molecular interactions and the efficacy and
safety of potential therapeutic agents. Moreover, the addition of tools
for identifying key fragments, calculating molecular properties, and
screening compounds for toxic groups will significantly enhance the
efficiency and precision of the drug discovery process. The focus on
improving the explainability and symbolic reasoning capabilities of
CACTUS will address common criticisms of large language models and
provide users with understandable, logical explanations for the tool&apos;s
recommendations and predictions.


As CACTUS continues to evolve and integrate with other computational
tools and autonomous discovery platforms, it has the potential to
revolutionize the way we approach drug discovery, catalyst design, and
materials science. By leveraging the power of AI and machine learning,
CACTUS can help researchers navigate the vast parameter spaces
associated with complex chemical systems, identifying promising
candidates for experimental validation and optimization. The future
development of CACTUS is geared towards creating an intelligent,
comprehensive cheminformatics tool that ensures a high degree of safety
and efficacy in the identification and design of therapeutic drugs,
catalysts, and materials for various application. Through the
integration of advanced computational techniques and models, alongside
improvements in usability and explainability, CACTUS is set to become an
indispensable resource for researchers across various scientific
disciplines.


In summary, CACTUS represents a significant milestone in the field of
cheminformatics, offering a powerful and adaptable tool for researchers
engaged in drug discovery, molecular property prediction, and beyond. As
we continue to advance AI-driven scientific discovery, agent like CACTUS
will play a pivotal role in shaping the future of research, innovation,
and human health. By embracing the potential of open-source language
models and cheminformatics tools, we can accelerate the pace of
scientific advancement and unlock new frontiers in the quest for novel,
effective, and safe therapeutic agents, catalysts, and materials.