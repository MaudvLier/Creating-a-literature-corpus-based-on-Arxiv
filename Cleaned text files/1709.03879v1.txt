Ultimate Intelligence Part III: Measures of Intelligence, Perception and Intelligent Agents


Introduction


The ultimate intelligence research program is inspired by Seth Lloyd&apos;s
work on the ultimate physical limits to computation [*REF*]. We
investigate the ultimate physical limits and conditions of intelligence.
This is the third installation of the paper series, the first two parts
proposed new physical complexity measures, priors and limits of
inductive inference [*REF*; *REF*].


We frame the question of ultimate limits of intelligence in a general
physical setting, for this we provide a general definition of an
intelligent system and a physical performance criterion, which as
anticipated turns out to be a relation of physical quantities and
information, the latter of which we had conceptually reduced to physics
with minimum machine volume complexity in [*REF*].


Notation and Background


Universal Induction


Let us recall Solomonoff&apos;s universal distribution [*REF*]. Let *MATH* be a
universal computer which runs programs with a prefix-free encoding like
LISP; *MATH* denotes that the output of program *MATH* on *MATH* is *MATH* 
where *MATH* and *MATH* are bit strings. Any unspecified variable or
function is assumed to be represented as a bit string. *MATH* denotes the
length of a bit-string *MATH*. *MATH* refers to function *MATH* rather than its application.


The algorithmic probability that a bit string *MATH* is
generated by a random program *MATH* of *MATH* is:
*MATH* which conforms to Kolmogorov&apos;s axioms [*REF*]. *MATH* 
considers any continuation of *MATH*, taking into account non-terminating
programs. *MATH* is also called the universal prior for it may be
used as the prior in Bayesian inference, for any data can be encoded as
a bit string. We also give the basic definitions of Algorithmic
Information Theory (AIT) [*REF*], where the algorithmic entropy, or
complexity of a bit string *MATH* is *MATH*. 


We use some variables in overloaded fashion in the paper, e.g., *MATH* 
might be a program, a policy, or a physical mechanism depending on the
context.


Operator induction


Operator induction is a general form of supervised machine learning
where we learn a stochastic map from *MATH* question and answer pairs
*MATH* sampled from a (computable) stochastic source
*MATH*. Operator induction can be solved by finding in available time a
set of operators *MATH*, each a conditional probability
density function (cpdf), such that the following goodness of fit is
maximized *MATH* for a stochastic source *MATH* where each
term in the summation is the contribution of a model:
*MATH*. *MATH* and *MATH* are question/answer pairs in the input dataset drawn from
*MATH*, and *MATH* is a computable cpdf in. We can use the found *MATH* 
operators to predict unseen data with a mixture model
[*REF*] *MATH*. The goodness of fit in this case strikes a balance between high a priori
probability and reproduction of data like in minimum message length
(MML) method [*REF*; *REF*], yet uses a universal
mixture like in sequence induction. The convergence theorem for operator
induction was proven in [*REF*] using Hutter&apos;s extension
to arbitrary alphabet, and it bounds total error by *MATH* 
similarly to sequence induction.


Set induction


Set induction generalizes unsupervised machine learning where we learn a
probability density function (pdf) from a set of *MATH* bitstrings
*MATH* sampled from a stochastic source *MATH*.
We can then inductively infer new members to be added to the set with:
*MATH*. Set induction is
clearly a restricted case of operator induction where we set *MATH* &apos;s to
null string. Set induction is a universal form of clustering, and it
perfectly models perception. If we apply set induction over a large set
of 2D pictures of a room, it will give us a 3D representation of it
necessarily. If we apply it to physical sensor data, it will infer the
physical theory -- perfectly general, with infinite domains -- that
explains the data, perception is merely a specific case of scientific
theory inference in this case, though set induction works both with
deterministic and non-deterministic problems.


Universal measures of intelligence


There is much literature on the subject of defining a measure of
intelligence. Hutter has defined an intelligence order relation in the
context of his universal reinforcement learning (RL) model AIXI
[*REF*], which suggests that intelligence corresponds to
the set of problems an agent can solve. Also notable is the universal
intelligence measure [*REF*; *REF*], which is again based on the
AIXI model. Their universal intelligence measure is based on the
following philosophical definition compiled from their review of
definitions of intelligence in the AI literature.


DEFINITION


It implies that intelligence requires an autonomous goal-following
agent. The intelligence measure of [*REF*] is defined as
*MATH* where *MATH* is a computable reward bounded environment, And *MATH* is the
expected sum of future rewards in the total interaction sequence of
agent *MATH*. *MATH*, where *MATH* is the instantaneous reward at time *MATH* generated from the
interaction between the agent *MATH* and the environment *MATH*, and
*MATH* is the time discount factor.


The free energy principle.


In Asimov&apos;s story titled &quot;The Last Question&quot;, the task of life is
identified as overcoming the second law of thermodynamics, however
futile. Variational free energy essentially measures predictive error,
and it was introduced by Feynmann to address difficult path integral
problems in quantum physics. In thermodynamic free energy, energies are
negative log probabilities like entropy. The free energy principle
states that any system must minimize its free energy to maintain its
order. An adaptive system that tends to minimize average surprise
(entropy) will tend to survive longer. A biological organism can be
modelled as an adaptive system that has an implicit probabilistic model
of the environment, and the variational free energy puts an upper bound
on the surprise, thus minimizing free energy will improve the chances of
survival. The divergence between the pdf of environment and an arbitrary
pdf encoded by its own mechanism is minimized in Friston&apos;s model
[*REF*]. It has been shown in detail that the free energy
principle adequately models a self-preserving agent in a stochastic
dynamical system [*REF*; *REF*], which we can
interpret as an environment with computable pdf. An active agent may be
defined in the formalism of stochastic dynamical systems, by
partitioning the physical states *MATH* of the environment into
*MATH* where *MATH* is an external
state, *MATH* is a sensory state, *MATH* an active state, and
*MATH* is an internal state. Self-preservation is defined
by the Markov blanket *MATH*, the removal of which partitions *MATH* 
into external states *MATH* and internal states *MATH* that influence
each other only through sensory and action states. *MATH* influences
sensations *MATH*, which in turn influence internal states *MATH*,
resulting in the choice of action signals *MATH*, which impact *MATH*, forming
the feedback loop of the adaptive system. The system states
*MATH* evolve according to the stochastic equation:


FORMULA


where *MATH* is the flow of system states and it is decomposed
into flows over the sets in the system partition, explicitly showing the
dependencies among state sets; *MATH* models fluctuations. Friston
formalizes the self-preservation (homeostasis) problem as finding an
internal dynamics that minimizes the uncertainty (Shannon entropy) of
the external states, and shows a solution based on the principle of
least action [*REF*] wherein minimizing free energy is
synonymous with minimizing the entropy of the external states (principle
of least action), which subsequently corresponds to active inference. We
have space for only some key results from the rather involved
mathematical theory. *MATH* is the generative pdf that generates
sensorium *MATH* and fictive (hidden) states *MATH* from probabilistic
model *MATH*, and *MATH* is the recognition pdf that predicts
hidden states *MATH* in the world given internal state. Generative pdf
factorizes as *MATH*. Free energy is defined as
energy minus entropy *MATH* which can be
subjectively computed by the system. Free energy is also equal to
surprise plus divergence between recognition and generative pdf&apos;s.
*MATH*. Minimizing divergence minimizes free energy, internal states *MATH* 
may be optimized to minimize predictive error using, and surprise is
invariant with respect to *MATH*. Free energy may be formulated as
complexity plus accuracy of recognition, as well.
*MATH*. In this case, we may choose an action that changes sensations to reduce
predictive error. Only the first term is a function of action signals.
Minimization of free energy turns out to be equivalent to the
information bottleneck principle of Tishby
[*REF*; *REF*]. The information
bottleneck method is equivalent to the pioneering work of Ashby, which
is simple enough to state here [*REF*; *REF*]:
*MATH* where the first term
is the mutual information between internal and hidden states, and the
second term is the mutual information between sensory states and
internal states. Both terms are expanded using conditional entropy, and
then two terms in the middle are eliminated because they are not
relevant to the optimization problem -- we do not know the hidden
variables in *MATH* and *MATH* is constant. *MATH*. Minimizing *MATH* thus minimizes the sum of
the entropy of internal states and the entropy required to encode
sensory states given internal states. In other words, it strikes an
optimal balance between model complexity *MATH*, and model
accuracy *MATH*. Friston further shows that directly derives
from the free energy principle, closing potential loopholes in the
theory. Please see [*REF*] for a comprehensive application of the
free energy principle to agents and learning. Note also that the bulk of
the theory assumes the ergodic hypothesis.


Perception as General Intelligence


Since we are chiefly interested in stochastic problems in the physical
world, we propose a straightforward informal definition of intelligence:


DEFINITION


Mechanism is any physical machine as usual, see [*REF*] which
suggests likewise. Therefore, a general formulation of Solomonoff
induction, operator induction, might serve as a model of general
intelligence, as well [*REF*]. Recall that operator
induction can infer any physically plausible cpdf, thus its
approximation can solve any classical supervised machine learning
problem. The only slight issue with might be that it seems to exclude
classical AI systems that are not agents, e.g., expert systems, machine
learning tools, knowledge representation systems, search and planning
algorithms, and so forth, which are somewhat more naturally encompassed
by our informal definition.


Is operator induction adequate?


A question naturally arises as to whether operator induction can
adequately solve every prediction problem we require in AI. There are
two strong objections to operator induction that we know of. It is
argued that in a dynamic environment, as in a physical environment, we
must use an active agent model so that we can account for changes in the
environment, as in the space-time embedded agent [*REF* -spacetime]
which also provides an agent-based intelligence measure. This objection
may be answered by the simple solution that each decision of an active
intelligent system may be considered a separate induction problem. The
second objection is that the basic Solomonoff induction can only predict
the next bit, but not the expected cumulative reward, which its
extensions can solve. We counter this objection by stating that we can
reduce an agent model to a perception and action-planning problem as in
OOPS-RL [*REF*]. In OOPS-RL, the perception module searches for the best
world-model given the history of sensory input and actions in allotted
time using OOPS, and the planning module searches for the best control
program using the world-model of the perception module to determine the
action sequence that maximizes cumulative reward likewise. OOPS has a
generalized Levin Search [*REF* -universalsearch-eng] which may be
tweaked to solve either prediction or optimization problems. Hutter has
also observed that standard sequence induction does not readily address
optimization problems [*REF*]. However, Solomonoff
induction is still complete in the sense of Turing, and can infer any
computable cpdf; and when the extension to Solomonoff induction is
applied to sequence prediction, it does not yield a better error bound,
which seems like a conundrum. On the other hand, Levin Search with a
proper universal probability density function (pdf) of programs can be
modified to solve induction problems (sequence, set, operator, and
sequence prediction with arbitrary loss), inversion problems (computer
science problems in P and NP), and optimization problems
[*REF*]. The planning module of OOPS-RL likewise requires
us to write such an optimization program. In that sense, AIXI implies
yet another variation of Levin Search for solving a particular universal
optimization problem, however, it also has the unique advantage that
formal transformations between AIXI problem and many important problems
including function minimization and strategic games have been shown
[*REF*]. Nevertheless, the discussion in
[*REF*] is rather brief. Also see [*REF*] for a
discussion of universal optimization.


PROPOSITION


More formally, the perceptual task of an RL agent would be inferring
from a history the cumulative rewards in the future, without loss of
generality. Let the chronology *MATH* be a sequence of sensory, reward, and
action data *MATH* where
*MATH* accesses *MATH* th element, and *MATH* accesses the subsequence
*MATH*. Let *MATH* be the cumulative reward
function where *MATH*. After
observing *MATH*, we construct dataset *MATH* as follows. For
every unique *MATH* pair such that *MATH*, we
concatenate history tuples *MATH*, and we form a question string
that also includes the next action, *MATH* and *MATH*,
FORMULA, and an answer string which is the cumulative
reward *MATH*. Solving the operator induction problem for
this dataset *MATH* will yield a cpdf which predicts cumulative rewards
in the future. After that, choosing the next action is a simple matter
of maximizing *MATH* where *MATH* is the
planning horizon. The reduction causes quadratic blow-up in the number
of data items. Our somewhat cumbersome reduction suggests that all of
the intelligence here comes from operator induction, surely an argmax
function, or a summation of rewards does not provide it, but rather it
builds constraints into the task. In other words, we interpret that the
intelligence in an agent model is provided by inductive inference,
rather than an additional application of decision theory.


Physical Quantification of Intelligence


corresponds to any kind of reinforcement-learning or goal-following
agent in AI literature quite well, and can be adapted to solve other
kinds of problems. The unsupervised, active inference agent approach is
proposed instead of reinforcement learning approach in [*REF*],
and the authors argue that they did not need to invoke the notion of
reward, value or utility. The authors in particular claim that they
could solve the mountain-car problem by the free-energy formulation of
perception. We thus propose a perceptual intelligence measure.


Universal measure of perception fitness


Note that operator induction is considered to be insufficient to
describe universal agents such as AIXI, because basic sequence induction
is inappropriate for modelling optimization problems
[*REF*]. However, a modified Levin search procedure can
solve such optimization problems as in finding an optimal control
program [*REF*]. In OOPS-RL, the perception module searches for the best
world-model given the history of sensory input and actions in allotted
time using OOPS, and the planning module searches for the best control
program using the world-model of the perception module to determine the
control program that maximizes cumulative reward likewise. In this
paper, we consider the perception module of such a generic agent which
must produce a world-model, given sensory input.


We can use the intelligence measure in a physical theory of
intelligence, however it contains terms like utility that do not have
physical units (i.e., we would be preferring a more reductive
definition). We therefore attempt to obtain such a measure using the
more benign goodness-of-fit (). Let the universal measure of the fitness
of operator induction be defined as *MATH* where
*MATH* is the set of possible stochastic sources in the observable universe
*MATH* and *MATH* is a physical mechanism, and *MATH* is relative to a
stochastic source *MATH* and a physical mechanism (computer) *MATH*. This
would be maximum if we assume that operator induction were solved
exactly by an oracle machine.


Note that *MATH* is finite; *MATH* is likewise bounded by
the amount of computation *MATH* will spend on approximating operator induction.


Application to homeostasis agent


In a presentation to Friston&apos;s group in January 2015, we noted that the
minimization of *MATH* is identical to Minimum Message Length
principle, which can be further refined as *MATH* using Solomonoff&apos;s entropy formulation that takes the
negative logarithm of algorithmic probability [*REF*]. In the
unsupervised agent context, solving this minimization problem
corresponds to inferring an optimal behavioral policy as *MATH* 
constitutes internal dynamics which may be modeled as a non-terminating
program. We could directly apply induction to minimize KL divergence, as
well. Note the correspondence to operator induction.


THEOREM


PROOF


PROPOSITION


The mechanism *MATH* that maximizes *MATH* achieves less error
with respect to a source (which may be taken to correspond to the whole
random dynamical system in the framework of free energy principle),
while *MATH* normalizes *MATH* with respect to a
random dynamical system. It holds for the same reasons Legg&apos;s measure
holds, which are not discussed due to space limits in the present paper.
We prefer the unsupervised homeostasis agent among the two agent models
we discussed because it provides an exceptionally elegant and
reductionist model of autonomous behavior, that has been rigorously
formulated physically. Note that this agent is conceptually related to
the survival property of RL agents discussed in [*REF*].


Discussion


The unsupervised model still achieves exploration and curiosity, because
it would stochastically sample and navigate the environment to reduce
predictive errors. While we either optimize perceptual models or choose
an action that would befit expectations, it might be possible to express
the optimal adaptive agent policy in a general optimization framework. A
more in-depth analysis of the unsupervised agent will be presented in a
subsequent publication. A more general reductive definition of
intelligence should also be researched. These developments could
eventually help unify AGI theory.