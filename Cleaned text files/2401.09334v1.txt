Large Language Models Are Neurosymbolic Reasoners


Introduction


The ability to perform reasoning is crucial for AI due to its significant impact on various real-world tasks. The widespread adoption of large language models (LLMs), such as ChatGPT and GPT-4 [*REF*], has led to a series of remarkable successes in reasoning tasks, ranging from question &amp; answering to solving math problems. Among these challenges, text-based games serve as important benchmarks for agents with natural language capabilities and have garnered significant attention in the realm of language-centric machine learning research [*REF*; *REF*; *REF*; *REF*; *REF*].
In these games, an agent uses language to interpret various scenarios and make decisions. The complexity of such games arises from the need for language comprehension, common sense, managing action spaces with combinatorial complexity, and the crucial importance of long-term memory and planning [*REF*; *REF*]. The challenges escalate in text-based games that involve symbolic tasks [*REF*].
For instance, contemporary agents might be tasked with a scenario where they are required to solve a mathematical problem and simultaneously gather a specified amount of fruits, with the quantity needed being the solution to the math problem.


Using symbolic modules or external tools for arithmetic, navigation, sorting, and knowledge-base lookup is crucial for language agents, especially in complex text-based games [*REF*; *REF*; *REF*; *REF*].
However, effectively integrating these aspects into language agents remains a relatively unaddressed challenge. Solving such text-based games requires interactive multi-step reasoning, and agents have most commonly been modeled using reinforcement learning [*REF*; *REF*; *REF*].
These methods, however, face challenges such as delayed rewards and difficulty in exploring large action spaces. Recently, there has been an exploration of imitation learning approaches, which utilize human play data [*REF*]. While Behavior Cloning (BC) shows potential in effectively addressing these challenges, it often necessitates substantial effort and resources. This is primarily due to the need for acquiring expert data.


Recently, large language models (LLMs) have demonstrated notable in-context generalization capabilities, suggesting the potential to elicit reasoning abilities by prompting these models [*REF*; *REF*]. However, the application of LLMs in performing symbolic reasoning remains an under-explored area.
Models like GPT-3.5 and GPT-4 have shown the ability to encode extensive information [*REF*]. A significant example of this is their acquisition of substantial knowledge during training, enabling them to approach human-level performance across a wide range of tasks [*REF*]. This indicates the feasibility of utilizing LLMs as neurosymbolic reasoners without relying on labeled gold training data. However, there is currently limited research on utilizing these models for reasoning tasks that involve logic, graphs, or symbolic formulas. The exploration and development of methods that leverage LLMs for symbolic reasoning is highly intriguing and holds significant potential impact.


FIGURE


In this paper, our aim is to investigate the role of Large Language Models (LLMs) in symbolic reasoning within the context of text-based games. When engaging in games that involve symbolic tasks, our LLM agent generates the most rational actions based on the observed game state in a zero-shot manner, assisted by external symbolic modules such as calculators or navigators, as illustrated in Figure [1]. The LLM agent employs both the text-based game environment and symbolic modules to generate a list of valid actions. These valid actions, along with the current observation, are integrated into the prompt to direct the LLM agent in selecting an appropriate action. Subsequently, the LLM agent executes this action, interacting with both the game environment and symbolic modules to complete the task.


In summary, our contributions include:


-  We introduce the use of LLMs for symbolic reasoning and provide a framework for employing the LLM agent as a neurosymbolic reasoner.
This achievement underscores the potential of LLMs, with the support of external modules, to function as neurosymbolic reasoners, capable of successfully completing complex tasks.
-  We have developed the LLM agent with tailored prompts, enabling it to effectively utilize symbolic modules and enhance its performance in text-based games that involve symbolic tasks.
-  Our experiments demonstrate that our agent outperforms strong baselines, including the Deep Reinforcement Relevance Network with symbolic modules and the Behavior Cloned Transformer trained with extensive expert data, achieving an average performance of 88% across all tasks.


Related Work


Large Language Models for Decision Making.


LLMs have demonstrated notable capabilities, enabling their application in tasks that extend beyond language generation [*REF*].
Furthermore, they are increasingly being grounded as policy models for decision-making in interactive contexts [*REF*]. Current studies focus on enhancing the decision-making capacity of LLMs through techniques such as prompting and in-context learning. For instance, *REF* introduce the Chain-of-Thought (CoT) approach, showing that a sequence of intermediate reasoning steps can enhance decision-making capabilities. *REF* present ReAct, a method for interleaved reasoning and action generation to improve performance in interactive decision-making tasks. Other studies [*REF*; *REF*; *REF*; *REF*; *REF*] have explored innovative strategies involving prompt engineering and the utilization of high-level function libraries to enhance the capabilities of LLMs. Additionally, some approaches incorporate mechanisms of self-critique and self-reflection into LLMs, enabling them to refine their generation. For example, *REF* introduce Reflexion, a technique that employs external feedback to detect ineffective actions and engage in self-reflection. *REF* enable an LLM to offer feedback on its previously generated text and refine it adaptively.
Recent attempts have also explored different aspects of LLMs for decision-making. *REF* utilize LLMs as proxy reward functions by prompting them with desired behaviors, while *REF* consider LLMs as world models, where the agent learns policy by interacting with the LLM-based world model. In our work, we focus on developing suitable prompting strategies to enhance the decision-making performance of LLMs in solving symbolic tasks.


Text-based Game.


Text-based games can be formally characterized as partially observable Markov decision processes (POMDPs) [*REF*]. In recent years, there has been a notable increase in the design of reinforcement learning (RL) agents to solve these games [*REF*; *REF*; *REF*]. Current research primarily addresses challenges such as long-term dependencies, partial state observations, sparse rewards, and complex action combinations in text-based games [*REF*; *REF*; *REF*; *REF*].
For instance, *REF* address the challenge of partial observability by exploring the acquisition of graph-structured state representations through data-driven methods. *REF* and *REF* employ a language model to generate a compact set of action candidates for RL agents, tackling the issue of the combinatorial action space. More recently, with the advancement of LLMs, research has shifted towards using prompts to enable LLMs to solve text-based games [*REF*; *REF*]. However, these efforts have primarily focused on the LLMs&apos; capability for in-context learning, while the exploration of their potential in symbolic reasoning has been relatively overlooked.


Neurosymbolic Reasoning.


The field of neurosymbolic reasoning combines the capabilities of deep neural networks with symbolic reasoning, significantly reducing the search space associated with symbolic techniques. This approach has been used to tackle various complex multi-step inference challenges, including tasks like multi-hop question answering [*REF*], language contextualization [*REF*], and semantic analysis [*REF*]. Text-based games that involve symbolic tasks serve as a valuable test-bed for addressing such challenges. Previous approaches have employed traditional optimization techniques or reinforcement learning agents. For example, *REF* decompose text-based games into collections of logical rules, which are then integrated with deep reinforcement learning. *REF* use Integer Linear Programming (ILP) to substantially improve agent performance, providing an interpretable framework for understanding agents&apos; selection of specific actions.


TABLE


Preliminaries


Text-based Games as POMDPs.


Text-based games can be formally defined as partially observable Markov decision processes (POMDPs), considering that the agent only observes partial information about the environment at each turn [*REF*]. In games with symbolic modules, at each discrete time step *MATH*, the agent is provided with an observation denoted as *MATH* and is given a task description denoted as *MATH*. The symbolic module then produces a collection of valid actions, denoted as *MATH*, while the text game environment concurrently establishes its own set of proper actions, denoted as *MATH*. Consequently, the set of acceptable actions at time step *MATH* is the union of these two sets, denoted as *MATH*. The agent&apos;s goal is to select an action *MATH* from the set of valid actions *MATH*, given the observation *MATH* and the task description *MATH*. If *MATH* belongs to the set *MATH*, the symbolic module generates the next observation *MATH*. Conversely, if *MATH* is not part of *MATH*, the text-based game environment processes *MATH* and produces both the subsequent observation *MATH* and the reward *MATH*.


Symbolic Tasks.


There are four distinct tasks within text-based games, namely Arithmetic, MapReader, Sorting and Text World Common Sense (TWC) [*REF*]. Each task is equipped with its own symbolic modules designed to assist agents in successfully accomplishing the task.


FIGURE


Methodology


We introduce an LLM agent, namely a language agent, for employing LLMs to engage in text-based games by leveraging symbolic modules in a zero-shot manner. We begin with an overview of playing games using symbolic modules, followed by a detailed description of the key design features of our language agent, including its prompting mechanism.


Playing Games with Symbolic Tasks


We describe the process of playing games that involve symbolic tasks, using the LLM agent in conjunction with external symbolic modules.


Symbolic Modules.


Symbolic modules play a crucial role in maximizing the reasoning capabilities of LLMs. For example, as shown in Figure [2], consider a scenario where a mathematical problem is presented, and a calculator is available. In such cases, the LLM&apos;s reasoning can effectively use the calculator to complete the task in a zero-shot manner. Furthermore, symbolic modules are adept at their functions, as employing an external tool like a calculator is considered an action in itself.


The scenarios include four distinct symbolic modules: the Calculation Module, Sorting Module, Knowledge Base Module, and Navigation Module.
Table shows examples of how these symbolic modules are utilized. The observation produced by a symbolic module indicates the current state of the game, while the action selected by the LLM agent serves as the input. Additionally, the Navigation Module requires the previous observation as input to accurately determine the player&apos;s current position. For instance, in a mathematical task, the LLM agent may select a computational action such as &quot;multiply 8 by 7&quot; (mul 8 7). This action triggers the symbolic module to calculate the product, and the resulting observation, &quot;Multiplying 8 and 7 results in 56,&quot; is then returned.


The process of engaging in text-based games with LLMs involves multiple stages. The specifics of these steps are detailed in Figure [2]. As mentioned earlier, the comprehensive environment, comprising both the symbolic modules and the text-based game environment, presents the LLM agent with a list of allowable actions. Upon receiving an observation, the LLM agent uses its symbolic reasoning to select an action from this list. If the chosen action involves the symbolic module, the module provides the next observation; otherwise, the text-based game environment supplies the subsequent observation.


LLM as the Neurosymbolic Reasoner


We investigate whether the accumulated world knowledge of LLMs can aid in making accurate decisions for downstream symbolic tasks. To ground LLMs in text-based games, we employ a prompting approach, which eliminates the need for costly additional training. Therefore, we construct prompts in a way that incorporates external context, enabling the LLM agent to generate reasonable actions.


We describe the role of the agent, incorporating the observation, valid actions, and the constraints of executing the action in the prompt, as it is not easy for the LLM agent to understand the underlying rules of the environment through interacting with game environments. The key components of our approach include:


- Role initialization: We initialize the agent by providing them with task descriptions and action constraints.
- Action Query: This step is repeated at each timestep. We prompt the LLM agent with the current observation, inventory state, valid action set, and a question.
- Answer by the LLM agent: The LLM agent chooses an action from the valid action set to complete the task.


TABLE


Role Initialization.


We initialize the role and provide instructions for a functional agent assigned to a task. This process informs the agent about its role, the task description, and the actions it can take, along with their explanations and constraints. These actions are necessary for interacting with text-based games or calling the symbolic module. The agent is instructed to choose from a valid set of actions, such as reading the map, getting paths to specific locations, and recalling the task. Additionally, the agent is advised to utilize the external symbolic module and to avoid unnecessary actions during the task.


Action Query.


At each timestep, we inform the LLM agent of the current game state, as outlined in Table [1]. This information includes the player&apos;s observation, the state of the inventory, the reward, and the valid action set. The inventory state refers to the current possessions of the agent. For instance, in mathematical tasks, the inventory state may consist of a mathematical problem, while in the MapReader task, it could include a map. Additionally, the inventory state can encompass tangible objects, such as toothpaste or a quantity of 18 avocados, acquired by the agent within the environment. The LLM agent is then tasked with selecting one action from the valid action set to continue with the task. It is important to note that the LLM agent is not allowed to decline or provide any text beyond the prescribed response. We also limit the number of valid actions provided by the symbolic module.


In addition, it is essential to develop appropriate prompts that effectively restrict the agent&apos;s actions according to the information provided in Table [2]. It is not feasible for the agent to acquire knowledge and infer the rules within trajectories solely through its interaction with the environment. In all tasks, there is typically a specific order of events, where the object is first taken and then placed in a designated location. This strategy is adopted to prevent scenarios where the object is placed before it is acquired, which would be considered unacceptable in the given context.


Experiments


We demonstrate the potential of LLMs in serving as neurosymbolic reasoners for text-based games. In particular, we present experimental results on four text-based games that involve different symbolic tasks.
In these tasks, we observe that LLMs can effectively function as symbolic reasoners.


Setup


We follow the evaluation framework and game environments in *REF*. These games are developed using the TextWorldExpress game engine [*REF*]. For our LLM agent, we use GPT-3.5-turbo. The LLM agent can interact with game environments and symbolic modules. The task descriptions and examples of how the symbolic modules are called are provided in Table. The evaluation includes four text-based games involving symbolic tasks. Each task is divided into &quot;Train&quot;, &quot;Dev&quot;, and &quot;Test&quot; sets. All evaluations are conducted on the &quot;Test&quot; set.


The evaluation metric is based on two factors: the average score achieved at the end of each game, and the average number of steps taken within a single episode.


TABLE 


Environments


We use four text-based game benchmark environments [*REF*]:


Arithmetic.


The task at hand involves a mathematical component, wherein an agent is required to read and solve a mathematical problem. This process determines the specific object from a given set of objects that they should select and place. The arithmetic game includes a calculator module equipped with the capability to perform basic mathematical operations, including addition, subtraction, multiplication, and division.


MapReader.


A pick-and-place game with a navigation theme, similar to the Coin Collector game [*REF*]. The agent is equipped with a map that may be exploited to optimize route planning. The map provides information on the connections between rooms, such as the lounge connecting to the cookery and supermarket. The navigation symbolic module has the capability to extract location information from the observation space. This includes specific information relating to the present location and geographical features leading to the intended destination. For instance, the instructions sent to the agent might indicate that in order to get from the cooking area to the recreation zone, one must pass through the bar, steam room, library, and finally reach the recreation zone.


Sorting.


This game involves an agent initially situated in a room containing a variable number of objects, ranging from three to five. The agent&apos;s task is to sequentially place these objects into a designated box, adhering to a specific sorting criterion based on increasing quantity. In this game, units related to volume, mass, or length are used, as exemplified by items such as 25g of oak, 12ml of marble, and 6cm of cedar. The sorting game includes a module capable of extracting information from the observation space. This module is specifically designed to identify items that include quantities and can arrange these objects in either ascending or descending order, following the user&apos;s instructions.


Text World Common Sense (TWC).


The challenges provided in this game serve as a baseline for evaluating common sense reasoning abilities [*REF*]. In this game, agents are required to gather objects from their surroundings, such as a clean brown shirt, and subsequently place these objects in their appropriate and commonly recognized locations, like a wardrobe. The incorporation of a symbolic module within this game enables agents to engage in knowledge-based queries. For instance, it allows them to deduce that a clean brown shirt is typically found in a wardrobe.


Baselines


We also compare our LLM agent with two baselines, namely the Deep Reinforcement Relevance Network (DRRN) [*REF*] and the T5-based Behavior Cloned Transformer [*REF*; *REF*], as follows:


-  DRRN: The primary concept of the DRRN is based on Q-learning.
The candidate action with the highest anticipated Q-value is chosen as the next action, based on the current observation. The DRRN employs a Deep Q-Network [*REF*] to estimate the Q-value for each observation-action pair. *REF* note that the DRRN is a fast and robust reinforcement learning baseline, frequently used to produce near state-of-the-art performance in a variety of text-based games.


-  Behavior Cloned Transformer: This method adopts an imitation learning approach, conceptualizing reinforcement learning as a sequence-to-sequence problem, similar to the Decision Transformer [*REF*]. It predicts the subsequent action based on a sequence of previous observations. This baseline aligns with the approach described in *REF*, where the model input at timestep *MATH* includes the task description, current state observation, previous action, and previous state observation.
Symbolic modules are utilized in the demonstrations, specifically employing gold trajectories.


Following *REF*, both baseline models include two variants: one with symbolic modules and one without. When using symbolic modules, we inject actions from these modules into the action space of each game for the baseline models.


Results


Based on the results presented in Table, it is evident that the use of the symbolic module in conjunction with the LLM agent yields a favorable average performance compared to other baseline approaches.
When comparing the outcomes of the Behavior Cloned Transformer with a symbolic module to those of the LLM agent, the performance of the LLM agent is observed to be slightly lower. However, the LLM agent demonstrates a similar level of competency in interacting with the game environment. Furthermore, unlike the Behavior Cloned Transformer models, the LLM agent does not require extensive training with a large volume of expert data. As a result, this approach saves significant training resources.


Table [3] demonstrates that the LLM agent possesses a robust capacity for reasoning, enabling effective handling of tasks involving symbolic tasks. It shows exceptional performance, particularly in mathematics. In the MapReader benchmark, the agent achieves commendable scores, though it requires a considerable number of steps to complete the task. This inefficiency is mainly due to the agent&apos;s tendency to forget the route obtained from the symbolic module, leading to the risk of reaching incorrect locations and necessitating repeated route queries. The complexity of map logic, which involves determining one&apos;s current location and desired destination, adds to the probabilistic nature of this task. In contrast, the Sorting task reveals suboptimal performance, as the LLM agent&apos;s understanding of sorting logic is not fully developed. This issue is largely attributed to the agent&apos;s limited memory capacity, hindering its ability to remember the ascending order of all objects.


In Table [4], it compares the performance of the model with constrained prompts to that of the model without constrained prompts. The results indicate that when the LLM agent is provided with the prompts outlined in Table [2], there is an improvement in performance across all tasks. Additionally, a reduction in the average number of steps required to interact with the game environment is observed. This demonstrates the effectiveness of our constrained prompts in these tasks. Furthermore, experimental results using GPT-4, as shown in Table [5], reveal that it significantly outperforms the GPT-3.5 agent in the MapReader and Sorting tasks, while showing weaker performance in the TWC task.


Discussion.


Our results demonstrate that the incorporation of external symbolic modules by the LLM agent leads to enhanced average accuracy compared to other baselines. This capability is achieved by leveraging the underlying patterns present in the training data. Instead of relying on symbolic thinking or explicit rules, this approach acquires knowledge by recognizing patterns and associations from the extensive corpus of text to which it has been exposed during its training phase, as exemplified by GPT-3.5 and GPT-4 [*REF*]. Although the LLM agent has the capability to connect with a symbolic module for specific tasks, it still exhibits uncertainty and is prone to making mistakes.


TABLE


Conclusion


This paper has demonstrated the effective application of Large Language Models (LLMs) in complex text-based games involving symbolic tasks.
Utilizing a prompting approach, we have guided the LLM agent to efficiently engage with symbolic modules within these games. The efficacy of our method, leveraging LLMs, has shown superior performance compared to alternative benchmarks, highlighting the potential of LLMs to enhance training procedures in text-based games. Consequently, it can be posited that Large Language Models can be considered as Neurosymbolic Reasoners, possessing significant potential for performing symbolic tasks in real-world applications.