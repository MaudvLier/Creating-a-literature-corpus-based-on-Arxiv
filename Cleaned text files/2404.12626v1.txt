Grasper: A Generalist Pursuer for Pursuit-Evasion problems


Introduction


The deployment of security resources to detect, deter, and catch
criminals is a critical task in urban
security [*REF*; *REF*]. Statistics show that
police pursuits probably &quot;injure or kill more innocent bystanders than
any other kind of force&quot; [*REF*]. Therefore, it is crucial to
come up with scalable approaches for effectively coordinating various
security resources, ensuring the swift apprehension of a fleeing
criminal to minimize harm and property damage. Due to the adversarial
nature between attackers and defenders, game-theoretic models have been
used to model various real-world urban security scenarios. In
particular, the pursuit-evasion game (PEG) has been extensively employed
to model the interactions between a team of pursuers (e.g., police
forces) and an evader (e.g., a criminal) on graphs (e.g., urban street
networks) [*REF*; *REF*; *REF*; *REF*].
To effectively solve PEGs under various settings, several methods, such
as counterfactual regret minimization (CFR) [*REF*] and
policy-space response oracles (PSRO) [*REF*], have been
developed in the literature. Among these algorithms, PSRO, a deep
reinforcement learning algorithm, provides a versatile framework for
learning the (approximate) Nash equilibria (NEs) of PEGs (refer to
Section for an introduction of the PSRO framework).
Furthermore, recent works have also integrated the pre-training and
fine-tuning paradigm into the PSRO framework to further enhance its
scalability [*REF*].


Although many existing works have achieved significant success, they
only focus on solving specific PEGs with predetermined initial
conditions, e.g., the initial locations of all players and exits, and
the time horizon of the game. Unfortunately, these conditions may vary
substantially in real-world scenarios, where crimes can occur at any
location in a city and at any time. When the initial conditions change,
existing algorithms must solve the new PEG from scratch, which is
computationally demanding and time-consuming [*REF*],
restricting the real-world deployment of current algorithms. Thus, there
is an urgent necessity to develop a new approach capable of solving
different PEGs with varying initial conditions effectively.


To this end, we introduce Grasper: a GeneRAlist purSuer for
Pursuit-Evasion pRoblems, which can effectively solve different PEGs by
generating the pursuer&apos;s policies conditional on the initial conditions
of the PEGs. Grasper consists of two critical components. First, as the
PEG is played on a graph, it is natural to use a graph neural network
(GNN) to encode the PEG with the given initial conditions into a hidden
vector. Then, inspired by recent work on generalization over games with
different population sizes [*REF*], we introduce a
hypernetwork to generate the base policy for the pursuer conditional on
the hidden vector obtained by the GNN. This generated base policy then
serves as a starting point for the pursuer&apos;s best response policy
training at each PSRO iteration.


To train the networks of Grasper, we find that naively applying
multi-task training [*REF*] is inefficient. Furthermore,
jointly training the GNN and hypernetwork could be time-consuming as the
GNN is only used to encode the initial conditions which are fixed during
the game playing. To address these challenges, we propose an efficient
three-stage training method to train the networks of Grasper. First, we
introduce a pre-pretraining stage to train the GNN by using
self-supervised graph learning methods such as
GraphMAE [*REF*]. Second, we fix the GNN and pre-train the
hypernetwork by using a multi-task training procedure where the training
data is sampled from different PEGs with different initial conditions.
In this stage, to overcome the low exploration efficiency due to the
pursuers&apos; random exploration and the evader&apos;s rationality, we propose a
heuristic-guided multi-task pre-training (HMP) where a reference policy
derived by heuristic methods such as Dijkstra is used to regularize the
pursuer policy. Finally, we follow the PSRO procedure and obtain the
pursuer&apos;s best response policy at each iteration by fine-tuning the base
policy generated by the hypernetwork.


In summary, we provide three contributions. First, we propose Grasper
which is the first generalizable framework capable of efficiently
providing highly qualified solutions for different PEGs with different
initial conditions. Second, to efficiently train the networks of
Grasper, we propose a three-stage training method: (i) a pre-pretraining
stage to train the GNN through GraphMAE, (ii) a pre-training stage to
train the hypernetwork through heuristic-guided multi-task pre-training
(HMP), and (iii) a fine-tuning stage to obtain the pursuer&apos;s best
response policy at each PSRO iteration. Finally, we perform extensive
experiments, and the results demonstrate the superiority of Grasper over
different baselines.


Related Work


Pursuit-evasion games (PEGs) have been extensively applied to model
various real-world problems such as security and
robotics [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
To efficiently solve PEGs and different variants, many algorithms such
as value iteration [*REF*] and incremental strategy
generation [*REF*; *REF*] have been introduced.
Nonetheless, these methods encounter scalability issues as they
typically rely on linear programming. On the other hand, PEG can be
viewed as a particular type of two-player imperfect-information
extensive-form game (IIEFG). Thus, the algorithms used for solving
large-scale IIEFGs, such as counterfactual regret minimization
(CFR) [*REF*] and Policy-Space Response Oracles
(PSRO) [*REF*], have been applied to tackle large-scale
PEGs [*REF*; *REF*]. However, when solving large-scale
PEGs using PSRO, there exist significant computational challenges as it
involves computing the best response strategy multiple times. To
mitigate this issue, recent research [*REF*] integrates the
pre-training and fine-tuning paradigm into PSRO to improve its
scalability. Despite the success, all these algorithms are tailored to
solve a specific PEG with predetermined initial conditions. When these
conditions change, they must recompute the NE strategy from scratch (one
to two hours for a PEG on a *MATH* grid map [*REF*]),
which hinders their real-world applicability. To address this
limitation, we propose Grasper, which uses PSRO to compute the NE
strategy and can generate different pursuers&apos; strategies for different
PEGs based on their initial conditions without recomputing the NE
strategy from scratch.


The generalizability of algorithms and models over different games has
gained increasing attention and remarkable progress has been achieved in
recent research. Neural equilibrium approximators that directly predict
the equilibrium strategy from game payoffs in normal-form games (NFGs)
have been theoretically proven PAC
learnable [*REF*; *REF*] and are able to
generalize to different games with desirable solution
quality [*REF*; *REF*; *REF*; *REF*].
However, it remains under-explored when going beyond NFGs. In this work,
we make the first attempt to consider the generalization problem in the
domain of PEGs, a type of game that has a wide range of real-world
applications [*REF*; *REF*] and is far more
complicated than NFGs. We propose a novel algorithmic framework that is
able to efficiently solve different PEGs with varying initial conditions
and demonstrate the generalization ability through extensive experiments.


Our work is also related to self-supervised graph learning and
multi-task learning. Recent works have shown that generative
self-supervised learning [*REF*] can be applied to graph
learning and outperform contrastive methods which require complex
training strategies [*REF*; *REF*], high-quality
data augmentation [*REF*], and negative samples that are
often challenging to construct from graphs [*REF*]. Therefore,
we employ the recent state-of-the-art, GraphMAE [*REF*], to
learn a good representation of a PEG with the given initial conditions.
Multi-task learning [*REF*; *REF*] has been
applied to various domains including natural language
processing [*REF*], computer vision [*REF*],
and reinforcement learning (multi-task RL) [*REF*; *REF*; *REF*; *REF*].
Due to its strong generalizability, we employ multi-task RL for the
pre-training process, enabling the pre-trained policy to be quickly
fine-tuned for efficient policy development in new tasks.


Finally, our work is related to the multi-agent patrolling problem where
the evader often anticipates patrolling strategies and may choose a
target or a single path as an action [*REF*; *REF*; *REF*; *REF*].
Conversely, our pursuit-evasion game features simultaneous actions with
the evader unaware of the pursuer&apos;s real-time locations.


Problem Formulation


In this section, we first present all the elements for defining the
PEGs. Then, we present the state-of-the-art (SOTA) method for solving
PEGs. Finally, we give the problem statement of this work.


Preliminaries


A pursuit-evasion game (PEG) is a two-player game played between a
pursuer and an evader, i.e., *MATH*. Following previous works
[*REF*; *REF*; *REF*], we assume that the
pursuer comprises *MATH* members denoted as *MATH*, and the
pursuer can obtain the real-time location information of the evader with
the help of tracking devices. In reality, PEGs are typically played on
urban road maps, which can be represented by a graph *MATH*, where
*MATH* is the set of vertices and *MATH* is the set of edges. Let
*MATH* denote the set of exit nodes from which the
evader can escape and *MATH* the predetermined time horizon of the game. At
*MATH*, the locations of the evader and pursuer are denoted by
*MATH* and *MATH*, respectively. Then,
the history of the game at *MATH* is a sequence of past locations of both
players, i.e., *MATH*. The available action set for both players is the neighboring vertices of the
player&apos;s current location, i.e., *MATH* and *MATH* 
where *MATH* denotes the set of neighboring vertices of vertex
*MATH*. According to the definition of history, we define the information
set for each player as the set consisting of indistinguishable
histories. As the evader cannot get the pursuer&apos;s real-time location
information, the information set of the evader is defined as
*MATH*, where *MATH* represents any possible location of the pursuer. Although the PEG is a
simultaneous-move game, we can model it as an extensive-form game (EFG)
by assuming that the evader acts first and then the pursuer commits
without any information about the evader&apos;s current action. The pursuer&apos;s
information set can be defined as *MATH* since the pursuer knows 
the evader&apos;s location but not the evader&apos;s current action.


A behavior policy of a player assigns a probability distribution over
the action set for every information set belonging to the player. Notice
that the pursuer&apos;s action space is combinatorial and expands
exponentially with the number of pursuer members. As a result, directly
learning a joint policy of the pursuer members would be computationally
difficult. To address this issue, instead of learning a joint policy,
previous works learn the individual policies either through value
decomposition [*REF*] or global critic [*REF*], which
are the paradigm of centralized training with decentralized execution
(CTDE) for the pursuers. Furthermore, previous works [*REF*]
also introduce a new state representation ignoring the game&apos;s historical
information, which leads to improved performance. In our work, we follow
the previously mentioned conventions to define the observations and
individual policies for the pursuers.


At each time step *MATH*, each pursuer member gets an observation
*MATH*, which includes all players&apos; current locations, the id of the pursuer member, and the time
step. Each pursuer member *MATH* constructs a policy
*MATH*, which assigns a probability
distribution over the action set *MATH*,
*MATH*. As for the evader&apos;s policy, we follow previous
works [*REF*; *REF*] that employ High-Level Actions
for the evader. That is, the evader only chooses the exit node to escape
from and then samples one shortest path from the initial location to the
chosen exit node, instead of deciding where to go in the next time step.
Specifically, at time step *MATH*, the evader determines an exit node
*MATH* using the policy *MATH*, samples a shortest path from
*MATH* to the chosen exit node *MATH*, and then takes
actions based on the path.


Here, we give some remarks on the assumption of High-Level Actions of
the evader. (i) In our game setting, the evader lacks real-time access
to the pursuers&apos; locations, requiring the evader to act without any
information about their whereabouts. Therefore, sampling one path for
the evader would not lose much information compared with the case where
the evader acts step by step. (ii) Training the pursuer against an
evader who chooses the shortest path, a worst-case scenario for the
pursuer, enhances the robustness of the pursuer&apos;s policy. (iii) Though
it is a simplification, the problem setting remains highly complex due
to the multiple exits and diverse players&apos; initial conditions, enlarging
the task space for the pursuer, as detailed in the Introduction and
Appendix A Q2.


In summary, given a graph *MATH* with the specific set of exit nodes
*MATH*, the initial locations of the pursuer and evader
(*MATH*), and the predetermined time horizon *MATH*, we can
define a specific PEG as *MATH*. In the PEG, players
will get the non-zero rewards only when the game is terminated. The
termination conditions include three cases: (i) the pursuer catches the
evader within the time horizon *MATH*, i.e., *MATH*;
(ii) the evader escapes from an exit node within the time horizon *MATH*,
i.e., *MATH*; (iii) the game reaches the time
horizon *MATH*. Let *MATH* be the time step that the game is
terminated. Then, for all *MATH*, *MATH*. For
*MATH*, in cases (i) and (iii), the pursuer receives a
reward *MATH* while the evader incurs a penalty *MATH*. In
case (ii), the evader gains a reward *MATH*, and the pursuer suffers
a loss *MATH*. Given the exit node chosen by the evader
*MATH*, we have *MATH* for the pursuer and *MATH* for
the evader, where the expectation is taken over the trajectories induced
by *MATH*. Then, for the policy pair *MATH*, we have
*MATH* for the pursuer and *MATH* for the evader.


Policy-Space Response Oracles


ALGORITHM


FIGURE


As one of the popular algorithms, PSRO [*REF*] can be
employed to solve a PEG *MATH*, shown in
Algorithm. It commences with each player using a random
policy (Line 1) and then expands the policy spaces of the pursuer and
evader in an iterative manner. At each epoch *MATH*: (1)
Compute the best response (BR) policies of the pursuer *MATH* and
evader *MATH* and add them to their policy spaces *MATH* and
*MATH* (Line 3--5); (2) Construct a meta-game *MATH* using all
policies in each player&apos;s policy space (Line 6); (3) Compute the
meta-strategy of the pursuer *MATH* and evader
*MATH* using a meta-solver (e.g.,
PRD [*REF*]) on the meta-game *MATH* (Line 7). These
processes are repeated for *MATH* epochs and then output the final
meta-strategy across the players&apos; policy spaces (Line 9).


As the evader&apos;s policy is a probability distribution over exit nodes, to
compute the BR policy (Line 3), we only need to estimate the value of
each exit node through simulations, i.e., *MATH*, *MATH*. Then, the evader&apos;s BR policy is
constructed by applying softmax operation on the values of all the exit
nodes. For the pursuer, computing the BR policy is to solve the problem
*MATH* (Line 4). As there are multiple pursuer members, we can use
MAPPO [*REF*] to learn the BR policy. In the traditional
PSRO algorithm, the pursuer&apos;s BR policy is learned from scratch, i.e.,
the BR policy is randomly initialized, which is inefficient. To address
this issue, recent works integrate the pre-training and fine-tuning
paradigm into PSRO to improve learning efficiency [*REF*].
Specifically, before running PSRO, a base pursuer policy is trained
through multi-task RL where each task is generated with a randomly
initialized evader&apos;s policy. Then, at each PSRO epoch, the pursuer&apos;s BR
policy is initialized with the pre-trained base policy, rather than
learning from scratch, which can largely improve the learning efficiency
of the PSRO algorithm.


Problem Statement


Although PSRO has been successfully applied to solve PEGs,
unfortunately, previous works typically focus on solving a specific PEG
with predetermined initial conditions which are not always fixed in
real-world scenarios: (i) The initial locations of the pursuers and the
evader (*MATH*) are not always fixed since attacks (thieves,
crimes, terrorists) can occur at any time and location in a city; (ii)
The locations of the exit nodes *MATH* may change due to temporary
closures and openings; (iii) The time horizon *MATH* might vary, as the
time required to pursue the evader is not always the same. When any of
the initial conditions change, the PEG adapts accordingly. As a
consequence, current algorithms can only solve the modified PEG from
scratch, leading to significant time consumption and inefficiency. Even
the SOTA method presented in the previous section -- PSRO with
pre-trained base pursuer policy -- still suffers from such an issue as
the base policy is pre-trained under the premise that the initial
condition of the PEG is fixed. In other words, a new base policy must be
pre-trained from scratch for the modified PEG since the original base
policy may not be a good starting point for the pursuer&apos;s BR policy in
the modified game (even worse than a randomly initialized BR policy). In
this paper, we aim to address this issue by developing a generalist
pursuer capable of learning and adapting to different PEGs with varying
initial conditions without the need to restart the training process from
the beginning.


Grasper


In this section, we introduce Grasper, illustrated in
Figure. We first present the architecture of
Grasper including several innovative components, and then the training
pipeline which consists of three stages to efficiently train the
networks of Grasper.


Architecture


Graphical Representations of PEGs.


To generate the pursuer&apos;s policy based on the specific PEG, we propose
to take the specific PEG as an input of a neural network. To this end,
we encode the initial conditions of a PEG except for *MATH* into a graph
(Figure (a)). The time horizon *MATH* can be directly
fed into the neural network. Specifically, given a PEG
*MATH*, these initial conditions *MATH* and *MATH* can be encoded into the
graph *MATH* by associating each node of the graph with a vector consisting
of the following parts: (i) a binary bit *MATH* where *MATH* indicates
that the node is an exit, (ii) a binary bit *MATH* where *MATH* 
signifies that the evader&apos;s initial location is this node, (iii) the
number of pursuers on this node *MATH* (the total number of
pursuers across all nodes equals to *MATH*), and (iv) additional
information regarding the topology of the graph, such as the degree of
the node. This provides a universal representation of any PEG with any
initial condition.


Game-conditional Base Policies Generation.


After representing a PEG as a graph, it is natural to leverage a graph
neural network (GNN) to encode the PEG with the given initial conditions
into a hidden vector. As shown in
Figure (b), we first feed the graphical
representation of the initial conditions into the GNN and get the
representations of all the nodes of the graph. Then, we use a pooling
operation to integrate all the node representations into a hidden vector
which will be concatenated with the time horizon *MATH* to get the final
representation of the PEG. Next, to generate a base policy conditional
on the PEG, we introduce a hypernetwork [*REF*], a neural
network that takes the final representation of the PEG as input and
outputs the parameters (weights and biases) of the policy network
(Figure (c)). Finally, the base policy network
serves as a starting point for the training of the pursuer&apos;s best
response policy in each iteration of the PSRO algorithm
(Figure (d)).


Observation Representation Layer.


As described earlier, the pursuer&apos;s policy is a mapping that associates
each observation with a probability distribution over the available
action set. Notably, an observation consists of the positions of both
players. Representing these positions by mere index numbers of vertices
in the graph does not provide much useful information for training,
though. Therefore, we seek a more compact and meaningful representation
of these observations. Previous works [*REF*; *REF*]
typically train a node embedding model for this purpose. Unfortunately,
such a model is often tailored and trained for a specific graph,
limiting its generalizability to other graphs. This lack of
generalizability makes this method unsuitable for our problem. To
address this issue, we adopt a representation layer to encode the
pursuer&apos;s observations, an approach that is not limited to a specific
graph.


As given in Section, the pursuer&apos;s observation
*MATH* includes three parts: the players&apos;
current locations *MATH*, the pursuer member&apos;s id *MATH*, and
the current time step *MATH*. Thus, the representation layer consists of
three components, each of which is a &quot;torch.nn.Embedding&quot; which has been
extensively used to encode an integer to a compact representation. The
outputs of the three components are concatenated to obtain the
representation of the pursuer&apos;s observation. This representation layer
will be trained jointly with the hypernetwork during the pre-training
process. Please refer to Appendix B for details on the architecture of
the representation layer, the GNN, and the hypernetwork.


Intuitively, the generalization ability of Grasper benefits from several
designs of our architecture. First, the graphical representation offers
a universal representation of any PEG, regardless of the graph&apos;s
topology. Second, GNN can encode different PEGs into fixed-size hidden
vectors, which can be directly fed into the hypernetwork (otherwise,
additional techniques are required if the sizes of the hidden vectors
are varied). Finally, the hypernetwork is designed to generate a
specialized policy tailored to a given PEG.


Training Pipeline


Now we introduce the training pipeline of Grasper, which involves three
stages: pre-pretraining, pre-training, and fine-tuning. Prior to delving
into the specifics, we first describe how the training set is generated.
The training set *MATH* should consist of different PEGs for
training. To this end, we generate the training set by randomizing the
initial conditions, denoted by *MATH*.
However, this approach may yield certain games that lack meaningful
training value. For example, when the evader&apos;s initial location is in
such close proximity to the exit nodes that the pursuer becomes
incapable of capturing the evader regardless of its movements. To
exclude these trivial cases, we introduce a filter condition when
generating the training set: the shortest path from the evader&apos;s initial
location to any exit nodes must exceed a predetermined length.


Stage I: Pre-pretraining.


As the hypernetwork takes a feature vector as input, we first use a GNN
to encode the graphical representation of the PEG into a fixed-size
hidden vector. As the GNN is solely employed to derive the effective
representation from the PEG&apos;s graphical interpretation, we introduce a
pre-pretraining stage (Figure (b)) to pre-train the GNN before the actual
pre-training stage. This approach is more efficient compared to jointly
training the GNN and hypernetwork in the pre-training stage.
Specifically, for each game in the training set
*MATH*, let *MATH* and *MATH* denote the adjacency matrix and feature matrix of
the underlying graph, respectively. We first obtain the latent code
matrix *MATH* by the GNN and train the GNN via any self-supervised graph learning
method (we use the recent SOTA method, GraphMAE [*REF*]).
Then, we get the hidden vector by pooling the latent code matrix
*MATH*, which will be fed into the hypernetwork.


ALGORITHM


Stage II: Pre-training.


Given a fixed evader&apos;s policy in a specific PEG, computing the pursuer&apos;s
best response policy can be seen as an RL task. Thus, we can apply the
multi-task RL algorithm to guide the pre-training process, which is
shown in Algorithm. Different from previous
work [*REF*], in these RL tasks, except for the change in the
evader&apos;s policy, the game&apos;s initial conditions also change. To obtain
these RL tasks for pre-training, we first randomly sample *MATH* games
from the training set (Line 3), and then for each game, we randomly
sample *MATH* evader&apos;s policies (Line 5). Once the game and the evader&apos;s
policy are fixed, the RL task is generated. During pre-training, for
each game, we first feed the hidden vector of the game (obtained by the
trained GNN) and the time horizon into the hypernetwork to generate the
pursuer&apos;s base policy, and then for each evader&apos;s policy, we collect the
training data using the pursuer&apos;s base policy (accompany by the
representation layer) into the episode buffer. Finally, we train the
hypernetwork and the representation layer jointly based on the episode
buffer (Lines 6-9). To deal with the multiple pursuer members cases, we
employ MAPPO [*REF*] as the underlying RL algorithm.


However, we found that simply applying the MAPPO under the multi-task
learning framework can result in low efficiency due to random
exploration in the environment. To clarify, consider the example
illustrated in Figure [1], which shows the need for a more efficient
pre-training method. Assume that the evader&apos;s policy is to take the
shortest path to one of the exits (denoted by the red path). If the
pursuer explores the environment randomly (the orange path), it will
probably lose the game and then receive a negative reward. This
situation can occur frequently at the beginning of the pre-training
process because the pursuer&apos;s initial policy is invariably random. To
mitigate this exploration inefficiency, we propose a novel scheme: heuristic-guided multi-task pre-training (HMP).


FIGURE


Note that in the RL tasks used for pre-training, we can acquire the
evader&apos;s policy, which can be used to guide the exploration of the
pursuer&apos;s policy. Specifically, given the exit node chosen by the
evader&apos;s policy *MATH*, we first induce a reference policy
*MATH* (represented by the green path) for the pursuer using
heuristic methods such as the Dijkstra algorithm. Then, apart from the
actions sampled by the generated policy *MATH* (Line 6),
we also sample the reference actions using the reference policy
*MATH* and add the data to the training buffer (Lines 8-9). Let
*MATH* denote the original loss function for training the
actor in the MAPPO algorithm. The HMP is implemented by introducing an
additional loss into the original loss function: *MATH* where
*MATH* controls the weight of the guidance of the reference
policy and *MATH* represents the Kullback--Leibler divergence (for
the reference policy *MATH*, the action probability distribution
is obtained by setting the probability of the reference action to 1
while all others to 0).


Stage III: Fine-tuning.


In this phase, we integrate the pre-trained pursuer policy into the PSRO
framework, as shown in Algorithm. The pursuer&apos;s policy *MATH* is
initialized using the output neural network from the pre-trained
Grasper, which takes the graphical representation of the specific PEG as
an input. Simultaneously, the evader&apos;s policy *MATH* is randomly
initialized (Line 2). Then we follow the standard PSRO framework: in
each iteration *MATH*, the best response (BR) policies for both players,
*MATH* and *MATH*, are computed using their respective BR oracles
(Lines 4-6). These BR policies are then added to the policy sets
*MATH* and *MATH* (Line 7), and the meta-game matrix *MATH* is
updated through simulation (Line 8). Finally, the meta distribution
*MATH* is computed using any meta-solver (Line 9).


The BR oracles for both players are the important components of the PSRO
algorithm. The evader&apos;s BR oracle follows the standard PSRO algorithm
given in Algorithm. The key difference between our fine-tuning
process and the standard PSRO algorithm lies in the training of the
pursuer&apos;s BR policy, which is highlighted in blue. Specifically, given
the pre-trained policy *MATH* conditional to the initial conditions,
we can use it as the starting point for the computation of the pursuer&apos;s
BR policy (Line 5), rather than training from scratch. This allows us to
simply fine-tune the pre-trained policy *MATH* over a few episodes
(Line 6) to quickly obtain the BR policy, significantly enhancing the
learning efficiency.


ALGORITHM 



figure


Experiments


In this section, we perform experiments to evaluate the performance of
Grasper and the effectiveness of different components.


Setup


Hyperparameters. The number of pursuers is *MATH*, the number of exit
nodes is 8, the time horizon *MATH* is *MATH*, and the number
of pre-training episodes is 20 million (20M). For PSRO, the number of
episodes used for training the best response is 10. We conduct
experiments on four maps: the grid map with size *MATH*, the
scale-free graph with 300 nodes, the Singapore map [*REF*]
with 372 nodes, and the Scotland-Yard map [*REF*] with 200
nodes. To simulate the situation where a road might be temporally
blocked due to congestion or traffic accidents, we set the probability
of an edge between two nodes to 0.8 for the grid map, 0.9 for the
Singapore map, and 1.0 (i.e., no congestion) for the other two maps.
More details on the hyperparameters can be found in Appendix B.


Worst-case Utility. Given that a PEG is a zero-sum game, we use the
pursuer&apos;s worst-case utility (as the evader always chooses the shortest
path from the initial location to the chosen exit) to measure the
quality of the solution: *MATH*,
where the inner expectation is taken over the trajectories induced by
*MATH* and *MATH* which are respectively sampled according to *MATH* and *MATH*.


Training and Test Sets. (1) We generate
*MATH* games as the training set. During the
generation, the minimum length of the evader&apos;s shortest path is set to 6
for the grid map and 5 for other maps. (2) We create two test sets,
*MATH* and *MATH*, each containing 30 games. (i)
*MATH* includes the games sampled from the training set
*MATH* (in-distribution test). (ii) *MATH* contains the games distinct from the training set
*MATH* (out-of-distribution test). To avoid trivial cases (the games that are either too difficult or too
simple for the pursuers), we constrain the zero-shot performance of
Grasper (i.e., the worst-case utility of the generated policy without
fine-tuning) within the range: *MATH* for *MATH* and *MATH* for *MATH*.


Baselines. (i) Multi-task PSRO (MT-PSRO): the state-of-the-art
(SOTA) approach adapted from [*REF*], which also uses the
observation representation layer and HMP. (ii) MT-PSRO with augmentation
(MT-PSRO-Aug): the hidden vector obtained from the pre-trained GNN and
the time horizon are concatenated to the output of the observation
representation layer. (iii) PSRO: the standard PSRO method. (iv) Random: 
the pursuer randomly selects actions.


Results


The experimental results are summarized in
Figure. The *MATH* -axis is the running time.
For the purpose of a fair comparison, apart from the running time of the
fine-tuning stage (the PSRO procedure), we also include the running time
of pre-pretraining and pre-training (called the pre-training time for
convenience). Since the games in the training set are uniformly randomly
sampled during pre-training, we approximate the pre-training time of
each game by averaging the total pre-training time over the training
set. Then, for each testing game, we add the pre-training time to the
running time (the horizontal gap between 0 and the start of the line).
Note that the amortized pre-training time for Grasper, MT-PSRO, and
MT-PSRO-Aug is similar since the pre-pretraining time is very short
(Table [2]). From the results, we can draw several conclusions.


(i) Given a fixed number of episodes for the fine-tuning process,
Grasper can start from and converge to a higher average worst-case
utility than the baselines, although it takes a certain pre-training
time, demonstrating the effectiveness of the pre-pretraining and
pre-training in accelerating the PSRO procedure. Note that MT-PSRO and
MT-PSRO-Aug also employ pre-pretraining or pre-training, but they
perform worse than Grasper, showcasing the superiority of Grasper. (ii)
For a fair comparison, MT-PSRO-Aug also integrates the information about
the initial conditions of the PEGs. The results clearly show the
necessity of the hypernetwork in Grasper. This can be also partly
verified by comparing MT-PSRO and MT-PSRO-Aug where their performance is
comparable, meaning that naively integrating the information about the
initial conditions does not bring much benefit and novel designs are
necessary. (iii) An interesting result is that even on the test set
*MATH* (in-distribution test), MT-PSRO and MT-PSRO-Aug, the
strongest baselines, perform worse on all the other maps than on the
grid map. We hypothesize the reason is that the other maps are more
heterogeneous than the grid map. For example, the degree of the nodes
varies from 1 to 16 in the Singapore map while it remains between 2 to 4
in the grid map. Thus, the games generated on the Singapore map share
much less similarity. In this sense, the information about the initial
conditions is particularly important when solving different PEGs. (iv)
In all cases, the performance of Grasper is much more stable than the
baselines (smaller standard error) as Grasper can generate distinct
policies for different PEGs. In contrast, other baselines either
entirely ignore or naively integrate the information about the initial
conditions of the PEGs, which renders them hard to generalize to
different PEGs, leading to larger performance variance than Grasper. (v)
The results on the test set *MATH* show that Grasper can solve
unseen games, exhibiting better generalizability than the baselines.


Ablations


Effectiveness of Different Modules. First, we study the contribution
of HMP and the observation representation layer (Rep.) to the
performance of Grasper, as shown in Table [1]. The results show that we can get
better performance (high worst-case utility and small standard error)
only when combining the two components, meaning that both two components
are indispensable for Grasper.


TABLE


Effectiveness of Pre-pretraining. Next, we investigate the
effectiveness of the pre-pretraining stage in accelerating the whole
training procedure of Grasper. Since jointly training GNN and the other
parts of Grasper for 20M pre-training episodes requires a long running
time, in this ablation study, we focus on the first 2M pre-training
episodes and compare the running time of Grasper with pre-pretraining
(w/ PP) and without pre-pretraining (w/o PP). The training curves are
shown in Figure [2], which shows that using pre-pretraining
can significantly accelerate the training procedure (*MATH* times faster
than without using pre-pretraining).


FIGURE


The quantitative values of the running time of the pre-pretraining and
pre-training are given in Table [2]. As the pre-pretraining time (304.2 seconds) is
much shorter than the pre-training time (9954.9 seconds), the curves of
Grasper, MT-PSRO, and MT-PSRO-Aug shown in
Figure start from a similar position in the *MATH* -axis.


TABLE


Influence of Evader&apos;s Initial Location. We perform some experiments
using Grasper to provide some insights into the PEG. In
Figure [3], we present the pursuer&apos;s
utility when the evader randomizes the initial location over the grid
map. We found that in some areas the pursuers can have high utility. For
example, in the top-right of the left figure, there are three pursuers
and only one exit, which means it could be hard for the evader to
escape. In the bottom-right of the right figure, as the pursuer&apos;s
initial location is near the two exits, it could be easy for the pursuer
to catch the evader, even though there is only one pursuer in this area.
The results reflect the intuition that Grasper can generate distinct
policies for different games and hence, the performance is more stable.


FIGURE


Conclusions


In this work, we investigate how to efficiently solve different PEGs
with varying initial conditions. First, we propose a novel generalizable
framework, Grasper, which includes several critical components: (i) a
GNN to encode a specific PEG into a hidden vector, (ii) a hypernetwork
to generate the base policies for the pursuers conditional on the hidden
vector and time horizon, (iii) an observation representation layer to
encode the pursuers&apos; observations into compact and meaningful
representations. Second, we introduce an efficient three-stage training
method which includes: (i) a pre-pretraining stage that learns robust
PEG representations through GraphMAE, (ii) a heuristic-guided multi-task
pre-training stage that leverages a reference policy derived from
heuristic methods such as Dijkstra to regularize pursuer policies, and
(iii) a fine-tuning stage that utilizes PSRO to generate pursuer
policies on designated PEGs. Finally, extensive experiments demonstrate
the superiority of Grasper over baselines in terms of solution quality
and generalizability. To the best of our knowledge, this is the first
attempt to consider the generalization problem in the domain of PEGs.
Future directions include (i) more efficient task sampling strategies
for pre-training, e.g., AdA [*REF*], (ii) a model capable of
generalizing to different PEGs with different underlying graph
topologies, e.g., generalizing from grid maps to scale-free maps, and
(iii) a model capable of tackling more complex settings, e.g.,
learning-based evader.