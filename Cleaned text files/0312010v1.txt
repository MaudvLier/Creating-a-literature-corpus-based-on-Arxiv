Self-organization in a simple model of adaptive agents playing 2 2
games with arbitrary payoff matrices


We analyze, both analytically and numerically, the self-organization
of a system of &quot;selfish&quot; adaptive agents playing an arbitrary
iterated pairwise game (defined by a 2×2 payoff matrix). Examples of
possible games to play are: the Prisoner&apos;s Dilemma (PD) game, the
chicken game, the hero game, etc. The agents have no memory, use
strategies not based on direct reciprocity nor &apos;tags&apos; and are chosen
at random i.e. geographical vicinity is neglected. They can play two
possible strategies: cooperate (C) or defect (D). The players measure their success by
comparing their utilities with an estimate for the expected benefits
and update their strategy following a simple rule.


Two versions of the model are studied: 1) the deterministic version
(the agents are either in definite states C or D) and 2) the
stochastic version (the agents have a probability c of playing C).
Using a general Master Equation we compute the equilibrium states into
which the system selforganizes, characterized by their average
probability of cooperation ceq. Depending on the payoff matrix, we show that ceq can take five different values.


We also consider the mixing of agents using two different payoff
matrices an show that any value of ceq can be reached by tunning the
proportions of agents using each payoff matrix. In particular, this
can be used as a way to simulate the effect a fraction d of
&quot;antisocial&quot; individuals - incapable of realizing any value to
cooperationon the cooperative regime hold by a population of neutral
or &quot;normal&quot; agents.


I. INTRODUCTION


Complex systems pervade our daily life. They are difficult to study
because they don&apos;t exhibit simple cause-and-effect relationships and
their interconnections are not easy to disentangle.


Game Theory has demonstrated to be a very flexible tool to study
complex systems. It coalesced in its normal
formREF during the second World War with the work
of Von Neumann and Morgenstern REF who first applied
it in Economics.


Later, in the seventies, it was the turn of Biology mainly with the
work of J. Maynard-Smith REF, who shown that the
Game Theory can be applied to various problems of evolution, and
proposed the concept of Evolutionary Stable Strategy (ESS), as an
important concept for understanding biological phenomena. Following
rules dictated by game theory to attain an ESS requires neither
consciousness nor a brain. Moreover, a recent experiment found that
two variants of a RNA virus seem to engage in two-player games
REF.


This opens a new perspective, perhaps the dynamic of very simple
agents, of the kind we know in Physics, can be modeled by Game Theory
providing an alternative approach to physical problems. For instance,
energies could be represented as payoffs and phenomena like phase
transitions understood as many-agents games. As a particular
application of this line of thought we have seen recently a
proliferation of papers addressing the issue of quantum games
REF- REF which might shed light on
the hot issue of quantum computing. Conversely, Physics can be useful
to understand the behavior of adaptive agents playing games used to
model several complex systems in nature. For instance, in some
interesting works Szabo et al REF,REF
applied the sophisticated techniques developed in non-equilibrium
statistical physics to spatial evolutionary games.


The most popular exponent of Game Theory is the Prisoner&apos;s Dilemma
(PD) game introduced in the early fifties by M. Flood and M. Dresher
REF to model the social behavior of &quot;selfish&quot;
individuals - individuals which pursue exclusively their own
self-benefit.


The PD game is an example of a 2 2 game in normal form: i) there are 2
players, each confronting 2 choices - to cooperate (C) or to defect
(D)-, ii) with a 2 2 matrix specifying the payoffs of each player for
the 4 possible outcomes: \[C,C\],\[C,D\],\[D,C\] and
\[D,D\] REF and iii) each player makes his choice
without knowing what the other will do. A player who plays C gets the
&quot;reward&quot; R or the &quot;sucker&apos;s payoff&quot; S depending if the other
player plays C or D respectively, while if he plays D he gets the
&quot;temptation to defect&quot; T or the &quot;punishment&quot; P depending if the
other player plays C or D respectively. These four payoffs obey the
relations: FORMULA and FORMULA.


Thus independently of what the other player does, by
REF, defection D yields a higher payoff than cooperation 
C (T \&gt; R and P \&gt; S) and is the dominant strategy. The
outcome \[D,D\] is thus called a Nash equilibrium
REF. The dilemma is that if both defect, both do
worse than if both had cooperated (P \&lt; R). Condition
REF is required in order that the average utilities for
each agent of a cooperative pair (R) are greater than the average
utilities for a pair exploitative-exploiter ((T + S)/2).


Changing the rank order of the payoffs - the inequalities
REF- gives rise to different games. A general taxonomy
of 2 2 games (one-shot games involving two players with two actions
each) was constructed by Rapoport and Guyer REF. A
general 2 2 game is defined by a payoff matrix FORMULA with
payoffs not necessarily obeying the conditions REF or
REF REF.


FORMULA


The payoff matrix gives the payoffs for row actions when confronting
with column actions.


Apart from the PD game there are other some well studied games. For
instance, when the damage from mutual defection in the PD is increased
so that it finally exceeds the damage suffered by being exploited: FORMULA 
the new game is called the chicken game. Chicken is named after the
car racing game. Two cars drive towards each other for an apparent
head-on collision. Each player can swerve to avoid the crash
(cooperate) or keep going (defect). This game applies thus to
situations such that mutual defection is the worst possible outcome
(hence an unstable equilibrium).


When the reward of mutual cooperation in the chicken game is decreased
so that it finally drops below the losses from being exploited: FORMULA
it transforms into the leader game. The name of the game stems from
the following every day life situation: Two car drivers want to enter
a crowded one-way road from opposite sides, if a small gap occurs in
the line of the passing cars, it is preferable that one of them take
the lead and enter into the gap instead of that both wait until a
large gap occurs and allows both to enter simultaneously.


In fact, every payoff matrix, which at a first glance could seem
unreasonable from the point of view of selfish individuals, can be
applicable to describe real life situations in different realms or
contexts. Furthermore, &quot;unreasonable&quot; payoff matrices can be used by
minorities of individuals which depart from the &quot;normal&quot; ones (assumed
to be neutral) for instance, absolutely D individuals incapable of
realizing any value to cooperation or absolutely C &quot;altruistic&quot;
individuals (more on this later).


In one-shot or non repeated games, where each player has a dominant
strategy, as in the PD, then generally these strategies will be
chosen. The situation becomes more interesting when the games are
played repeatedly. In these iterated games players can modify their
behavior with time in order to maximize their utilities as they play
i.e. they can adopt different strategies. In order to escape from
the non-cooperative Nash equilibrium state of social dilemmas it is
generally assumed either memory of previous interactions
REF or features (&quot;tags&quot;) permitting cooperators and
defectors to distinguish one another REF; or
spatial structure is required REF.


Recently, it was proposed REF a simple model of
selfish agents without memory of past encounters, without tags and
with no spatial structure playing an arbitrary 2 2 game, defined by a
general payoff matrix like REF. At a given time t,
each of the Nag agents, numbered by an index i, has a
probability ci(t) of playing C (1 ci(t) of playing D).
Then a pair of agents are selected at random to play. All the players
use the same measure of success to evaluate if they did well or badly
in the game which is based on a comparison of their utilities U with
an estimate of the expected income ǫ and the arithmetic mean of
payoffs µ (R + S + T + P)/4. Next, they update their
ci(t) in consonance, i.e. a player keeps his ci(t) if he
did well or modifies it if he did badly.


Our long term goal is to study the quantum and statistical versions of
this model. That is, on one hand to compare the efficiency and
properties of quantum strategies vs. the classical ones for this model
in a spirit similar to that of ref. REF. On the
other hand, we are also interested in the effect of noise, for
instance by introducing a Metropolis Monte-Carlo temperature, and the
existence of power laws in the space of payoffs that parameterize the
game, of the type found in ref. REF and
REF, for a spatial structured version of this
model. Before embarking on the quantum or statistical mechanics of
this model, the objective in this paper is to complete the study of
the simplest non-spatial M-F version. In particular, to present an
analytic derivation of the equilibrium states for any payoff matrix
i.e. for an arbitrary 2 2 game using elemental calculus, both for
the deterministic and stochastic versions. In the first case the
calculation is elementary and serves as a guide to the more subtle
computation of the stochastic model. These equilibrium states into
which the systems self-organizes, which depend on the payoff matrix,
are of three types: &quot;universal cooperation&quot; or &quot;all C&quot;, of
intermediate level of cooperation and &quot;universal defection&quot; or &quot;all D&quot;
with, respectively, ceq = 1.0, 0 \&lt; ceq \&lt; 1.0 and 0.0. We
also consider the effect of mixing players using two different payoff
matrices. Specifically, a payoff matrix producing ceq=0.0 and the
canonical payoff matrix are used to simulate, respectively, absolutely
D or &quot;antisocial&quot; agents and &quot;normal&quot; agents.


II. THE MODEL


We consider two versions of the model introduced in ref.
REF. First, a deterministic version, in which the
agents are always in definite states either C or D i.e. &quot;black and
white&quot; agents without &quot;gray tones&quot;. Nevertheless, it is often remarked
that this is clearly an over-simplification of the behavior of
individuals.Indeed, their levels of cooperation exhibit a continuous
gamma of values. Furthermore, completely deterministic algorithms fail
to incorporate the stochastic component of human behavior. Thus, we
consider also a stochastic version, in which the agents only have
probabilities for playing C. In other words, the variable ci,
denoting the state or &quot;behavior&quot; of the agents, for the deterministic
case takes only two values: ci = 1 (C) or 0 (D) while for the
stochastic case ci is a real variable \[0, 1\].


The pairs of players are chosen randomly instead of being restricted
to some neighborhood. The implicit assumptions behind this are that the population is
sufficiently large and the system connectivity is high. In other
words, the agents display high mobility or they can experiment
interactions at a distance (for example electronic transactions,
etc.). This implies that Nag the number of agents needs to be
reasonably large. For instance, in the simulations presented in this
work the population of agents will be fixed to Nag = 1000.


The update rule for the ck of the agents is based on comparison of
their utilities with an estimate. The simplest estimate ǫk that
agent number k for his expected utilities in the game is provided by
the utilities he would made by playing with
himself REF, that is: FORMULA where ck is the probability that in the game the agent k plays C.
From equation REF we see that the estimate for C-agents
(ck = 1) ǫC and D-agents (ck = 0) ǫD are given by FORMULA.


The measure of success we consider here is slightly different from the
one considered in ref. REF: To measure his success
each player compares his profit Uk(t) with the maximum between
his estimate ǫk(t), given by REF, and the
arithmetic mean of the four payoffs given by FORMULA REF. If FORMULA the player assumes
he is doing well (badly) and he keeps (changes) his FORMULA as follows: if player k did well he assumes his
ck(t) is adequate and he keeps it. On the other hand, if he did badly he assumes his ck is inadequate and he changes it
(from C to D or from D to C in the deterministic version).


We are interested in measuring the average probability of cooperation
c vs. time, and in particular in its value of equilibrium ceq,
after a transient which is equivalent to the final fraction of
C-agents fC.


III. COMPUTATION OF THE EQUILIBRIUM STATES


A. Deterministic version


For the deterministic case the values of ceq are obtained by
elementary calculus as follows. Once equilibrium has been reached, the
transitions from D to C, on average, must equal those from C to D.
Thus, the average probability of cooperation ceq is obtained by
equalizing the flux from C to D, JCD, to the flux from D to C,
JDC. The players who play C either they get R (in \[C,C\]
encounters) or S (in \[C,D\] encounters), and their estimate is
ǫC = R; thus, according to the update rule, they change to D if
R \&lt; µ or S \&lt; max{R, µ} respectively. For a given average probability of
cooperation c, \[C,C\] encounters occur with probability c^2^
and \[C,D\] encounters with probability c(1 − c). Consequently, JCD can be written as: FORMULA with
FORMULA where θ(x) is the step function given by: FORMULA.


On the other hand, the players who play D either they get T (in
\[D,C\] encounters) or P (in \[D,D\] encounters) and their estimate
is ǫD = P ; thus, according to the update rule, they change to C
if T \&lt; max{µ, P } or P \&lt; µ respectively. As \[D,C\] encounters
occur with probability (1 − c)c and \[D,D\] encounters with probability (1 − c)^2^,
JCD can be written as: FORMULA. As there are 2 possibilities for each coefficient aXY, we have a
total of 2^4^ = 16 different equations governing all the possible
equilibrium states (actually there are 15 since this includes the
trivial equation 0 ≡ 0). The roots REF of these
equations are: FORMULA.


In addition, we have to take into account the case when: FORMULA.


In this case we can see from REF and
REF that JCD JDC identically, so we have that
peq co, (being the initial mean probability), whatever the initial conditions
are.


For instance, for the canonical payoff matrix we have aCC = 0 =
aDC and aCD = 1 = aDD, therefore we get FORMULA with the root ceq = 1/2 corresponding to the stable dynamic
equilibrium in which the agents change their state in such a way that,
on average, half of the transitions are from C to D and the other half
from D to C.


B. Stochastic version


In the case of a continuous probability of cooperation ck, the
calculation is a little bit more subtle: now the estimate ǫk for
the agent k is not only R or P, as it happened in the discrete case,
but it can take a continuum of values as the probability ck varies
in the interval \[0,1\]. From now on we will use the estimate as given
in REF, but instead of a ǫk as a function of time
we will use a generic ǫ that is a function of the []{#_bookmark12
.anchor}cooperation probability (and implicitly of time, off course),
that is: FORMULA.


So we have: FORMULA.
To calculate ceq we begin by writing a balance equation for the
probability ci(t). The agents will follow the same rule as
before: they will keep their state if they are doing well (in the
sense explained earlier) and otherwise they will change it. If two
agents i and j play at time t, with probabilities ci(t)
and cj(t) respectively, then the change in the probability
ci, provided he knows cj(t), would be given by: FORMULA, being θ the step function. 
The equation of evolution for cj(t)
is obtained by simply exchanging i j in equation
REF. Certainly, the assumption that each agent knows
the probability of cooperation of his opponent is not realistic.
Later, when we perform the simulations, we will introduce a procedure
to estimate the opponent&apos;s probability (more on this in Section V.b)


In REF if at time t the payoff obtained by agent
i, X (= R, S, T or P) is less than max ǫ^RST^ ^P^
(ci(t)), µ, the first two terms in the RHS decrease the
cooperation probability of agent i, while the two last terms
increase it. The terms give no contribution if the payoff X is
greater or equal than max{ǫ (ci(t)), µ}.


We will use the canonical payoff matrix M ^3051^ to illustrate how
the above equation of evolution for ci(t) works. In this case, the estimate function is, by
FORMULA, thus it is easy to see that: FORMULA. In addition we have for this case FORMULA.
We can then write, to a very good approximation (we are assuming that
the last line of REF is valid for FORMULA.


Defining the mean probability of cooperation as FORMULA, FORMULA
summing eq. REF over i and j leads to: FORMULA, within an error of O(1/Nag) since REF is valid
∀ i /= j but we are summing over all the Nag agents. Thereof
we can calculate the equilibrium mean probability of cooperation
ceq: FORMULA obtaining the two roots: FORMULA
being ceq = 1/2 the stable solution. Hence we obtain the same
result that in the deterministic case.


Using analog reasoning for the general case,
we can conclude that if FORMULA or FORMULA the results for the mean cooperation probability for the deterministic
version and the stochastic version, are the same.


There is an easy way to evaluate ǫ^RST^ ^P^ in practice. It can be
seen -see appendix - that FORMULA. When there is a payoff X such that FORMULA
things can change because agents who get X update in general their
probability of cooperation ci(t) differently depending whether
X \&lt; ǫ(ci) or X ǫ(ci). So as the probability takes
different values in the interval \[0, 1\], we have different
equations of evolution, which somehow &quot;compete&quot; against each other in
order to reach the equilibrium. The different equations that can
appear are off course restricted to the ones generated by the
coefficients aXY as they appear in REF. It is
reasonable to expect then that the final equilibrium value for the
mean probability will be somewhere in between the original equilibrium
values for the equations competing. We will analyze some particular
cases of this type in Section V.b to illustrate this point.


Although at first sight one may think that the universe of
possibilities fulfilling condition REF is very vast,
it happens that no more than three different balance equations can
coexist. This can be seen as follows: from eqs. REF
and REF, ǫ^RST^ ^P^ ≥ max{R, P }, and besides we
know that the estimate never could be greater than all the payoffs, so there is at least one X
such that ǫ^RST^ ^P^ \&lt; X. So this leaves
us with only two payoffs that effectively can be between µ and
ǫ^RST^ ^P^, and this results in at most three balance equations playing in a given game.


IV. AN EXAMPLE OF COEXISTENCE OF AGENTS USING DIFFERENT PAYOFF MATRICES: COOPERATION IN PRESENCE OF &quot;ALWAYS D&quot; AGENTS


Let us analyze now the situation where there are a mixing of agents
using two different payoff matrices, each leading by separate to a
different value of ceq. For simplicity we consider the
deterministic version but the results for the stochastic version are
similar. We call &quot;antisocial&quot; individuals those for whom cooperation
never pays and thus, although they can initially be in the C state,
after playing they turn to state D and remain forever in this state.
They can be represented by players using a payoff matrix that always
update ci to 0; for instance M^1053^. Notice that these
individuals are basically different from those which use a payoff
matrix fulfilling conditions REF and REF
who, even though they realize the value of cooperation i.e. R \&gt; P
and 2R \&gt; T + S, often may be tempted to &quot;free ride&quot; in order to
get a higher payoff. However, with the proposed mechanism -which
implies a sort of indirect reciprocitywhen D grows above 50 % it
punishes, on average, this behavior more than C favoring thus a net
flux from D to C. Conversely, if C grows above 50 % it punish, on
average, this behavior more than D favoring thus the opposite flux
from C to D. In other words, small oscillations around fC = 0.5
occur. On the other hand, agents using M ^1053^ are &quot;immune&quot; to the
former regulating mechanism. Let us analyze the effect they have on
cooperation when they &quot;contaminate&quot; a population of neutral agents
(using the canonical payoff matrix). In short, the two types of
individuals play different games (specified by different payoff
matrices) without knowing this fact, a situation which does not seem
too far from real life.


The asymptotic average probabilities of cooperation can be obtained by
simple algebra combining the update rules for M^3051^ and M^1053^. The
computation is completely analogous the one which leads to REF. We have to calculate JDC and JCD as a
function of the variable c and the parameter d and by equalizing them at equilibrium we get the equation for ceq. To
JDC only contribute the fraction (1-d) of normal players using
the canonical payoff matrix who play D against a player who also plays
D (normal or antisocial). That is, JDC is given by  FORMULA.


On the other hand, contributions to JCD come from one of these 3
types of encounters: i) \[C,D\] no matter if agents are neutral or
antisocial, ii) \[C,C\] of two antisocial agents and iii) \[C,C\] of a
neutral and antisocial agent (the neutral agent remains C and the
antisocial, who started at t = 0 playing C and has not played yet,
changes from C to D). The respective weights of these 3 contributions
are: FORMULA and FORMULA. Therefore, JCD is given
by FORMULA. In equilibrium JDC = JCD and the
following equation for ceq arises: and solving it: FORMULA.


We must take the roots with the &quot;-&quot; sign because those with &quot;+&quot; are
greater than 1 for non null values of d. We thus get the following
table for ceq for different values of the parameter d: TABLE.


V. SIMULATIONS


A. Deterministic version


In this subsection we present some results produced by simulation for
the deterministic version. Different payoff matrices were simulated
and it was found that the system self-organizes, after a transient, in
equilibrium states in total agreement with those calculated in
REF.


The update from ci(t) to ci(t + 1) was dictated by a
balance equation of the kind of REF. The measures are
performed over 1000 simulations each and ceq denotes the
average of ceq over these milliard of experiments. In order to
show the independence from the initial distribution of probabilities
of cooperation, Fig. 1 shows the evolution with time of the average
probability of cooperation for different initial proportions of
C-agents fC0 for the case of the canonical payoff matrix M^3051^
(i.e. R = 3, S = 0, T = 5 and P = 1).


FIGURE 


Depending on the payoff matrix the equilibrium asymptotic states can
be of three types: of &quot;all C&quot; (ceq = 1.0), &quot;all D&quot; (ceq
= 0.0) or something in between (0 \&lt; ceq \&lt; 1).


We have seen that the canonical payoff matrix M^3051^ provides an
example of matrix which gives FORMULA.


Let us see examples of payoff matrices which produce other values of
ceq. A payoff matrix which produces ceq = 1.0 is
obtained simply by permuting the canonical values of S (0) and T
(5), i.e. M ^3501^. For this matrix we have, by inspection of
REF and REF: aCC = aCD = 0 aDC = aDD = 1. (38)


Hence, after playing the PD game the pair of agents always ends
\[C,C\] since JCD 0 by REF.


On the other hand, a payoff matrix which leads ceq = 0.0 is
obtained simply by permuting the canonical values of R (3) and P
(1), i.e. M ^1053^, for which: FORMULA.


That is, all the changes are from C to D since in this case.
The rate of convergence to the possible values of ceq depends
on the values of JCD and JDC.


FIGURE


Finally, we simulated the mixing of agents using payoff matrices
M^3051^ and M^1053^. The evolution to equilibrium states for different
fixed fractions d of agents using M^1053^ is presented in Fig. 3.
The results are in complete agreement with the asymptotic
probabilities of cooperation which appear in Table 1.


B. Stochastic version


In this case simulations were made updating the probability of
cooperation according to eq. REF. However, as we
anticipated, we have to change slightly this eq. to reflect reality: two agents i and j interact and they obtain the payoffs Xi and
Xj, respectively. For each of them there is no way, from this only
event, to know the probability of cooperation ck of his opponent.
What they can do then is to (roughly) estimate this ck as follows.
The player i average utility in an encounter at time t with agent
j is given by: FORMULA. (When he plays he gets the payoff Xi, so his
best estimate c˜^i^ for the probability of agent j is obtained
by j replacing Uij (t) for Xi in eq. RE). Then
he will have: FORMULA. 


Exchanging i for j in this eq. gives the estimate of the
probability ci(t) that makes agent j. Equation
REF can retrieve any value of c˜^i^ (t) and not
just in the interval \[0, 1\], so it is necessary to make the
following j FORMULA.


FIGURE


When this happens, the agent is making the roughest approximation,
which is to assume that the other player acts like in the
deterministic case.


For the canonical payoff matrix, the result was the expected one as
this is a matrix obeying condition


REF: as predicted by the analytical calculation of
Section III.b, the value for the equilibrium mean probability is
ceq = 1/2 as in the deterministic case, despite the change
introduced in REF. Simulations for other payoff
matrices satisfying conditions REF or
REF were also made and in all the cases the
deterministic results were recovered.


We will illustrate the case in which some FORMULA with two particular examples. One of them is the case of the
normalized matrix M ^1S10^, with S varying from 1 to 2, both
limiting cases in which condition REF ceases to be
valid. So for S 1 the update equation is given simply by: FORMULA for which ceq = √5−1 is the corresponding equilibrium value. When
S (1, 2\], both balance equations play a
role, the general equation for the update follows from eq.
REF applied to this particular case: FORMULA.


So we can see that for R = T = 1 ǫ, eq. REF
reduces to REF while if R = T = 1 \&lt; ǫ we
obtain REF. When the simulation takes place, cj
has to be replaced by c˜^i^.


The same analysis can be done for the matrices M ^11T^ ^0^, with
T varying from 1 to 2 also. In this case the other root competing
with ceq = 1 is ceq = ^3−^√5.


The results of the simulations for both cases are presented in the
next table, data for S \&gt; 2 and T \&gt; 2 -for which REF is validis also included
REF: FORMULA.


As it can be seen from the data, for 1 \&lt; X 2, that is, when
condition REF is valid, the results for the
stochastic case are the same that they would be if we were working
with the deterministic model. This is a consequence of the estimate
REF together with conditions REF.


For values of T and S greater than 2, for which condition
REF does not hold any more, we can observe what at
first may seem a curiosity: for T or S near 2, the equilibrium
values for the deterministic case are recovered as expected, but as we
increase the values of T or S, the value of ceq = 1/2 is
approached. After a little thought, it is clear that this is also a
consecuence of the estimation of REF, since it
depends on the payoffs. It can be easily seen that in the case of FORMULA.
If we take then cj = 0 in eq. REF, and
remembering that T implies that µ, we will obtain that ceq =
1/2. In an analogous way for FORMULA, which toghether with eq. REF again leads to ceq =
1/2. The encounters for which Xi = S or T are responsible
for that the exact value ceq = 1/2 is not attained. A similar
analysis can be done when R or P → ∞.


VI. SUMMARY AND OUTLOOK


The proposed strategy, the combination of measure of success and
update rule, produces cooperation for a wide variety of payoff
matrices.


In particular, notice that: A cooperative regime arises for payoff matrices representing &quot;Social
Dilemmas&quot; like the canonical one. On the other hand spatial game
algorithms like the one of ref. REF produce
cooperative states (ceq \&gt; 0) in general for the case of a &quot;weak
dilemma&quot; in which P = S = 0 or at most when P is significantly
below R REF.


Payoff matrices with R = S = 0 which, at least in principle, one
would bet that favor D, actually generate equilibrium states with
ceq /= 0, provided that P \&lt; µ -see eqs.
REF-REF.


Any value of equilibrium average cooperation can be reached in
principle, even in the case of the deterministic model, by the
appropriate mixing of agents using 2 different payoff matrices. This
is an interesting result that goes beyond the different existent
social settings. For instance we have in mind situations in which one
wants to design a device or mechanism with a given value of ceq
that optimizes its performance.


In this work we adopted a Mean Field approach in which all the
spatial correlations between agents were neglected. One virtue of this
simplification is that it shows the model does not require that agents
interact only with those within some geographical proximity in order
to sustain cooperation. Playing with fixed neighbors is sometimes
considered as an important ingredient to successfully maintain the
cooperative regime REF,REF.
(Additionally, the equilibrium points can be obtained by simple
algebra.)


To conclude we mention different extensions and applications of this
model as possible future work. We mentioned, at the beginning,
&quot;statistical mechanic&quot; studies. For instance, by dividing the four
payoffs between say the reward R reduces the parameters to three: a = S/R, b = T/R and d = P/R, and we are interested to
analyze the dependence of ceq on each one of these 3 parameters in
the vicinity of a transition between two different values. It is also
interesting to introduce noise in the system, by means of an inverse
temperature parameter β, in order to allow irrational choices. The
player i changes his strategy with a probability Wi given by FORMULA.


We are planning also a quantum extension of the model in order to deal
with players which use superposition of strategies FORMULA instead of
definite strategies.


The study of the spatial structured version and how the different
agents lump together is also an interesting problem to consider.
Results on that topic will be presented elsewhere.


Finally, a test for the model against experimental data seems
interesting. In the case of humans the experiments suggest, for a
given class of games (i.e. a definite rank in the order of the
payoffs), a dependency of fc with the relative weights of R, S,
T and P, which is not observed in the present model. Therefore, we
should change the update rule in such a way to capture this Feature.
Work is also in progress in that direction.