Rethinking ChatGPT&apos;s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs&apos; Prompting


Introduction


ChatGPT has emerged as the most popular AI application, with a vast user
base. The success of GPT models can be attributed to the scaling of
transformer-based neural networks and the extensive pre-training data,
as explored in previous studies [*REF*; *REF*].
The scope of this paper is directed towards Large Language Models (LLMs)
that are sufficiently large to acquire world knowledge, commonsense, and
the linguistic capabilities required to attain high performance on
benchmarks such as GLUE [*REF*].


Although LLMs are commonly perceived as general-purpose language
intelligence models, the practice often diverges from employing a
singular, all-encompassing model for every task. Instead, the deployment
frequently entails developing a suite of specialized models tailored to
specific tasks. This specialization is facilitated through the
introduction of task-specific channels, modifying the model&apos;s structure
or its pre-trained parameters to better suit the nuances of individual
tasks. This highlights a departure from the ideal of a universal,
one-size-fits-all model, while the broad capabilities of LLMs suggest
they could serve as jack-of-all-trades in language processing. This
trend towards creating task-specific models may stem from the tradition
of evaluating linguistic intelligence through a variety of distinct
tasks and benchmarks [*REF*], with researchers striving to excel
in these tasks independently to set new benchmarks. In this paper, we
delve into the mechanisms behind prevalent deployment paradigms
including AR-LLMs&apos; prompting, which underpins ChatGPT&apos;s operation, and
highlight several critical observations: 1) Models tailored with
optimized task-specific channels often suffer from issues related to
task customizability, transparency, and user-level complexity during
deployment, affecting their overall usability; 2) Anticipated to mimic
human-like intelligence, they often exhibit slow thinking through
shortcuts [*REF*]; 3) They frequently fall short in
showcasing advanced cognitive behaviors, which we contend are vital for
convincing users of the models&apos; intelligence. Conversely, AR-LLMs&apos;
prompting paradigms introduce a more natural, human-like channel (verbal
free-form context) for representing a wide array of real-life tasks and
employ form-form output modalities to showcase cognitive behaviors in
complex scenarios.


Specifically, in this paper, we commence by examining the foundational
principles of language modeling, revisiting the notable split in
language modeling approaches that emerged in the late 2010sauto-encoding LMs 
(AE-LMs) exemplified by BERT [*REF*] and
auto-regressive LMs (AR-LMs) exemplified by the GPT series
[*REF*; *REF*]. Rather than delve into an
extensive array of deployment paradigms, we introduce and discuss the
concepts of modalities and channels to investigate how LLMs are deployed
(§[2]). Upon evaluating different deployment
paradigms for LLMs, it becomes clear that aside from the AR-LLMs&apos;
prompting approach, other paradigms struggle to demonstrate advanced
human-like cognitive behaviors. This shortfall is attributed to the
constraints within modalities and channels, coupled with a tendency
towards superficial learning, i.e., slow thinking
(§[3] and §[4.1]). In contrast, via specified context in
the free-from text, the AR-LLMs&apos; prompting strategy imitate human-like
cognitive behaviors, such as reasoning, planning, and feedback learning,
which are elucidated in Table [1] (§[4]). Finally, we explore how understanding
cognitive behaviors can help overcome the tuning and deployment
obstacles encountered by LLMs functioning as autonomous entities and
within multi-agent frameworks (§[5]).


Deploying Large Language Models 


This section elucidates the dual objectives underlying language models,
which both aim to model the joint probability distribution of text
sequences through self-supervised learning techniques and generate text
that is relevant to the given context. After this introduction, we
present a novel framework that facilitate the characterization of
various deployment paradigms through two types of data modalities, which
support language comprehension, coupled with six unique channels for
processing these modalities.


The Fundamental Dichotomy in Language Modeling 


Objective of Language Modeling


The goal of language modeling is to estimate the joint probability
distribution of sequences of text [*REF*]. This
involves developing two distinct yet relaxed formulations for
constructing LLMs that leverage self-supervised learning from vast
quantities of unlabeled text data. The self-supervised approach enables
the training of LLMs on extensive text corpora, a practice that has been
thoroughly investigated in various studies
[*REF*; *REF*]. This paper focuses on how the
intrinsic design of language models impacts their usability and
potential to express cognitive behaviors.


Auto-Regressive (Left-to-Right) Language Modeling


Typically, language modeling is approached by predicting the subsequent
token in a sequence based on the preceding tokens. This prediction is
quantified as the product of conditional probabilities for each
subsequent token, considering its previous tokens, in accordance with
the chain rule [*REF*]. *MATH*. Here, *MATH* serves as a marker for the beginning of text.


Auto-Encoding (Denoising) Language Modeling


In the context of auto-encoding language modeling, noise is
intentionally introduced to an input sequence *MATH*. The
primary aim is to optimize *MATH* represents the altered,
noise-added version of the input sequence. The approach of masking
specific tokens in the text at random, known as token-level masked
language modeling [*REF*], is a widely adopted strategy. This
involves substituting original tokens with a special token, such as
&quot;\[MASK\]&quot;, and training the model to predict these original tokens
based on the context of the surrounding, unmasked tokens. The
discrepancy between the original and reconstructed sequences is
quantified through a reconstruction loss: *MATH*.
This denoising methodology also includes other variants such as
span-level masked language modeling [*REF*], text
infilling [*REF*], among others.


Exploring the Modalities within Large Language Models


This section delves into the concept of &quot;modalities&quot; within LLMs, a term
often implicitly associated with research on multimodal systems to
describe diverse, human-like channels of communication, such as text,
speech, gestures, and visual inputs [*REF*]. Here,
&quot;modalities&quot; specifically refer to the various forms of input and output
data utilized in LLM deployment.


In the operation of both AR-LLMs and AE-LLMs, we identify three primary
modalities: a unique textual modality for both the input and output in
AR-LLMs (unrestricted text), a distinct textual modality for AE-LLMs
(masked text or contextualized n-grams), and a shared modality of
intermediate dense representations applicable to both models: 1)
Intermediate Dense Representations: Fundamentally, LLMs convert each
word (or subword) in a sequence into dense vector embeddings. These
embeddings are generated through a series of mathematical operations,
such as the self-attention mechanism, at every layer of the neural
network, and are represented as *MATH* for every
position *MATH* within the sequence and for every layer *MATH* in the model.
Here, *MATH* ranges from *MATH* to *MATH*, with *MATH* indicating the total number
of elements in the sequence, and *MATH* spans from *MATH* to *MATH*, where *MATH* 
represents the complete count of layers within the model. 2) Textual
Modalities: AE-LLMs feature an input modality of masked text, with the
output modality being contextualized n-grams designed to reconstruct the
masked sections. Conversely, due to their auto-regressive design,
AR-LLMs are capable of encoding any text as context and generating
free-form text outputs, thereby employing unrestricted text for both
input and output. These modalities are inherently linked to their
respective language modeling strategies.


TABLE


Task-specific Channels for Deployment


To tailor the core capabilities of LLMs for specific downstream tasks,
both input and intermediate modalities can be altered directly (for
instance, by appending prefixes or incorporating verbal context) or
indirectly through the use of parametric modules such as neural
networks, including adapters and output layers as described
subsequently. It&apos;s worth noting that direct modifications, such as
prefixes, can also be achieved using parametric modules. These
parametric modules undergo optimization via task-specific supervised
learning. In this context, we describe the means for modality
transformation aimed at specific tasks as task-specific channels. For
clarify, modalities are the types of data or the form in which data is
processed, while channels are the pathways or methods through which
these data modalities are adapted or transformed for specific tasks.
Task-specific channels encompass: 1) Adapter: Adapters are compact
neural networks that can be embedded between an LLM&apos;s layers. A
well-known approach, adapter tuning [*REF*], involves
optimizing the adapter&apos;s parameters while leaving the original LLM
parameters intact. These adapters are designed to adjust the
intermediate layer representations to better align with task-specific
needs. 2) LLMs Themselves: An alternative strategy involves
modifying the LLM directly to produce task-specific representations by
fine-tuning the model&apos;s weights across all or selected layers. This
method of fine-tuning is prevalent for AE-LLMs [*REF*] and has
also been applied to AR-LLMs in early use of GPT-like models
[*REF*]. 3) Output Layers: Once task-specific
representations are produced by either adapters or the LLM directly, the
function of the output layers is to translate these representations into
a designated output space. These layers typically consist of one or
several linear layers. For example, linear functions are frequently used
for tasks involving classification, while tasks that involve extractive
question answering often necessitate the use of two linear functions to
determine the beginning and concluding positions of the answer within a
text passage. 4) Activation Prefixes: Within the scope of deploying
LLMs via task-specific supervised learning, where training neural
networks is common, prefix tuning [*REF*] presents an
innovative method that employs prefixes to directly modify intermediate
representations. These prefixes are essentially embeddings that are
added at various layers, with dimensions identical to those of token
embeddings, functioning as virtual tokens. Introducing these prefixes at
earlier stages in the model allows for the infusion of task-specific
information into more advanced layers, thereby improving the model&apos;s
alignment with the desired task objectives.


Beyond the four channels previously outlined, verbal channels offer a
unique approach for articulating the task context in which LLMs can
identify and execute the intended tasks. These channels include: 5)
Verbal Free-form Context: In this approach, a context is articulated
using free-form text, such as task instructions and few-shot
demonstrations, which can activate complex cognitive functions. By
merely incorporating task instructions within the context, AR-LLMs are
enabled to undertake a multitude of tasks through zero-shot prompts.
Another widely adopted method is few-shot prompting
[*REF*; *REF*], which involves learning from a
limited number of examples for in-context learning without the need for
gradient updates, showcasing a human-like efficiency in acquiring new
tasks. This method is particularly effective in eliciting cognitive
behaviors akin to those observed with few-shot demonstrations, with
further details discussed in Section [4]. It&apos;s important to recognize that, in
contrast to channels that are easily differentiated by input-side
modalities (such as task-specific examples), this channel (e.g., task
instructions) can intertwine with model inputs, e.g., task-specific
examples. This allows for the seamless integration of the models&apos; world
knowledge into tasks, for instance, &quot;summarize deep learning
technology&quot;. 6) Contextual Text Patterns: Given their training on a
denoising language model objective, AE-LLMs excel in completing texts by
filling in missing words, a trait that can be leveraged for downstream
tasks. Task-specific patterns, in this regard, serve as a mechanism to
alter given task-specific examples. Typically, this involves appending
the examples with a cloze-style phrase or sentence (text with missing
words) tailored to the task, allowing the model to predict the intended
task outcomes based on the placeholders filled within the text. Pattern
Exploitation Training (PET) [*REF*] involves
the creative design of task-specific patterns and the fine-tuning of
LLMs to these patterns. Conversely, auto-prompt methods
[*REF*] seek to optimize task-specific patterns to
better fit the models, enhancing their ability to interpret and respond
to the given tasks effectively.


Evaluation of Modalities and Channels 


Evaluating Usability of Deployment Channels


This section introduces a framework for assessing the usability of
language model deployment channels, focusing on their customizability,
transparency, and complexity, as summarized in Table.


Customizability of User-level Tasks: Extent of User Control over Channels


Essentially, any task can be articulated in human languages, such as
English, using free-form context. This adaptability is a testament to
the evolution of human language over thousands of years, which has been
refined to describe a vast array of everyday and complex scientific
problems. Typically, in a zero-shot learning context, the channel
consists solely of task instructions within the prompts, capable of
encompassing a wide range of tasks. For instance, Wang et al.,
*REF* -etal-2022-super have converted standard NLP datasets designed for
optimized channels into instruction-based formats for 76 different
tasks. Moreover, free-form task instructions allow for nuanced control
mechanisms, including explicit directives (such as specifying output
formats or initiating reasoning processes) and subtle cues (such as
inducing cognitive behaviors through few-shot examples). These aspects
will be further explored in Section [4] and summarized in Table
[1]. In contrast, since other channels are
set during the optimization process for specific tasks, they lack the
flexibility for user-directed modifications. Channels that require
adjustments, such as fine-tuning the LLM, adapter tuning, or prefix
tuning, rely on supervised learning methods for configuration. Although
prompting in AE-LLMs could, in theory, facilitate task adjustments at
inference time without prior task-specific fine-tuning --- akin to
AR-LLMs&apos; prompting approach --- it often requires task-specific
optimization to achieve effective channel performance. For example,
techniques like Pattern Exploitation Training (PET)
[*REF*] utilize mathematical optimization to
adapt models to specific patterns, whereas Auto-prompt
[*REF*] optimizes text patterns for language
models. The question of whether this need for optimization arises from
the inherent complexities of auto-encoding language models invites
further research.


User-level Transparency: Can Channel Formulation Be Easily Understood by Users?


The focus here is on the understandability of the channels themselves to
lay users, rather than their functional effectiveness, as this greatly
influences the user experience. For example, the objective of an output
layer is clear  ---  transforming LLM representations into a specific
output format. However, the process involving dense representations
through matrix multiplication is not intuitively understandable to the
non-specialist. Moreover, text patterns refined through AE-LLMs&apos;
Auto-prompting often lack the straightforwardness found in manually
created prompts.


User-level Complexity: Assessing the Number of Conceptual Components


This analysis evaluates the conceptual load required to deploy *MATH* tasks
using various channels, moving away from the parameter size metric,
which is more pertinent to researchers and developers. Assuming each
task is accommodable across all channels, we quantify the complexity as
follows: For fine-tuned LLMs, prefixes, adapters and output layers, each
task-specific adjustment equates to a complexity of *MATH*, with *MATH* 
denoting the total number of tasks. Additionally, *MATH* text patterns are
devised per task, resulting in a complexity of *MATH*, where *MATH* 
represents the number of patterns per task. The complexity for verbal
free-form context is considered negligible, as these are formulated
spontaneously by users at the time of use. From this framework, we can
deduce the complexity inherent to each deployment paradigm. For
instance, LLM fine-tuning, which necessitates one LLM and one output
layer per task, carries a complexity of *MATH*.


Evaluating Expressiveness of Modalities


During LLM fine-tuning and adapter tuning, the task-specific output
layers strictly limit the range of possible outputs, hindering the
potential for detailed expressiveness and, by extension, advanced
cognitive behaviors. The output space is tightly defined, with actions
or labels being pre-determined and given specific meanings through
task-specific supervised learning. Nonetheless, certain probing
techniques allow us to uncover the thought processes behind their
predictions, a topic we will explore further in Section [4.1]. When it comes to AE-LLMs prompted with
text patterns, these models are limited to generating only specific
tokens or words, constrained by the patterns set in advance. These
constraints, such as token positions and quantities dictated by the
input patterns, along with the need for grammatical and coherent text
completion, restrict the models&apos; ability to articulate complex ideas,
plans, and actions. On the other hand, AR-LLMs&apos; prompting capitalizes on
their auto-regressive nature to produce unbounded, free-form text,
influenced solely by the given input context. This capability is further
demonstrated in Section [4] and summarized in Table
[1], showcasing the open-ended expressiveness unique to the AR-LLM prompting paradigm.


Cognitive Behaviors Under AR-LLMs&apos; Prompting Paradigm 


This section elucidates the capability of AR-LLM prompting paradigms to
exhibit cognitive behaviors expressed by the free-form modalities by
mainpulating the free-form channels. It&apos;s important to clarify that not
every AR-LLM demonstrates cognitive behaviors --- smaller models like
GPT-2 [*REF*] may not. Specifically, we analyze four
cognitive behaviors: thinking, reasoning, planning, and feedback
learning, leaving the examination of their interrelationships for future
research.


Thinking, Fast And Slow 


At the core of cognitive behavior lies thinking. The Kahneman&apos;s
framework [*REF*] divides thinking into two distinct
systems: the fast system operates through intuitive shortcuts for quick
navigation of daily situations without extensive analysis. Conversely,
the slow system, or System 2, involves conscious, detailed and
methodical examination of information, necessitating logical
deliberation to arrive at decisions and address challenges.


Fast Thinking via Task-specific Channels


Using channels trained through task-specific supervised learning can
achieve performances that rival or exceed human performance.
Nonetheless, they often struggle with generalizing to data from natural
domain shifts, adversarial perturbations and debiased data, as
summarized by Li et al., *REF*. This limitation is
consistently attributed to shortcut learning, such as classifying
sentences containing the word &quot;No&quot; as &quot;contradiction&quot; in text entailment
tasks [*REF*; *REF*]. The intriguing
question arises whether task-specific channels can also develop System 2
 ---  the fast system. While the limited expressiveness of task-specific
outputs does not offer straightforward evidence, Li et al.,
*REF* employ a technical probe [*REF*]
to reveal that indulgence in shortcut learning during task-specific
training impedes the development of the slow system. While the mentioned
research primarily examines the LLM fine-tuning paradigm, it&apos;s our
contention that shortcut learning and the fast thinking are likely
prevalent across all the parametric channels, including prefixes and
adapters, trained on supervised datasets to some degree. This is
attributed to the inherent characteristics of gradient descent
optimization, as demonstrated by empirical findings in Li et al.,
*REF*. Another empirical evidence shows that methods like
prefix and adapter tuning, although more resilient, still notably falter
under distribution shifts and adversarial attacks
[*REF*; *REF*]. The mitigated impact observed in
prefix and adapter tuning is attributed to the fact that the underlying
LLMs are not directly engaged as task-specific channels, as explored by
[*REF*]. While we draw parallels between reliance on
shortcuts and fast thinking within human cognition, some research within
the NLP field argues that such dependency on shortcuts (dataset biases)
detracts from the models&apos; relevance to human-level cognition
[*REF*]. This perspective arises from the view that the
shortcuts might not reflect genuine human cognitive activities within
the field of NLP.


Minimal Fast Thinking Evident with AR-LLMs Prompting


Research findings [*REF*; *REF*]
consistently indicate the difficulty of inducing fast thinking in
AR-LLMs through prompting techniques. These models typically remain
unfazed by various distributional shifts, such as domain shift and
adversarial perturbations. Min et al., *REF* demonstrate that, 
even with few-shot demonstrations for in-context
learning, the models tend to leverage the structure of these
demonstrations to organize the generation rather than relying on
simplistic input-to-label mappings for predictions. Additionally, Raman
et al., *REF* show that PET prompting improve the
AE-LLMs&apos; ability to withstand adversarial attacks. Nonetheless, this
enhanced robustness is somewhat restricted. The constrained
effectiveness could be attributed to the dependency on task-specific
channels inherent during the deployment of the PET prompting.


Slow Thinking in Prompting Paradigms


The remainder of this section will illustrate the capacity of AR-LLMs&apos;
prompting to replicate the human slow thinking process through the
exhibition of effortful mental activities, as encapsulated in Table [1].


Reasoning


Reasoning is a thinking process to conclusions or decisions with the
sequential and interconnected nature, i.e., chain-of-thoughts (CoTs)
[*REF*]. This is the most common definition in the NLP/LLM are
to investigate the LLMs&apos; reasoning ability. With a reasoning path in
free-form modality, models can better solve complicated tasks requiring
multi-step reasoning compared to the conclusion without CoTs. As an
illustration, Wei et al., *REF* substantially boosts model
efficacy in solving mathematical reasoning bechmarks.


Reasoning is defined as the process of arriving at conclusions or
decisions through a sequential and interconnected series of thoughts,
often referred to as a chain-of-thoughts (CoTs) [*REF*]. This
definition is widely accepted in the field of Natural Language
Processing (NLP) for exploring the reasoning capabilities of LLMs. By
employing a reasoning path via the modality of free-form text, models
are more adept at tackling complex tasks that necessitate multi-step
reasoning, as opposed to reaching conclusions without the aid of CoTs.
Technically, the auto-regressive nature employs the thoughts or
intermediate steps generated as the prior for generating subsequent
thoughts and, ultimately, the final predictions.


Context for Eliciting Reasoning


Two primary contexts are employed to facilitate the creation of
intermediate reasoning steps: incorporating a Chain of Thought (CoT)
triggers in task instructions (zero-shot CoTs), such as &quot;Let&apos;s think
step-by-step&quot; [*REF*], within prompts, or integrating
manually crafted reasoning steps in a few-shot learning context
(few-shot CoTs) [*REF*]. To circumvent the manual
compilation of few-shot demonstrations with reasoning sequences, Zhang
et al., *REF* developed a method to automatically generate
few-shot demonstrations by choosing several queries and utilizing
zero-shot CoTs to craft reasoning sequences for each query (Auto
CoTs). Given that simple greedy decoding (producing a single chain) is
prone to error accumulation in intermediate steps, Wang et al.,
*REF* propose generating multiple chains and
consolidating them through majority voting, thereby enhancing model
accuracy in both scenarios (CoTs-SC).


Planning


Planning involves the forethought and organization of actions or steps
to achieve a predetermined objective. This process fundamentally
requires a comprehension or representation of the environment and
involves breaking down tasks into smaller, manageable subgoals. It
represents a key cognitive behavior modeled within the fields of AI.
Typical planning methods break down tasks into subgoals through explicit
symbolic representation [*REF*]. For instance,
partial-order planning ensures the logical sequencing of actions by
modeling actions, preconditions, effects, and the relations among
actions in such a way that actions are logically sequenced to meet the
goal&apos;s preconditions. Differing from traditional approaches that rely on
explicitly modeled knowledge and reasoning mechanisms, LLMs leverage
their inherent knowledge and inferential capabilities to mimic planning.
They do this by producing text sequences that suggest a logical
progression of steps or actions directed towards an objective
[*REF*; *REF*; *REF*].
This skill stems from the models&apos; proficiency in forecasting the
subsequent most likely word sequence based on a context indicative of
planning or reasoning processes.


Context to Elicit Plans


Similar to the activation of reasoning processes, the process of
planning can be prompted through the inclusion of specific planning cues
in zero-shot scenarios, such as the prompt &quot;let&apos;s carry out the plan&quot;
[*REF*], or through the demonstration of planning steps
in few-shot examples [*REF*]. Experimental findings
indicate that instructions tailored to tasks significantly enhance the
performance of LLMs on various tasks. For instance, directives like &quot;pay
attention to calculation&quot; [*REF*] or &quot;identify key
variables and their corresponding figures to formulate a plan&quot;
[*REF*] have been shown to improve outcomes in tasks
requiring numerical reasoning.


Applying Planning for Sequential Decision-making


This ability is essential for addressing problems requiring a series of
decisions, especially when deploying LLMs in open-world scenarios like
robotics. In such environments, tasks typically need physical actions
(grounded), involve translating broad objectives into actionable steps
(high-level), and present a vast range of possible actions (open-ended).
Research has demonstrated the effectiveness of LLMs in deconstructing
complex goals into actionable sequences within such dynamic
environments, as seen in projects like ALFWorld [*REF*],
VirtualHome [*REF*], and Minecraft [*REF*]. An
example from ALFWorld illustrates this: achieving the objective of
&quot;examining paper under desklamp&quot; necessitates LLMs to devise practical
plans (e.g., initially approaching the coffee table, then acquiring the
paper and utilizing the desklamp) and subsequently generate textual
instructions for execution in real-world settings.


Feedback Learning


As Kahneman et al., *REF* elucidates, although System 1
may rush to judgments that are biased or erroneous, System 2 has the
capacity to identify and rectify these mistakes through introspection on
the rapid decisions made by System 1. Similarly, LLMs have shown the
ability to mimic this aspect of human cognition.


Feedback Generation and Contextual Basis


LLMs are adept at generating feedback by reflecting on their previously
given responses or observations from interactions with the external
environment. There are two primary scenarios for feedback generation: 1)
Feedback based on previous actions and external feedback: In the work by
Yao et al., *REF*, LLMs engaging with a Wikipedia API to search
for entities that do not exist, such as &quot;Search\[goddess frigg\]&quot;, may
encounter a 404 error, delivered in JSON format. In response, LLMs can
articulate feedback about the error related to their action, such as
stating, &quot;Could not find goddess frigg.&quot;. 2) Feedback based solely on
prior responses: This approach is relevant in various situations where
external environmental feedback is absent
[*REF*; *REF*]. In such cases, LLMs can
give feedback on previous answers by applying certain evaluation
metrics, such as determining the relevance of a sub-question to a
broader question requiring intricate, multi-step reasoning
[*REF*]. This process involves using a prompt that
asks the LLM to judge the utility of a sub-question in addressing the
main question (Prompt: &quot;Given a question, assess if the subquestion aids
in solving the original question. Answer &apos;Yes&apos; or &apos;No&apos;. Question; Subquestion:. Is the subquestion useful?&quot;).
Furthermore, feedback may be presented as numerical scores, such as the
confidence scores (normalized logits) for &apos;Yes&apos; or &apos;No&apos; answers, instead
of in verbal form. The decision to use numerical rather than verbal
feedback is contingent on the specific requirements of the feedback
mechanism, as explored in subsequent discussions.


Mechanism for Feedback Learning


After generating feedback, Shinn et al., *REF* directly
include verbal feedback LLMs have produced to enhance the accuracy of
their responses or decisions. Meanwhile, Hao et al.,
*REF* detail a methodology where numerical feedback
serves as a reward system for guiding LLMs for action selection.
Following this, the LLMs function as world models to predict the
subsequent state of state-action pairs during the planning phase,
utilizing Monte-Carlo Tree Search (MCTS) to achieve this aim. Instead of
allowing LLMs to directly process the feedback, an implicit feedback
learning strategy is employed where a feedback loop is deliberately
established to influence the sequence of actions undertaken by the LLMs
via the update of state values during the propagation phase of MCTS.


TABLE


Cognitive behaviors enabled by free-form context. For the &quot;Feedback
Learning&quot; sections, we illustrate the contexts utilized to produce
feedback. It&apos;s worth noting that the methods for feedback adaptation
might not always employ free-form context; for instance, they may
involve advanced search techniques as outlined in our study. The final
column presents examples of tasks for demonstration purposes, though
the list is not comprehensive.



Bridging LLM Deployment Gaps with Insights from Cognitive Behaviors 


This section investigates how insights into cognitive behaviors can aid
in addressing the tuning and deployment challenges faced by LLMs
operating as autonomous agents and within multi-agent systems.


Autonomous Cognitive Behaviors


Instead of relying on explicit contextual cues to trigger advanced
cognitive functions, an intelligent system is expected to independently
engage in reasoning, planning, and decision-making as it interacts with
the external world --- for instance, by seeking input from humans or
utilizing available tools. To foster such autonomous behaviors, various
algorithms aim to tune LLMs for independently exhibiting behaviors that
align with human cognitive processes. For instance, Liu et al.,
*REF* have developed techniques for instruction tuning that
facilitates autonomous reasoning. Yet, the challenge remains in creating
instructional data that encapsulates higher-order cognitive functions. A
pivotal question emerges: How can various cognitive behaviors be
encapsulated within free-form text (instruction data)? Addressing this
question is crucial for ensuring that the data used for tuning mirrors
human cognitive processes, thereby making the resulting model actions
more human-like. Unraveling this issue might necessitate insights from
both cognitive psychology and linguistics. Another approach to tuning
involves the use of reliable reward models, such as reinforcement
learning from human feedback (RLHF) [*REF*] and behavior
cloning [*REF*]. Many studies [*REF*; *REF*] develop reward models based on
comparisons of model-generated responses, with human evaluators ranking
these responses. An unresolved inquiry remains: How can reward models
be devised to truly reflect human cognitive preferences?


Navigating Free-form Contexts in Multi-turn Interactions Within Multi-agent Systems


In exploring planning and feedback learning strategies, it becomes
evident that multi-agent systems are designed to facilitate interactions
between LLMs and the external world, as well as among LLMs themselves.
Examples include LLMs acting as both evaluators and actors in feedback
learning environments [*REF*] or taking on roles as
evaluators, actors, and environmental simulators in planning scenarios
[*REF*]. Such setups necessitate more than a one-time
interaction between LLMs and users, requiring instead sustained,
multi-turn dialogues. Challenges emerge within these complex
interactions: How do agents process and integrate information from
other agents and their own previous dialogues? How can environmental
data be stringified into a format understandable by LLMs? What
strategies can simplify the management of extended sequences of
interaction trajectories?


Advancing Towards A Unified Inference Framework for Multi-agent Systems


Despite the recent development of various LLM-based multi-agent systems,
there remains an absence of a unified framework across these models.
Exploring such framework serves as a key drive for this research on a
cognitive framework. Organizing LLM-based agents by their cognitive
behaviors offers a pathway to this unification. For instance, as
indicated in Table [1], the ReAct framework [*REF*]
employs a reasoning agent for decision-making, whereas the RAP framework
[*REF*] utilizes a reasoning agent for
decision-making alongside a feedback learning agent for evaluation.
Delving deeper into the relationships between cognitive behaviors might
benefit from insights in cognitive psychology. Taking the concept of
Self-Regulated Learning (SRL) as defined by *REF*,
planning and feedback learning are intertwined, enhancing the learning
process.


&quot;Self-regulated learning refers to self-generated thoughts, feelings,
and actions that are planned and cyclically adapted to the attainment of
personal goals&quot;


This rationale lays the groundwork for future efforts to combine
planning frameworks (like RAP) with feedback learning frameworks (such
as Reflexion). Furthermore, the structure of certain inference models
showcases their interconnections and the presence of common detailed
components. For instance, both feedback learning frameworks, exemplified
by Reflexion *REF*, and planning frameworks, such as RAP
*REF*, incorporate a common detailed module: a
feedback learning agent. This agent plays a pivotal role in
decision-making by facilitating the selection of appropriate actions
from an assortment of tools and environments. It is evident that the
interconnection within the intricate inference framework can be
examined, especially at the level of cognitive behavior. Our
comprehensive analysis aims to pave the way for the systematic creation
of multi-agent LLM-based frameworks, or inversely, to stimulate research
in cognitive psychology.


Conclusion


In summary, our analysis seeks to inspire further research in AI, within
the domain of language intelligence and beyond, to move away from
heavily optimized task-specific channels. Instead, we advocate for the
adoption of natural and free-form modalities throughout the pretraining
phase via self-supervised learning, followed by straightforward
inference-time deployment that eschews the necessity for mathematically
optimizing task-specific channels. We developed an analytical framework
to examine the deployment of LLMs to reach the conclusion. Besides, the
auto-regressive nature of free-form modalities, leveraged during
pretraining, enhances the capacity for exhibiting a range of human-like
cognitive behaviors by utilizing the free-form channel. It is important
to clarify that we do not advocate that LLMs possess conscious thought.
Rather, our findings illustrate how LLMs, such as ChatGPT, can imitate
the outcomes of human cognitive activities via the free-form modality
given suitable verbal context. Lastly, we highlight the opportunity to
address challenges in LLM deployment through the integration of
cognitive behavior concepts.