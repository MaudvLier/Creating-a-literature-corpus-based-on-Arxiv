Agents meet OKR: An Object and Key Results Driven Agent System with Hierarchical Self-Collaboration and Self-Evaluation


Introduction


The widespread application of Large Language Models (LLMs) has elicited transformative advancements in various sectors. However, the intricate potential of LLMs remains underexplored, especially in tasks of high complexity [*REF*; *REF*], such as curating movie scenes or designing sophisticated travel plans, where LLMs face challenges related to knowledge and reasoning intensity, due to issues such as hallucination [*REF*; *REF*] and lack of slow thinking [*REF*; *REF*].


In this study, we explore two crucial dimensions of utilizing LLMs for intricate tasks: enhancing self-collaboration and enabling self-evaluation. Our investigation is motivated by two key observations.
Firstly, existing studies such as CoT [*REF*] and ToT [*REF*] reveal that introducing intermediate steps and adopting a multi-persona approach can markedly enhance the performance of LLMs. Nevertheless, these methodologies necessitate manual workflow configuration and agent assignment. The study of SPP [*REF*] demonstrates the potential of LLMs to dynamically allocate agents, based on task inputs and user-specified requirements, and to produce rational outputs. This observation has led us to explore deeper into the capability of LLMs to autonomously decompose input tasks into meaningful goals and to formulate guidelines for agent collaboration from varied perspectives. Secondly, the inherent multiplicity of potential solutions in any collaborative undertaking necessitates the critical ability to discern, assess, and choose the most apt solutions [*REF*]. Hence, we aim to unearth a mechanism that enables LLMs to effectively evaluate and pinpoint the most promising solutions from a myriad of generated possibilities.


FIGURE


Inspired by the renowned success of the Objectives and Key Results (OKR) system in guiding large corporations and organizations towards achievement, we have developed a unique goal-setting framework within our Large Language Model (LLM) task-solving pipeline, namely OKR-Agent.
Specifically, OKR-Agent works in a hierarchical and self-collaboration manner. Given a concise description of the task, a primary agent undertakes the initial analysis and generates a spectrum of potential objectives, proceeding to select the most apt ones. Subsequently, this leading agent incorporates additional agents based on the finalized objectives, assigning to each the corresponding key results necessary for fulfillment. It is imperative to note that this process of agent assignment is comprehensive, encompassing both the delineation of roles for each agent and the establishment of inter-agent dependencies throughout the workflow. These newly incorporated agents possess the capability to further break down the key results into subordinate objectives and key results, enabling a substantial enlargement of the agent roster. This iterative and multi-level approach ensures that each layer of the task has a dedicated focus, fostering a nuanced and thorough exploration of potential solutions and strategies.


Another core component of OKR-Agent is multi-level self-evaluation. As illustrated in recent works [*REF*; *REF*], assimilating evaluations from varied personas and perspectives substantially enhances the accuracy and quality of outputs generated by LLM agents. However, prevailing methodologies solicit feedback from agents based solely on their designated roles, often neglecting a holistic overview of the content. We postulate that an effective agent evaluation should not only be reflexive but also encompass assessments from agents in close relational proximity, offering a more rounded perspective. With the architectural design of OKR-Agent, each agent is cognizant of its relative position within the workflow, enabling it to furnish evaluations that encompass all correlated perspectives and make modifications accordingly. Moreover, proficient evaluations at both strategic and executional levels are crucial to guarantee the efficacy of the solution. The self-evaluation works in a coarse-to-fine manner, where top-level agents scrutinize overarching strategies, subsequently transitioning to lower-level agents who meticulously attend to execution details.


We further validate the efficacy of OKR-Agent through experiments encompassing three diverse tasks: short video storyboard generation, multi-day trip planning, and trivia creative writing. Our empirical results demonstrate that OKR-Agent surpasses preceding LLM-based task-solving methodologies, exhibiting superior performance in both overarching planning and the intricacy of details, presenting a consistent enhancement across diverse domains.


To summarize, our main contributions are as below:


- We introduce a new hierarchical and self-collaborative approach for task solving. It analyzes and decomposes tasks into distinct objectives and assigns key results to various agents, based on their roles and the workflow&apos;s relative positions, enabling a more structured and coherent approach to task execution.
- We propose a novel multi-level self-evaluation mechanism, allowing each agent to offer evaluations from all related perspectives. This feature not only refines the accuracy and quality of the outputs by incorporating various assessments and feedback but also ensures that the evaluations are comprehensive, covering both strategic and executional levels.
- OKR-Agent has demonstrated its supremacy over existing LLM-based task-solving models on on diverse tasks. It has shown consistent enhancements in both overall planning and detail execution, making it a robust solution for complex task-solving scenarios.


Related work


Prompting Framework and Pipeline


[*REF*] introduces the concept of Chain-of-Thought, which effectively enhances the reasoning ability of LLMs by generating a series of intermediate reasoning steps in response to a question.
[*REF*] proposes to explore multiple feasible paths and combine searching and backtracking, which significantly improves the effectiveness in solving complex logical reasoning problems.
[*REF*] proposes a reasoning method with automatic evaluation. This approach involves automating the setup of multiple agents with different capabilities to evaluate and refine reasoning results multiple times. It endows LLMs with stronger reasoning abilities while effectively reducing hallucinations. [*REF*] introduces a GoT structure that can comprehensively utilize the optimal results generated during the reasoning process. While ongoing advancements aim to augment the task-solving capabilities of LLMs through the implementation of diverse reasoning pipelines, a notable performance gap persists, particularly in generating intricate content requisite for domains such as creative writing and storyboard generation. In this study, we explore an innovative pipeline imbued with a hierarchical structure specifically engineered to address the complexities inherent in such creative generation tasks.


Specific Tasks-Solving with Agent and LLMs


Numerous studies have delved into the enhancement of LLMs&apos; task-solving capabilities, exploring innovative paradigms to boost their creative and problem-solving prowess in specific scenarios. Some studies introduce systems optimizing inter-agent communication [*REF*], whereas others harmoniously amalgamate the behavioral propensities of agent groups with the advanced reasoning capabilities of LLMs [*REF*]. These explorations, inclusive of works cited as [*REF*; *REF*; *REF*], have manifested remarkable advancements in tailoring Agent-Pipeline solutions, demonstrating unparalleled creativity and proficiency in problem resolution. Conversely, some research ventures [*REF*] have employed hierarchical and structured approaches combined with specific role assignments to leverage the capabilities of LLMs in generating long-form creative content, such as continuous scripts enriched with detailed contextual elements. These studies, along with [*REF*; *REF*; *REF*], have exemplified commendable strides in integrating domain-specific knowledge to guide LLMs in executing tasks with enhanced precision in respective domains.


Despite the plethora of advancements and innovations in LLMs, a common limitation is evident, i.e., the reliance on manual specification for both problem-solving processes and the determination of agent attributes, impeding their versatility in generalized applications. To address this issue, our study proposes leveraging LLMs as agents capable of self-collaboration and self-evaluation, a methodology we posit is adaptable across a myriad of tasks.


Cognitive Science and LLMs


Researchers such as [*REF*; *REF*; *REF* -WASPOR-3; *REF*] have delved into the realms of human psychology and cognition, influencing subsequent developments in artificial intelligence theory [*REF*].
[*REF*; *REF*; *REF*; *REF*; *REF*] demonstrate the capabilities of large language models (LLMs) to the public.
[*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*], by integrating cognitive science with Large Language Models (LLMs), continuously explore methods to enhance the capabilities of LLMs.


All of these approaches represent new explorations in both theoretical and methodological aspects, providing fresh perspectives for the enhancement and development of LLMs in the future.


Method


In this section, we formally introduce OKR-Agent, as demonstrated in Figure [1]. We first revisit the definition of task-solving of LLM Agents, and then provide details about OKR-Agent including Self-Collaboration, Self-Evaluation, and the complete workflow.


Given an input instruction *MATH* and a model *MATH*, if we denote the final output to be *MATH*, then the Standard Prompting can be formulated as: *MATH* y = \mathcal M(x) *MATH* with the additional prompt *MATH* and intermediate generations *MATH*, Chain-Of-Thought(CoT)and Solo-Performance-Prompting(SPP) can be described as below respectively: *MATH* *MATH* *MATH* *MATH* *MATH* *MATH* where *MATH* is the intermediate step in CoT, *MATH* are the multi-personas and multi-run feedback in SPP. For OKR-Agent, the goal is to hierarchically generate Objects(*MATH*) and Key Results(*MATH*) from user input *MATH*, and assign persona *MATH* accordingly. We also want the system can generate evaluations(*MATH*) as guidance during execution. Our system then can formulated as: *MATH* y *MATH* *MATH* We now provide the details of the corresponding intermediate generations (*MATH*) in OKR-Agent.


Objects and Key Results.


At the core of our OKR-Agent pipeline is the hierarchical derivation of Objects and Key Results. Given a user input *MATH*, the LLM acts as a Leading Agent (LA), assigned to produce a set of potential targets that align with the user&apos;s intentions. This generative process can occur multiple times, allowing the LA to consolidate a selection of the most relevant targets, referred to as the first level of Objects (*MATH*).
Subsequently, each object can be elaborated into sub-objects, providing more nuanced details and forming the Key Results *MATH*.


Agent Assignment.


Given the generated Objects(*MATH*) and Key Results(*MATH*), each can be paired with an agent, constituting a structured set of agents. It is noteworthy that, unlike the SPP in [*REF*], our agents are not chosen from a predefined list but are dynamically incorporated based on OKR decomposition. This approach ensures that agent selection is not only task-driven but also inherently adaptive to the tasks at hand.
Specifically, each agent is accountable for its assigned Object or Key Result: agents assigned to Objects oversee the broader aspects of the task, ensuring overarching coherence, while those paired with Key Results focus on the fine details, maintaining precision in execution.


Evaluations.


For each agent, it is crucial to develop evaluation criteria, represented as *MATH*, to direct its work. Once again, we employ LLM for criteria generation. Given the role definition and corresponding OKR for an agent, we prompt the LLM to formulate a concise, one-sentence criterion serving as the evaluation metric for the relevant segment in the final output.


Hierarchical Generation


As depicted in Algorithm , the decomposition process of OKR, along with the generation of agents and evaluations, can be conducted iteratively, allowing for a meticulous and organized delineation of tasks, which ensures accuracy and uniformity throughout the task-solving trajectory.
Formally, the output of this generation stage is the structured OKR *MATH* with its associated agents *MATH* and the evaluations *MATH*.
This hierarchical configuration intrinsically forms inter-dependencies among agents, enabling streamlined and efficient pathways for execution and evaluation.


Prompt generation.


To prompt an LLM to follow the generation procedure as mentioned above, we also designed the prompts of OKR decomposition and evaluation criteria, which help LLM to perform as expected. Specifically, we use the following for OKR decomposition:


\&quot;You are an expert in &apos;Objective templates&apos;. It is known that a key element of the &apos;Objective template&apos; includes &apos;KR0,KR1,\...,KRn&apos;. Expand the keywords &apos;KRx&apos; within this Objective. List keywords and separate each word with a comma, no extra words.\&quot;


For agent generation, we use:


You are an expert in &apos;Objective&apos;, tasked with creating content about &apos;KR0,KR1,\...,KRn&apos;. Summarize the required job positions as &apos;JobTitle&apos;.
Connect the &apos;JobTitle&apos; with &apos;-\&gt;&apos;, without unnecessary words.


Moreover, we use this prompt to generate evaluation criteria:


You are an outstanding &apos;JobTitle Expert&apos;. Provide a sentence that thoroughly describes the role of this position in &apos;Objective&apos; work, taking into account the professional characteristics of &apos;JobTitle&apos;. The sentence should follow the structure \&quot;An excellent &apos;Objective&apos; should have the characteristic of\... in the aspect of XXX;\&quot; and should not exceed 200 words. Avoid unnecessary words.


ALGORITHM


OKR-Agent Workflow


Following the Hierarchical Generation stage, the LLM advances to the execution phase. In this phase, each agent is tasked with extending the solution per its assigned OKR, scrutinizing the extended solution against its evaluation criteria and the accumulated ones from preceding agents, and refining the solution based on the evaluations before forwarding the refined version to the subsequent agent. This progression operates hierarchically, cascading from the Leading Agent down to the leaf-node agents, culminating in the formulation of the final solution.
We now provide details for each component, the overall workflow is shown in Algorithm .


Solution expanding.


For each agent, the essential task is to elaborate on the existing solution per its designated OKR, which may involve enumerating sub-targets or enriching the details of key results. Given that we employ OKR decomposition for the input tasks, it offers an intuitive key-value method to represent the solution, wherein the keys can serve as objects and the values can retain the details. In our experiments, we discovered that such a representation is more conducive for LLMs in maintaining complicate information. We will provide examples in experiment section.


Solution review.


Once the solution is expanded, the subsequent step for the agent is its review. Our findings indicate that errors introduced at higher levels tend to propagate and accumulate in subsequent levels, making it challenging for later agents to rectify them. Therefore, post-generation review by the agent becomes imperative. To safeguard previously generated parts from unintended alterations, we catalog the evaluation criteria of all preceding agents. These criteria are then amalgamated with the current agent&apos;s criteria, enabling a more precise and comprehensive evaluation.


Solution modification.


Given the current solution and evaluation, the agent will be asked to modify the solution in case there are errors, missing parts and so on.
This refinement process can be executed multiple times, allowing the agent to choose the iteration with the highest evaluation score.


ALGORITHM


Experiments


In this section, we deploy the OKR-Agent to three distinct tasks: storyboard generation, creative writing, and trip planning, to evaluate its efficacy and versatility in varying applications. Across these diverse tasks, OKR-Agent exhibited exceptional capabilities in global task planning, maintaining high levels of correctness, and generating rich details.


Short Video Storyboard Generation


Storyboard generation.


In the creation of a short video, a storyboard plays a pivotal role by taking a writer&apos;s narrative and structuring it for video production, delineating essential components like shot composition, scene setup, actor performance, and dialogue through textual representations. It acts as the linchpin, coordinating tasks across various roles in production.
Nonetheless, crafting a proficient storyboard script necessitates extensive professional knowledge. To democratize the creation of visually compelling videos and make it accessible to everyone, we explored the proficiency of OKR-Agent in executing this intricate task.


OKR Generation.


In our experiment, we used the following user input: \&quot;A storyboard of &apos;The boy picked up a robot, the boy repaired the robot, the boy and the robot became friends&apos;.\&quot; Referencing [2], we contrast the Objects and Key Results produced by OKR-Agent and ChatGPT.
Owing to its hierarchical structure, OKR-Agent elucidates more detailed and significant elements and requirements for video production than ChatGPT. Additionally, our method autonomously generates the requisite agents, deriving from the OKRs; for instance, the agents for OKR-L-2 include &apos;script writer&apos;, &apos;director&apos;, &apos;camera operator&apos;, &apos;actor&apos;, and &apos;musician&apos;.


FIGURE


Storyboard visuallization.


Given the substantial visual and artistic elements in the storyboard produced by OKR-Agent, we proceeded with an evaluation, employing user-study based subjective assessments. We initiated the evaluation by generating results from OKR-Agent using the uniform story input (\&quot;The boy found a robot, fixed it, and they became friends.\&quot;) and visually represented the script content utilizing AI \&quot;text to image\&quot; tools like MidJourney [*REF*], where the generated outputs served as prompts for deriving results. For comparative analysis, results were similarly procured using DramaTron [*REF*], with the comparative outputs illustrated in Figure [3]


We subsequently conduct a user-study involving 30 participants, comprised of 20 professionals from the field of &apos;Digital Art Creation&apos; and 10 non-professionals. Participants were queried on various aspects including &apos;Plausibility of the Story,&apos; &apos;Text/Image Consistency,&apos; and &apos;Visual Continuity,&apos; with the responses statistically illustrated in Table . The compiled data revealed that OKR-Agent, leveraging its enhanced capability for text detail generation, demonstrated superior control over visual elements in the text-to-image transformation process, marking a 24% improvement, and yielded more consistent results, with a 28.9% increase compared to Dramatron. However, Dramatron exhibited a richer storyline content, surpassing OKR-Agent by 5.2%, attributed to its distinctive ability to define &apos;storyline styles.&apos; The comparison between &apos;Professional&apos; and &apos;Amateur Evaluation&apos; discernibly shows that professionals applied stricter assessment criteria, especially evident from Dramatron&apos;s scores. However, this stringent assessment did not translate to substantial differences in the evaluation of OKR-Agent, underscoring the resilience and efficacy of OKR-Agent&apos;s detailed output. More visualization results of another input is provied in [4].


FIGURE


FIGURE


Multi-day Trip Planning


This task aims to assess the coordination of individual sub-item arrangements within OKR-Agent in multiple parallel planning scenarios, as well as the continuity and rationality of global planning. This feature is particularly prominent in real-world travel planning tasks.
Such planning is conducted on a &apos;day&apos;-by-&apos;day&apos; basis, where multiple factors need to be taken into account within a single day, each with various inter-dependencies. Simultaneously, the overall arrangement also needs to be considered for its rationality.


In this task, OKR-Agent breaks down the mission into four objectives: determining the travel destination and itinerary, booking transportation and accommodation, planning daily activities, and preparing contingency plans. This results in the corresponding agents: &apos;Travel Planner&apos;, &apos;Accommodation and Transportation Booking Officer&apos;, &apos;Emergency Management Commissioner&apos;, responsible for &apos;itinerary arrangement&apos;, &apos;transportation and accommodation planning&apos;, and &apos;safety and financial management tasks&apos; respectively.


Table  illustrates the divergent output results from OKR-Agent and ChatGPT, both generated from identical user input---&apos;Family Three-Day Hawaii Travel Plan.&apos; OKR-Agent exhibits a more thorough consideration of elements such as &apos;transportation&apos; and &apos;financial management&apos; compared to ChatGPT, which is evidenced by the detailed inclusion of aspects like &apos;round-trip time arrangements for the first and last days&apos; and &apos;airfare and car rental costs&apos; in the planning results. This reflects OKR-Agent&apos;s superior capability in producing comprehensive and realistic travel plans, catering to multiple facets of trip planning.


TABLE


Trivia Creative Writing


Creative Writing is the art of crafting written content that explores thoughts, feelings, and ideas, utilizing narrative craft, character development, and literary tropes, often diverging from formal writing styles to convey emotions, create imagery, and experiment with language across various genres and forms like novels, poetry, and screenplays. In this task, we test the same input as in the SPP paper: \&quot;Generate a paragraph of fantasy creative story.\&quot;


Table depicts the visual rendition of a fantastical adventure narrative, conceived by the OKR-Agent. Our method elaborated on the prompt: \&quot;one-paragraph background story of an NPC for the next Legend of Zelda game. The background story should mention \... by Jay Chou,\&quot; resulting in the creation of four enriched scenes.


In this experiment, OKR-Agent demonstrated its prowess by producing narratives with richer and more layered content in comparison to SPP.
The following is a narrative developed by OKR-Agent. Words highlighted in red represent the accurate responses to the input queries. Owing to space limitations, the comprehensive story has been relegated to the supplemental material:


TABLE


Conclusion


In this study, we demonstrate the effectiveness of OKR-Agent in optimizing task-solving pathways within intricate domains such as storyboard generation and creative writing. By leveraging hierarchical structures and LLMs, it has enabled precise, coherent, and adaptive task planning and execution. The findings from the comparative experiments underscore OKR-Agent&apos;s superior global task planning and detail generation capabilities, offering substantial contributions to artificial intelligence research. It will be worth investigating incorporating human-in-the-loop approaches to fine-tune and augment the capabilities of OKR-Agent, enabling real-time human interaction to enhance creative content generation and problem-solving processes.