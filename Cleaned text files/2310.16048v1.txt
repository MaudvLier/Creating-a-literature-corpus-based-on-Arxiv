AI Alignment and Social Choice: Fundamental Limitations anD Policy Implications


Introduction


The challenge of ensuring that artificial intelligence (AI) systems are aligned with human intentions, ethics, and values has been recognized since the early days of the development of AI. In his seminal article Some Moral and Technical Consequences of Automation, Norbert Wiener [*REF*] highlighted the need to ensure that the &quot;purpose put into the machine is the purpose which we really desire.&quot; More recently, Russell [*REF*] has highlighted the need to explicitly align AI with objectives aligned with human intentions and values.


But how can we embed human intentions and values in machines? And whose values should be embedded? With the rise of autonomous systems, these questions have garnered significant attention in recent years. This has led to new disciplines like Machine Behavior [*REF*] and Machine Ethics [*REF*] which have explored how machines can be &quot;taught&quot; to learn human norms. As expected, there is no unique way to teach norms to machines. Building on Rawlsian ideas of fairness [*REF*], Dwork et al. [*REF*] suggested that machine norms need to be &quot;an approximation as agreed upon by society.&quot; Awad et al. [*REF*] operationalized this idea for the case of autonomous vehicles by crowd-sourcing moral decisions from millions of online participants through the Moral Machine Experiment.
Noothigattu et al. [*REF*] used data from the Moral Machine Experiment to build a model of aggregated moral preferences using tools from computational social choice [*REF*; *REF*].


More recently, in the context of Large Language Models (LLMs), the project of AI alignment has been focused on mitigating harms i.e.
ensuring that LLMs follow user instructions, do not generate biased or toxic text, or factually inaccurate information [*REF*].
Reinforcement learning using human feedback (RLHF) has been the key technical innovation that has led to remarkable progress in developing aligned AI agents [*REF*; *REF*; *REF*]. For instance, the most widely deployed LLMs like GPT4 [*REF*], Claude [*REF*], Bard [*REF*], and Llama [*REF*] all use RLHF to fine-tune their models. While these approaches focus on criteria such as &quot;helpfulness, honesty, and harmlessness&quot; [*REF*], there remain open questions about how such alignment can be realized practically and at scale. More broadly, whose norms or values should they be aligned with? And how can we align AI systems by respecting democratic norms?


The limitations of RLHF in aligning AI systems at scale remain poorly understood. Casper et. al. [*REF*] provide an exhaustive review of fundamental limitations and open challenges in RLHF for AI alignment. In particular, they highlight the limitation of selecting &quot;representative humans&quot; to act as reinforcers during the training process. For example, while training GPT4, OpenAI restricts human reinforcers who agree with &quot;expert&quot; researcher preferences [*REF*]. Given systemic issues in researcher training and hiring, this can lead to biased outcomes. Additionally, the demographics of reinforcers are not representative of end users.


This paper examines the limits of building aligned AI systems through a democratic process, even when reinforcers are representative of the underlying population of users. More specifically, we ask whether it is possible to design voting rules that allow a group of reinforcers, representative of a population of diverse users, to train an AI model using RLHF. Using two widely known results from social choice theory, we show that there exist no unique voting rules that allow a group of reinforcers to build an aligned AI system by respecting democratic norms i.e. by treating all users and reinforcers the same. Further, we show that it is impossible to build a RLHF model democratically that respects the private preferences of each user in a population simultaneously.
As far as we know, this fundamental limitation of RLHF has not been highlighted in the literature.


This paper builds on ideas from social choice theory, machine ethics, and computer science to highlight some fundamental barriers to embedding human intentions to AI systems while following democratic norms. Our key result borrows from two widely known theorems in social choice theory - the impossibility theorems by Arrow and Sen - which are widely known constraints in voting theory. Gabriel [*REF*] briefly touched on the implications of impossibility theorems on AI alignment but as far as we know, these arguments have not been formally investigated in the context of RLHF.


The rest of the paper is organized as follows. In Section [2], we provide formalize reinforcement learning with multiple reinforcers, in Section [3] we outline the Arrow-Sen impossibility theorems in the context of RLHF and we discuss the implications of these theorems in Section [4].


Reinforcement Learning with Multiple Reinforcers 


Early approaches to reinforcement learning explicitly specified a reward function [*REF*] that was used to train an AI model. However, specifying a well-defined reward function to achieve alignment for AI language agents is impossible. Reinforcement Learning with Human Feedback (RLHF) [*REF*; *REF*] was suggested as a way to &quot;communicate&quot; human intentions to language models through a small group of non-expert human reinforcers. The key innovation in RLHF is training AI agents to be aligned with humans without knowing the explicit reward function. Instead, the reward function is &quot;discovered&quot; via human feedback.


Consider an AI agent receiving feedback from a human reinforcer over a sequence of steps; at each time *MATH* the agent receives a question/prompt *MATH* from the human and then sends an answer/response *MATH* to the human. The reinforcement function is not known a priori. Instead, the human reinforcer expresses its preferences between trajectory segments. A trajectory segment is a sequence of observations and actions, *MATH*.
When a human reinforcer prefers trajectory segment *MATH* to trajectory segment *MATH*, we denote it as *MATH*.


Consider a set of trajectory segments *MATH* and a set of human reinforcers *MATH*. Each human reinforcer *MATH* ranks the trajectory segments in *MATH* according to an individual preference ranking denoted by *MATH*.
The preferences *MATH* between two trajectory segments *MATH* and *MATH* are generated by a reward function *MATH* for *MATH*, such that: *MATH* *MATH* *MATH* whenever, *MATH* *MATH* *MATH*.


If reinforcer *MATH* likes trajectory *MATH* as much as or more than alternative *MATH*, we write *MATH*. If they like *MATH* strictly more than *MATH* we write *MATH*, and if they are indifferent we write *MATH*. Each reinforcer *MATH* is rational i.e. the preferences are complete (*MATH* or both) and transitive (if *MATH* and *MATH* then *MATH*).


Thus, the ranking *MATH* is a weak ordering of the alternatives in *MATH*, and the set of all weak orderings of *MATH* is denoted by *MATH*. Finally, we denote the profile of all individuals&apos; preferences by *MATH*. A preference aggregation function *MATH* takes a preference profile *MATH* as input and produces a collective preference relation, *MATH*, that compares the alternatives.


Arrow-Sen Impossibility Theorems for RLHF 


Consider a set of reinforcers *MATH* who have to vote on their preferences for a set of trajectories *MATH* and *MATH*. There can be multiple voting rules to choose a preference.
For example, a plurality rule would choose a preference that has the largest number of votes, even when a preference does not have a clear majority (*MATH*) votes. Another alternative is the simple majority rule that which chooses a preference with the majority of votes in a ranked choice. The two voting rules are shown in Tables [1] and [2]. The two simple voting rules above lead to different outcomes: for the plurality rule selects trajectory *MATH*, whereas the majority rule chooses trajectory *MATH*. One can similarly design many other voting rules.


In the above scenario, which among all possible voting rules is the &quot;best&quot;? Arrow [*REF*] showed that under very general considerations, no such voting rule exists. While this is a widely known result, its application to RLHF has important implications for building scalable and aligned AI systems. We outline Arrow&apos;s arguments below in the context of RLHF.


TABLE


Arrow [*REF*] posited four axioms that any reasonable aggregation/voting rule should satisfy. The axioms are:


-  Pareto or Consensus: an aggregation function *MATH* satisfies Pareto if, whenever every individual *MATH* strictly prefers *MATH* to *MATH*, the function *MATH* ranks *MATH* strictly higher than *MATH*.
-  Independence of irrelevant alternatives (IIA): the group members&apos; preferences about some alternative *MATH* does not affect how the aggregation rule *MATH* ranks two different alternatives, *MATH* and *MATH*.
-  Transitivity: If *MATH* produces an ordering in which *MATH* and *MATH*, then it must also be the case that *MATH*.
-  No Dictator: a rule *MATH* is dictatorial when there exists one voter *MATH* such that every time *MATH* (i.e., the dictator *MATH* strictly prefers *MATH* to *MATH*), the aggregation rule *MATH* produces a strict ranking *MATH*. An aggregation rule *MATH* satisfies no dictator if it is not dictatorial.


Arrow&apos;s impossibility theorem states that no aggregation rule can simultaneously satisfy all four axioms listed above.


THEOREM


To understand the implications of Arrow&apos;s theorem for RLHF, suppose we want to design an AI agent through a democratic process. Arrow&apos;s theorem implies that any voting rule that is Pareto efficient, transitive, and independent of irrelevant alternatives must grant all decision-making authority to a single individual/reinforcer.


Sen [*REF*] extended the idea of Arrow&apos;s theorem to understand the implications for individual rights in a society where decisions are made by the preferences of the majority. Consider, for instance, an individual *MATH* &apos;s rights within a social choice framework.
The individual can choose between two alternatives, *MATH* and *MATH*, and the two alternatives differ only with respect to features that are private to *MATH* i.e. other individuals should not have a say in *MATH* &apos;s preference between two alternatives. In his original paper Sen summarized this impossibility result in the following way: &quot;given other things in the society, if you prefer to have pink walls rather than white, then society should permit you to have this, even if a majority of the community would like to see your walls white.&quot; In the case of RLHF, this can be a specific definition of &quot;harmlessness&quot; that individual *MATH* cares about (e.g. expecting the AI system to avoid certain stereotypes based on gender or race).


Now consider an individual&apos;s &quot;protected domain&quot; *MATH*, which is a collection of pairs of alternatives between which that person&apos;s preference should be protected.
Given any individual *MATH*, a social choice *MATH* is said to respect *MATH* &apos;s individual rights if, for all preference profiles *MATH*, *MATH* *MATH* *MATH* With this definition of protecting individual preferences or rights, Sen proposed two axioms:


-  Pareto: *MATH* satisfies Pareto if, for all preference profiles *MATH*, if *MATH* for all *MATH*, then *MATH*.
-  Minimal Liberalism: A choice correspondence *MATH* respects minimal liberalism if there are at least two individuals, *MATH*, for whom *MATH* and *MATH*, such that *MATH* respects both *MATH* &apos;s and *MATH* &apos;s individual rights.


THEOREM 2


Sen&apos;s theorem implies that any choice *MATH* satisfying universal domain and Pareto can respect the individual rights of at most one individual. The theorem demonstrates that protecting the preferences of multiple individuals is at odds with the most basic notions of utilitarian ethics.


To understand the application of Sen&apos;s theorem to AI alignment, consider a modified version of the scenario presented in his original paper [*REF*]. Consider two reinforcers, A and B, and three outputs from an AI model (in the pre-RLHF stage). The three outputs correspond to normative statements about a political party X:


-  Output-1: Political party *MATH* is fascist
-  Output-2: Political party *MATH* is not fascist
-  Output-3: Political party *MATH* has a complicated agenda but it does not neatly fall into the above two categories.


Reinforcer *MATH*, who is very anti- *MATH*, prefers output 1, but given the choice between revealing their political bias and staying neutral, they would prefer to stay neutral over revealing their political affiliation.
In decreasing order of preference, their ranking is 3, 1, 2.
Reinforcer- *MATH*, however, does not mind revealing their political beliefs and would rather assert Output 2 over staying neutral. Their ranking is: 2, 3, 1.


If the choice is between Outputs 1 and 3, a liberal - someone who cares about individual rights above all else - might argue that Reinforcer- *MATH* &apos;s preference should count; since Reinforcer-A would be OK not revealing their preference, and should not be forced to. Thus the final output after RLHF would lead to Output-3.


Similarly, in the choice between Output-2 (&quot;X is not fascist&quot;) and Output-3 (&quot;its complicated&quot;), liberal values require that Reinforcer-B&apos;s preference should be decisive, and since they clearly feel strongly about opposing the stance that X is fascist, they should be permitted to do this. In this case, Output-2 should be judged as &quot;socially acceptable&quot; better than Output-3.


Thus, respecting individual preferences or liberal values would lead to preferring Output-3 over Output-1 and Output-2 over Output-3. Choosing the outputs from only one Reinforcer will break the Pareto axiom. We are thus left with an inconsistency.


Note that unlike Arrow&apos;s theorem, Sen&apos;s theorem does not require the transitivity axiom, or the independence of irrelevant alternatives. It only requires the existence of a best alternative (the Pareto axiom).
Sen&apos;s result is thus stronger because it relies on fewer axiomatic constraints.


Implications for AI Governance and Policy 


Can we build aligned AI agents using democratic norms? The results in the previous section show the answer is not straightforward. Arrow&apos;s impossibility theorem implies that there is no unique voting rule that can allow us to train AI agents through RLHF while respecting democratic norms i.e., treating each reinforcer equally. Note that this is distinct from the obvious result that any democratic process will always result in a minority that will be unhappy about the outcome; however, Arrow&apos;s theorem implies that no unique voting rule exists even when deciding what the majority preference is.


Sen&apos;s theorem has even more serious implications for AI alignment using RLHF. It implies that a democratic method of aligning AI using RLHF cannot let more than one reinforcer encode their (privately held) ethical preferences via RLHF, irrespective of the ethical preferences of other reinforcers. As Sen argued in his original paper, &quot;liberal values conflict with the Pareto principle.&quot; We now discuss some policy implications of the Arrow-Sen results for RLHF.


Non-Uniqueness of AI Alignment and Model Transparency


Consider multiple AI model developers/companies who want to build aligned AI agents. The choice of voting rule for each developer is private, i.e., there is no sharing of voting protocol between the developers. Even when the preferences of the reinforcers hired by the different developers are consistent, the resulting AI models might not be consistently aligned since voting rules across developers might differ. This will likely pose challenges for an alignment audit for AI models unless model builders disclose voting rules for the reinforcers.


One solution to this problem is to include the voting rule in a model card [*REF*]. This will allow consistent comparisons between different RLHF models. However, to avoid confusion for users the simplest solution is for all AI model builders to agree on a specific voting rule. It is not obvious how this voting rule can be agreed upon and poses a new governance challenge for AI alignment. Currently, model developers like OpenAI do not report voting rules in their system cards [*REF*].


Universal vs. Narrow AI Alignment


Sen&apos;s theorem has important implications for aligning AI systems that respect personal preferences of individual users or a small subset of users with shared values, especially those who might not be represented amongst reinforcers. Consider, for example, an AI conversational agent trained via RLHF to be used in educational settings. Each user will likely have individual (private) preferences that other users or model developers should not have any say on. For example, a user of an AI conversational agent in a classroom might prefer not to encounter language deemed racist by them. Sen&apos;s theorem implies that it is impossible to build an RLHF model via democratic methods, such that every individual&apos;s private preferences are respected. While prima facie, this conclusion might seem obvious, it implies that we cannot build artificial general intelligence aligned with all users&apos; intentions. Indeed any AI agent built using RLHF will be misaligned with every user in some dimension.


This result has important implications for developing universally aligned AI agents. At the outset, building universally aligned AI agents using RLHF is impossible. We can build AI agents that are narrowly aligned with certain preferences of a group of users but these preferences need to be explicitly communicated during the reinforcement process. For example, we can build an AI conversational agent that avoids racist speech (suitably defined). But we cannot expect the agent to be aligned along other dimensions, such as gender, nationality, political background, etc., that were not explicitly considered during the training process. This implies we can build aligned AI agents for narrow preferences/domains. In practice, this means we will have a family of aligned models for specific tasks/groups but not generally aligned models.


Conclusion 


The development of a universally accepted ethical framework for building and regulating AI is still a work in progress. However, there has been convergence in key principles that communities around the world want in AI systems: transparency, fairness, non-maleficence, responsibility and privacy [*REF*]. There is however no consensus between entities developing and regulating AI around how to embed these principles into AI models.


RLHF has shown remarkable success in improving model outputs of LLMs.
However, whether it is possible to align models in a scalable manner, while respecting democratic norms, remains unclear. This paper argues that even in the presence of diverse reinforcers who are representative of a population of users, alignment might not be unique. Further, we argue that there will always be private preferences of users that an RLHF AI model built via democratic norms will violate.


Our results have important implications for the governance and development of aligned AI systems. While the use of RLHF for real-world applications has exploded in recent months, regulations and policies for deploying RLHF models at scale are still nascent. Our results highlight the collective action challenges that model developers and policymakers will have to grapple with while aligning AI agents in the near future.
Finally, our results indicate that the future of aligned AI development might be better served by incentivizing smaller model developers working on aligning their models to a narrow set of users (&quot;narrow AI alignment&quot;) as opposed to trying to build universally aligned AI (&quot;aligned AGI&quot;).