LOA: Logical Optimal Actions for Text-based Interaction games


 Introduction


Neuro-symbolic (NS) hybrid approaches have been proposed for overcoming
the weakness of deep reinforcement learning [*REF*; *REF*; *REF*; *REF*; *REF*],
including less training data with generalization, external knowledge
utilization, and direct explainability of what is learned. Study of
reinforcement learning (RL) in non-symbolic environments, such as those
with natural language and visionary observations, would be an important
step towards the real-world application of the approaches beyond classic
and symbolic environments.


Under certain controls necessary for studying RL, text-based games
provide complex, interactive, and a variety of simulated environments
where the environmental game state observation is obtained through the
text description, and the agent is expected to make progress by entering
text commands. In addition to language understanding [*REF* *REF*; *REF*],
successful play requires skills such as long-term memory [*REF*],
exploration [*REF*], observation pruning [*REF*], and common sense
reasoning [*REF*]. However, these studies are not
using the neuro-symbolic approach which is a combination of the neural
network and the symbolic framework.


FIGURE


A recent neuro-symbolic framework called the Logical Neural
Networks (LNN) [*REF*] simultaneously provides key
properties of both neural networks (learning) and symbolic
logic (reasoning). The LNN can train the constraints and rules with
logical functions in the neural networks, and since every neuron in the
network has a component for a formula of weighted real-valued logics, it
can calculate the probability and contradiction loss for each of the
propositions. At the same time, trained LNN follow symbolic rules, which
means they yield a highly interpretable disentangled representation.
Using this benefit of LNN, we proposed a neuro-symbolic RL method that
uses pre-defined external knowledge in logical networks, and the method
successfully plays on the text-based games [*REF*].


In this demonstration (demo site: WEBSITE), we present
a Logical Optimal Actions (LOA) architecture for neuro-symbolic RL
applications with LNN [*REF*] for text-based interaction
games. While natural language-based interactive agents are the ambitious
but attractive target as real-world applications of neuro-symbolic, it
is not easy to provide an environment for the agent. The proposed
demonstration uses text-based games learning environment, called
TextWorld [*REF*], as a miniature of a natural
language-based interactive environment. The demonstration provides a
web-based user interface for visualizing the game interaction, which is
including displaying the natural text observation from the environment,
typing the action sentence, and showing the reward value from the taken
action. The LOA in this demonstration also visualizes trained and
pre-defined logical rules in LNN via the same interface, and this will
help the human user understand the benefits of introducing the logical
rules via neuro-symbolic frameworks. We also supply an open-sourced
implementation for demo environment and some RL methods. This
implementation contains our logical approaches and other
state-of-the-art agents.


 Logical Optimal Action


Our proposing LOA is an RL framework which is combining logical
reasoning and neural network training. These training and reasoning are
provided from functionalities of LNN [*REF*] that is
simultaneously providing key properties of both neural networks and
symbolic logic. Figure [1] shows the overview architecture for LOA.
The LOA model receives the logical state value as logical fact from the
language understanding component which receives raw natural language
state value from the environment. The model forwards into LNN for the
input to get the optimal action for it, the action goes into the
environment to execute the action command, then reward is input to LOA
agent. The LOA will be trained the action decision network in LNN by
using the acquired reward value and chosen action from the network.


 LOA Demo


FIGURES


The proposing web-based LOA demonstration supports two
functionalities: 1) play the text-based game by human interactions, 2)
visualize the trained and pre-defined LNN to increase interpretability
for acquired rules.


For playing the games by web interface,
Fig. [2] shows an initial view for the LOA demonstration. On the left-hand side, we can
choose the game from some existing text-based interaction games,
such as TextWorld Coin-Collector game [*REF*],
TextWorld Cooking game [*REF*], TextWorld
Commonsense Cleanup game [*REF*], and Jericho
game [*REF*]. Figure [3] shows the view for playing the TextWorld game,
and Fig. [4] shows the view for another game (cleanup task). The human player can
input any action by natural language then the demonstration system
displays the raw observation output from the environment.


FIGURE


For visualizing the trained and pre-defined neuro-symbolic network in
LNN, Fig. [4] and Fig. [5] show the example of the LNN output. In these
figures, the LNN contains simple rules for the TextWorld Coin-Collector
game; for example, the rule is the agent takes &apos;go east&apos; action, when
the agent finds the east room (&quot;found west&quot; *MATH* 
&quot;go west&quot;). The round box explains the proposition from the given
observation inputs, the circle with a logical function means a logical
function node of LNN, and the rectangle box explains an action candidate
for the agent. The highlighted nodes (red node) have &apos;true&apos; value, and
non-highlighted nodes (white node) have &apos;false&apos; value. In
Fig. 4, the agent found the north exit from the given
observation (&quot;Observation (t=1)&quot;) by using semantic parser, then
the going north room action (&quot;go north&quot;) are activated. In
Fig. [5], if the user clicks the selectable box, the LOA recommends only one action which
is &apos;go north&apos;. In this demonstration, we show the benefit of introducing
the LNN into an RL agent, we don&apos;t prepare to automatically choose the
action by LOA framework. However, if we execute the RL with LOA
framework, the RL agent can converge faster than other non-symbolic and
neuro-symbolic methods.


FIGURE


After selecting &quot;go north&quot; action at *MATH*, next observation sentence
and LNN output for next step are shown in
Fig 6. In this step, the agent found two doors, which
are east and south; however, the south door is connected to the previous
room because the agent took going north action at the previous step.
Since this LNN is simple LNN, the &quot;go south&quot; action is also
recommended in Fig 6. Figure [6] shows the output of the complicated LNN which has
functionality for avoiding revisiting the visited room. By using such
the LNN, LOA can output only &quot;go east&quot; action by having contradiction
loss in LNN. This is a benefit of introducing the neuro-symbolic
framework, and the human user can easily understand the reason for the
taken action by the agent with this interpretability by LOA.


 Conclusion


We propose a novel demonstration (URL: WEBSITE) which
provides to play the text-based games on the web interface and visualize
the benefit of the neuro-symbolic algorithm. This application helps the
human user understand the trained network and the reason for taken
action by the agent. We also extend more complicated LNN for other
difficult games on the demo site. At the same time, we open the source
code for the demonstration (URL: WEBSITE).