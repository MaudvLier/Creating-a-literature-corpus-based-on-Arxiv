Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents


FIGURE


Introduction


Large language models (LLMs) have made impressive advances in language
generation and understanding in recent years [*REF*; *REF*; *REF*; *REF*].
See [*REF*] for a recent summary of their
capabilities and impacts. Being trained on large corpora of
human-produced language, these models are thought to contain a lot of
information about the world [*REF*; *REF*; *REF*] - albeit in linguistic form.


We ask whether we can use such knowledge contained in LLMs not just for
linguistic tasks, but to make goal-driven decisions that can be enacted
in interactive, embodied environments. But we are not simply interested
in whether we can train models on a dataset of demonstrations collected
for some specific environment -- we are instead interested in whether
LLMs already contain information necessary to accomplish goals without
any additional training.


More specifically, we ask whether world knowledge about how to perform
high-level tasks (such as &quot;make breakfast&quot;) can be expanded to a series
of groundable actions (such as &quot;open fridge&quot;, &quot;grab milk&quot;, &quot;close
fridge&quot;, etc) that can be executed in the environment. For our
investigation, we use the recently proposed VirtualHome
environment [*REF*]. It can simulate a large variety of
realistic human activities in a household environment and supports the
ability to perform them via embodied actions defined with a
&apos;verb-object&apos; syntax. However, due to the open-ended nature of the
tasks, it is difficult to autonomously evaluate their success. We rely
on human evaluation (conducted on Mechanical Turk) to decide whether
sequences of actions meaningfully accomplish posed tasks.


We find that large GPT-3 [*REF*] and
Codex [*REF*] models, when prompted with a single fixed
example of a task description and its associated sequence of actions,
can produce very plausible action plans for the task we&apos;re interested
in. Such completions reflect the information already stored in the model
-- no model fine-tuning is involved. Additionally, we only observe this
effect in the larger models. Unfortunately, despite their semantic
correctness, the produced action plans are often not executable in the
environment. Produced actions may not map precisely to admissible
actions, or may contain various linguistic ambiguities.


We propose several tools to improve executability of the model&apos;s
outputs. First, we enumerate all admissible actions and map the model&apos;s
output phrases to the most semantically-similar admissible action (we
use similarity measure between sentence embeddings produced by a RoBERTa
model [*REF*] in this work, but other choices are possible).
Second, we use the model to autoregressively generate actions in a plan
by conditioning past actions that have been made admissible via the
technique above. Such on-the-fly correction can keep generation anchored
to admissible actions. Third, we provide weak supervision to the model
by prompting the model with a known task example similar to the query
task. This is somewhat reminiscent of prompt tuning approaches but does
not require access to gradients or internals of the model.


Using the above tools to bias model generation, we find that we improve
executability of action plans from 18% to 79% (see Figure
[1]) without any invasive modifications to model parameters or any extra gradient or
internal information beyond what is returned from the model&apos;s forward
pass. This is advantageous because it does not require any modifications
to the model training procedure and can fit within existing model
serving pipelines. However, we do find there to be some drop in
correctness of the action sequences generated with the above tools (as
judged by humans), indicating a promising step, but requiring more
research on the topic.


To summarize, our paper&apos;s contributions are as follows:
-  We show that without any training, large language models can be
prompted to generate plausible goal-driven action plans, but such
plans are frequently not executable in interactive environments.
-  We propose several tools to improve executability of the model
generation without invasive probing or modifications to the model.
-  We conduct a human evaluation of multiple techniques and models and
report on the trade-offs between executability and semantic
correctness.


FIGURE


Evaluation Framework


Simulating open-ended tasks that resemble naturalistic human activities
requires an environment to support a rich set of diverse interactions,
rendering most existing embodied environments unsuitable for our
investigation. One exception is VirtualHome [*REF*],
which we evaluate on as it models complex human activities, though only
in a household setting. To measure correctness of the generated action
plans, for which evaluating computationally is inherently difficult for
these open-ended tasks, we conduct a human evaluation similar
to  *REF*. We note that since no further training is
involved throughout our investigations, the observations and findings
presented in this paper should also translate to similar embodied
environments, likely even beyond the household domain.


Evaluated Environment: VirtualHome 


Preliminaries


In VirtualHome, activities are expressed as programs. Each program
consists of a sequence of textual action steps, where each step is
written as: &apos;[action] &apos; *MATH* &apos;arg&apos; *MATH* &apos;(idx)&apos;. Each &apos;action&apos;
refers to one of the 42 atomic actions supported in VirtualHome, such as
&quot;walk&quot; and &quot;open&quot;. Full list of atomic actions can be found in
Appendix [8.4]. Different actions take in different
numbers of &apos;arg&apos;, such as &quot;bedroom&quot; and &quot;fridge&quot;, that are necessary for
specifying an interaction. Associated with each &apos;arg&apos; is a unique &apos;id&apos;
specifying the corresponding node in the environment graph, in case of
multiple instances of the same object class are present in the graph.
For the sake of simplicity, we omit the &apos;id&apos; in the remaining
discussions of this paper and allow automatic assignment by the
environment. An example program is shown below for the task &quot;Relax on
sofa&quot;: TABLE.


Evaluated Tasks


We use the ActivityPrograms knowledge base collected
by  *REF* for evaluation. It contains 2821 different
entries annotated by Amazon Mechanical Turk (MTurk) workers. Each entry
contains 1) a high-level task name (e.g. &quot;Watch TV&quot;), 2) detailed
instructions expressed in natural language to complete the task (e.g.
&quot;Sit on my couch directly opposite my TV, switch on my TV with the
remote control and watch&quot;), and 3) an executable program containing all
necessary steps for a robotic agent (example above). We omit the use of
detailed instructions (2) as we desire direct extraction of executable
programs (3) from only high-level task names (1). There are 292 distinct
high-level tasks in the knowledge base, from which we randomly sample 88
held-out tasks for evaluation. The remaining 204 tasks are used as
demonstration set from which we are allowed to select as example(s)
for prompting language models, or in the case of supervised fine-tuning
baselines, they are used to fine-tune pre-trained language models.


ALGORITHM


Metrics 


A program that commands the agent to wander around in a household
environment is highly executable but is mostly not correct. On the other
hand, a program composed of natural language instructions annotated by
humans is likely correct but cannot be executed, because its format is
ambiguous and may lack necessary common-sense actions (e.g. fridge must
be opened before an agent can grab things from it). We thus consider two
axes for evaluation: executability and correctness.


Executability


Executability measures whether an action plan can be correctly parsed
and satisfies the common-sense constraints of the environment. To be
correctly parsed, an action plan must be syntactically correct and
contain only allowed actions and recognizable objects. To satisfy the
common-sense constraints, each action step must not violate the set of
its pre-conditions (e.g. the agent cannot grab milk from the fridge
before opening it) and post-conditions (e.g. the state of the fridge
changes from &quot;closed&quot; to &quot;open&quot; after the agent opens it). We report the
average executability across all 88 tasks and all 7 VirtualHome scenes.


Correctness


Unlike most embodied environments where the completion of a task can be
easily judged, the ambiguous and multimodal nature of natural language
task specification makes it impractical to obtain a gold-standard
measurement of correctness. Therefore, we conduct human evaluations
for the main methods. For the remaining analysis, we rely on a
match-based metric that measures how similar a generated program is to
human annotations. Specifically, we follow  *REF* and
calculate the longest common subsequence (LCS) between two programs,
normalized by the maximum length of the two. In the presence of multiple
human-written programs for a single task, we take the maximum LCS across
them. However, we note that the majority of the tasks only have one
human annotation, but there are often many plausible ways to complete a
certain task, making this metric imperfect at evaluation program
correctness. Although correlation between the two is shown
by  *REF*, we consider it only as a proxy metric in
replacement of unscalable human evaluation.


Method


In this section, we investigate the possibility of extracting actionable
knowledge from pre-trained language models without further training. We
first give an overview of the common approach to query large language
models (LLMs) and how it may be used for embodied agents in Section [3.1].
Then we describe an inference-time procedure that addresses several
deficiencies of the LLM baseline and offers better executability in
embodied environments. We break down the proposed procedure into three
individual components, each discussed in
Section [3.2], [3.3], [3.4]. Pseudo-code is in Algorithm.


Since LMs excel at dealing with natural language text instead of the
specific format required by VirtualHome as described in
Section [2.1], we only expose natural language text to
LMs. To do this, we define a bi-directional mapping for each atomic
action that converts between the natural language format and the program
format. For instance, &quot;walk to living room&quot; is mapped to &apos; *MATH* &apos;living_room&apos; *MATH* &apos;(1)&apos;. 
Full list of the mappings is in Appendix [8.4].


Querying LLMs for Action Plans 


Previous works have shown that large language models pre-trained on a
colossal amount of data would internalize rich world knowledge that can
be probed to perform various downstream
tasks [*REF*; *REF*]. Notably,
autoregressive LLMs can even perform in-context learning, an ability to
solve tasks using only contextual information without gradient
updates [*REF*]. Contextual information is given as part of
the input prompt and LMs are asked to complete the remaining text. It
often consists of natural language instructions and/or a number of
examples containing the desired input/output pairs.


We adopt the same approach to query LLMs to generate action plans for
high-level tasks. Specifically, we prepend one example high-level task
and its annotated action plan from the demonstration set to the query
task, as shown in Figure [2]. To obtain text completion results, we sample
from autoregressive LLM using temperature sampling and nucleus
sampling [*REF*]. We refer to this LM as Planning LM
and the approach using this LM for plan generation as Vanilla
*MATH* &apos;LM&apos; *MATH*, where *MATH* &apos;LM&apos; *MATH* is replaced
by specific language model such as GPT-3.


To improve the generation quality, we follow  *REF* to
sample multiple outputs for each query. However,
unlike  *REF* who investigate program synthesis and can
choose the sample with highest unit test pass rate, we only consider the
setting where one sample is allowed to be evaluated for each task. This
is because repetitive trial-and-error is equivalent to probing the
environment for privileged information, which should not be considered
viable in our setting. For Vanilla  *MATH* &apos;LM&apos; *MATH*, to choose the
best action plan *MATH* among *MATH* samples *MATH*, each
consisting of *MATH* tokens *MATH*, we select the sample
with highest mean log probability as follows: *MATH*.


Admissible Action Parsing by Semantic Translation 


One issue arises when naively following the above approach to generate
action plans: the plan expressed in free-form language often cannot be
mapped to unambiguous actionable steps and thus is not executable by a
robotic agent. Many reasons can cause such failures: 1) the output does
not follow pre-defined mappings of any atomic action (e.g. &quot;I first walk
to the bedroom&quot; is not of the format &quot;walk to
*MATH* &apos;PLACE&apos; *MATH* &quot;), 2) the output may refer to atomic action
and objects using words unrecognizable by the environment (e.g.
&quot;microwave the chocolate milk&quot; where &quot;microwave&quot; and &quot;chocolate milk&quot;
cannot be mapped to precise action and object), or 3) the output
contains lexically ambiguous words (e.g. &quot;open TV&quot; should instead be
&quot;switch on TV&quot;).


Instead of developing a set of rules to transform the free-form text
into admissible action steps, we propose to again leverage world
knowledge learned by language models to semantically translate the
action. For each admissible environment action *MATH*, we calculate its
semantic distance to the predicted action phrase *MATH* by cosine
similarity: *MATH*. To embed the output action phrase and environment
actions, we use a BERT-style LM [*REF*; *REF*]
pre-trained with Sentence-BERT [*REF*] objective, to
which we refer as Translation LM. The action embedding is
obtained by mean-pooling the last layer hidden states across all tokens
in that action phrase. While the set of admissible actions in our
environment is discrete and possible to exhaustively enumerate, sampling
or projection can be employed in larger discrete or continuous action
spaces.


Autoregressive Trajectory Correction 


Translating each step of the program after the entire program has been
synthesized lacks consideration of achievability of individual steps and
subjects to compounding errors. In practice, LLMs might output
compounded instructions for a single step, even though it cannot be
completed using one admissible action in the environment. To this end,
we can instead interleave plan generation and action translation to
allow for automatic trajectory correction. At each step, we first query
Planning LM to generate *MATH* samples for a single action
*MATH*. For each sample *MATH*, we
consider both its semantic soundness and its achievability in the
environment. Specifically, we aim to find admissible environment action
*MATH* by modifying the ranking scheme described in
Equation as follows: *MATH*. Then we append the translated environment action *MATH* 
to the unfinished text completion. This way all subsequent steps will be
conditioned on admissible actions instead of free-form action phrases
generated by Planning LM. Furthermore, we can use Translation LM to
detect out-of-distribution actions, those outside the capabilities of a
robot, and terminate a program early instead of mapping to a faulty
action. This can be achieved by setting a threshold *MATH* such that
if *MATH* at step *MATH*, the program is terminated early. Since we now sample
Planning LM for individual steps instead of an entire sequence, another
termination condition we consider is when *MATH* of current-step
samples are 0-length (excluding leading or trailing non-English text
tokens).


Dynamic Example Selection for Improved Knowledge Extraction 


So far in the text, we always give the same example in the prompt for
all query tasks. However, consider the task of &quot;ordering pizza&quot;.
Prompting LLMs with this task may give the assumption that the agent is
initialized in front of a computer, and the LLMs may guide the agent to
search for a pizza store and click &quot;checkout my cart&quot;. Although these
are reasonable and feasible in the real world, such assumption cannot
always be made as these interactions may not be supported in simulated
environments. In fact, the closest series of actions that human experts
give in VirtualHome may be &quot;walking to a computer&quot;, &quot;switching on the
computer&quot;, and &quot;typing the keyboard&quot;. Without being fine-tuned on these
data, LLMs would often fail at these tasks.


To provide weak supervision at inference time, we propose to select the
most similar task *MATH* and its example plan *MATH* from the demonstration
set to be used as the example in the prompt. Specifically, we re-use
the same Translation LM introduced in Section [3.2] and select *MATH* whose high-level
task name *MATH* maximizes *MATH*, where *MATH* is the query
task. This approach bears resemblance to several recent
works [*REF*; *REF*; *REF*; *REF*].
An example is shown in Figure [2] where &quot;Shave&quot; is the most similar to the query
task &quot;Apply lotion&quot;.


[Final Method]


Combining the various improvement discussed above, we refer to the final
method as Translated *MATH* &apos;LM&apos; *MATH*, where
*MATH* &apos;LM&apos; *MATH* is replaced by specific language model used
such as GPT-3.


FIGURE


Results


In this section, we first show that language models can generate
sensible action plans for many high-level tasks, even without any
additional training. Then we highlight its inadequacy when naively
applied to embodied environments and demonstrate how this can be
improved by again leveraging world knowledge learned by LLMs.
Visualization of generated programs is shown in Figure [3].


Sampling from LMs


Pre-trained LMs are sensitive to sampling parameters and the specific
example given in the prompt. For all evaluated methods, we perform
hyperparameter search over various sampling parameters, and for methods
using a fixed prompt example, we report metrics averaged across three
randomly chosen examples. To select the best run for each method, we
rank the runs by the sum of LCS and executability, each normalized by
human-expert scores. Further details are in Appendix [8.1].


Model Choices


For Planning LM, we evaluate a representative set of causal language
models. For Translation LM, we mainly use Sentence-RoBERTa-355M and
provide relevant ablations in Section [5.3]. GPT-3 and Codex are accessed using OpenAI
API, and the remaining models are accessed through open-source packages,
Hugging Face Transformers [*REF*] and
SentenceTransformers [*REF*], all without additional
training (except for the fine-tuning baseline).


TABLE


Do LLMs contain actionable knowledge for high-level tasks? 


We first investigate whether LLMs can generate sensible action plans
expressed in free-form language. We use the approach described in
Section [3.1] to
query pre-trained LLMs. To evaluate the correctness of generated action
plans, we conduct human evaluations. For each model, we ask 10 human
annotators to determine -- by answering &quot;Yes&quot; or &quot;No&quot; -- whether each
task can be completed using provided action steps. To provide a
reference of how humans might rate the action plans provided by other
humans, we also ask annotators to rate the human-written action plans
included in the VirtualHome dataset for the same set of tasks. In
contrast to the free-form text output by LLMs, humans wrote the plans
using a graphical programming interface that enforces strict syntax and
a chosen set of atomic action vocabulary, which limit the expressivity
and the completeness of their answers. More details of our human
evaluation procedure can be found in Appendix [8.2].


We show the human evaluation results in Figure [1],
where the y-axis shows correctness averaged across all tasks and all
annotators. Surprisingly, when LLMs are large enough and without imposed
syntactic constraints, they can generate highly realistic action plans
whose correctness -- as deemed by human annotators -- even surpasses
human-written action plans. We also observe some level of correctness
for smaller models such as GPT-2. However, inspection of its produced
output indicates that it often generates shorter plans by ignoring
common-sense actions or by simply rephrasing the given task (e.g. the
task &quot;Go to sleep&quot; produces only a single step &quot;Go to bed&quot;). These
failure modes sometimes mislead human annotators to mark them correct as
the annotators may ignore common-sense actions in their judgment as
well, resulting in a higher correctness rate than the quality of the
output shows.


How executable are the LLM action plans? 


We analyze the executability of LLM plans by evaluating them in all 7
household scenes in VirtualHome. As shown in Table [1], we find
action plans generated naively by LLMs are generally not very
executable. Although smaller models seem to have higher executability,
we find that the majority of these executable plans are produced by
ignoring the queried task and repeating the given example of a different
task. This is validated by the fact that smaller models have lower LCS
than larger models despite having high executability, showing that this
failure mode is prevalent among smaller models. In contrast, larger
models do not suffer severely from this failure mode. Yet as a result of
being more expressive, their generated programs are substantially less
executable.


Can LLM action plans be made executable by proposed procedure? 


We evaluate the effectiveness of our proposed procedure of action
translation. We first create a bank of all allowed 47522 action steps in
the environment, including all possible combinations of atomic actions
and allowed arguments/objects. Then we use an off-the-shelf
Sentence-RoBERTa [*REF*; *REF*] as Translation
LM to create embeddings for actions and output text. For better
computational efficiency, we pre-compute the embeddings for all allowed
actions, leaving minor computation overhead for our procedure over the
baseline methods at inference time. As shown in
Table [1], executability of generated programs is significantly improved.
Furthermore, we also observe improved LCS because the translated action
steps precisely follow the program syntax and thus are more similar to
the plans produced by human experts. Sample output is shown in
Figure [1] and a larger random subset of generated samples can be found in
Appendix [8.5].


To validate their correctness, we again perform human evaluations using
the same procedure from Section [4.1]. Results are shown in Table [1]. We find
that despite being more similar to human-written plans as they follow
strict syntax, the programs are deemed less correct by humans compared
to their vanilla counterparts. By examining the output, we observe two
main sources of errors. First, we find Translation LM is poor at mapping
compounded instructions to a succinct admissible action, e.g. &quot;brush
teeth with toothbrush and toothpaste&quot;. Second, we find that the
generated programs are sometimes terminated too early. This is partly
due to the imperfect expressivity of the environment; certain necessary
actions or objects are not implemented to fully achieve some tasks, so
Translation LM cannot map to a sufficiently similar action. This is also
reflected by our human evaluation results of the programs written by
other humans, as only 70% of the programs are considered complete.


Analysis and Discussions


Ablation of design decisions


We perform ablation studies for the three components of our proposed
procedure, described in Section [3.2], [3.3], and [3.4] respectively. As shown in
Table [2], leaving out any of the three components would all lead to decreased
performance in both executability and LCS. An exception is Translated
GPT-3 w/o Trajectory Correction, where we observe a slight improvement
in LCS at the expense of a considerable drop in executability. Among the
three proposed components, leaving out action translation leads to the
most significant executability drop, showing the importance of action
translation in extracting executable action plans from LLMs.


TABLE


Are the generated action plans grounded in the environment? 


Since successful execution of correct action plans directly measures
grounding, we calculate the percentage of generated action plans that
are both correct and executable. We deem an action plan to be
correct if 70% or more human annotators decide it is correct.
Human-written plans are 100% executable, of which 65.91% are deemed
correct. Results for LMs are shown in Figure [4].


Although smaller LMs such as GPT-2 can generate highly executable action
plans as shown in Table [1], these executable plans mostly are not correct, as
they often repeat the given example or do not contain all necessary
steps. Increasing model parameters can lead to some improvement in
generating plans that are both executable and correct, yet it scales
poorly with the parameter count. In the meantime, action translation
offers a promising way towards grounding actionable knowledge by
producing executable and correct plans, though a large gap remains to be
closed to reach human-level performance (65.91%).


FIGURE


Effect of Different Translation LMs 


In this section, we study the effect of using different Translation LM.
We compare two size variants of Sentence BERT and Sentence
RoBERTa [*REF*; *REF*; *REF*] trained
on the STS benchmark [*REF*] and a baseline using averaged
GloVe embeddings [*REF*]. Results are shown in
Table [3]. Notably, we do not observe significant
differences in executability and LCS across different variants of BERT
and RoBERTa. We hypothesize that this is because any language models
trained on reasonably large datasets should be capable of the
single-step action phrase translation considered in this work. However,
simply using average GloVe embeddings would lead to significantly
reduced performance.


FIGURE


Can LLMs generate actionable programs by following step-by-step instructions?


Prior works often focus on translating step-by-step instructions into
executable programs. Specifically, instead of only providing a
high-level task name, how-to instructions are also provided, as shown
in Figure. Although this setting is easier as it does not
require rich prior knowledge, how-to instructions can help resolve
much ambiguity of exactly how to perform a high-level task when multiple
solutions are possible. To investigate whether pre-trained LLMs are
capable of doing this without additional training, we include these
instructions in the prompt and evaluate LLMs with the proposed
procedure. We compare to a supervised baseline from VirtualHome that
trains an LSTM [*REF*] from scratch on human-annotated
data. Since the code to train the baseline is not publicly released and
a different train/test split is likely used, we only show results
reported in  *REF* as a crude reference. We also cannot
compare executability as it is not reported. Results are shown in
Table. Surprisingly, without being fine-tuned on any
domain data, Translated Codex/GPT-3 can attain LCS close to supervised
methods while generating highly executable programs.


FIGURE


Analysis of program length


Shorter programs have a natural advantage of being more executable as
they need to satisfy less pre/post-conditions, albeit being prone to
incompleteness. To validate the proposed approach does not simply
generate very short programs, we calculate the average program length
across the 88 evaluated tasks. Results are shown in Table [4].
Mirroring the observations made in
Section [4.1] and Section [4.2], we find smaller LMs such as GPT-2 tend to
generate shorter programs than larger models do while frequently
repeating the given executable example. In contrast, larger models like
Codex and GPT-3 can generate more expressive programs with high realism,
yet consequently, they often suffer from executability. We show proposed
procedure can find appropriate balance and is capable of generating
programs that are highly executable while maintaining reasonable
expressiveness as measured by program length.


TABLE


Related Works


Large-scale natural language modeling has witnessed rapid advances since
the inception of the Transformer architecture [*REF*].
It has been shown by recent works that large language models (LLMs)
pre-trained on large unstructured text corpus not only can perform
strongly on various down-stream NLP tasks [*REF*; *REF*; *REF*; *REF*]
but the learned representations can also be used to model relations of
entities [*REF*], retrieve matching visual
features [*REF*], synthesize code from
docstrings [*REF*; *REF*], solve math
problems [*REF*; *REF*], and even as valuable
priors when applied to diverse tasks from different
modalities [*REF*; *REF*]. Notably, by
pre-training on large-scale data, these models can also internalize an
implicit knowledge base containing rich information about the world from
which factual answers (e.g. &quot;Dante was born in *MATH* &apos;PLACE&apos; *MATH* &quot;) can be
extracted [*REF*; *REF*; *REF*; *REF*; *REF*].
Compared to prior works in single-step knowledge extraction, we aim to
extract sequential action plans to complete open-ended human
activities while satisfying various constraints of an interactive
environment.


Many prior works have looked into grounding natural language in embodied
environments. A series of them parse language instructions into formal
logic or rely mainly on lexical analysis to resolve various linguistic
ambiguities for embodied agents [*REF*; *REF*; *REF*; *REF*].
However, they often require many hand-designed rules or scale
inadequately to more complex tasks and environments. Recently, many
efforts have been put into creating more realistic environments with the
goal to further advances in this
area [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
At the same time, by leveraging the better representation power of
neural architectures, a number of works have looked into creating
instruction-following agents that can perform
manipulation [*REF*; *REF*], navigation [*REF*; *REF*; *REF*],
or both [*REF*; *REF*; *REF*]. Recent
works also use language as hierarchical abstractions to plan actions
using imitation learning [*REF*] and to guide exploration in
reinforcement learning [*REF*].


Notably, many prior works do not leverage full-blown pre-trained LLMs;
most investigate smaller LMs that require considerable domain-specific
data for fine-tuning to obtain reasonable performance. Perhaps more
importantly, few works have evaluated LLMs in an embodiment setting that
realizes the full potential of the actionable knowledge these models
already contain by pre-training on large-scale unstructured text: the
tasks evaluated are often generated from a handful of templates, which
do not resemble the highly diverse activities that humans perform in
daily lives [*REF*; *REF*]. The development of VirtualHome environment [*REF*] enables
such possibility. However, relevant works [*REF*; *REF*] rely on
human-annotated data and perform supervised training from scratch. Due
to the lack of rich world knowledge, these models can only generate
action plans given detailed instructions of how to act or video
demonstrations. Concurrent work by  *REF* validates similar
hypothesis that LMs contain rich actionable knowledge. They fine-tune
GPT-2 with demonstrations to incorporate environment context and to
predict actions in VirtualHome, and evaluate on tasks that are generated
from pre-defined predicates. In contrast, we investigate existing
knowledge in LLMs without any additional training and evaluate on human
activity tasks expressed in free-form language.


Conclusion, Limitations &amp; Future Work 


In this work, we investigate actionable knowledge already contained in
pre-trained LLMs without any additional training. We present several
techniques to extract this knowledge to perform common-sense grounding
by planning actions for complex human activities.


Despite promising findings, there remain several limitations of this
work which we discuss as follows:


Drop in Correctness


Although our approach can significantly improve executability of the
generated plans, we observe a considerable drop in correctness. In
addition to the errors caused by the proposed action translation
(discussed in Section [4.3]), this is partially attributed to the
limited expressivity of VirtualHome, as it may not support all necessary
actions to fully complete all evaluated tasks (correctness is judged by
humans). This is also reflected by that Vanilla LMs can even surpass
human-written plans, which are restricted by environment expressivity.


Mid-Level Grounding


Instead of grounding the LLM generation to low-level actions by using
downstream data from a specific environment, we focus on high-level to
mid-level grounding such that we evaluate raw knowledge of LLMs as
closely and broadly as possible. Hence, we only consider the most
prominent challenge in mid-level grounding that the generated plans must
satisfy all common-sense constraints (characterized by executability
metric). As a result, we assume there is a low-level controller that can
execute these mid-level actions (such as &quot;grab cup&quot;), and we do not
investigate the usefulness of LLMs for low-level sensorimotor behavior
grounding. To perform sensorimotor grounding, such as navigation and
interaction mask prediction, domain-specific data and fine-tuning are
likely required.


Ignorant of Environment Context


We do not incorporate observation context or feedback into our models.
To some extent, we approach LLMs in the same way as how VirtualHome asks
human annotators to write action plans for a given human activity by
imagination, in which case humans similarly do not observe environment
context. Similar to human-written plans, we assume the plans generated
by LMs only refer to one instance of each object class. As a result,
successful plan generation for tasks like &quot;stack two plates on the right
side of a cup&quot; is not possible.


Evaluation Protocol


We measure quality of plans by a combination of executability and
correctness instead of one straightforward metric. To the best of our
knowledge, there isn&apos;t a known way to computationally assess the
semantic correctness of the plans due to the tasks&apos; open-ended and
multi-modal nature. Prior work also adopt similar combination of
metrics [*REF*]. We report two metrics individually to
shine light on the deficiencies of existing LLMs which we hope could
provide insights for future works. To provide a holistic view, we report
results by combining two metrics in Section [5.2].


We believe addressing each of these shortcoming will lead to exciting
future directions. We also hope these findings can inspire future
investigations into using pre-trained LMs for goal-driven
decision-making problems and grounding the learned knowledge in embodied
environments.