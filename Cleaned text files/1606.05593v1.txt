Introspective Agents: Confidence Measures for General Value Functions


Predictive Knowledge. The ability to autonomously construct
knowledge directly from experience produced by an agent interacting with
the world is a key requirement for general intelligence. One
particularly promising form of knowledge that is grounded in experience
is predictive knowledge -- here defined as a collection of multi-step
predictions about observable outcomes that are contingent on different
ways of behaving. Much like scientific knowledge, predictive knowledge
can be maintained and updated by making a prediction, executing a
procedure, and observing the outcome and updating the prediction -- a
process completely independent of human intervention.
Experience-grounded predictions are a powerful resource to guide
decision making in environments which are too complex or dynamic to be
exhaustively anticipated by an engineer [*REF*; *REF*].


A value function from the field of reinforcement learning is one way
of representing predictive knowledge. Value functions are a learned or
computed mapping from state to the long-term expectation of future
reward. Sutton et al. recently introduced a generalization of value
functions that makes it possible to specify general predictive questions
[*REF*]. These general value functions (GVFs), specify a prediction
target as the expected discounted sum of future signals of interest
(cumulants) observed while the agent selects actions according to some
decision making policy. Temporal discounting is also generalized in GVFs
from the conventional exponential weighting of future cumulants to an
arbitrary, state-conditional weighting of future cumulants. This enables
GVFs to specify a rich class of predictive questions where discounting
acts as a stochastic termination function [*REF*]. A single GVF
specifies a predictive question, and the answer takes the form of an
approximate GVF that can be learned by temporal-difference (TD) learning
algorithms solely from unsupervised interaction with the world. A
collection of GVFs contributes to the agent&apos;s knowledge of the world.


Ultimately, the purpose of acquiring knowledge is to improve the agent&apos;s
ability to achieve its goals. The agent&apos;s collection of GVFs are only
useful to the extent that they help with reward maximization. While GVFs
are relatively new, there have been several recent demonstrations of
their usefulness in robot tasks, from reflexive action in mobile robots
[*REF*], to the control of prosthetic arms
[*REF*; *REF*]. In this paper we take the next step
by specifying GVFs whose cumulants are internal signals defined by the
agent&apos;s own learning process.


Predicting Internal Signals. GVFs have been previously used to
specify predictions about signals external to the agent -- signals in the
agent&apos;s sensorimotor stream of interactions. However, agents also have
access to a set of internal signals not previously considered as
cumulants for GVFs. Specifically, there are a number of signals
available to an agent that relate to the agent&apos;s own learning
mechanisms -- for example, its predictions&apos; errors, weight changes, and
other time-varying meta-parameters. There are also a range of signals
that quantify the agent&apos;s interactions with its sensorimotor
stream -- for example, statistics about state or feature-space visitation
and statistical properties of input signals. Integrating predictions of
these internal signals should improve an agent&apos;s decision making
abilities towards human-level intelligence [*REF*].


One representative class of internal signals relates to an agent&apos;s
certainty in its own predictions. An agent might make better use of its
knowledge given some sense of how much each approximate GVF is to be
trusted. That is, given a GVF, how confident is the agent that the
learned prediction is accurate and precise? Methods such as confidence
intervals or ensemble forecasting are used in many domains and may also
be appropriate here [*REF*]. For our purposes, we desire an
approach that is compatible with function approximation and supports
online and incremental prediction and learning with only linear
complexity (in the size of the input features). GVFs and TD methods used
to approximate them satisfy these criteria and are therefore a promising
approach to incorporating confidence measures. Indeed, this presents an
appealing architecture where both predictive questions and measures of
their confidence are represented in a single form.


Further, we propose that an agent&apos;s decision making process can be
improved by using several confidence measures, rather than solely
relying on a single value of confidence. Each measure can then provide a
different perspective on the accuracy of an approximate GVF, enabling
the agent to make more informed decisions. Encoding these measures as
GVFs enables these internal predictions to participate in the agent&apos;s
representation of state [*REF*], which can lead to more
efficient reward maximization [*REF*; *REF*] and more accurate prediction [*REF*].


FIGURE


While there are many measures that could be useful for an agent in
determining confidence [*REF*], we here provide three examples
which are readily represented as GVFs (see
FigureÂ 1 for examples). The first is visitation. A measure of state visitation
can be expressed as a GVF by simply using a constant valued cumulant. An
agent might reasonably decide to only trust a prediction in states that
have been visited many times. The second is prediction error. On each
time step, a temporal-difference error is used to update each
approximate GVF [*REF*]; this TD error can itself be used as the
cumulant of another GVF. Such a prediction gives an agent an expectation
of how much each approximate GVF will differ from the true outcome
specified by the corresponding GVF. The third is variance. The
variance of a cumulant or an approximate GVF can easily be represented
as the difference between two GVFs, although the process for
approximating these nonstationary cumulants is somewhat more involved [*REF*].


Example: Exploration with Confidence Measures. A collection of
approximate GVFs, combined with their corresponding confidence measures
provide the agent with a way to measure how much it should trust what it
knows. In a safe environment an agent might view low confidence as an
opportunity to learn more about its world
[*REF*; *REF*]. In a dangerous environment, low
confidence might be a strong indicator to proceed with caution or not at
all. Further, confidence itself can be a goal for an agent&apos;s behavior.
That is, an agent could choose to seek out predictability (high
confidence) or novelty (low confidence). This naturally plays a role in
the trade off between exploration and exploitation.


Concluding remarks: Predictive knowledge is essential to a generally
intelligent agent in maximizing its reward. We advocate that internal
measures relating to prediction learning can and should also be
represented as GVFs and learned in the same way. These new predictions
provide additional knowledge that enables an agent to improve its
decision making abilities. GVFs present a novel approach to the general
problem of introspection within intelligent agents.