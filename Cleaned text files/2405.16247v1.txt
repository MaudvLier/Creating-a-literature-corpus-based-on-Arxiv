AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning


Introduction 


Recently, autonomous agents based on Large Language Models (LLM), e.g.,
ReAct [*REF*], Reflexion [*REF*], SayCan [*REF*],
WebGPT [*REF*], and Voyager [*REF*], have demonstrated their
potential to complete long-horizon tasks in grounded environments. These
LLM agents operate by generating thoughts and actions that are
executable in the environment. For customized environments, such as
robotics [*REF*; *REF*; *REF*; *REF*] and
games [*REF*; *REF*; *REF*], prior methods provide detailed
instructions and in-context examples to familiarize LLM with action
functions (API) and the target environment. However, unlike these
agents, humans can autonomously build and update their understanding of
an unfamiliar environment through dynamic interaction.


Several existing methods enable LLM agents to reflect on
feedback [*REF*; *REF*] or save successful experiences as
skills [*REF*; *REF*; *REF*] to enhance the performance and
reduce the reliance on human-provided examples. However, these
reflections and skills have not been well exploited to foster a deeper
understanding of the environment. As a result, directly using saved
skills as in-context examples can lead to the Path Dependence
problem, i.e., the agent blindly replicates the paths of previous
successes, failing to appropriately adapt to new scenarios. Such
problems are more severe in real-world situations characterized by high
variability.


A previous work, ExpeL [*REF*], gathers the trajectories of LLM agents
and extracts cross-task rules from them. However, these rules are
extracted offline, making ExpeL suffer from the same distributional
shift problem as Offline RL [*REF*]. Meanwhile, due to the
simplicity of rule management, its rules are always armchair general and
unhelpful for the Path Dependency problem. In this paper, we propose a
novel framework called AutoManual to build a well-organized
understanding of the environment that can guide multi-task planning
effectively. AutoManual leverages a dynamic rule system that not only
extracts valuable experience, including skills and reflections, into
different types of rules but also allows for continuously updating these
rules in response to new situations. Additionally, error-prone details
are explicitly described in the rules to improve the robustness of
planning.


AutoManual follows two alternating iterative processes to optimize the
rules. First, given the observation and task of an episode, the Planner
agent utilizes currently discovered rules to write free-form code as an
actionable plan. The interaction between the environment and the Planner
will loop until the episode ends. Second, based on this trajectory, the
Builder agent will update relevant rules through the rule system. This
online updating mechanism can timely verify whether the rules have
deviations and are applicable to the Planner. After rules optimization,
the Formulator agent categorizes these rules according to their
application scenarios and compiles a comprehensive manual in Markdown
format.


The challenge lies in enabling the Builder to accurately extract
applicable rules from a long trajectory, as LLM are prone to generating
hallucinations. To address this, we employ a case-conditioned
prompting strategy, which directs the Builder to focus on specific
rules according to the case of the trajectory. For example, if errors
occurred in a trajectory, the Builder is first asked to determine which
caused the error: an unrecorded situation occurred, or the Planner
failed to follow existing rules. Based on this answer, the Builder will
be given corresponding prompts to update relevant rules.


To summarize, our contributions are the following:
-  We adopt free-form code as the way for the Planner agent to interact
with the environment. We introduce a structured rule system that
allows the Builder agent to manage multiple types of knowledge from
these code-based interactions.
-  We propose an alternating process between the Planner and Builder
agents to optimize rules in an online manner and resolve the Path
Dependency problem. To improve readability, the Formulator agent is
introduced to reorganize and formalize the rules into a Markdown
manual.
-  To facilitate rule management, we employ a case-conditioned
prompting strategy, which guides the Builder to manage specific
types of rules for different trajectory cases.
-  Starting from a single demonstration, AutoManual can generate
detailed instruction manuals for complex environments like ALFWorld
and MiniWoB++. These manuals allow LLM agents to achieve remarkable
success rates of 97.4% with GPT-4-turbo and 86.2% with GPT-3.5-turbo
on ALFWorld, 98.3% with GPT-4-turbo and 92.7% with GPT-3.5-turbo on
MiniWoB++.


Related Works 


LLM for Agents Planning


Large Language Models (LLM) exhibit powerful reasoning and planning
capabilities [*REF*; *REF*; *REF*; *REF*; *REF*] while requiring much
fewer demonstrations than traditional learning methods. With this
planning capability as the core, LLM agents are being developed for use
in robotics [*REF*; *REF*; *REF*; *REF* -Planner; *REF*],
game-playing [*REF*; *REF*; *REF*; *REF*], software
development [*REF*; *REF*], and other fields [*REF*]. Prior
studies [*REF*; *REF*; *REF*] allow agents to adjust actions
or plans based on environmental feedback to improve planning
performance. Given the powerful programming capability of LLM, several
works, e.g., CodeAsPolicy [*REF*], ProgPrompt [*REF*] and
AdaPlanner [*REF*], propose to use Python code as the plan of LLM
agents. This form of output can automatically respond to in-plan
feedback and achieve better performance than the action and JSON
format [*REF*; *REF*].


Self-improvement of LLM Agents


Embodied agent research has long sought to enable agents to self-improve
through interactive experiences. Unlike traditional learning-based
agents that require extensive iterations for optimization,
Reflexion [*REF*] allows LLM agents to reflect on previous failures
and quickly improve their plans. Some works [*REF*; *REF*; *REF*]
combine tree search with reflection to deliberately seek a better
solution. Apart from failure experiences, prior
studies [*REF*; *REF*; *REF*] utilize successful experiences as
skills to assist future planning. Voyager [*REF*] stores generated
and verified programs into the skill library as a new skill for more
complex tasks. AdaPlanner [*REF*] also discovers and archives
successful programs into skill memory for future similar tasks. However,
these methods stop updating skills after storing them, which inevitably
leads to the Path Dependency problem.


Memory Management of LLM Agents


For LLM agents, learning from past experiences can also be viewed as
managing the episodic memory [*REF*]. CLIN [*REF*] proposes to keep
updating a memory centered on causal abstractions for new trials.
Retrieval-Augmented Planning (RAP) [*REF*] retrieves past experiences
corresponding to the current situation. MemGPT [*REF*] allows LLM to
select content to retain in working memory and to search for information
in long-term memory. Generative Agents [*REF*] retrieve memories
based on recency, importance, and relevance to the current situation.
Generative Agents also generate tree-structured reflections, but they
focus on a continuous scenario rather than task-oriented rules.


LLM for Rule Discovery


Several recent works also investigate the rule discovery capabilities of
LLM. Zhu et al. [*REF*] propose Hypotheses-to-Theories (HtT), enabling
LLM to induce and deduce rules for basic reasoning tasks. For LLM
agents, ExpeL [*REF*] gathers the trajectories of Reflexion agents and
extracts cross-task rules from them. Furthermore, AutoGuide [*REF*]
generates state-aware rules and retrieves rules relevant to the
test-time state. Unlike ExpeL and AutoGuide, which extract rules from
offline experiences, we update rules in an online manner, verifying
their reliability and applicability. For more discussion of differences,
refer to Appendix C.


Methods 


AutoManual Overview


Our AutoManual framework, shown in
Fig 1, consists of three main stages. Building stage: The Planner agent and Builder
agent collaborate to build rules from the interactive environment. The
Consolidator agent merges or deletes redundant rules when the rules
exceed the maximum rule number. Formulating stage: The Formulator
agent categorizes the rules, summarizes the key points, and formulates
them into a manual in Markdown form. Testing stage: Based on the
generated manual, a test-time Planner agent will be evaluated through
test tasks and scenarios.


Formally, an Interactive Environment can be modeled as a Partially
Observable Markov Decision Process (POMDP): *MATH*. At
the start of each episode, a scenario *MATH* will be
initialized, a text-grounded task *MATH* and the initial
observation *MATH* (processed into textual form) will be
given. The environment can be interacted with through permissible
actions (API) set *MATH*. After executing an action
*MATH*, the environment will return the result of the action
and the new observation *MATH* based on the dynamics
*MATH* and *MATH*. Finally, when the episode is
done, a binary reward *MATH* indicating the failure or success
of the task will be returned.


FIGURE


We approach the learning of environmental rules as an optimization
problem: *MATH* where
*MATH* denotes all rules in our rule system, *MATH* 
denotes the policy of the Planner given the current rules *MATH* and
*MATH* denotes a trajectory of *MATH* starting from
*MATH*. Classic policy gradient methods [*REF*] solve such
problems through stochastic gradient ascent, i.e., executing the current
policy to obtain the episodic reward and back-propagating gradients to
update the parameters.


Inspired by this online reinforcement learning paradigm, we follow two
alternative processes to optimize the rules *MATH*: 1. The Planner
practices the current rules through interaction within an episode. 2.
The Builder updates the rules *MATH* based on this trajectory.
Compared to traditional parameter optimization, sample-inefficient
gradient ascent is replaced by text-based rule management. We design a
well-structured rule system described in
Section [3.3] to ensure the rule updating contributes
to rewards. Additionally, to limit the role of human expertise, we only
provide a simple example demonstrating the output format to agents. We
derive initial rules from this example as the starting point of the
optimization.


Planner Agent for Interactive Planning 


As demonstrated by the success of Voyager [*REF*] and
AdaPlanner [*REF*], code-based planning can leverage the powerful
programming capability of LLM and automatically react to in-plan
feedback. Voyager and AdaPlanner output and refine a complete solution
function for the task, which is potentially reusable. However, this
function-form output is difficult to adjust in response to environmental
feedback, as it requires maintaining the integrity of the plan
throughout.


Our Planner Agent outputs free-form code as its plan, which aligns more
with the natural programming capabilities of LLM [*REF*; *REF*].
This form simplifies planning by only generating code necessary for the
current environmental situation and feedback without the overhead of
integrating previously executed code. As shown in
Fig 2, at the start of a new episode, the Planner receives system prompts, current
rules *MATH*, relevant samples from the skill and reflection
libraries, the target task *MATH*, and initial observation *MATH*. System
prompts contain the role, permissible actions *MATH*, response
guidelines, and a simple example (detailed in Appendix H). The output of
the Planner is structured into four segments during each cycle:
1. Analysis: The understanding of the current situation and
reflection on previous errors if exist.
2. Related Rules: Rules (along with their IDs) that need to be
considered in this situation.
3. Overall Plan: The general plan to complete the task.
4. Free-Form Code: A block of Python code divided into steps. The
Planner is encouraged to define helpful functions in the code, which
might be reusable in similar scenarios.


We denote this response of the Planner as *MATH*, where *MATH* 
denotes the first three segments. *MATH* executed in the
environment is followed by feedback *MATH*, which informs the subsequent
output cycle. This process iterates until the episode ends or a response
limit is reached.


As shown in Fig 2, according to the episodic reward, we categorize the
result into Direct Success, Indirect Success (errors occurred
but were solved later), and Failure. In the case of Direct or
Indirect Success, the Planner will be prompted to organize its previous
code into a code block. For Indirect Success, it additionally summarizes
the mistakes and misunderstandings that cause errors. For the Failure
case, the Planner will be prompted to reflect on the reason for the
failure carefully, suggest reasonable corrections, and specify the code
segment that caused the error. We denote this response of the Planner as
*MATH*. Finally, we obtain a trajectory of the Planner: *MATH*. 


FIGURE 


Skill Library and Reflection Library: Apart from rules, we also
manage and transmit conclusions from previous episodes, which provide
essential details for generating planning code. In the case of Direct or
Indirect Success, we save the code block in conclusion as a skill for
that task type [^1] into the skill library [*REF*; *REF*]. In
the Failure case, we save its conclusion as a reflection for that task
type into the reflection library. When a new task comes, the code block
of the most similar task is retrieved from the skill library. If there
is no existing skill for the new task type, the reflection for that task
type will be returned. As mentioned in the Introduction, compared with
rules, these skills and reflections contain more programming details but
are less generalizable to new scenarios. Considering this Path
Dependence problem, we prompt the Planner that the rules should be
prioritized.


Cooperation between Agents: In our framework, rule management is not
solely the responsibility of the Builder; the Planner also plays a
critical role by explicitly identifying the rules it engages in its
response. This cooperation is facilitated by including the Planner&apos;s
thoughts within the trajectory *MATH*, which is provided to the Builder.
This synergy enhances the identification and adjustment of problematic
rules. In addition, conclusion from the Planner contains the detailed
success process or reflections on errors, which further assist the
Builder in managing corresponding types of rules.


Builder and Consolidator Agents for Rule Management 


Upon receiving the trajectory *MATH*, the Builder has to manage
the rules through the rule system.


FIGURE


Rule System: We intuitively identify rules as the kinds of knowledge
that help task completion, including the analyses of the observed
phenomenon *MATH*, the mechanism *MATH*, and the correlation
between the reward *MATH* and *MATH*, i.e., the success process or
the occurred error. Therefore, unlike ExpeL [*REF*] and
AutoGuide [*REF*], which derive general insight from the
trajectory, our system categorizes six specific rule types to extract
environmental knowledge that targets different aspects of the
trajectory. Furthermore, each rule in our system is enhanced with an
\&quot;Example\&quot; attribute to illustrate its application and important
details, making it grounded and well-understood. Specifically, each rule
in the rule system has these four attributes:
1. Rule Type: The type of the rule, options include &quot;Special
Phenomenon&quot;, &quot;Special Mechanism&quot;, &quot;Success Process&quot;, &quot;Useful Helper
Method&quot;, &quot;Corrected Error&quot; and &quot;Unsolved Error&quot;;
2. Rule Content: A description of the rule, beginning with the
scope of its applicable scenarios;
3. Example: An example or code from the trajectory demonstrates
this rule, where additional remarks, e.g. error-prone details, can
also be added to it;
4. Validation Logs: Logs that track the rule&apos;s application and
updates, including episode and rule IDs that trace the rule&apos;s
evolution, serving as a reference for the Builder and Consolidator.


The Builder manages the rules through the following functions of the
rule system:
-  write_rule(\&#13;ule_attributes): Write down a new rule with its
four attributes.
-  update_rule(rule_id, \&#13;ule_attributes): Rewrite the attributes
of a existing rule.
-  stop_generating(): When the trajectory is not needed or
insufficient to derive any more new rules, the function should be
called.


Similar to hierarchical reflections in Generative
Agents [*REF*], we allow the Builder to utilize existing rules
to induce more general or deeper rules and record their dependence in
Rule Content or Validation Logs, more discussed in Appendix D.


Case-Conditioned Prompting: To mitigate the risk of erroneous rule
creation, such as deriving rules of success from a failed trajectory, we
employ case-conditioned prompts. As illustrated in
Fig 3, the Builder first analyzes and determines if the major errors stem from
&quot;Imperfect Rules&quot; or &quot;Imperfect Agent&quot;. Based on this analysis and the
trajectory results, targeted prompts guide the Builder in rule
management [^2]. For example, in a case of indirect success due to
imperfect rules (Case *MATH* ), the prompts will guide the Builder to
extract or update the success process, helper methods, and error
reflections in corresponding rule types. Finally, the Builder responds
with the potential rules detailing their relation with existing rules
and uses the functions of the rule system to manage rules.


Rule Consolidation: When the number of rules in the rule system
exceeds *MATH*, the Consolidator agent steps in to consolidate
related rules and delete redundant rules. It uses three functions of the
rule system: get_trajectory(episode_id), update_rule(rule_id,
\&#13;ule_attributes) and delete_rule(rule_id). Given the current
rules, the Consolidator identifies potentially relevant or overlapped
rules, uses get_trajectory function to investigate the trajectories
they depend on, and finally calls the remaining functions to manage the
rules. During the management, the Consolidator ensures that
consolidation retains details of rules and examples.


Manual Formulation


Once the building stage is complete, we can obtain a set of rules
targeted to different situations. Although these rules have been
validated through online optimization to ensure their applicability, our
next goal is to enhance their readability and global understanding. To
achieve this, we introduce the Formulator agent, designed to transform
these rules into a user-friendly manual, like a teacher conveys complex
subjects through easily digestible lessons. As depicted in
Fig 1, the Formulator begins by categorizing all rules based on their target
scenarios. This categorization aids in structuring the manual and
ensures that related rules are discussed together, which enhances the
logical flow and accessibility of the information. For each category,
the Formulator drafts an introduction, summarizing the rules it contains
and highlighting the key points and overall principles that govern the
specific scenarios. Finally, the Formulator compiles the rules and their
introductions into a comprehensive manual formatted in Markdown.


Experiments 


In line with AdaPlanner [*REF*], we conduct the experiments on two
interactive environments: (1) ALFWorld [*REF*] is a text-based
virtual household environment containing six distinct task types. We run
the building stage on 36 tasks (6 tasks for each task type) sampled from
the training set of ALFWorld, and each task is run only once. Following
previous works [*REF*; *REF*; *REF*], we run the testing
stage on the validation unseen set containing 134 tasks across these six
types. (2) MiniWoB++ [*REF*] is a simulated web environment where
agents complete diverse tasks on the Internet by performing keyboard and
mouse actions. Prior works [*REF*; *REF*] selects 9 task types with
environmental feedback and 44 task types without feedback from MiniWoB++
tasks. We perform experiments on 9 task types with feedback and all 53
task types. At each stage, we randomly sample 6 tasks for each task
type.


During building and formulating stages, we use GPT-4-turbo
(&apos;gpt-4-1106-preview&apos;) as the LLM for all agents. At the testing stage,
we equip the Planner agent with GPT-4-turbo or GPT-3.5-turbo
(&apos;gpt-3.5-turbo-1106&apos;), which can evaluate the effect of the generated
manual on relatively smaller LLM. More details of the implementation and
prompts for AutoManual can be found in the Appendix.


Compared Methods: In the experiments, we compare AutoManual with the
following methods of LLM Agent: (1) ReAct [*REF*] prompts LLM to
generate the reasoning trace using CoT [*REF*] and next-step action; (2)
Reflexion [*REF*] agents generate reflection on task feedback
signals, which is saved in the memory for subsequent trials; (3)
ExpeL [*REF*] extract insights and skills from the offline
trajectories of Reflexion agents; (4) RCI [*REF*] agent recursively
criticizes and improves its output for solving computer tasks; (5)
AdaPlanner [*REF*] allows the LLM agent to generate and
adaptively refine a code-style plan; (6) Planner+Lib. represents our
Planner agent equipped with skill and reflection libraries
(§[3.2]) during building and testing stages
without any rules. We re-implement prior methods with GPT-3.5 and GPT-4
versions the same as ours for fair comparisons.


ReAct, Reflexion, and ExpeL provide LLM agents with *MATH* human examples
(*MATH* examples per task type) of ALFWorld. For AdaPlanner, they provide 6
human examples (*MATH* example per task type) of ALFWorld as the start of
skill discovery. For our methods, agents are provided only one human
example of the simplest task (&apos;Put&apos;) on ALFWorld. On MiniWob++, our
agents are provided one human example (&apos;search-engine&apos;) for tasks with
feedback and 4 examples for all tasks. Reflexion agents are allowed to
try at most *MATH* trials for each task. For AdaPlanner and our methods, we
allow the Planner agent to replan at most *MATH* times in response to the
environmental feedback. To reduce randomness, we performed each
experiment three times and reported the average.


Main Results


Main Results on ALFWorld: As shown in
Tab. 1, AutoManual significantly outperforms the existing methods, evidenced by
overall success rates of 86.2% when using GPT-3.5-turbo for the testing
stage and 97.4% when using GPT-4-turbo. Noticeably, AutoManual requires
little expert prior knowledge about the environment and is only provided
with one human example to achieve excellent results. In comparison, the
rules induced by ExpeL hardly improve performance, as its offline
trajectories are composed of individual actions rather than code. We
find the performance of AdaPlanner is lower than reported. One reason is
that AdaPlanner requires LLM to output specific formats to complete its
function-form code, which is difficult for creative LLM, e.g.,
GPT-4-turbo. In addition, AdaPlanner and Planner+Lib. are inferior to
AutoManual because they only store successful paths as skills and
inevitably face the Path Dependence problem. Especially, tasks in Put
Two have various scenarios, such as &quot;two objects can occur at the same
receptacle or different receptacles&quot;, that require different processes
to solve (Appendix G shows an example). Furthermore, Planner+Lib. often
does not mark error-prone points in its skills, such as &quot;target objects
may appear in unconventional locations&quot;.


FIGURE


Main Results on MiniWoB++: As shown in
Tab. 2, the performance of AutoManual exceeds the previous methods and Planner+Lib.
by a large margin. Especially in 9 task types with feedback, these tasks
have higher diversity and require LLM agents to cope with various
situations. For example, the tasks in login-user-popup type will
interrupt the agent&apos;s plan at any time, requiring the agent to cope with
unexpected situations. Therefore, solely imitating previous successful
experiences without extracting targeted rules will lead to task failure.
Additionally, due to the flexibility of free-form codes, our method
shows better adaptability while requiring fewer expert examples than
prior methods.


Learning Curves. We show the success rate curves (testing with
GPT-4-turbo or GPT-3.5-turbo) when gradually increasing the tasks of the
building stage in Fig [1](fig4).
In the left image, we share rules across all task types (Cross-task
Type), as in AutoManual, or each task type builds a separate set of
rules (Single-task Type) during the building stage.
Fig [1](fig4) (a) demonstrates
that sharing rules across task types can facilitate rule optimization.
The rules for each task type deepen understanding of the environment,
thereby boosting the planning of other tasks. In
Fig [1](fig4) (b), we compare
AutoManual and Planner+Lib. on 9 tasks with feedback in MiniWob++. We
find that Planner+Lib. tends to get stuck with a lower success rate. In
the face of highly diverse scenarios, Skill Library cannot express the
rules behind the environment, thus falling into the Path Dependency
problem.


TABLE


In this ablation study, we quantify the impact of each core component of
the AutoManual framework on performance, specifically focusing on
success rates and error reduction during task execution. Since we
allowed the Planner to replan up to 3 times, each task could have up to
4 error steps.


Online v.s. Offline Rule Management: We perform offline AutoManual
by collecting all trajectories and then managing rules from them. As
Tab [1](tab3) shows, without
online rule management, the generated manual can only slightly improve
planning (from 88.1% to 90.7%). This is because more mundane mistakes
and fewer direct successes will occur in the trajectories (the
distributional shift problem), and the rules cannot be verified by
feedback from the environment.


Skill&amp;Reflection Libraries: Retrieving historical conclusions is
essential for correct planning, as they record massive interacting
details that can complement the rules. Without them, there will be more
errors in the details, and the success rate drops from 97.4% to 89.5%.
However, as discussed previously, using plain experiences without
inducing rules will lead to Path Dependency.


Case-Conditional Prompts: This strategy further improves the rule
management process by reducing the hallucination, as evidenced by an
increase in success rate from 93.8% to 97.4%. These prompts ensure that
the Builder updates rules reasonably and grounded.


Effect of Manual Formulation: The final formulation of rules into a
comprehensive manual contributed to the success rate of 97.4% and
decreased average error steps, demonstrating the effectiveness of
presenting rule-based knowledge in an organized and accessible format.
It not only aids the Planner in mastering multiple rules but is also
friendly for human reading.


Conclusion 


In this paper, we introduce AutoManual, a framework significantly
advancing LLM agents by enabling adaptability and continual learning
through online rule optimization. Utilizing the structured rule system,
AutoManual autonomously generates comprehensive manuals, achieving high
success rates in benchmarks like ALFWorld and MiniWoB++. This approach
reduces reliance on human-provided examples and expert interventions,
illustrating a robust method for enhancing agent generalization and
addressing the Path Dependency problem in diverse environments.


Limitations


Despite the significant contributions of the AutoManual framework,
several limitations warrant further discussion. First, our method
heavily relies on the capabilities of GPT-4-turbo to generate reliable
rules, which may limit the framework&apos;s applicability with less advanced
language models.


Secondly, the current implementation places all rules directly within
the context of LLM, which, while effective in smaller or moderately
complex environments, may not scale well to larger, more dynamic
settings. For such expansive environments, integrating the rule system
with Retrieval-Augmented Generation (RAG) techniques, similar to the
approach taken by AutoGuide, could enhance performance by dynamically
selecting relevant rules based on the context, thereby managing the
cognitive load on the LLM more efficiently.


Lastly, there remains a challenge in ensuring that the Planner agent
consistently adheres to the rules outlined in the manual. In practice,
the Planner may sometimes ignore these rules and cannot generate the
ideal solution code that can be applied to different situations. This
indicates a need for additional mechanisms to enforce or verify rule
compliance during operations. This issue underscores the potential
necessity for developing more sophisticated methods to ensure rule
adherence or to introduce more robust validation steps within the
planning process.


Broader Impacts


The AutoManual framework, leveraging LLM agents, presents positive and
negative impacts on safety. On the positive side, by autonomously
generating reliable manuals, our method enhances the predictability and
reliability of LLM behaviors in dynamic environments, potentially
reducing errors and increasing operational safety. However, relying on
LLMs also introduces risks of unpredictable behaviors when agents
encounter unanticipated scenarios or when rule adherence is not fully
ensured.


Furthermore, the manuals generated by our method can be invaluable tools
for human workers. They encapsulate a distilled form of
interaction-based learning that can aid in training, provide decision
support, and improve task efficiency in various domains. This can not
only enhance productivity but also ensure that human workers are better
informed and prepared to manage the complex systems with which they
interact.


Lastly, AutoManual&apos;s ability to generate structured, context-aware
manuals from interactive experiences suggests a promising avenue for
constructing comprehensive knowledge bases for AI. These manuals can
contribute to a global knowledge base shared by LLMs of different sizes,
promoting broader AI research and development. It offers a method to
systematically organize and retrieve complex interaction data in a way
that is accessible and useful for both machines and humans.