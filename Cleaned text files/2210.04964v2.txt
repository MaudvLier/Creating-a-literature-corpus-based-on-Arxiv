Generating Executable Action Plans with Environmentally-Aware Language Models 


INTRODUCTION


Recent work in the natural language processing (NLP) and machine learning (ML) communities has made tremendous breakthroughs in several core aspects of computational linguistics and language modeling driven by advances in deep learning, data, hardware, and techniques. These advancements have led to the release of pretrained large (million and billion+ parameter) language models (LLMs) that have achieved the state-of-the-art across a variety of tasks such as text classification, generation, and summarization, question answering, and machine translation, that demonstrate some abilities to meaningfully understand the real world [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
LLMs also demonstrate cross-domain and cross-modal generalizations, such as retrieving videos from text, visual question answering and task planning [*REF*]. In particular, recent works have explored using LLMs to convert high-level natural language commands to actionable steps (e.g. &quot;bring water&quot; *MATH* &quot;grab glass&quot;, &quot;fill glass with water&quot;, &quot;walk to table&quot;, &quot;put glass on table&quot;) for intelligent agents [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Trained on diverse and extensive data, LLMs have the distinct ability to form action plans for varied high-level tasks.


FIGURE 1


While promising, the action steps generated by LLMs in prior work are not always executable by a robot platform. For instance, for a task &quot;clean the room&quot; a LLM might generate an output &quot;call cleaning agency on phone&quot;; while being correct, this action plan might not be executable since the agent might not grasp the concept of &quot;call&quot; or have the &quot;phone&quot; object in it&apos;s environment. This limitation arises because LLMs are trained solely on large text corpora and have essentially never had any interaction with an embodied environment. As a result, the action steps they generate lack context on the robot&apos;s surroundings and capabilities.


To address this issue, prior works have explored grounding LLMs by fine-tuning models using human interactions [*REF*; *REF*; *REF*] or training models for downstream tasks using pretrained LLMs as frozen backbones [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
However, these methods often require training on extensive annotated data, which can be expensive or infeasible to obtain, or can lead to loss of generalized knowledge from the LLM. Instead, recent research has investigated biasing LLM output without altering their weights by using prompt engineering [*REF*; *REF*; *REF*] or constraining LLM output to a corpus of available action steps defined a priori that are known to be within a robot&apos;s capabilities [*REF*; *REF*]. This line of research focuses on methods that can utilise the capabilities of LLMs while preserving their generality and with substantially less additional annotated data.


While these systems effectively perform common sense grounding by extracting knowledge from an LLM, they employ a one-fits-all approach without considering the variations possible in the actionable environment. As a result, executing the action plans generated by these systems either requires approximations to the agent&apos;s environment or time-consuming and costly pretraining to generate an affordance score to determine the probability that an action will succeed or produce a favourable outcome towards task completion, given the current agent and environment states. Additionally, since prior systems are environment agnostic, it is not possible to use them to generate executable action plans for tasks requiring object disambiguation. For example, to generate correct action plans for tasks that require interaction with multiple objects with the same name, the system needs to be able to distinguish among object instances.


We propose a novel method to address these issues while generating low-level action plans from high-level task specifications. Our approach is an extension to Huang et al., 2022 [*REF*]. From an Example set (see §[4]) using the ActivityPrograms knowledge base collected by Puig et al., 2018 [*REF*], we sample an example similar to the query task and environment and use it to design a prompt for a LLM (details of which are given in §[3.1]). We then use the LLM to autoregressively generate candidates for each action step. To rank the generated candidates, we design multiple scores for the actions and their associated objects (see §[3.2] and §[3.3]). After the top candidate is selected, we append it to the action plan and repeat the process until the entire action plan is generated.


To evaluate our action plans, we use the recently released VirtualHome interface [*REF*] (Figure 1 shows a visualization of an example action plan running in VirtualHome).
We use several metrics (details in §[4.1]), including executability, Longest Common Sub-sequence (LCS), and final graph correctness to autonomously test generated action plans on VirtualHome. Overall, we found that our method increased action plan executability and correctness by 310% and 147% respectively over a state-of-the-art baseline.


RELATED WORK


Our work builds upon recent efforts in robotics to leverage the potential of LLMs. For instance, researchers are beginning to explore LLMs in the context of applying commonsense reasoning to natural language instructions [*REF*], providing robotic agents with zero-shot action plans [*REF*], and supplying high-level semantic knowledge about robot tasks [*REF*]. Below, we review related research in task planning, LLMs, and action plan grounding.


Task Planning


The problem of task planning involves generating a series of steps to accomplish a goal in a constrained environment. Historically, this problem has been widely studied in robotics [*REF*; *REF*; *REF*], with most approaches solving it by optimizing the generated plan, given environment constraints, [*REF*; *REF*] and using symbolic planning [*REF*; *REF*]. Recently, machine learning methods have been employed to relax the constraints on the environment and allow higher-level task specifications by leveraging techniques such as reinforcement learning or graph learning to learn task hierarchy [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
However, most of these methods require extensive training from demonstrations, or explicitly encoded environmental knowledge and may not generalize to unseen environments and tasks. The use of LLMs, which encapsulate generalized world knowledge, may help plan for novel tasks and new environments.


Large Language Models


Large language models (LLMs) are language models, usually inspired by the transformer architecture [*REF*], tens of gigabytes in size and trained on enormous amounts of unstructured text data. Recent advances in the field of natural language processing have shown that LLMs are useful for several downstream applications including interactive dialogue, essay generation, creating websites from text descriptions, automatic code completion, etc.
[*REF*; *REF*; *REF*; *REF*].
During their pretraining, LLMs can accumulate diverse and extensive knowledge [*REF*; *REF*; *REF*] that enables their use in applications beyond NLP, such as retrieving visual features [*REF*] and solving mathematical problems [*REF*; *REF*] or as pretrained models for other modalities [*REF*; *REF*]. In robotics, knowledge embedded in LLMs can be utilised to generate actionable plans for agents from high-level queries. However, in order for a plan to be executable by a robot, the outputs from the LLMs need to be grounded in the context of the robot&apos;s environment and capabilities.


Grounding Natural Language in Action Plans


There has been considerable work towards grounding natural language in actionable steps. Prior research has focused on parsing natural language or analysing it as series of lexical tokens to remove ambiguity and map language commands to admissible actions [*REF*; *REF*; *REF*; *REF*].
However, these methods usually require extensive, manually coded rules and thus fail to generalize to novel environments and tasks. More relevant to our approach, recent work has explored grounding language models using additional environment elements [*REF*; *REF*; *REF*; *REF*; *REF*].
Techniques include prompting [*REF*; *REF*] and constraining language model outputs to admissible actions [*REF*; *REF*; *REF*; *REF*].
To also ground the output of language models in the environment of the agent, prior works have tried using LLMs as fixed backbones, [*REF*; *REF*; *REF*; *REF*; *REF* -etal-2021-piglet; *REF*; *REF*; *REF*] fine-tuning or ranking model outputs through interactions with the environment [*REF*; *REF*; *REF*]. Our work extends such approaches, where we use additional inputs from the environment (i.e., objects and their properties) to condition the model output without any fine-tuning of the LLM or extra training to learn value functions for ranking LLM outputs.


APPROACH 


In this section, we discuss our proposed method to generate directly executable action plans from high-level tasks (Figure 2 provides a visual overview). Motivated by Huang et al., 2022 [*REF*], our approach uses two language models, a planning LM (*MATH*) to generate the action plan and calculate a score for the similarity of an object with the other objects associated with the action plan; and a translation LM (*MATH*) to calculate embeddings for objects and actions.


FIGURE 2


LLM Action Plan Prompt Generation 


Large language models have the ability to learn from context during inference, i.e., when autoregressively sampled, LLMs can generate meaningful text to complete or extend a given textual prompt [*REF*]. We leverage this capability in designing prompts for LLM sampling that generate action plans. Specifically, we select an example from an Example set of task and action plans synthesized from the ActivityPrograms dataset (see §[4]) and construct a prompt for the LLM by prepending the example task and action plan to the current task.


We dynamically select the example during inference to design a prompt similar to the query. As in Huang et al., 2022 [*REF*], we use the query task to select the example. However, one of our novel extensions is to also use the environment associated with the query to construct a prompt, keeping in mind the objects (and their states) the agent can currently interact with. For a query (*MATH*) with task &quot;Play video games&quot;, an example (*MATH*) with task &quot;play board games&quot; may be chosen considering just the task similarity. However, another example (*MATH*) with task &quot;Use the computer&quot; may be more relevant because the action plan for both *MATH* and *MATH* would have actions involving similar objects, such as &quot;switch on computer&quot;, &quot;type on keyboard&quot;, &quot;push mouse&quot;, etc., which may not be present in the action plan for *MATH*.
Considering the environment in selecting the example may also help in disambiguating between examples with high task similarity but different objects. For example, a &quot;clean room&quot; action plan, which uses a rag, and a &quot;clean floor&quot; plan, which uses a mop, may both have high task similarity to a &quot;clean the house&quot; query task. Considering the objects present in the environment (*MATH*) of the query (*MATH*) (e.g., a rag is present, but not a mop) can help determine the better example.


We start by selecting *MATH* examples that have tasks *MATH* similar to the task *MATH* of the query *MATH*. Here *MATH* is a hyperparameter. We use the cosine similarity (*MATH*) of task embeddings to calculate task similarity given by: *MATH*. We then compare the environments *MATH* of the selected examples with the environment (*MATH*) of the query (*MATH*). An environment from a sample in our dataset is structured like a graph, with the graph nodes representing the available objects. The nodes also have information about object properties (eg.
grabbable, openable, movable, etc.) and the current states of the objects (eg. clean, closed, etc.). The edges in the graph represent the relations between objects (eg. inside, on, facing, close to, etc.). We calculate the environment similarity as the mean of the intersection over unions of the nodes and edges respectively: *MATH*. Finally, from the selected *MATH* examples, we select one example *MATH* that maximises the example score given by: *MATH* where *MATH* is a hyperparameter.
With the example task *MATH*, action plan *MATH*, and query task *MATH*, we form a prompt (*MATH*) for generating the action plan and a set of objects (*MATH*) associated with the actions plan *MATH*. We use *MATH* to calculate the similarity scores between any new objects and the objects already associated with the action plan (See §[3.3]).


Action Step Generation 


As in Huang et al., 2022 [*REF*], we sample the *MATH*  multiple times using prompt *MATH* to get *MATH* samples for each action step, and the LLM generation probability associated with each sample step (*MATH*). *MATH* gives a score for how relevant the planner LM thinks the sample is to the current action plan and prompt. However, since the output of the language model is unconstrained, it can include infeasible steps that the agent cannot actually execute. To make sure the actions generated are executable, we map each sample to its closest admissible action step that maximises the action matching score given by: *MATH*. Where *MATH*. *MATH* is the corpus of all admissible atomic action steps, constructed by matching every possible action with every known object.


Object Association 


Each action step discussed so far is of the format ’[Action] &lt;Object names&gt;’. For an agent to execute an action step, it needs to associate the object names in the step with objects in the environment. However, action plans generated without considering the environment often contain object names that cannot be directly mapped to objects present in the environment, making the plan not executable.


We propose a way to autonomously associate objects in the query environment with action steps during action plan generation, using only a list of the objects present in the environment and their locations, without the use of any hard-coded information from the environment. As a result, our action plans can be executed directly in any query environment. Additionally, since our action plans consider the agent&apos;s environment, we are able to generate action plans for tasks that were previously infeasible. For example, a task &quot;set the table&quot; has an action plan with steps &quot;find a plate&quot;, &quot;put plate on table&quot;, &quot;grab cup&quot;, &quot;put cup on table&quot;, etc. repeated multiple times, each time for different &quot;plate&quot; and &quot;cup&quot; objects. A system that does not have any scene information may generate an action plan that has these action steps repeated in a correct order, but map all &quot;cup&quot; objects to the same &quot;cup&quot; and all &quot;plate&quot; objects to the same &quot;plate&quot; during execution, creating an incorrect final result. Since our method can distinguish among the &quot;cup&quot; and &quot;plate&quot; objects in the scene, we can associate different objects of the same name with repeated action steps, leading to correct execution.


Since the admissible action steps for our agent all follow the same schema, we first parse the step to extract the object names (e.g., &quot;put cup on table&quot; has the object names &quot;cup&quot; and &quot;table&quot;). For each extracted object name (*MATH*), we then assign an object from the environment. To do so, we first use a translation LM (*MATH*) to find *MATH*, the closest available object name to *MATH* that is present in the agent&apos;s environment, chosen by maximizing an object matching score given by: *MATH* where *MATH* is the corpus of all object names present in the agent&apos;s current environment. We also calculate an object relevance score that denotes the similarity of *MATH* with the object names in the action plan of the example (*MATH*) and those associated with the previous steps of the generated action plan. For each object name *MATH* in *MATH*, we construct the text &quot; *MATH* and *MATH* are related&quot;. We forward the text through the planning LM (*MATH*) to calculate the associated cross-entropy loss (*MATH*). We compute the object relevance score (*MATH*) as the mean of *MATH* over all objects in *MATH*.


The object matching and relevance scores enable a consideration of environment objects for action plan generation, but will be the same for all objects in the scene with the same name. To disambiguate among such objects, we calculate an object disambiguation score (*MATH*) as being inversely related to mean distance (*MATH*) of the object from the objects that the agent interacted with in the last generated step: *MATH*.


This score prioritizes objects that are near the agent&apos;s location in the prior step. Thus, promoting shorter and more efficient routes for the agent to complete an action step. For some object names, the location is absent in the dataset, and object disambiguation is impossible. In such cases we set *MATH*. However since disambiguation occurs among objects with the same object name, this doesn&apos;t create an unfair bias for objects that have location information present. For each object, we also look for repetitions, i.e., instances where the current (*MATH*, *MATH*) pair is found in the already generated action plan. For each such instance we incur a negative penalty to encourages the agent to interact with a different object with the same name. This negative score builds for every instance of repetition and can also reduce the overall score for the (*MATH*, *MATH*) pair, deterring our action plans from loops of multiple *MATH* (e.g., &quot;walk to table&quot;, &quot;walk to chair&quot; repeated over and over) that prior works suffer from. We assign the object with the greatest *MATH* to *MATH*.


Ranking and Termination 


We take a weighted sum of the scores described above to rank all (*MATH*, *MATH*) pairs: *MATH*.


Where *MATH* and *MATH* are hyperparameter weights for each score. If an action step has multiple objects, we calculate a mean for each of the 3 object scores (*MATH*), over the objects. We then select the highest ranking (*MATH*, *MATH*) pair and append it to the action plan. We also append *MATH* to the prompt for action generation (*MATH*) and *MATH* to the set for object relevance score (*MATH*). If the score for the highest ranking (*MATH*, *MATH*) pair is below a cutoff hyperparameter, we terminate the action plan.


The resulting action plans generated by the language model are in natural language. To execute and evaluate plans, we used the VirtualHome simulation platform (see §[4]), which required parsing this plan to create an action plan matching the VirtualHome agent schema. This parsing was performed using a predefined mapping since all natural language action steps follow a fixed pattern.


EVALUATION 


We evaluated our approach in generating environmentally-aware action plans against plans generated using the method in Huang et al., 2022 [*REF*], the state-of-the-art baseline for a LLM action plan generation system, which does not consider environment information.
We executed all plans using VirtualHome, a multi-agent simulation platform. VirtualHome provides diverse and customizable household environments that support a wide array of possible interactions in the form of atomic action steps. An atomic action step is specified by ’[Action] &lt;Objects&gt; (object_ids)’ (e.g.
’[PutBack] &lt;glass&gt; (2) &lt;sink&gt; (1)’).


For our experiments, we used the ActivityPrograms Knowledge Base released by Puig et al., 2018 [*REF*]. This dataset contains 292 unique high-level household tasks, with 1374 unique action plans and 6201 unique environments in total extracted from VirtualHome, and task and action plan samples manually annotated by Amazon Mechanical Turk workers. Each data point consists of a high-level task, a graphical representation of the agent&apos;s environment, and an action plan consisting of atomic actions. Out of the 292 tasks, 285 tasks have action plans and environments that execute without error in the VirtualHome interface. We performed our experiments on these 285 tasks, randomly split into 3 sets: an Example set of 160 tasks, a Validation set of 25 tasks and a Test set of the remaining 100 tasks.


Metrics 


We evaluate our action plans across three metrics: executability, longest common sequence, and correctness.


Executability measures whether the generated actions follow a logical order and satisfy the constraints of the environment (e.g., action preconditions are met by preceding actions, action plan objects are present in the environment, and object states support planned actions). We executed the action plan step-by-step on the VirtualHome interface and calculated the number of steps that executed without the action plan failing. We report the percentage of steps that executed as the action plan executability score.


Following Puig et al., 2018 [*REF*], we computed Longest Common Sub-sequence (LCS) as the length of the longest common sub-sequence of steps between a generated action plan and a &quot;ground truth&quot; action plan from the ActivityPrograms dataset written by human annotators; divided by the length of the longer action plan. We required that the arrangement of the steps in the sub-sequence remains the same, but allowed gaps between them. LCS provides a metric to understand the number of correct actions being generated and also their short term order, while penalizing longer action plans that have irrelevant or repeated actions. However, it does not judge the correctness of the action plan as a whole because it does not consider the position of the longest common sequence in the action plan. Also, as LCS only compares the natural language action plans, it does not offer a way to judge whether the action plan is able to disambiguate among objects of the same name.


We propose Final Graph Correctness as a new metric to evaluate the final correctness of the environment graph after execution of the action plans. We executed each generated action plan in VirtualHome up to the last executable step and extracted the graph for the resulting environment (*MATH*). We then compared this graph with the graph of the environment formed after executing the ground truth action plan (*MATH*).
We computed the set of nodes and edges that changed in the initial environment (*MATH*) after executing the output and ground truth action plans respectively: *MATH*, *MATH*, *MATH*, *MATH*.
The final graph correctness was calculated as the mean of intersection over unions of the two sets of nodes and edges thus obtained: *MATH*.


Experimental Setup


We evaluated and ablated over the Test set (100 tasks) and used the Example set (160 tasks) for prompt engineering. The Validation set (25 tasks) was used for hyperparameter search. We ran all our experiments on Google Colab using a NVIDIA A100-SXM4-40GB GPU.


In action plans generated by the baseline, objects in action steps were not originally associated with the objects in the environment. For each object in each action step, we randomly selected an object of the same name if it is available, and assigned it to the action step.


We used open-source resources from Hugging Face Transformers [*REF*] and SentenceTransformers [*REF*] for our model choices. Our primary results used ’GPT2-large’ [*REF*] as the Planning LM and ’all-roberta-large-v1’ [*REF*] as the Translation LM.


FIGURE 3


RESULTS


In this section we present our results. The step count is measured as the number of steps and all other metrics reported are a percentage of 100, unless mentioned otherwise. Some action plans generated by our method are illuminated in Figure 3.


Environment Aware Action Plans


We computed the action plans for each of the samples in our Test set, using our method and the baseline [*REF*] that doesn&apos;t use any scene information. As each task can have multiple action plans and associated environments, we conducted 5 runs every time we generated an action plan; in each run we randomly selected an action plan and environment for each task. We report the average results over all the runs. Table [1] reports the mean step count, executability, LCS and final graph correctness for both methods.


TABLE 1


We observed that on average our method produced longer action plans, which allowed the agent to complete more complicated and longer-horizon tasks. Our method, using the information from the scene, generated action plans that were not only more executable but also led to final environment graphs that were closer to the desired results. As seen from the average LCS, using the environment information to select the example and inform action step generation also led to action plans that were closer to the ground truth action plans. However, we found that the extra computation associated with the agent&apos;s environment slowed down our method as compared to the baseline. On average, computing a step with our method took *MATH* seconds, as compared to the *MATH* seconds of the baseline.


These results are promising as our method, which implicitly considers object states and properties in our example selection module, could be readily integrated with a robot perception stack during real deployments.


Ablations


TABLE 2


Ablation of Implementation Choices


We ablated the scores we propose and show results over the test set in Table [2].
Compared to the baseline, we saw our biggest jump in executability and final correctness by incorporating object matching with the action steps and using the object scores (*MATH*) to inform action step ranking. The additional scores also encouraged longer and more complicated action plans. Adding in the disambiguation score (*MATH*) enabled the agent to distinguish between objects with the same name and allowed for more accurate action plans. It also discouraged redundant or repeated actions, thus resulting in shorter and more executable plans.
Finally, since we used the objects in the environment to inform action step selection, also including this information in example selection (*MATH*) boosted results further as it made the resulting prompt more relevant to the current environment of the agent.


TABLE 3


To exactly evaluate the usefulness of object disambiguation, we separately evaluated and compared the performance of the *MATH* action plans which had a (*MATH*, *MATH*) pair repeated in the action plan when object disambiguation was omitted. The results for this ablation are shown in Table [3].


Ablation over Planning LMs


We ablated over different sized Planning LMs (ranging from 117M parameters to 1.5B parameters in size) from two families of models [*REF*; *REF*], while fixing ’all-roberta-large-v1’ as the Translation LM (Table [4]). We observed that within a family of models, the medium sized models (’GPT2-large’, ’OPT-350M’) gave the best results. We found that the small models (’GPT2’, ’OPT-125M’) resulted in shorter action plans that were unable to capture the details required in action steps and ended up generating high-level instructions which were vague and not executable by the agent (e.g. a task &quot;work in office&quot; generated an action plan - &quot;Step 1: Go to office&quot;, &quot;Step 2: work&quot;). On the other hand, the large models (’GPT2-xl’, ’OPT-1.3B’) often generated complicated samples that couldn&apos;t effectively be mapped to any available actions and thus resulted in action plans that were not relevant to the query task (e.g., a task &quot;Shampoo hair&quot; generated a sample &quot;grab a shampoo bottle and get in the shower&quot; which couldn&apos;t be mapped to any atomic action step).


TABLE 4


Ablation over Translation LMs


We also explored using different size models of Sentence BERT and Sentence RoBERTa [*REF*; *REF*; *REF*] for the Translation LM, fixing ’GPT2-large’ as the Planning LM (Table [5]). We found that larger translation LMs (’stsb-bert-large’, ’stsb-roberta-large’, ’all-roberta-large-v1’) created better performing and slightly shorter action plans compared to smaller model variants (’stsb-bert-base’, ’stsb-roberta-base’). We speculate that larger models may create more meaningful embeddings for actions and objects and thus better guide the Planning LM to correct actions; however they are harsher towards planning LM outputs that don&apos;t effectively match any atomic action step and thus caused the action plans to terminate faster.


TABLE 5


FUTURE WORK &amp; CONCLUSIONS


In this paper, we propose a method to condition large language models on the information contained in an agent&apos;s environment to generate environmentally-aware action plans from high-level tasks. We propose multiple scores to rank the outputs of LLMs and ground them in the agent&apos;s surroundings. We discuss the performance of our generated action plans for complex and diverse tasks on the VirtualHome interface. While our results demonstrate improved performance in terms of plan executability and correctness over the state-of-the-art baseline, there are several areas for further improvements. For example, our approach makes implicit use of various object properties and states when selecting examples for prompt generation, but cannot make use of this information directly during plan generation. Future research might explore how to further improve plan executability by addressing this limitation. In addition, future work is needed to validate our approach in a real-world robot deployment, beyond the VirtualHome simulator, where object information can be derived from a robot perception and affordance reasoning stack. We hope this work spurs further investigations into how robotics may leverage LLMs in dynamic environments.