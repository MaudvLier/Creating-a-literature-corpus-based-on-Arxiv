Introduction


FIGURE


Human life is all about making decisions. Intelligent agents have been
developed to help humans with real-world decision making, such as
traffic control [*REF*], energy management [*REF*],
and drug discovery [*REF*]. Prevalent paradigms to train
agents include reinforcement learning (RL), imitation learning (IL),
planning and search as well as optimal control. Recent advances in these
algorithms have achieved superhuman performance in mastering the game of
Go [*REF*], playing video games [*REF*],
and robotic locomotion and manipulation [*REF*]. However,
traditional approaches to decision making exhibit limited sample
efficiency and poor generalization. For instance, expert systems heavily
rely on human knowledge and manual crafting, and conventional RL methods
necessitate agent training from scratch for each task.


In contrast, foundation models [*REF*] in language
and vision achieve rapid adaptation to a wide variety of tasks with
minimal fine-tuning or prompting. These models, pretrained on vast and
diverse datasets, demonstrate unprecedented capabilities in
understanding and generating text [*REF*; *REF*],
image [*REF*; *REF*] and some
multi-modalities [*REF*; *REF*]. Therefore, this
position paper argues that foundation agents as generally capable
agents across physical and virtual worlds, will be the paradigm shift
for decision making, akin to LLMs as general-purpose language models
to solve linguistic and knowledge-based tasks.


To arrive at this position, we first identify three fundamental
characteristics of foundation agents: (1) a unified representation of
variables involved in decision process, including state-action spaces,
feedback signals (e.g., rewards or goals) and environment dynamics, (2)
a unified policy interface across tasks and domains from robotics and
game play to healthcare and beyond, and (3) interactive
decision-making in physical and virtual worlds by reasoning about
behaviors, handling environment stochasticity and uncertainty, and
potentially navigating competitive or cooperative multi-agent scenarios.
These characteristics constitute the uniqueness and challenges for
foundation agents[^1], empowering them with multi-modality perception,
multi-task and cross-domain adaptation as well as few- or zero-shot
generalization. Particularly, foundation agents exhibit enhanced credit
assignment and planning in scenarios requiring long-horizon reasoning
[*REF*; *REF*; *REF*] and involving sparse
rewards or partial observability [*REF*; *REF*].
Such unique set of characteristics and capabilities enable foundation
agents to improve sample efficiency and generalization, showing their
versatility in navigating diverse contexts of decision making.


With the formulation of foundation agents, we specify their roadmap in
Figure *REF*. Firstly, large-scale interactive data
can be collected from the internet (e.g., YouTube videos, tutorials,
audios, etc) and physical environments, or generated through real-world
simulators [*REF*; *REF*; *REF*] for supervision
and scale-up. Secondly, pretrain in an unsupervised manner from large,
unlabeled, and probably suboptimal interactive data for decision-related
knowledge representation learning and downstream adaptation with
knowledge reasoning. Thirdly, align with LLMs to integrate world
knowledge and human values into foundation agents for reasoning,
generalization, and interpretability. This roadmap is motivated by three
key ingredients we observe for the success of LLMs: (1) leveraging
internet-scale text data to absorb broad knowledge described by
languages, (2) self-supervised pretraining to learn unified text
representation [*REF*] and task interface [*REF*] through
generative modeling, and (3) safety and preference
alignment [*REF*; *REF*] to fulfill user
commands.


For foundation agents, however, it is challenging to reproduce the
roadmap of LLMs. Firstly, the broad information in physical and virtual
worlds are low-level details instead of high-level abstractions
expressed by languages, posing challenges to a unified representation.
Therefore, we discuss the morphology of foundation agents (e.g, a
unified or compositional foundation model) for structuring diverse
environments and tasks in (§6.1). Secondly, the large domain gap between
different decision making scenarios makes it difficult to develop a
unified policy interface, while linguistic tasks can be simply unified
by text generation. We thus posit that theoretical guarantees for policy
optimization with foundation agents may address this issue in (§6.2).
Thirdly, while language and vision models focus on understanding and
generating content, foundation agents are involved in dynamic process of
choosing optimal actions based on complex environment information.
Open-ended tasks and environments are then another critical research
question for foundation agents as discussed in (§6.3). Finally, we
showcase some examples supported by recent work and a case study towards
building a potential foundation agent in real-world decision making.


Preliminaries


Decision Making Formalism


Decision making refers to the process of making a series of decisions to
achieve a goal in continuous time, based on previous actions and
observations while considering possible future states and rewards. The
process can be simplified as a markov decision process (MDP)
*MATH*, where *MATH* is the state space, *MATH* the action
space, *MATH* the dynamics transition function
*MATH*, *MATH* the reward function
*MATH*, and *MATH* is a
discount factor for calculating discounted cumulative rewards. When the
underlying state is not accessible (e.g, a video game), the process can
be modified as a partially observable MDP *MATH*, 
where *MATH* is the observation space, and *MATH* denotes the
observation emission function. Solutions to MDP typically involve RL to
learn an optimal policy that maximizes the expected discounted
cumulative rewards *MATH* through
trial and error within an environment, or IL to learn an expert&apos;s policy
from expert demonstrations in a supervised learning manner. However,
real-world decision making is more complex and not limited to MDP, such
as stock market dynamics, epidemiological modeling, and sequential games
with incomplete information (e.g, Poker and Bridge).


Self-Supervised Learning for RL


To improve sample efficiency and generalization, conventional
self-supervised learning for RL involves representation learning of
separate RL components, such as action representation
[*REF*], reward representation [*REF*], policy
representation [*REF*], and environment or task
representation [*REF*]. Recently, by formulating RL as a
sequence modeling problem [*REF*; *REF*], the
representation of various RL components can be simultaneously learned
via trajectory optimization
[*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Such a formulation draws upon the simplicity and scalability of
Transformer [*REF*], and benefits representation
learning of knowledge concerning different aspects of decision process.


Large Language Models and Agents


Large language models
[*REF*; *REF*; *REF*] are
pretrained on internet text and fine-tuned on human instructions and
preferences. Through this process, LLMs acquire extensive world
knowledge and are aligned with human values
[*REF*; *REF*; *REF*; *REF*].
State-of-the-art LLMs demonstrate convincing capabilities in not only
natural language processing
[*REF*; *REF*; *REF*], but also tasks
requiring strong reasoning abilities like coding [*REF*] 
and text-based games [*REF*; *REF*]. As a
source of world knowledge and human values, we argue that LLMs enable
foundation agents to align with the world model and human society
created by languages, enhancing their reasoning and planning
capabilities.


Learning from Large-Scale Interactive Data


Large-scale interactive data is an essential component for building
foundation agents, akin to the significance of internet text and image
data for foundation models in language and vision. In this section, we
first demonstrate how a potential foundation agent can be trained via
offline IL when large, multi-modal and multi-task demonstrations are
available. We then discuss the potential use of data generation systems
or real-world simulators for training foundation agents at scale.
Finally, we identify the constraints and alternatives to offline RL or
IL methods in establishing foundation agents, as well as the limitations
of real-world simulators realized by video generation models.


Offline IL from Large Demonstrations


Inspired by sequence modeling of RL problems, recent work attempt to
train a generalist agent on extensive interactive demonstrations simply
via offline IL. For example, by imitating multi-modal demonstrations,
Behavior Transformer [*REF*] leverages a modified
Transformer with action discretization and a multi-task action
correction to capture the modes present in large behavior data.
Gato [*REF*] unifies multi-modal and multi-task expert
episodes into sequences through autoregressive sequence modeling, and is
capable of diverse tasks and domains from robotic manipulation and Atari
play to visual question answering. Similarly, Unified-IO 2
[*REF*] scales autoregressive multi-modal model across vision,
language, audio, and action data, unifying different modalities into a
shared semantic space. With a single encoder-decoder Transformer, it
learns varieties of skills through fine-tuning with prompts and
augmentations. However, collecting demonstrations via humans or online
RL algorithms is costly and time-consuming, which is not applicable in
real-world decision making.


Potentials of Real-World Simulators


Generative models as data generation systems [*REF*] or
real-world simulators [*REF*; *REF*; *REF*] open
another path for training foundation agents through learning behaviors
from generated data. For instance, RoboGen [*REF*] realizes a
propose-generate-learn cycle to scale up robot learning via generative
simulation, of which policy learning algorithms are not restricted to IL
but also involve motion planning or trajectory optimization. Moreover,
video generation models such as Sora [*REF*] and Genie [*REF*] 
can be a general-purpose simulator of interactive decision making
scenarios in physical and virtual worlds. These models, by unifying the
representation of visual worlds [*REF*], generate diverse and
open-ended training environments across tasks and domains for scaling up
the foundation agent in a never-ending curriculum of new and generated
worlds. The foundation agent pretrained with these generated
environments learns to reason about behaviors, handle environment
stochasticity and uncertainty, and navigate competitive or cooperative
multi-agent settings, thus enhancing adaptability and generalization in
unseen scenarios. This could be particularly valuable for domains where
real-world interactive data is scarce or expensive to obtain, such as
robotics and self-driving.


TABLE


Discussion


Despite the preliminary efforts, real-world interactive data scales far
beyond internet text or visual data, making it impossible to solely rely
on offline RL or IL to train foundation agents. Alternative approaches,
such as self-supervised (unsupervised) pretraining, can be utilized to
harness large and unlabeled interactive data. Moreover, current
generalist agents remain small in model size compared to LLMs, but
already consume significant computational resources even when trained on
a single domain. We provide a summary of computational requirements of
recently proposed generalist agents and a case we build in
Appendix [9].


In addition, although we posit that universal video generation models
(e.g., Sora and Genie) hold promise for enhancing the training of
foundation agents, they are not the whole story. Firstly, not all
real-world decision making tasks can be adequately represented by video,
such as situations when visual and textual information is not available
like wireless communication and grid management. Secondly, the video
generation task may not be a unified policy interface, since simply
predicting next frame may not induce a reasonable agent behavior.
Specifically, video generation objectives such as generating realistic
video sequences given a fixed context, cannot be directly compatible
with decision making objectives that optimize policies given feedback
signals. Thirdly, as the real world is much more complicated than
simulators, the training of foundation agents should be grounded in
simplified world such as a state formed as a result of an action. Then
the agent can be trained on high-level results rather than low-level
pixels and videos, which remains an open question for future work.


Self-Supervised Pretraining and Adaptation


Interactive data captures various aspects of information in the decision
process, including state transitions, state-action causality and state
or action values if rewards are provided. These decision-related
knowledge should be learned during pretraining and transferred to
downstream inference tasks so as to improve sample efficiency and
generalization of agents. Similar to foundation models in language and
vision, we posit that the pretraining and adaptation pipeline can be
considered as knowledge representation learning and reasoning of
foundation agents. In this section, we highlight strategies in
self-supervised pretraining for decision making, and discuss the
potential of pretraining and adaptation in building foundation agents.


Self-Supervised Pretraining


Self-supervised (unsupervised) pretraining for decision making allows
foundation agents to learn without reward signals and encourages the
agent to learn from suboptimal offline datasets. This is particularly
applicable when large, unlabeled data can be easily collected from
internet or real-world simulators. Specifically, given a sequence of
trajectories *MATH*, 
self-supervised pretraining aims to learn a representation function
*MATH* *MATH* to
distill valuable knowledge from trajectory data for downstream
inference. The knowledge can be temporal information about the same
modality (e.g., *MATH*), causal information between
different modalities (e.g., *MATH*), as well as dynamics
*MATH* and reward *MATH* information.


Generally, there are two steps in pretraining foundation agents based on
Transformer architecture. The first step is to learn embeddings of
trajectory data. Specifically, the tokenization of trajectory sequences
comprises three components:(1) trajectory encoding that transforms raw
trajectory inputs into a common representation space, (2) timestep
encoding that captures absolute or relative positional information, and
(3) modality encoding to disambiguate between different modalities in
trajectories [*REF*]. Particularly, two levels of tokenization
granularity have been studied: (1) discretization at the level of
modalities [*REF*; *REF*], and (2) discretization at the
level of dimensions [*REF*; *REF*].


The second step is devising self-supervised pretraining objectives to
discover the underlying structure and semantics of trajectory data for
knowledge representation learning.
Table  *REF* shows pretraining objectives for decision
making in domains including control [*REF*], navigation
[*REF*], and game play [*REF*]. These objectives are
mainly inspired by autoregressive
[*REF*; *REF*] or masked
[*REF*; *REF*] prediction in language and vision model
pretraining. The primary pretraining objective for decision making
involves learning control information by predicting the next action in
an autoregressive way. This objective is further modified by
conditioning on different variables within trajectories, such as reward
or value signals [*REF*; *REF*], next state
(observation) information [*REF*], or latent future
sub-trajectory information [*REF*]. As an alternative, random
masking [*REF*] learns the context of trajectory data by
filling in missing information. Various masking schemes conditioned on
different RL components are designed. For example, reward-conditioned
random mask prediction [*REF*] recovers masked trajectory
segments conditioned on the first-step return, while [*REF*] 
only recovers masked actions conditioned on the final-step action to
capture global temporal relations for multi-step control. Combining
masked and autoregressive modeling, [*REF*] constrains the last
variable in trajectories to be masked to force the pretrained model to
be causal at inference time. Moreover, contrastive prediction objective
[*REF*; *REF*; *REF*; *REF*] 
has been commonly used in self-supervised pretraining to learn state
representation via a contrastive loss, which can benefit dynamics
learning.


Downstream Adaptation


During adaptation, extensive decision-related knowledge acquired from
pretraining is transferred to downstream tasks via fine-tuning or
prompting. The knowledge transfer facilitates the optimization of
learning objectives *MATH* in downstream inference, such as the
value function *MATH* or *MATH*, policy, dynamics and reward
functions. The optimized objectives empowered by knowledge reasoning can
finally improve sample efficiency and generalization compared to
learning from scratch in traditional RL.


Generally, there are two cases requiring fine-tuning: (1) when the
pretraining data is a mix of a small proportion of near-expert data
and a large proportion of exploratory trajectories, and (2) when the
pretraining objective significantly differs from the inference
objective of downstream decision making tasks. For instance, traditional
RL aims to maximize cumulative rewards according to a specified reward
function, whereas self-supervised pretraining tasks are usually
reward-free. In such cases, RL algorithms are demanded for fine-tuning
policies in online or offline settings
[*REF*; *REF*; *REF*; *REF*; *REF*].


In addition to fine-tuning, prompting directly adapts the pretrained
model to downstream inference without altering or introducing any
parameters. It concerns: (1) aligning pretraining objectives with
downstream inference objectives, and (2) prompting the model with
interactive demonstrations [*REF*; *REF*; *REF*; *REF*].
For example, using a random masking pretraining objective with a
variable mask ratio for different goals, [*REF*] achieves
zero-shot generalization to goal-reaching tasks. This is attributed to
the natural alignment of the masked pretraining objective with
goal-reaching scenarios, where the model is required to recover masked
actions based on remaining states.


Discussion


Previous attempts at self-supervised RL pretraining have been mostly
limited to a single task [*REF*], or performing
pretraining and fine-tuning within the same task
[*REF*]. This is not generic and flexible for
adapting to various decision making tasks. Therefore, we argue for
multi-modal and multi-task self-supervised pretraining in the evolution
of foundation agents. Particularly, self-supervised pretraining empowers
foundation agents to acquire a nuanced understanding of large
interactive data, laying a robust foundation for knowledge learning and
reasoning in adaptation. However, challenges arise in optimizing
foundation agents, such as determining the optimal granularity and
representation of trajectory data. Furthermore, striking a balance
between versatility and task specificity remains an ongoing challenge.
Despite these problems, combining self-supervised pretraining and
versatile adaptation strategies, underscores the potential of foundation
agents in capable of robust performance across a broad array of decision
making scenarios.


Knowledge and Value Alignment via LLMs


The broad real-world knowledge and human values embedded in LLMs pave
the way for foundation agents with improved reasoning, generalization
and interpretability. In this section, we discuss the roles of LLMs in
enhancing decision making realized by memory, planning and action
modules. We then outline some major challenges and potential solutions
to align foundation agents with LLMs.


Memory and Information Processing


Foundation agents aligned with LLMs are often equipped with a memory
module. The memory module functions as a repository, storing both
task-specific information and past interaction history that enables
agents to retrieve and refine relevant historical data for future
planning. Similar as human memory [*REF*], the agent&apos;s
memory can be categorized into short-term and long-term components.
Short-term memory encapsulates recent information within the context
window, while long-term memory encompasses historical information stored
in external storage.


After perceiving and processing environmental state, agents can decide
what information to remember and what is crucial for current decision
making that demands retrieval. For example, mastered skills
[*REF*] are explicitly stored to prevent forgetting and
retrieved as needed to enhance decision making efficiency. Further,
through alignment with LLMs, foundation agents can refine their existing
memories, generate new insights based on current knowledge, and assist
future decision making [*REF*; *REF*].


Planning with World Models


The abundant world knowledge and human values embedded in LLMs enables
LLMs to serve as world models [*REF*], simulating transition
dynamics and reward functions [*REF*] for foundation agents to
perform planning. The agent then is able to reason about current
observations and historical information, decomposing a complicated task
into a sequence of strategic plans or sub-goals. Few-shot prompting
[*REF*] can be one efficient means to utilize LLMs for
enhancing agent&apos;s understanding and planning by providing high-quality
demonstrations in short-term memory. For example, Chain-of-Thought (CoT)
[*REF*] decomposes tasks into logically coherent sub-goals, and
Tree-of-Thought (ToT) [*REF*] enables agents to generate
potential solutions, allowing them to select the most reliable one.
Furthermore, ReAct [*REF*] generates logically correct reasoning
paths, improving the quality of generated plans by assessing correctness
and logical coherence of previously generated actions.


Moreover, feedback from the environment
[*REF*; *REF*; *REF*; *REF*; *REF*],
LLM critics [*REF*; *REF*; *REF*],
or humans [*REF*] can also be leveraged to improve the
planning of foundation agents. These feedback enables agents to adjust
behaviors and plans, addressing obstacles encountered during execution
and thus improving task fulfillment.


Action and Decision Making


For ultimate action execution and interaction with real or simulated
environments, an action module is required for foundation agents to
interpret plans from the planning module, translate subgoals into
executable actions, and combine atomic actions to form more complex
structured actions.


Specifically, the action module specifies action goals and available
actions. Action goals articulate intended objectives, such as movement
[*REF*], communication [*REF*], and code generation
[*REF*]. Clear goal descriptions enable agents to comprehend
and generate reasonable actions based on observations. Available
actions, on the other hand, define the agent&apos;s action space, which can
be expanded by adding recently acquired skills into memory
[*REF*; *REF*]. Additionally, equipped with LLMs,
foundation agents can possess the ability to call external APIs or tools
for problem solving. HuggingGPT [*REF*] exemplifies this
by using LLMs as the controller to manage existing AI models from the
internet via APIs to execute each sub-task. Further, by connecting to
external databases [*REF*; *REF*], the agent can acquire
extra task-related knowledge via external tools and plugins created by
domain experts [*REF*] for reliable and creative decision
making.


Discussion


The integration of LLMs into agent learning offers advantages over
conventional RL and IL methods. Firstly, LLMs require significantly
fewer training samples [*REF*], as they leverage pretraining
on internet-scale text, acquiring extensive real-world knowledge and
demonstrating strong generalization abilities. Secondly,
state-of-the-art LLMs are inherently human-aligned, enabling them to
make decisions consistent with human preferences and values
[*REF*]. Thirdly, by providing detailed reasoning for
actions during planning, the interpretability of foundation agents
aligned with LLMs addresses the challenge of deploying black-box AI
models [*REF*] in real-world decision making.


However, a substantial challenge for aligning foundation agents with
LLMs lies in the hallucination of LLMs. Hallucination refers to the
phenomenon where the language model generates false content that is
misaligned with real-world knowledge [*REF*]. Since
hallucination detection still remains an open problem, aligning
foundation agents with LLMs amplifies the risk of inexplicable abnormal
behaviors. Given the broad range of tasks assigned to foundation agents,
particularly those that are safety-critical, the hallucination in LLMs
could result in serious consequences during decision making. Therefore,
it is crucial to remain vigilant about the potential risks of foundation
agents aligned with LLMs, and propose solutions to this issue for a safe
and responsible deployment in real-world decision making.


Trends for Foundation Agents


Derived from the formulation and challenges for foundation agents, some
critical research questions remain to be solved. In this section, we
propose these issues and potential solutions, and discuss the trends for
foundation agents with examples in recent work as well as a case study
in real-world decision making scenarios.


A Unified or Compositional Foundation Agent 


One central challenge in building foundation agents stems from the
substantial diversity among decision making tasks, involving variations
in state and action spaces, environment dynamics, and reward functions.
In contrast, the unification of tasks in language is facilitated by the
text-to-text generation paradigm [*REF*; *REF*]. To
structure diverse environments and tasks, some attempts have been made
along two primary directions: (1) the pursuit of a unified foundation
model [*REF*; *REF*], and (2) the exploration of
compositions involving multiple existing foundation models
[*REF*; *REF*] for decision making.


To establish a unified foundation model for decision making, a core
requirement is the unified representation of RL components. This entails
encoding states, actions, and rewards from diverse environments and
tasks into standardized tokens through sequence modeling
[*REF*; *REF*; *REF*]. The
subsequent step involves transforming these tokens into a consistent
data modality, such as text descriptions [*REF*], generated
videos [*REF*], code [*REF*], or text-image pairs
[*REF*]. However, due to the multi-modal and heterogeneous
characteristics of interactive data, uncertainties persist regarding the
efficacy of tokenization methods in compressing raw trajectories into
compact tokens. In addition, whether a single data modality can
comprehensively represent interactive data and effectively convey its
underlying information and knowledge warrants further investigation.


While a unified foundation model might offer a comprehensive solution,
it could also lead to increased complexity and challenges in
interpretability. Instead, we consider whether integrating existing
foundations models (e.g., large language or vision models) with moderate
decision models is sufficient to address most (if not all) of decision
making tasks. The integration of existing foundation models allows for
leveraging their domain-specific strengths but may introduce issues in
harmonizing different modalities and functionalities. Additionally, a
compositional foundation agent could inherit both merits and demerits
from other foundation models. Therefore, striking a balance between
these approaches requires careful evaluation of specific requirements
and constraints of target tasks, offering an intriguing avenue for
future research in foundation agents.


Policy Optimization with Foundation Agents


In traditional RL, theoretical guarantees underpin algorithmic
advancements, such as the Bellman optimality update in temporal
difference learning and dynamic programming. Recent theoretical analyses
have shed light on the delusions in sequence models for interaction and
control [*REF*], yet the optimization of a generalist
policy with foundation agents still lacks a solid theoretical
foundation.


Establishing theoretical foundations for policy optimization with
foundation agents is a complex but crucial endeavor. Firstly, policies
generated by foundation agents will directly affect changes in
real-world applications, requiring rigorous theoretical guarantees to
ensure their effectiveness, optimality, safety and robustness.
Meanwhile, the theoretical foundations for policy optimization can in
turn help to understand the principles underlying the agent&apos;s behavior
and performance, thereby enabling the development of algorithms and
methodologies for training foundation agents. Specifically, we posit
that the theoretical foundations require an interdisciplinary view
(e.g., control theory, RL, and optimization), and we outline some ideas
here.


- Define Pretraining and Task-Specific Objectives: Unlike
traditional RL objectives that focus on maximizing cumulative
rewards, the optimization objectives for foundation agents involve
both pretraining and task-specific objectives. Pretraining
objectives address the unified representation of variables involved
in decision process and the unified policy interface across tasks
and domains. Task-specific objectives may be employed in fine-tuning
phase, which have been established in many control, planning, and RL
algorithms [*REF*; *REF*].


- Understand the Interplay between Pretraining and Task-Specific
Optimization: Establish a mathematical framework to investigate
how the pretraining phase shapes the optimization landscape for
task-specific policy optimization, and how different adaptation
algorithms interact with the previously acquired knowledge. This
could involve probing convergence properties, sample efficiency, and
generalization bounds of foundation agents.


- Extend Existing Theoretical Frameworks: Adapt and extend
existing theoretical frameworks from RL, control theory and
optimization to accommodate the peculiarities of foundation agents.
This may involve relaxing assumptions made in traditional RL and
developing new notions of optimality for generalist policies. For
example, (1) extend the MDP framework with generalized state
spaces to incorporate multi-modal inputs and extended action spaces
spanning different domains (e.g., physical movements and linguistic
responses), (2) leverage the probabilistic representations and
principled reasoning offered by the control as inference
framework [*REF*] to derive optimal control policies
and analyze their properties, and (3) ground policy optimization for
foundation agents in information-theoretic principles, such as
mutual information or entropy, to capture the agent&apos;s ability to
influence the environment and gather informative experiences.


Open-Ended Tasks for Foundation Agents


In the evolution of foundation agents, a notable trend is the shift
towards learning from open-ended tasks at scale
[*REF*; *REF*]. Traditionally, RL agents are confined
to a single and individual task
[*REF*; *REF*], limiting their
applicability to massive dynamic scenarios. Contemporary approaches
instead prioritize the scalability of generative models, especially via
LLMs to effectively handle the complexities inherent in open-ended
tasks [*REF*; *REF*]. The open-ended agents, endowed
with continuously learning capabilities, interact with environments or
external tools [*REF*; *REF*; *REF*],
iteratively refining their decision process. This paradigm aligns
seamlessly with RL principles, where agents acquire optimal strategies
through iterative learning.


Despite the promising advances in the development of open-ended agents,
several challenges persist particularly for open-ended tasks.
Importantly, open-ended tasks do not have a predefined goal or endpoint,
push the boundaries of agent adaptability, generalization, and continual
learning, and require flexible adaptation, creativity, and the ability
to discover new goals or objectives. Inspired by these features, we
discuss the issues associated with foundation agent learning in
open-ended tasks, along with insights into how these issues can be
addressed.


- Continual Learning and Adaptation: The dynamic nature of
open-ended tasks poses challenges in model adaptability without
catastrophic forgetting of acquired skills and knowledge. Agents
need to handle an unlimited variety of open-ended goals with
training objectives and task distributions dynamically changing. Two
lines of recent work attempt to address the issue by (1) leveraging
pretrained language or vision model to convert world knowledge into
actionable insights [*REF*; *REF*], and (2)
pretraining (or meta-learning) large transformer models from scratch
on multi-tasks [*REF*; *REF*; *REF*].
However, these studies require mostly human-defined goals as input.
Instead, [*REF*] formulates goal as energy function and
learns without goal-conditioning via diffusion models. Nevertheless,
it is also limited by human-designed functions for goals which could
be replaced by neural networks in future work.


- Unboundedness and Novelty Detection: In open-ended tasks, the
environment might be infinite or continually evolving, making it
difficult to exhaustively explore or anticipate all possible states
and outcomes. Agents need to deal with novelty and unexpected
situations throughout their lifetime. Current research attempts to
address this issue through novelty difficulty
measures [*REF*], hierarchical
RL [*REF*] and self-initiated open world learning
framework [*REF*], yet it still remains an open question.


- Curriculum and Autotelic Learning: Agents learning in open-ended
tasks have to learn and explore without explicit external rewards.
Existing studies apply procedural environment generation to create
diverse and increasingly complex environments [*REF*], and
intrinsic motivation [*REF*] and
curiosity-driven learning to encourage exploration. Future work can
consider automatically generate curriculum via
LLMs [*REF*], universal video generation
models [*REF*; *REF*] or multi-modal models.


- Creativity and Innovative Solutions: Generating novel and
valuable solutions is a hallmark of open-ended learning. This
necessitates agents that can go beyond mere imitation or
optimization, possibly by combining known concepts in unique ways or
inventing entirely new ones. Potential solutions include (1)
Quality-Diversity algorithms [*REF*] to produce
a diverse set of high-performing solutions, (2) neuroevolution
methods [*REF*] to produce innovative solutions,
and (3) deep generative models to generate novel and sometimes
unexpectedly creative outputs.


![Visualized fine-tuning performance of our pretrained agent on unseen
task and game. Frames are selected from the video recorded in the last
evaluation episode. Total number of evaluation episodes is 50. Results
are from one training seed.](visualization.png)


Case Studies in Real-World Decision Making


Autonomous Control. The progress in robotics continually enhances
productivity and resource efficiency. Recently, robots with general
capabilities have been developed towards foundation agents. For example.
RT-1 [*REF*] is a generalist and language-conditioned robotic
agent, exhibiting zero-shot generalization to new tasks, environments
and long-horizon scenarios by scaling on a large set of manipulation
demonstrations [*REF*]. RT-2 [*REF*] further
improves generalization and reasoning in response to user commands by
co-fine-tuning pretrained vision-language models on demonstrations and
internet vision-language tasks. Moreover, equipped with real-time human
feedback, robots can also improve their high-level decision making
iteratively in long-horizon tasks [*REF*]. We also conduct a case
study in robotic locomotion and game play by jointly pretraining on 5
tasks in the DeepMind Control (DMC) suite [*REF*] and 5
games in Atari video play [*REF*] through autoregressive
modeling. Visualization of the fine-tuning performance on a new
locomotion task and a new game is shown in
Figure [1]. Details of our case study can be found in
Appendix [10].


In addition to robotics and game play, self-driving is another critical
domain where foundation agents have major impact opportunities. As a
paradigm shift from previous perception-prediction-planning framework,
LLMs are exploited to integrate common-sense knowledge and human
cognitive abilities into autonomous
driving [*REF*; *REF*], and enable multiple vehicles
to realize collaboration over time [*REF*]. Despite
these trends for foundation agents in autonomous control, safety,
robustness and reliability of decisions made by agents requires rigorous
evaluation and assurance in real-world deployment.


Healthcare. Human-Agent collaborative decision making in healthcare
has the opportunity to enhance diagnostic accuracy, streamline
operations, and provide personalized treatment options. Current research
in foundation agents for healthcare is rare but has shown effectiveness
in assessing patient&apos;s state in ICU (intensive care unit)
rooms [*REF*] and designing treatment plans through
discussions among LLMs as different medical
experts [*REF*]. However, agents in healthcare still raise
high uncertainty and distrust from clinicians. Inspired by drawing
biomedical literature evidence via LLM [*REF*] or
involving human experts in the intermediate stages of medical decision
making [*REF*], real-world healthcare can be realized
through consistent collaboration between humans and foundation agents in
the future.


Science. Scientific discovery and research can be revolutionized by
foundation agents with significantly accelerated data analysis and
experimentation processes, leading to faster and more accurate insights.
AlphaFold 3 [*REF*] has demonstrated the feasibility of
high accuracy modelling across biomolecular space within a single
unified model, consistent with the characteristics of foundation agents
that have a unified representation and policy interface across
state-action spaces. Aligned with LLMs and empowered by external tools,
chemistry agents [*REF*; *REF*] show their
potential for accelerating research and discovery. They can be applied
in organic synthesis, reaction optimization, drug discovery, and
materials design with capabilities of (semi-) autonomous hypothesis
generation, experimental design and execution. However, the deployment
of foundation agents in scientific research poses potential biases in
data interpretation, unintended experimental errors, and ethical
concerns regarding the transparency and accountability of findings,
requiring rigorous oversight, comprehensive validation and ethical
guidelines.


Conclusion


We take the position that foundation agents hold the potential to alter
the landscape of agent learning for decision making, akin to the
revolutionary impact of foundation models in language and vision. We
support this position by formulating the notion of foundations agents
and specifying their roadmap and key challenges. Demonstrated by recent
work and a case study, we show that the enhanced perception, adaptation,
and reasoning abilities of agents not only address limitations of
conventional RL, but also hold the key to unleash the full potential of
foundation agents in real-world decision making. However, due to the
intrinsic complexity of decision making, existing work towards
foundation agents is still at a starting point. Future directions
involve but are not limited to learning a unified representation of
diverse environments and tasks, establishing theoretical foundations for
a unified policy interface and learning from open-ended tasks at scale.
Bridging these gaps will pave the way for more robust and versatile
foundation agents, contributing to the evolution of artificial general
intelligence.