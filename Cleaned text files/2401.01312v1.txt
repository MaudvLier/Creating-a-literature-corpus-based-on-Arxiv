LLM Harmony: Multi-Agent Communication for Problem Solving 


Introduction


The rise of Large Language Models (LLMs) [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] has undeniably revolutionized the software industry, particularly in the realm of Natural Language Processing (NLP) [*REF*]. These models have demonstrated a remarkable ability not only to generate text but also to grasp the underlying structures of written language, extending their utility to tasks ranging from text generation to code understanding [*REF*] [*REF*]. However, despite their prowess, it has become evident that LLMs, while adept at handling familiar topics, often falter when confronted with novel challenges.


One notable limitation lies in their tendency to hallucinate information when presented with unfamiliar subjects [*REF*]. Moreover, while these models exhibit commendable proficiency in addressing technical coding challenges across multiple programming languages, they struggle with fundamental reasoning questions. Addressing such limitations necessitates an approach that goes beyond conventional methodologies [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*].


In response to this challenge, our paper introduces a novel strategy aimed at enhancing LLM performance on novel problems. Drawing inspiration from the effectiveness of chain-of-thought prompting [*REF*] in breaking down complex problems, we seek to leverage the synergy of multiple LLM agents working collaboratively [*REF*] [*REF*]. Each agent is endowed with a distinct persona, a concept inspired by the CAMEL framework, and engages in role-playing communication methods.


Unlike traditional approaches that might be limited to binary agent personas, such as &quot;user&quot; and &quot;assistant,&quot; or simplistic positive and negative agent roles, our proposed framework adopts a nuanced and flexible strategy. The intricacies of various novel scenarios demand a more sophisticated approach. Our design incorporates industry best practices to guide the creation of diverse agent personas, ensuring adaptability to a multitude of problem-solving contexts.


The paper not only addresses the limitations of existing LLM models but also explores the potential of harnessing the collective intelligence of multiple agents to tackle a broader range of challenges. We aim to demonstrate that the proposed framework not only outperforms traditional methodologies but also provides a foundation for autonomous problem-solving, minimizing the need for explicit human guidance.


In the subsequent sections, we delve into the existing techniques that have been developed to address similar challenges, highlighting their strengths and limitations [*REF*]. Following this, we present our multi-agent communication design, emphasizing its improvements over current methodologies [*REF*]. The extendability of our approach to various LLM models is a key focus, underlining its potential as a versatile and reusable solution. Through rigorous experimentation and analysis, we aim to showcase the effectiveness and adaptability of our proposed framework in enhancing the quality of LLM outputs across a spectrum of tasks.


This is what we propose:


- An innovative framework employing multiple large language model workers/agents, each characterized by a unique persona and guided by a chain-of-thought prompt.
- The collaborative effort of all agents is directed toward devising solutions for novel problems.
- The framework&apos;s versatility allows the incorporation of any persona and chain-of-thought prompt, aligning with the specific problem to be addressed.
- The framework is available. WEBSITE. It is built on top of CAMEL&apos;s and ChatDev&apos;s framework.


Methodology


In the realm of collaborative problem-solving, particularly evident in scenarios like software development, the complexities arise from coordinating efforts among diverse individuals. Software development teams, for instance, often undergo iterative cycles before converging on a viable solution, and even then, compromises may be accepted as the best available option [*REF*] [*REF*]. Our proposed strategy addresses the need for comprehensive validation of Large Language Models (LLMs), not just in code evaluation but also in complex reasoning and arithmetic challenges [*REF*] [*REF*] [*REF*] [*REF*] [*REF*]. Focused on breaking down problem statements and leveraging distinct personas [*REF*], the framework provides agents with tailored chain-of-thought prompts [*REF*], enabling multi-agent models to tackle novel problems collaboratively.


Understanding personas is crucial; in a software development organization, roles such as CEO, VP of Engineering, developers, testers, and product managers contribute unique perspectives to problem-solving [*REF*]. The interplay of these personas allows agents to validate each other&apos;s responses effectively [*REF*] [*REF*] [*REF*]. The concept of chain-of-thought mirrors human problem-solving strategies: decomposing a problem, identifying solutions to sub-problems, and constructing a comprehensive answer. The chain-of-thought prompt extends this strategy, offering the LLM an input, a problem-solving approach, and the desired output. The fusion of personas [*REF*] and chain-of-thought [*REF*] prompts capitalizes on mimicking human analytical and execution strategies [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*].


While some might question the necessity of this approach over retraining the model for novel problems, the practicality lies in the belief that emulating human-like analysis and execution strategies can create autonomous agents capable of diverse tasks without extensive model retraining [*REF*] [*REF*]. Additionally, the high cost and challenges associated with retraining LLMs [*REF*], particularly for problems with limited training data, support the appeal of our strategy.


In the realm of multi-agent communication [*REF*] [*REF*], adherence to assigned personas and chain-of-thought prompts is foundational to mitigate agent hallucination and improving cooperation [*REF*] [*REF*]. The framework&apos;s versatility extends beyond problem-solving to applications in multi-player games [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*] [*REF*], accommodating both team-oriented and individual goal-driven scenarios, and offering flexibility in achieving optimal outcomes.


Experiments


In our experimentation, we adopt a two-agent strategy comprising an expert agent and an evaluator agent, both instances of OpenAI&apos;s &quot;gpt3.5-turbo.&quot; Each agent is configured with specific parameters: temperature set to 0.0, representing the trade-off between coherence and creativity, and a conversation limit of 5, indicating the maximum iterations allowed for inter-agent conversation. The evaluator agent assesses the responses generated by the expert agent, guiding it to rectify inaccuracies. Leveraging OpenAI&apos;s LLM object, our framework, integrated with CAMEL&apos;s architecture, facilitates seamless communication among multiple LLM agents, aiming to establish a collaborative problem-solving capability for advanced LLM models.


Arithmetic Reasoning


In the arithmetic reasoning segment, we address challenges that assess the arithmetic proficiency of LLMs, an area where traditional large language models have historically faced limitations. The experiment employs the GSM8K benchmark and the SVAMP data set, specifically designed for math word problems. We evaluate the performance of three LLMs: a standalone GPT-3 model, a multi-agent GPT-3 model, and a multi-agent GPT-3 model integrated into our framework. The objective is to gauge the effectiveness of our collaborative approach in enhancing arithmetic reasoning capabilities.


&quot;GSM8K consists of 8.5K high-quality grade school math problems created by human problem writers. We segmented these into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ - /) to reach the final answer. A bright middle school student should be able to solve every problem.&quot; [*REF*]


&quot;We first show that existing models achieve reasonably high accuracies on these datasets even after removing the &quot;question&quot; part of the MWP at test time. We further show that a simple model without any word-order information can also solve a majority of MWPs in these datasets. Our experiments indicate that existing models rely on shallow heuristics in benchmark MWP datasets for achieving high performance. Our experiments render the benchmark datasets unreliable to measure model performance. To enable more robust evaluation of automatic MWP solvers, we created a challenge set called &quot;SVAMP&quot;.&quot; [*REF*]


EXAMPLE 1.1


EXAMPLE 1.2


Results


The outcomes from both experiments can be found in Table [1] and Table [2]. In the initial experiment (example can be found here [3.1.1]) using the GSM8K data set, the single-agent GPT-3 achieves approximately 50% accuracy, and the multi-agent GPT-3 performs slightly better at 55%.
However, our multi-agent approach significantly enhances accuracy, surpassing other large language models (LLMs) such as Google&apos;s PALM 540B parameter model, which we haven&apos;t directly tested but are referencing from their paper. This improvement is notable in terms of accuracy, and it&apos;s noteworthy that we haven&apos;t retrained the model to achieve this enhancement. Assigning personas to the agents enables the LLM model to concentrate on specific aspects of the problem, and the use of chain-of-thought prompts equips it with efficient means to solve sub-problems.


In the second experiment (example can be found here [3.1.2]) using the SVAMP data set, similar results are observed to those in the first experiment. The single-agent GPT-3 achieves an accuracy of approximately 70%, while the multi-agent version of GPT-3 attains an accuracy of 73%.
Even in the SVAMP data set experiment, our multi-agent approach surpasses others, delivering an impressive 77% accuracy.


If our approach is unsuccessful in arriving at the correct answer, it is often due to an arithmetic error occurring in one of the sub-steps of the provided problem, accounting for more than half of the instances.
This implies that if subsequent iterations of large language models (LLMs) enhance their proficiency in basic arithmetic operations, we can potentially attain an even greater accuracy with our framework.


TABLE 1


TABLE 2


Commonsense Reasoning


The subsequent experiment delves into commonsense reasoning, requiring logical inference from a given problem statement, assuming general knowledge. Unlike traditional natural language processing systems, LLMs are uniquely equipped for such tasks due to their predictive nature, where they anticipate the next word, inherently understanding the contextual nuances of sentences. We aim to harness this inherent capability and build a versatile, context-aware framework driven by multiple agents. Evaluations on the CSQA dataset involve a one-agent GPT-3 model, a multi-agent GPT-3 model, and a multi-agent GPT-3 model integrated into our framework. The goal is to assess the performance gains achieved through collaborative, context-driven approaches in enhancing commonsense reasoning tasks.


EXAMPLE 2.1


EXAMPLE 2.2


Results


The results of the third experiment are detailed in Table [3]. The single-agent GPT-3 achieves an impressive 77% accuracy, while the multi-agent GPT-3 performs slightly better at 78%. However, our multi-agent approach substantially improves accuracy, surpassing the other two and reaching approximately 83% accuracy. Notably, this accuracy is attained through Few-Shot training, indicating there is room for further enhancement. Few-shot training involves providing the LLM with a few examples of a specific problem type, enabling it to learn the correct answer without retraining the entire model for novel problems.
Examples can be found here [3.2.1] and here [3.2.2].


In instances where the model provided incorrect answers, it often stemmed from making inaccurate correlations within the provided options.
This incorrect correlation is a result of the underlying data set on which this particular model was trained. While retraining the model could improve the accuracy of this experiment, it&apos;s considered a costly option. This poses an open question for future researchers to address: How can the model be trained with enough context to establish accurate correlations in commonsense word problems?


TABLE


Limitations


While we recognize that our framework addresses numerous challenges, there are still a few aspects that remain unaddressed. Some of the reasoning capabilities may not see improvement unless the dataset used to train OpenAI&apos;s &quot;gpt3.5-turbo&quot; is sufficiently diverse to comprehend the entirety of our surroundings. Additionally, the framework requires the capability to incorporate new information to stay updated with the constantly evolving data. Another necessary step involves implementing a data processing mechanism to filter out redundant information and prevent the inclusion of duplicate data.


Another limitation pertains to the context limit of each agent in multi-agent communication. Each agent is constrained by the maximum context, defined by the underlying model; for instance, the &quot;gpt3-turbo&quot; model has a context limit of 4096 tokens.


We intend to address these limitations in our future work and strive to find effective solutions.


Conclusion


In conclusion, our exploration into the realm of multi-agent communication for Large Language Models (LLMs) has unveiled promising avenues for overcoming inherent limitations. While LLMs have revolutionized natural language processing, their efficacy is not without challenges, particularly in addressing novel problems, reasoning, and commonsense understanding. Our proposed approach leverages personas and chain-of-thought prompts, inspired by industry best practices and cognitive processes. By assigning distinct personalities and thoughtful prompts to each agent, we mitigate issues like hallucination and enhance the overall performance of multi-agent communication.


Through a series of experiments, we demonstrated the effectiveness of our approach, showcasing improvements in arithmetic reasoning and commonsense understanding. Our strategy of employing multiple agents, each with a specific role and reasoning prompt, facilitates collaborative problem-solving, providing a feasible alternative to the costly retraining of LLMs for novel challenges.


By reducing reliance on human intervention, our approach paves the way for LLMs to tackle a myriad of tasks independently. The scalability and adaptability of our role-playing framework position it as a valuable asset in various domains, from software development to complex decision-making scenarios.


In an era where artificial intelligence continues to evolve, our research contributes a novel perspective on enhancing the capabilities of LLMs through cooperative multi-agent communication. The journey from understanding limitations to proposing effective solutions marks a significant step forward, opening doors to future advancements in autonomous, context-aware language models.