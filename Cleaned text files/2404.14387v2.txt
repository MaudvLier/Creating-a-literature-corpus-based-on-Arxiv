A Survey on Self-Evolution of Large Language Models


Introduction


With the rapid development of artificial intelligence, large language
models (LLMs) like GPT-3.5 [*REF*],
GPT-4 [*REF*], Gemini [*REF*],
LLaMA [*REF*; *REF*], and Qwen [*REF*]
mark a significant shift in language understanding and generation. These
models undergo three stages of development as shown in
Figure [1](fig:intro): pre-training on large and diverse corpora to gain a general
understanding of language and world
knowledge [*REF*; *REF*], followed by supervised
fine-tuning to elicit the abilities of downstream
tasks [*REF*; *REF*]. Finally, the human
preference alignment training enables the LLMs to respond as human
behaviors [*REF*]. Such successive training paradigms
achieve significant breakthroughs, enabling LLMs to perform a wide range
of tasks with remarkable zero-shot and in-context capabilities, such as
question answering [*REF*], mathematical
reasoning [*REF*], code generation [*REF*], and
task-solving that require interaction with environments [*REF*].


FIGURE


Despite these advancements, humans anticipate that the emerging
generation of LLMs can be tasked with assignments of greater complexity,
such as scientific discovery [*REF*] and future events
forecasting [*REF*]. However, current LLMs encounter
challenges in these sophisticated tasks due to the inherent difficulties
in modeling, annotation, and the evaluation associated with existing
training paradigms [*REF*]. Furthermore, the recently developed
Llama-3 model has been trained on an extensive corpus comprising 15
trillion tokens. It&apos;s a monumental volume of data, suggesting that
significantly scaling model performance by adding more real-world data
could pose a limitation.


This has attracted interest in self-evolving mechanisms for LLMs, akin
to the natural evolution of human intelligence and illustrated by AI
developments in gaming, such as the transition from
AlphaGo [*REF*] to AlphaZero [*REF*].
AlphaZero&apos;s self-play method, requiring no labeled data, showcases a
path forward for LLMs to surpass current limitations and achieve
superhuman performance without intensive human supervision.


Drawing inspiration from the paradigm above, research on the
self-evolution of LLMs has rapidly increased at different stages of
model development, such as self-instruct [*REF*], self-play
[*REF*], self-improving [*REF*], and self-training
[*REF*]. Notably, DeepMind&apos;s AMIE system
[*REF*] outperforms primary care physicians in diagnostic
accuracy, and Microsoft&apos;s WizardLM-2 exceeds the performance of the
initial version of GPT-4. Both models are developed using
self-evolutionary frameworks with autonomous learning capabilities and
represent a potential LLM training paradigm shift. However, the
relationships between these methods remain unclear, lacking systematic
organization and analysis.


Therefore, we first comprehensively investigate the self-evolution
processes in LLMs and establish a conceptual framework for their
development. This self-evolution is characterized by an iterative cycle
involving experience acquisition, experience refinement, updating, and
evaluation, as shown in Figure [2]. During the cycle, an LLM initially gains
experiences through evolving new tasks and generating corresponding
solutions, subsequently refining these experiences to obtain better
supervision signals. After updating the model in-weight or in-context,
the LLM is evaluated to measure progress and set new objectives.


The concept of self-evolution in LLMs has sparked considerable
excitement across various research communities, promising a new era of
models that can adapt, learn, and improve autonomously, akin to human
evolution in response to changing environments and challenges.
Self-evolving LLMs are not only able to transcend the limitations of
current static, data-bound models but also mark a shift toward more
dynamic, robust, and intelligent systems. This survey deepens
understanding of the emerging field of self-evolving LLMs by providing a
comprehensive overview through a structured conceptual framework. We
trace the field&apos;s evolution from the past to the latest cutting-edge
methods and applications while examining existing challenges and
outlining future research directions, paving the way for significant
advances in developing self-evolution frameworks and next-generation
models.


The survey is organized as follows: We first present the overview of
self-evolution (§ [2]), including background and conceptual
framework. We summarize existing evolving abilities and domains of
current methods(§ [3]). Then, we provide in-depth analysis and
discussion on the latest advancements in different phases of the
self-evolution process, including experience acquisition
(§ [4]), experience refinement (§ [5]), updating
(§ [6]), and evaluation (§ [7]). Finally, we outline open problems and
prospective future directions (§ [8]).


FIGURE


Overview 


In this section, we will first discuss the background of self-evolution
and then introduce the proposed conceptual framework.


Background


Self-Evolution in Artificial Intelligence. Artificial Intelligence
represents an advanced form of intelligent agent, equipped with
cognitive faculties and behaviors mirroring those of humans. The
aspiration of AI developers lies in enabling AI to harness
self-evolutionary capabilities, paralleling the experiential learning
processes characteristic of human development. The concept of
self-evolution in AI emerges from the broader fields of machine learning
and evolutionary algorithms [*REF*]. Initially influenced by
the principles of natural evolution, such as selection, mutation, and
reproduction, researchers have developed algorithms that simulate these
processes to optimize solutions to complex problems. The landmark paper
by *REF*, which introduced the genetic algorithm, marks
a foundational moment in the history of AI&apos;s capability for
self-evolution. Subsequent developments in neural networks and deep
learning have furthered this capability, allowing AI systems to modify
their own architectures and improve performance without human intervention [*REF*].


Can Artificial Entities Evolve Themselves? Philosophically, the
question of whether artificial entities can self-evolve touches on
issues of autonomy, consciousness, and agency. While some philosophers
argue that true self-evolution in AI would require some form of
consciousness or self-awareness, others maintain that mechanical
self-improvement through algorithms does not constitute genuine
evolution [*REF*]. This debate often references the
works of thinkers like *REF*, who explore the
cognitive processes under human consciousness and contrast them with
artificial systems. Ultimately, the philosophical inquiry into AI&apos;s
capacity for self-evolution remains deeply intertwined with
interpretations of what it means to &apos;evolve&apos; and whether such processes
can purely be algorithmic or must involve emergent consciousness [*REF*].


Conceptual Framework 


In the conceptual framework of self-evolution, we describe a dynamic,
iterative process mirroring the human ability to acquire and refine
skills and knowledge. This framework is encapsulated within
Figure [2], emphasizing the cyclical nature of learning
and improvement. Each iteration of the process focuses on a specific
evolution goal, allowing the model to engage in relevant tasks, optimize
its experiences, update its architecture, and evaluate its progress
before moving to the next cycle.


Experience Acquisition


At the *MATH* iteration, the model identifies an evolution objective
*MATH*. Guided by this objective, the model embarks on new
tasks *MATH*, generating solutions *MATH* and
receiving feedback *MATH* from the environment,
*MATH*. This stage culminates in the acquisition of new experiences *MATH*.


Experience Refinement


After experience acquisition, the model examines and refines these
experiences. This involves discarding incorrect data and enhancing
imperfect ones, resulting in refined outcomes *MATH*.


Updating


Leveraging the refined experiences, the model undergoes an update
process, integrating *MATH* into its
framework. This ensures the model remains current and optimized.


Evaluation


The cycle concludes with an evaluation phase, where the model&apos;s
performance is assessed through an evaluation in external environment.
The outcomes of this phase inform the objective *MATH*,
setting the stage for the subsequent iteration of self-evolution.


The conceptual framework outlines the self-evolution of LLMs, akin to
human-like acquisition, refinement, and autonomous learning processes.
We illustrate our taxonomy in Figure [3].


FIGURE


Evolution Objectives 


Evolution objectives in self-evolving LLMs serve as predefined goals
that autonomously guide their development and refinement. Much like
humans set personal objectives based on needs and desires, these
objectives are crucial as they determine how the model iteratively
self-updates. They enable the LLM to autonomously learn from new data,
optimize algorithms, and adapt to changing environments, effectively
\&quot;feeling\&quot; its needs from feedback or self-assessment and setting its
own goals to enhance functionality without human intervention.


We define an evolution objective as combining an evolving ability and an
evolution direction. An evolving ability stands for an innate and
detailed skill. The evolution direction is the aspect of the evolution
objective aiming to improve. We formulate the evolution objective as
follows: *MATH*  where *MATH* is the evolution objective, composed by
evolving abilities *MATH* and evolution directions
*MATH*. Take \&quot;reasoning accuracy improving\&quot; as an
example, \&quot;reasoning\&quot; is the evolving ability and \&quot;accuracy
improving\&quot; is the evolution direction.


TABLE 


In Table [1], we summarize and categorize the targeted
evolving abilities in current self-evolution research into two groups: LLMs and LLM Agents.


LLMs


These are fundamental abilities underlying a broad spectrum of
downstream tasks.


Instruction Following: The capability to follow instructions is
essential for effectively applying language models. It allows these
models to address specific user needs across different tasks and
domains, aligning their responses within the given context [*REF*].


Reasoning: LLMs can self-evolve to recognize statistical patterns,
making logical connections and deductions based on the information. They
evolve to perform better reasoning involving methodically dissecting
problems in a logical sequence [*REF*].


Math: LLMs enhance the intricate ability to solve mathematical
problems covering arithmetic, math word, geometry, and automated theorem
proving [*REF*] towards self-evolution.


Coding: Methods improve the LLM coding abilities to generate more
precise and robust programs [*REF*; *REF*].
Furthermore, EvoCodeBench [*REF*] provides an evolving
benchmark that updates periodically to prevent data leakage.


Role-Play: It involves an agent understanding and acting out a
particular role within a given context. This is crucial in scenarios
where the model must fit into a social structure or follow a set of
behaviors associated with a specific identity or
function [*REF*].


Others: Apart from the above fundamental evolution objectives,
self-evolution can also achieve and a wide range of NLP
tasks [*REF*; *REF*; *REF*; *REF*; *REF*].


LLM-based Agents


The abilities discussed here are characteristic of advanced artificial
agents used for task-solving or simulations in digital or physical
world. These capabilities mirror human cognitive functions, allowing
these agents to perform complex tasks and interact effectively in
dynamic environments.


Planning: It involves the ability to strategize and prepare for
future actions or goals. An agent with this skill can analyze the
current state, predict the outcomes of potential actions, and create a
sequence of steps to achieve a specific objective [*REF*].


Tool Use: This is the capacity to employ objects or instruments in
the environment to perform tasks, manipulate surroundings, or solve
problems [*REF*].


Embodied Control: It refers to an agent&apos;s ability to manage and
coordinate its physical form within an environment. This encompasses
locomotion, dexterity, and the manipulation of
objects [*REF*].


Communication: It is the skill to convey information and understand
messages from other agents or humans. Agents with advanced communication
abilities can participate in dialogue, collaborate with others, and
adjust their behaviour based on the communication
received [*REF*].


Evolution Directions


Examples of evolution directions include but are not limited to: 
Improving Performance: The goal is to continuously enhance the
model&apos;s understanding and generation across various languages and
abilities. For instance, a model initially trained for question
answering and chitchat can autonomously extend its proficiency and
develop abilities like diagnostic dialogue [*REF*], social
skills [*REF*], and role-playing [*REF*].


Adaptation to Feedback: This involves improving model responses
based on feedback to better align with preferences or adapt to
environments [*REF*; *REF*].


Expansion of Knowledge Base: The aim is to continuously update the
model&apos;s knowledge base with the latest information and trends. For
example, a model might automatically integrate new scientific research
into its responses [*REF*].


Safety, Ethic and Reducing Bias: The goal is to identify and
mitigate biases in the model&apos;s responses, ensuring fairness and safety.
One effective strategy is to incorporate guidelines, such as
constitutions or specific rules, to identify inappropriate or biased
responses and correct them through model updates [*REF*; *REF*].


Experience Acquisition 


Exploration and exploitation [*REF*] are fundamental
strategies for learning in humans and LLMs. Among that, exploration
involves seeking new experiences to achieve objectives and is analogous
to the initial phase of LLM self-evolution, known as experience
acquisition. This process is crucial for self-evolution, enabling the
model to autonomously tackle core challenges such as adapting to new
tasks, overcoming knowledge limitations, and enhancing solution
effectiveness. Furthermore, experience is a holistic construct,
encompassing not only the tasks encountered [*REF*] but
also the solutions developed to address these
tasks [*REF*], and the feedback [*REF*]
received as a result of task performance.


Inspired by that, we divide experience acquisition into three parts: task evolution, 
solution evolution, and obtaining feedback. In task
evolution, LLMs curate and evolve new tasks aligning with evolution
objectives. For solution evolution, LLMs develop and implement
strategies to complete these tasks. Finally, LLMs may optionally collect
feedback from interacting with the environment for further improvements.


Task Evolution 


To gain new experience, the model first evolves new tasks according to
the evolution objective *MATH* in the current iteration.
Task evolution is the crucial step in the engine that starts the entire
evolution process. Formally, we denote the task evolution as: *MATH*
where *MATH* is the task evolution function.
*MATH*, *MATH*, and *MATH* denote the
evolution objective, the model, and the evolved task at iteration *MATH*,
respectively. We summarize and categorize existing studies on the task
evolution method *MATH* into three groups: Knowledge-Based, Knowledge-Free, and Selective. We detail each
type in the following parts and show the concepts in
Figure [4].


FIGURE


Knowledge-Based


The objective *MATH* may associate with external knowledge
to evolve where the knowledge is not inherently comprised in the current
LLMs. Explicitly sourcing from knowledge enriches the relevance between
tasks and evolution objectives. It also ensures the validity of relevant
facts in the tasks. We delve into the Knowledge-Based methods seeking to
evolve new tasks of the evolving objective assisted by external
information.


The first kind of knowledge is structured. Structured knowledge is dense
in information and well-organized. Self-Align [*REF*] guides
task generation by covering 20 topics, such as scientific and legal
expertise, to ensure diversity. Apart from topic knowledge,
DITTO [*REF*] includes character knowledge from Wikidata and
Wikipedia. The knowledge comprises attributes, profiles, and concise
character details for role-play conversations. SOLID [*REF*]
generates structured entity knowledge as conversation starters.


The second group consists of tasks evolving from an unstructured
context. Unstructured context is easy to obtain but is sparse in
knowledge. UltraChat [*REF*] gathers unstructured knowledge
of 20 types of text materials to construct questions or instructions.
SciGLM [*REF*] derives questions from the text of diversified
science subjects, which covers rich scientific knowledge.
EvIT [*REF*] derives event reasoning tasks based on large-scale
unstructured events mined from the unsupervised corpus. Similarly,
MEEL [*REF*] evolves multi-modal events in both image and text to
construct the tasks for MM event reasoning.


Knowledge-Free


Unlike previous methods that require extensive human effort to gather
external knowledge, Knowledge-Free approaches operate independently
using the evolving object *MATH* and the model itself.
These efficient methods can generate more diversified tasks without
additional knowledge restrictions.


First, the LLMs can prompt themselves to generate new tasks according to
*MATH*. Self-Instruct [*REF*; *REF*; *REF*]
is a typical methodology of Knowledge-Free task evolution. These methods
self-generate a variety of new task instructions based on evolution
objectives. Ada-Instruct [*REF*] further proposes an adaptive task
instruction generation strategy that fine-tunes open-source LLMs to
generate lengthy and complex task instructions for code completion and
mathematical reasoning.


Second, extending and boosting original tasks increases the quality of
instructions. WizardLM [*REF*] proposes Evol-Instruct that
evolves instruction following tasks with in-depth and in-breadth
evolving and further expands it in code
generation [*REF*]. MetaMath [*REF*] rewrites
the question in multiple ways, including rephrasing, self-verification,
and FOBAR. It evolves a new MetaMathQA dataset for fine-tuning LLMs to
improve mathematical task-solving.
Promptbreeder [*REF*] evolves seed tasks via
mutation prompts. It further evolves mutation prompts via the hyper
mutation prompts to increase the task diversity.


Third, deriving tasks from plain text is another way.
Backtranslation [*REF*] extracts self-contained segments in
unlabelled data and regards it as the answers to tasks. Similarly,
Kun [*REF*] presents a task self-evolving algorithm utilizing
instruction harnessed from unlabelled data towards back-translation.


Selective


Instead of task generation, we may start with large-scale existing
tasks. At each iteration, LLMs can select tasks that exhibit the highest
relevance to the current evolving objective *MATH* without
additional generation. This approach obviates the intricate curation of
new tasks, streamlining the evolution
process [*REF*; *REF*; *REF*].


A simple task selecting method is to randomly sample tasks from the task
pool like REST [*REF*], REST *MATH*  [*REF*], and GRATH [*REF*] do. Rather
than random selection, DIVERSE-EVOL [*REF*] introduces a data
sampling technique where the model selects new data points based on
their distinctiveness in the embedding space, ensuring diversity
enhancement in the chosen subset. SOFT [*REF*] then splits the
initial training set. Each iteration selects one chunk of the split set
as the evolving task.


*REF* propose Selective Reflection-Tuning and select a subset
of tasks via a novel metric calculating to what extent the answer is
related to the question. V-STaR [*REF*] selects the correct
solutions in the previous iteration and adds their task instructions to
the task set of the next iteration.


Solution Evolution 


FIGURE 


After obtaining evolved tasks, LLMs solve the tasks to acquire the
corresponding solution. The most common strategy is to generate the
solution directly according to the task
formulation [*REF*; *REF*; *REF*; *REF*; *REF*].
However, this straightforward approach might reach solutions irrelevant
to the evolution objective, leading to suboptimal
evolution [*REF*]. Therefore, solution evolution uses
different strategies to solve tasks and enhance LLM capabilities by
ensuring that solutions are not just generated but are also relevant and
informative. In this section, we comprehensively survey these strategies
and illustrate them in
Figure [5]. We first formulate the
solution-evolution as follows: *MATH* 
where *MATH* is the model&apos;s strategy to approach the
evolution objective.


We then categorize these methods into positive and negative according to
the correctness of the solutions. The positive methods introduce various
approaches to acquire correct and desirable solutions. On the contrary,
negative methods elicit and collect undesired solutions, including
unfaithful or mis-align model behaviors, which are then used for
preference alignment. We elaborate on the details of each type in the
following sections.


Positive


Current studies explore diverse methods beyond vanilla inference for
positive solutions to obtain correct solutions aligned with evolution
objectives. We categorize the task-solving process into four types: 
Rationale-Based, Interactive, Self-Play, and Grounded.


Rationale-Based


The model incorporates rationale explanations towards approaching the
evolving objective when solving the tasks and can self-evolve by
utilizing such rationales. These methods enable models to explicitly
acknowledge the evolution objective and complete this task in that
direction [*REF*; *REF*; *REF*; *REF*].


*REF* proposes a method where an LLM self-evolves using
\&quot;high-confidence\&quot; rationale-augmented answers generated for unlabeled
questions. Similarly, STaR [*REF*] generates rationale when
solving the task. If the answer is wrong, it further corrects the
rationale and the answer. Then, it uses the answer and rationale as
experiences to fine-tune the model. Similarly,
LSX [*REF*] proposes the novel paradigm to generate an
explanation of the answer, incorporating an iterative loop between a
learner module performing a base task and a critic module that assesses
the quality of explanations given by the learner. *REF* 
[*REF*] obtain rationales in the ReAct [*REF*] style
when solving the tasks. The rationales are further engaged in training
the agents in the following step.


Interactive


Models can interact with the environment to enhance the evolution
process. These methods can obtain environmental feedback that is
valuable for guiding self-evolution directions.


SelfEvolve and LDB [*REF*; *REF*] improve code
generation ability via self-evolution. They allow the model to generate
code and acquire feedback via running the code on the interpreter. As
another environment, *REF* [*REF*] interact in
embodied scenarios and acquire feedback. They learn to take proper
actions based on their current state. For agent abilities,
AutoAct [*REF*] introduces self-planning from scratch,
focusing on an intrinsic self-learning process. In this process, agents
enhance their abilities through recursive planning iterations with
environment feedback. Following AutoAct, [*REF*] further
enhances agent training by integrating self-evolution and an external
action knowledge base. This approach guides action generation and boosts
planning ability through environment-driven corrective feedback loops.


Self-Play


It&apos;s the situation where a model learns to evolve by playing against
copies of itself. Self-play is a powerful evolving method because it
enables systems to communicate with themselves to get feedback in a
closed loop. It&apos;s especially effective in environments where the model
can simulate various sides of the roles, like multi-player
games [*REF*; *REF*]. Compared with
interactive methods, self-play is an effective strategy to obtain
feedback without an environment.


*REF* investigate the systematic biases in
simulations of debates by LLMs. On the contrary to debating,
*REF* engage LLMs in conversations following generated
principles. Another kind of conversation via role-playing. *REF* 
proposes self-simulated role-play conversation. The process involves
instructing the LLM with character profiles and aligning its responses
to maintain consistency with the character&apos;s knowledge and style.
Similarly, *REF* propose SOLID to generate large-scale
intent-aware role-play dialogues. This self-playing aspect harnesses the
expansive knowledge of LLMs to construct information-rich exchanges that
streamline the dialog generation process. *REF* introduces a
novel approach whereby each LLM follows a role and communicates with
each other to achieve their goals.


Grounded


To reach the evolving objective and reduce exploration space, models can
be grounded on existing rules [*REF*] and previous
experiences for further explicit guidance when solving the tasks.


LLMs can generate desirable solutions more effectively by being grounded
on pre-defined rules and principles. For instance,
Self-Align [*REF*] generated self-evolved questions with
principle-driven constraints to guide the task-solving process.
SALMON [*REF*] design a set of combined principles that
requires the model to follow when solving the task.
Self-Talk [*REF*] ensures the LLMs generate a
workflow-aligned conversation based on preset agent characters. They
generate the workflow in advance based on GPT-4.


Besides pre-defined rules, grounding on previous experiences can improve
the solutions. MemoryBank [*REF*] and TiM [*REF*]
answer current questions by incorporating previous question-answer
records. Rather than previous solution histories, MoT [*REF*],
IML [*REF*], and TRAN [*REF*] incorporate induced
rules from the histories to answer new questions.
MemGPT [*REF*] combines these merits and retrieves previous
questions, solutions, induced events, and user portrait knowledge.


Negative


In addition to acquiring positive solutions, recent research illustrates
that LLMs can benefit from negative ones for
self-improvement [*REF*]. This strategy is analogous to
trial and error in human behavior when learning skills. This section
summarises typical methods of gaining negative solutions to assist in
self-evolution.


Contrastive


A widely used group of methods is to collect multiple solutions for a
task and then contrast the positive and negative ones to get
improvements.


For instance, Self-Reward [*REF*] generates responses, predicts
corresponding rewards by the model itself, and constructs preference
pairs based on rewards. SPIN [*REF*] uses the results generated
by the current model as negatives and the ground truth of SFT as
positives for iterative self-training. Similarly, GRATH [*REF*]
generates both correct and incorrect answers. It then trains the model
by comparing these two answers. Self-Contrast [*REF*]
contrasts the differences and summarizes these discrepancies into a
checklist that could be used to re-examine and eliminate discrepancies.
In ETO [*REF*], the model interacts with the embodied
environment to complete tasks and optimizes from the failure solutions.
A^T [*REF*] improves ETO by adding rationale after each
action for solving tasks. STE [*REF*] implements trial and error
where the model solves the tasks with unfamiliar tools. It learns by
analyzing failed attempts to improve problem-solving strategies in
future tasks. More recently, COTERRORSET [*REF*] obtains
incorrect solutions generated by PALM-2 and proposes mistake tuning,
which requires the model to void making mistakes.


Perturbative


Compared to Contrastive, Perturbative methods seek to add
perturbations to obtain negative solutions intentionally. Models can
later learn to avoid generating these negative answers. Adding
perturbations to obtain negative solutions is more controllable than
contrastive methods.


Some methods add perturbation to generate harmful
solutions [*REF*; *REF*]. Given a task,
RLCD [*REF*] curates both positive and negative instructions and
generates positive and negative solutions. DLMA [*REF*] gathers
both positive and negative instructional prompts and subsequently
produces corresponding positive and negative solutions.


Rather than harmful perturbation, incorporating negative context is
another way. Ditto [*REF*] adds negative persona characters to
generate incorrect conversations. The model then learns from the
negative conversations to evolve persona dialogue ability.


Feedback 


FIGURE


As humans learn skills, feedback plays a critical role in demonstrating
the correctness of the solutions. This key information enables humans to
reflect and then update their skills. Akin to this process, LLMs should
obtain feedback during or after the task solution in the cycle of
self-evolution. We formalize the process as follows: *MATH* 
where *MATH* is the method to acquire feedback.


In this part, we summarize two types of feedback. Model feedback refers
to gathering the critique or score rated by the LLMs themselves.
Besides, Environment denotes the feedback received directly from the
external environment. We illustrate these concepts in
Figure [6].


Model


Current studies demonstrate that LLMs can play well as a
critic [*REF*]. In the cycle of self-evolution, the model
judges itself to acquire the feedback of the solutions.


One type of feedback is a score that indicates correctness.
Self-Reward [*REF*], LSX  *REF*, and
DLMA [*REF*] rate their own solutions and output the scores via
LLM-as-a-Judge prompting. Similar to that, SIRLC [*REF*]
utilizes self-evaluation results of LLM as the reward for further
reinforcement learning. Self-Alignment [*REF*] leverages the
self-evaluation capability of an LLM to generate confidence scores on
the factual accuracy of its outputs.


Another type provides a textual description, offering multi-dimensional
information. To alter the distribution of the responses via supervised
learning, CAI [*REF*] asks the model to critique its
response according to a principle in the constitution. In contrast to
supervised learning and reinforcement learning approaches,
Self-Refine [*REF*] allows the model to generate natural
language feedback on its own output in a few-shot manner.


Environment


Another form of feedback comes from the environment, common in tasks
where solutions can be directly evaluated. This feedback is precise and
elaborate and can provide sufficient information for model updating.
They may be derived from code
interpreter [*REF*; *REF*; *REF*],
tool execution [*REF*; *REF*], the embodied
environment [*REF*; *REF*; *REF*], and
other LLMs or agents [*REF*; *REF*; *REF*].


For code generation, Self-Debugging  *REF* utilizes execution
results on test cases as part of feedback while
SelfEvolve [*REF*] receives the error message from the
interpreter. Similarly, Reflexion [*REF*] also obtains the
run-time feedback from the code interpreter. It then further reflects to
generate thoughts. This run-time feedback contains the trace-back
information that can point out the key information for improved code
generation.


Recently, methods endow tool-using ability to LLMs and agents. Executing
tools leading to feedback in return [*REF*; *REF*; *REF*; *REF*; *REF*].


RoboCat [*REF*] and SinViG [*REF*] act in the
robotic embodied environment. This type of feedback is precise and
strong to guide self-evolution.


Communication feedback is common and effective in LLM-based multi-agent
systems. Agents can correct and support each other, enabling
co-evluation [*REF*; *REF*; *REF*].


Experience Refinement 


After experience acquisition and before updating in self-evolution, LLMs
may improve the quality and reliability of their outputs through
experience refinement. It helps LLMs adapt to new information and
contexts without relying on external resources, leading to more reliable
and effective assistance in dynamic environments. This process is
formulated as follows: *MATH* where *MATH* is the methods of experience refinement,
*MATH* are the refined tasks and solutions. We classify the methods into two categories: filtering and correcting.


FIGURE


Filtering


Refinement in self-evolution involves two primary filtering strategies: Metric-Based and Metric-Free. The former uses external metrics to
assess and filter outputs, while the latter does not rely on these
metrics. This ensures that only the most reliable and high-quality data
is utilized for further updating.


Metric-Based


By relying on feedback and pre-defined criteria, metric-based filtering
improves the quality of the outputs [*REF*; *REF*; *REF*; *REF*],
ensuring the progressive enhancement of LLM capabilities through each iteration of refinement.


For example, ReST *MATH*  [*REF*] incorporates a reward
function to filter the dataset sampled from the current policy. The
function provides binary rewards based on the correctness of the
generated samples rather than a learned reward model trained on human
preferences in ReST [*REF*].
AutoAct [*REF*] leverages F1-score and accuracy as rewards
for synthetic trajectories and collects trajectories with exactly
correct answers for further training.
Self-Talk [*REF*] measures the number of completed
subgoals to filter the generated dialogues, ensuring that only
high-quality data is used for training. To encourage diversity of the
source instructions, Self-Instruct [*REF*] automatically filters
low-quality or repeated instructions using ROUGE-L similarity and
heuristics before adding them to the task pool.


The filtering criteria or metrics are crucial for maintaining the
quality and reliability of the generated outputs, thereby ensuring the
continuous improvement of the model&apos;s capability.


Metric-Free


Some methods seek filtering strategies beyond external metrics, making
the process more flexible and adaptable. Metric-Free filtering
typically involves sampling outputs and evaluating them based on
internal consistency measures or other model-inherent
criteria [*REF*; *REF*; *REF*]. The filtering
in Self-Consistency [*REF*] is based on the consistency of the
final answer across multiple generated reasoning paths, with higher
agreement indicating higher reliability. LMSI [*REF*] utilizes
CoT prompting plus self-consistency for generating high-confidence
self-training data.


Designing internal consistency measures that accurately reflect output
quality can be challenging. Self-Verification [*REF*] allows the
model to select the candidate answer with the highest interpretable
verification score, calculated by assessing the consistency between the
predicted and original condition values. For the code generation task,
CodeT [*REF*] considers both the consistency of the outputs
against the generated test cases and the agreement of the outputs with
other code samples.


These methods emphasize the language model&apos;s ability to self-assess and
filter its outputs based on internal agreement, showcasing a significant
step forward in self-evolution without the direct intervention of
external metrics.


Correcting


Recent advancements emphasize the importance of iterative
self-correction in LLMs, allowing for the refinement of experiences.
This section divides the methods employed into two categories: 
Critique-Based and Critique-Free correction. Critiques often serve
as strong hints that include the rationale behind perceived errors or
suboptimal outputs, guiding the model towards improved iterations.


Critique-Based


These methods rely on additional judging processes to draw the critiques
of the experiences. Then, the experiences are refined based on the
critiques. By leveraging either
self-generated [*REF*; *REF*; *REF*; *REF*]
or environment-interaction generated
critiques [*REF*; *REF*; *REF*], the
model benefits from detailed feedback for nuanced correction.


LLMs have demonstrated their ability to identify errors in their
outputs. Self-Refine [*REF*] introduces an iterative process in
which the model refines its initial outputs conditioned on actionable
self-feedback without additional training. To evolve from the
correction, CAI [*REF*] generates critiques and
revisions of its outputs in the supervised learning phase, significantly
improving the initial model. Applied to an agent automating computer
tasks, RCI [*REF*] improves its previous outputs based on the
critique finding errors in the outputs.


Since weaker models may struggle significantly with the self-critique
process, several approaches enable models to correct the outputs using
critiques provided by external tools. CRITIC [*REF*] allows
LLMs to revise the output based on the critiques obtained during
interaction with tools in general domains.
SelfEvolve [*REF*] prompts an LLM to refine the answer
code based on the error information thrown by the interpreter. ISR-LLM
[*REF*] helps the LLM planner find a revised action plan by using
a validator in an iterative self-refinement process.


The primary advantage of this method lies in its ability to process and
react to detailed feedback, potentially leading to more targeted and
nuanced corrections.


Critique-Free


Contrary to critique-based, critique-free methods correct the
experiences directly leveraging objective
information [*REF*; *REF*; *REF*; *REF*].
These methods offer the advantage of independence from nuanced feedback
that critiques provide, allowing for corrections that adhere strictly to
factual accuracy or specific guidelines without the potential bias
introduced by critiques.


One group of critique-free methods modifies the experiences on the
signal of whether the task was correctly resolved. Self-Taught Reasoner
(STaR) [*REF*] proposes a technique that iteratively
generates rationales to answer questions. If the answers are incorrect,
the model is prompted again with the correct answer to generate a more
informed rationale. Self-Debug [*REF*] enables the model to
perform debugging steps by investigating execution results from unit
tests and explaining the code on its own.


Different from depending on the task-solving signal, other information
produced during the solving process can be leveraged.
IterRefinement [*REF*] relies on a series of refined prompts
that encourage the model to reconsider and improve upon its previous
outputs without any direct critique. For information extraction tasks,
Clinical SV [*REF*] grounds each element in evidence from the
input and prunes inaccurate elements using supplied evidence.


These critique-free approaches simplify the correction mechanism,
allowing for easier implementation and faster adjustments.


FIGURE


Updating 


After experience refinement, we enter the crucial updating phase that
leverages the refined experiences to improve model performance. We
formulate updating as follows: *MATH* where *MATH* is the updating functions. These update
methods keep the model effective by adapting to new experiences and
continuously improving performance in changing environments and during
iterative training.


We divide these approaches into in-weight learning, which involves
updates to model weights, and in-context learning, which involves
updates to external or working memory.


In-Weight


Classical training paradigms in updating LLMs in weight encompass
continuous pretraining [*REF*; *REF*],
supervised fine-tuning [*REF*], and preference alignment
[*REF*; *REF*]. However, in the iterative
training process of self-evolving, the core challenge lies in
achieving overall improvement and preventing catastrophic
forgetting, which entails refining or acquiring new capabilities while
preserving original skills. Solutions to this challenge can be
categorized into three main strategies: replay-based,
regularization-based, and architecture-based methods.


Replay-based


Replay-based methods reintroduce previous data to retain old knowledge.
One is experience replay, which mixes the original and new training data
to update LLMs [*REF*; *REF*; *REF*; *REF*; *REF*].
For example, Reinforced Self-Training (ReST)
[*REF*; *REF*] method iteratively updates
large language models by mixing seed training data with filtered new
outputs generated by the model itself. AMIE [*REF*] utilizes a
self-play simulated learning environment for iterative improvement and
mixes generated dialogues with supervised fine-tuning data through inner
and outer self-play loops. SOTOPIA- *MATH* [*REF*] leverages
behavior cloning from the expert model and self-generated social
interaction trajectory to reinforce positive behaviors.


Another is generative replay, which adopts the self-generated
synthesized data as knowledge to mitigate catastrophic forgetting. For
instance, Self-Synthesized Rehearsal (SSR) [*REF*]
generates synthetic training instances for rehearsal, enabling the model
to preserve its ability without relying on real data from previous
training stages. Self-Distillation Fine-Tuning (SDFT) [*REF*]
generates a distilled dataset from the model itself to bridge the
distribution gap between task datasets and the LLM&apos;s original
distribution to mitigate catastrophic forgetting.


Regularization-based


Regularization-based methods constrain the model&apos;s updates to prevent
significant deviations from original behaviors, exemplified by function-
and weight-based regularization. Function-based regularization focuses
on modifying the loss function that a model optimizes during training
[*REF*; *REF*]. For example, InstuctGPT
[*REF*] employs a per-token KL-divergence penalty from the
output probabilities of the initial policy model *MATH* on the
updated policy model *MATH*. FuseLLM [*REF*]
employs a technique akin to knowledge distillation
[*REF*], leveraging the generated probability
distributions from source LLMs to transfer the collective knowledge to
the target LLM.


Weight-based regularization [*REF*] directly
targets the model&apos;s weights during training. Techniques such as Elastic
Reset [*REF*] counters alignment drift in RLHF by
periodically resetting the online model to an exponentially moving
average of its previous states. Furthermore, *REF* introduced
WARM, which combines multiple reward models through weight averaging to
address reward hacking and misalignment. Moreover, AMA
[*REF*] adaptively average model weights to optimize the
trade-off between reward maximization and forgetting mitigation.


Architecture-based


Architecture-based methods explicitly utilize extra parameters or models
for updating, including decomposition- and merging-based approaches.
Decomposition-based methods separate large neural network parameters
into general and task-specific components and only update the
task-specific parameters to mitigate forgetting. LoRA
[*REF*; *REF*] inject trainable low-rank matrices to
significantly reduce the number of trainable parameters while
maintaining or improving model performance across various tasks. This
paradigm is later adopted by GPT4tools [*REF*], OpenAGI
[*REF*] and Dromedary [*REF*]. Dynamic ConPET
[*REF*] combines pre-selection and prediction with
task-specific LoRA modules to prevent forgetting, ensuring scalable and
effective adaptation of LLMs to new tasks.


Merging-based methods, on the other hand, involve combining multiple
models or layers to achieve general improvements, including but not
limited to merging multiple generic and specialized model weights into a
single model [*REF*; *REF*; *REF*; *REF*],
through mixture-of-expert approach [*REF*] or even
layer-wise merging and up-scaling such as EvoLLM [*REF*].


In-Context


In addition to directly updating model parameters, another approach is
to leverage the in-context capabilities of LLMs to learn from
experiences, thereby enabling fast adaptive updates without expensive
training costs. The methods could be divided into updating external and
working memory.


TABLE


External Memory


This approach utilizes an external module to collect, update, and
retrieve past experiences and knowledge, enabling models to access a
rich pool of insights and achieve better results without updating model
parameters. The external memory mechanism is common in AI Agent
systems [*REF*; *REF*; *REF*].
This section provides a detailed overview of the latest methods for
updating external memory, emphasizing the aspects of memory Content
and Updating Operations, and summarized in Table [2].


Content: External memory mainly stores two types of content: past
experiences and reflected rationale, each serving distinct purposes. For
instance, past experience provides valuable historical context, serving
as a guiding force toward achieving improved outcomes. MoT [*REF*]
archives filtered question-answer pairs to construct a beneficial memory
repository. Additionally, the FIFO Queue mechanism in
MemGPT [*REF*] maintains a rolling history of messages,
encapsulating interactions between agents and users, system
notifications, and inputs and outputs of function calls.


On the other hand, reflected rationales offer condensed explanations,
such as rules, that support decision-making. For instance,
TRAN [*REF*] archives rules inferred from experiences
alongside information on mistakes to mitigate future errors.
Correspondingly, TiM [*REF*] preserves inductive reasoning,
defined as text elucidating the relationships between entities.
Moreover, IML [*REF*] and ICE [*REF*] store
comprehensive notes and rules derived from a series of trajectories,
demonstrating the broad spectrum of content types that memory systems
can accommodate.


MemoryBank [*REF*] and AesopAgent [*REF*]
reflect on experience and rationale, save them as external memory, and
retrieve them when needed to achieve better performance.


Updating Operation: We categorize the operations to the memory into
Insert, Reflect, and Forget. The most common operation is insert,
methods insert text content into the memory for
storage [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Another operation is reflection, which is to think and summarize
previous experiences to conceptualize rules and knowledge for future
use [*REF*; *REF*; *REF*; *REF*].
Last, due to the limited storage of memory, forgetting content is
crucial to keeping memory efficient and the content valid.
MemGPT [*REF*] adopts the FIFO queue to forget the contents.
MemoryBank [*REF*] establishes a forgetting curve on the
insert time of each item.


Working Memory


The methods use past experience to evolve the capabilities of agents by
updating internal memory streams, states, or beliefs, known as working
memory, often in the form of verbal cues.
Reflexion [*REF*] introduces verbal reinforcement learning
for decision-making improvement without conventional model updates.
Similarly, IML [*REF*] enables LLM-based agents to
autonomously learn and adapt to their environment by summarizing,
refining, and updating knowledge based on past experience directly in
working memory.


EvolutionaryAgent [*REF*] aligns agents with dynamically changing
social norms through evolution and selection principles, leveraging
environmental feedback for self-evolution. Agent-Pro [*REF*]
employs policy-level reflection and optimization, allowing agents to
adapt their behavior and beliefs in interactive scenarios based on past
outcomes. Lastly, ProAgent [*REF*] enhances cooperation in
multi-agent systems by dynamically interpreting teammates&apos; intentions
and adapting behavior.


These collective works demonstrate the importance of integrating past
experiences and knowledge into the agents&apos; memory stream to refine their
state or beliefs for improved performance and adaptability across
various tasks and environments.


Evaluation 


Much like the human learning process, it is essential to ascertain
whether the present level of ability is adequate and meets the
application requirements through evaluation. Furthermore, it is from
these evaluations that one can identify the direction for future
learning. However, how to accurately assess the performance of an
evolved model and provide directions for future improvements is a
crucial yet underexplored research area. For a given evolved model
*MATH*, we conceptualize the evaluation process as follows: *MATH* 
where *MATH* represents the evaluation function that measures the
performance score (*MATH*) of the current model and
provide evolving goal (*MATH*) for the next iteration.
Evaluation function *MATH* can be categorized into
quantitative and qualitative approaches, each providing valuable
insights into model performance and areas for improvement.


Quantitative Evaluation


This method focuses on providing measurable metrics to reliably assess
LLM performance, such as automatic [*REF*; *REF*]
and human evaluation. However, traditional automatic metrics struggle to
accurately evaluate increasingly complex tasks, and human assessment is
not an ideal option for autonomous self-evolution. Recent trends use
LLMs as human proxy for automatic evaluators, offering cost-effective
and scalable solutions for evaluations.


For example, reward model score has been widely used to measure model or
task performances [*REF*] and select the best checkpoint
[*REF*]. LLM-as-a-judge [*REF*] using LLMs to
evaluate LLMs, employing methods like pairwise comparison, single answer
grading, and reference-guided grading. It shows that LLMs can closely
match human judgment, enabling efficient large-scale evaluations.


Qualitative Evaluation


Qualitative evaluation involves case studies and analysis to derive
insights, offering evolving guidance for subsequent iterations.
Initiatives such as LLM-as-a-judge [*REF*] provide the
reasoning behind its assessments; ChatEval [*REF*] explores
the strengths and weaknesses of model outputs through debate mechanisms.
Furthermore, TRAN [*REF*] leverages past errors to formulate
rules that enhance future LLM performances. Nonetheless, compared with
instance-level critic or reflection, qualitative evaluation at the task-
or model-level still needs comprehensive investigation.


Open Problems 


Objectives: Diversity and Hierarchy 


Section [3] summarizes existing evolution objectives and
their coverage. Nonetheless, these highlighted objectives can only
satisfy a small fraction of the vast human needs. The extensive
application of LLMs across various tasks and industries highlights
unresolved challenges in establishing self-evolution frameworks for
evolving objectives that can comprehensively address a broader spectrum
of real-world tasks [*REF*].


Furthermore, the concept of evolving objectives entails a potential
hierarchical structure; for instance, UltraTool [*REF*] and
T-Eval [*REF*] categorize the capability of tool usage into various
sub-dimensions. Exploring evolutionary objectives into manageable
sub-goals and pursuing them individually emerges as a viable strategy.


Overall, a clear and urgent need exists to develop self-evolution
frameworks that effectively address diversified and hierarchical
objectives.


Level of Autonomy: From Low to High 


Self-evolution in large models is emerging, yet lacks clear definitions
for its autonomous levels. We categorize self-evolution into three
tiers: low, medium, and high-level autonomy.


Low-level


In this level, the user predefined the evolving object *MATH* 
and it remains unchanged. The user needs to design the evolving
pipeline, namely all modules *MATH*, on its own. Then, the model
completes the self-evolution process based on the designed framework. We
denote this level of self-evolution in the following formula: *MATH*
where *MATH* denotes the model to be evolved. *MATH* 
is the evolving output. *MATH* is the environment. Most of the
current works lie at this level.


Medium-level


In this level, the user only sets the evolving object *MATH* 
and keeps it unchanged. The user doesn&apos;t need to design the specific
modules *MATH* in the framework. The model can construct each
module *MATH* independently for self-evolution. This level
denotes as follows: *MATH*.


High-level


In the final level, the model diagnoses its deficiency and constructs
the self-evolution methods to improve itself. This is the ultimate
purpose of self-evolution. The user model sets its own evolving object
*MATH* according to the evaluation *MATH* output. The evolving
objective would change during the iteration. Besides, the model designs
the specific modules *MATH* in the framework. We represent this
level as: *MATH*.


As discussed in the previous open problem
(§ [8.1]), there are a large of unfulfilled
objectives. However, most of the existing self-evolution frameworks are
at the Low-level which requires specifically designed
modules [*REF*; *REF*; *REF*]. These
frameworks are objective-dependent and rely on large human efforts to
develop. Exhausting all objectives are not deployment-efficient which
brings about the urgent need to develop medium and high levels
self-evolution frameworks. At the medium level, it doesn&apos;t require
expert efforts to design specific modules. LLMs can self-evolve
according to targeted objectives. Then at the high level, LLMs can
investigate their current deficiencies and evolve in a targeted manner.
In all, developing highly autonomous self-evolution frameworks remains
an open problem.


Experience Acquisition and Refinement: From Empirical to Theoretical


Suppose we have addressed the previous two challenges and developed
promising self-evolution frameworks, but the exploration of
self-evolution LLMs still lacks solid theoretical grounding. This idea
posits that LLMs can self-improve or correct their outputs, with or
without feedback from the environment. However, the mechanisms behind it
remain unclear. Studies show mixed results: *REF* observed
self-corrective behavior in models with over 22 billion parameters,
while *REF* finds LLMs struggle to self-correct reasoning
errors without external feedback.


A related challenge is the use of self-generated data for learning.
Critics argue this approach could reduce linguistic diversity
[*REF*] and lead to \&quot;model collapse,\&quot; where models fail to
capture complex, long-tailed data distributions [*REF*].
Furthermore, *REF* reveal that generative models trained
on their synthetic outputs progressively lose output quality and
diversity. *REF* extend this by theoretically analyzing the
impact of self-consuming training loops on model performance,
emphasizing the importance of balancing synthetic and real data to
mitigate error accumulation.


Recent studies [*REF*; *REF*] also show that current
methods struggle to improve after more than three rounds of
self-evolution. One hypothesized reason is that the self-critic of LLM
has not co-evolved with the evolving objective, but more experimental
and theoretical support is still needed. These findings highlight a
pressing need for more theoretical exploration in self-evolving LLMs.
Addressing these concerns is crucial for advancing the field and
ensuring that models can effectively learn and improve over time.


Updating: Stability-Plasticity Dilemma


The stability-plasticity dilemma represents a crucial yet unresolved
challenge that is essential for iterative self-evolution. This dilemma
reflects the difficulty of balancing the need to retain previously
learned information (stability) while adapting to new data or tasks
(plasticity). Existing LLMs either overlook this issue or adopt
conventional methods that may be ineffective. While training models from
scratch could mitigate the problem of catastrophic forgetting, it is
highly inefficient, particularly as model parameters increase
exponentially and autonomous learning capabilities advance. Finding a
balance between acquiring new skills and preserving existing knowledge
is crucial for achieving effective and efficient self-evolution, leading
to overall improvement.


Evaluation: Systematic and Evolving


To effectively assess LLMs, a dynamic, comprehensive benchmark is
crucial. This becomes even more pivotal as we progress towards
Artificial General Intelligence (AGI). Traditional static benchmarks
risk obsolescence due to LLMs&apos; evolving nature and potential access to
test data through interacting with environments, such as search engines,
undermining their reliability. A dynamic benchmark, like Sotopia
[*REF*], proposes a solution by creating an LLM-based
environment tailored for evaluating the social intelligence of LLMs,
thereby avoiding the limitations posed by static benchmarks.


Safety and Superalignment


The advancement of LLMs opens the possibility for AI systems to achieve
or even surpass expert-level capabilities in both supportive and
autonomous decision-making. For safety, ensuring these LLMs align with
human values and preferences is crucial, particularly to mitigate
inherent biases that can impact areas such as political debates, as
highlighted by *REF*. OpenAI&apos;s initiative,
Superalignment [*REF*], aims to align a superintelligence
by developing scalable training methods, validating models for
alignment, and stress-testing the alignment process through scalable
oversight [*REF*], robustness [*REF*], automated
interpretability [*REF*], and adversarial testing. Although
challenges remain, Superalignment marks an initial attempt to develop a
self-evolving LLM that closely aligns with human ethics and values in a
scalable way.


Conclusion 


The evolution of LLMs towards self-evolution paradigms represents a
transformative shift in artificial intelligence akin to the human
learning process. It is promising to overcome the limitations of current
models that rely heavily on human annotation and teacher models. This
survey presents a comprehensive framework for understanding and
developing self-evolving LLMs, structured around iterative cycles of
experience acquisition, refinement, updating, and evaluation. By
detailing advancements and categorizing the evolution objectives within
this framework, we offer a thorough overview of current methods and
highlight the potential for LLMs to adapt, learn, and improve
autonomously. We also identify existing challenges and propose
directions for future research, aiming to accelerate the progress toward
more dynamic, intelligent, and efficient models. This work deepens the
understanding of self-evolving LLMs. It paves the way for significant
advancements in AI, marking a step towards achieving superintelligent
systems capable of surpassing human performance in complex real-world
tasks.