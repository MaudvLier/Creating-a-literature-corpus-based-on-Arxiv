ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy


Introduction


The rapid development of Large Language Models (LLMs) [(OpenAI, 2023; Touvron et al., 2023; Team et al., 2023; Jiang et al., 2024)] has led to the prosperity of language agents.
Leveraging the ability of LLMs, language agents have demonstrated impressive performances in diverse decision-making scenarios by interacting with the environments autonomously [(Wang et al., 2023; Mirchandani et al., 2023; Zheng et al., 2024; Wu et al., 2024)].


Recently, increasing efforts have been made to train language agents with open-sourced LLMs. The multi-step trajectories that describe the entire task-solving process of a language agent are used as training data, which consist of environmental observations, internal reasoning texts, and external actions. The collection of such trajectories is therefore essential, which are currently categorized into two paradigms in Fig. [1] (a) and (b). The first paradigm is to leverage expert demonstrations [(Yao et al., 2022)]. However, the expense of human labor hampers the scalability of the approach. Another paradigm is implementing different agent frameworks to gather diverse trajectories with proprietary LLMs [(Qin et al., 2023; Zeng et al., 2023; Chen et al., 2023; Aksitov et al., 2023)]. However, the exploration coverage in the training data is still upper-bounded by the full set of prompting techniques. Besides, implementing diverse agent frameworks requires considerable human efforts and proprietary LLM calls [(Yang et al., 2024)]. To ease the data collection process in diverse scenarios, [Yin et al. 2023] and [Zhang et al. 2024] propose unified data formats by elucidating the comprising submodules in agent trajectories. However, as obtained by converting human-annotated data or one single defaulted prompting scheme, the agent trajectories are still limited in diversity and scalability. Considering that an environment automatically returns observations and rewards with action inputs, it should serve as an infinite data generator. While [Song et al. 2024] propose an exploration-based agent framework for self-improvement, the gathered trajectories consist of only interleaved external actions and environmental observations, without textual rationales that could steer better behavior of language agents. We ask the following question: Can a language agent autonomously gather high-quality trajectories, with textual annotations suitable for its further training?


FIGURE 1


In this work, we propose A3T, a framework that enables Autonomous Annotation of Agent Trajectories in the style of ReAct [(Yao et al., 2023)] for self-improvement with minimal human supervision. The central idea is to exploit both the in-context language ability and the decision-making ability of a language agent: To collect diverse trajectories, an agent could randomly sample external actions from the action space at arbitrary steps. However, the corresponding reason for the sampled action should be annotated for a ReAct-style agent. To facilitate this, we propose ActRe, an act-then-reason prompting agent that explains the reason for the sampled action. With ActRe, the ReAct-style agent composes extra reason-then-act trajectories for each failed task by inversely prepending the ActRe-prompted reason to the randomly sampled action.
After the execution of each composed trajectory, the agent receives a terminal reward from the environment, which automatically annotates the quality of the trajectory. The gathered successful trajectories are then supplemented with the failed trajectory by the ReAct-style agent alone for contrastive self-training, where we use policy gradient methods [(Williams, 1992)] with binarized rewards for LLM fine-tuning. As new agents are trained, more trajectories can be gathered and accumulated, which forms a closed loop for the self-improvement of language agents as shown in Fig. [1-(c)].


We validate our A3T framework in the textual embodied environment AlfWorld [(Shridhar et al., 2021)] and the online shopping environment WebShop [(Yao et al., 2022)]. We use QLoRA [(Dettmers et al., 2023)] to fine-tune Mistral-7B-Instruct-v0.2 [(Jiang et al., 2023)] in the training experiments. Experimental performances demonstrate significant improvement over state-of-the-art agent techniques: On AlfWorld, our trained agent achieves a 96% success rate in unseen scenarios with a single trial. On WebShop, the success rate of our agent reaches 49%, which matches the average human performance (50%). In the setting of iterative refinement, after four rounds of data collection and contrastive self-training, the accumulated success rate becomes 100% on AlfWorld and 54.8% on WebShop, narrowing the gap with human experts (59.6% on WebShop). A3T paves the way for agents with improved autonomy through the closed loop of self-annotation and contrastive self-training.


A3T for Closed-Loop Self-Improvement


In this section, we introduce the closed-loop self-improvement for agents facilitated by the A3T framework. The loop contains two parts: autonomous trajectory annotation with the ActRe agent (Sec.
[2.1]), and the contrastive self-training process (Sec. [2.2]).


1. Autonomous Trajectory Annotation with ActRe


Agents are able to gather diverse trajectories by exploration.
However, for language agents like ReAct, the actions are inferred by first reasoning with LLMs. When the agent randomly samples an action that differs from the self-inferred one, a modified reason is needed to compose a full reason-then-act trajectory. [Yao et al. 2023] show that humans can modify the reasoning in the trajectory and prompt a ReAct-style agent for desirable actions. As humans can provide in-progress supervision, such a human-in-the-loop process still lacks scalability.


To automate the process, we propose a complementary ActRe prompting agent to synthesize the modified reasons by leveraging the in-context language ability of an LLM. ActRe inverts the causality of ReAct: while ReAct conditions the external action with a reason a priori, ActRe explains the reason a posteriori for an external action. The synergy of ActRe with ReAct facilitates the autonomous annotation of textual reasons: when the language agent randomly samples an external action, the reason for the sampled action is obtained by querying the ActRe prompting agent. The synthetic reason is then used as the condition of the sampled action for the ReAct-style agent. The progress of a trajectory is synchronized between the ReAct-style agent and the ActRe prompting agent, with the only difference in the order of intermediate reasoning and actions. The detailed workings are depicted below:


Denote ot, RSt, EAt as the environmental observation, internal reasoning, and external action at the t-th step, respectively. The trajectory of a ReAct-style agent reads: *MATH*.


At the end of each trajectory, the environment provides a terminal reward R to indicate whether the execution is successful. The reward R automatically annotates the quality of the entire trajectory. In this way, the language agent autonomously composes diverse ReAct-style trajectories without human annotation effort, paving the way for self-training.


2. Contrastive Self-Training


Language agents are trained by fine-tuning an LLM with the accumulated trajectories. While supervised fine-tuning (SFT) with high-quality data is widely adopted [(Zhou et al., 2023b; Singh et al., 2023)], in this work, we improve the awareness of the agent about the contrast between failed and successful trajectories in the same task with policy gradient methods [(Williams, 1992)]. In the context of ReAct-style language agents, a gathered trajectory τ with T steps reads τ = {o1, a1, o2, a2, · · ·. ot, aT}, with ot in token strings representing the t-step environmental observation, and at representing the textual action of the agent being either the internal reasoning RSt or the external action EAt. Given a total of M trajectories {τm}, we maximize the following objective as the estimation of policy gradient: *MATH* with R(τm) ≤ 1 as the score of the trajectory τm, and pθ as the LLM with parameters θ to be fine-tuned. While traditional policy gradient methods omit the pθ (om|am. om) world modeling part, in our work, we keep the term and tune pθ to learn a joint model of action and world modeling. This instructs the LLM to better align with the tasks and the environment.


For the gathered trajectories in each task, we filter the composed ones that result in unsuccessful task completion. This ensures that all the failed trajectories generated solely by the agent are paired with successful trajectories in the same task, and all the successful trajectories are retained in the set. Assume that in the same task, we have K ≥ 1 successful trajectories τ1, τ2, · · · τK and a failed trajectory τf. Then [Eq.(1)] for the K+1 trajectories can be structured as *MATH* where we use the fact that R(τk) = 1 for all k as they are successful trajectories. According to Eq. [(2)], we have the following remarks about shaping the reward of the failed trajectory:


REMARK 1


REMARK 2


REMARK 3


In implementation, we binarize the reward of the failed trajectories with R(τf) = −1. To address Remark 3, we let the agent collect multiple successful trajectories via diverse exploration to satisfy K &gt; 1. After training, the new agent would follow Sec.
[2.1] to gather more annotated trajectories. The trajectory set then continually grows as looping more rounds of data collection and agent training. For the training in each round, we use the accumulated trajectory set to fine-tune an LLM with Eq. [(1)]. Another implementation detail is that in the initial round, we use 1-shot ReAct prompting to gather the training trajectories instead of exploration and annotation for bootstrapping. The failed trajectory for each task is directly excluded as it is not paired with sampled successful trajectories. Eq.
[(1)] is therefore reduced to ReAct supervised fine-tuning in [Yao et al. 2023] for the training in Round 0. The latter rounds leverage explored trajectories via autonomous annotation, and self-training by Eq.
[(1)] with binarized rewards. Other details are covered in Appendix [A].


TABLE 1


Experiments


We conduct experiments on two benchmarks to valid the effectiveness of A3T: the textual embodied environment AlfWorld [(Shridhar et al., 2021)], and the online shopping environment WebShop [(Yao et al., 2022)]. The two benchmarks require a language agent to perform multi-step decision-making to accomplish a certain goal introduced in each task.


In A3T, we loop for 4 rounds of trajectory collection and agent training, with the initial round using ReAct prompting as the bootstrap of training data. No trajectories are gathered from testing tasks for training. We use gpt-3.5-turbo-instruct-0914 to implement the initial ReAct prompting, as well as the ActRe prompting agent that helps the trajectory composition in the latter 3 rounds. We use the open-sourced Mistral-7B-Instruct-v0.2 [(Jiang et al., 2023)] with QLoRA [(Dettmers et al., 2023)] finetuning for the training experiments.


We compare our A3T framework with multiple strong baselines, including methods like advanced prompting frameworks using GPT-4, specialized LLMs by full fine-tuning, and gpt-3.5-turbo-1106 fine-tuning. The results are reported in the following sections.


1. AlfWorld


Alfworld is a textual embodied environment where an agent needs to accomplish a high-level goal by reasoning about its situation and performing sequential low-level actuation.


TABLE 3


Covering 6 task types, the benchmark provides 3,553 tasks for training and 134 held-out tasks for unseen scenarios evaluation. We use 660 out of the 3,553 tasks for our experiments: 600 for training and 60 for validation. In each round, 40 trajectories are composed for each training task failed by the policy agent. See Appendix [A] for other implementation details.


Baseline methods are divided into two categories: the methods that make only a single trial in each test task, and the methods that perform iterative refinement in a test task. In the former category, we select BUTLER [(Shridhar et al., 2021)] with supervised training over 105 expert trajectories on each task type. We also select LM-BUTLER [(Micheli &amp; Fleuret, 2021)] that fine-tunes a full GPT2-medium by collecting expert demonstrations with the interactions from all the 3,553 tasks (with interleaved observations and external actions in each trajectory). We also compare with the best version of the fully fine-tuned AgentLM [(Zeng et al., 2023)] in the AlfWorld task (AgentLM-70B), which leverages trajectories from all 3,553 training tasks in AlfWorld and other tasks in different benchmarks. The ReAct prompting [(Yao et al., 2023)] is also categorized into this category, and we also rerun the method with gpt-3.5-turbo-instruct-0914, following their setting to use 6 distinct prompts and report the best performance. In the latter category, we select Reflexion [(Shinn et al., 2023)] that prompts GPT-3.5 to self-reflect with failed trials. We also compare with RAFA [(Liu et al., 2023)], a principled iterative planning framework using GPT-4 as the critic.


Tables [1] and [2] show the performance comparison of our framework. For the single trial setting, the overall success rate of our agent reaches 96% at 2-nd round and matches the prior SoTA (LM-BUTLER). However, our agent is trained with a QLoRA of 26M parameters and 600 training tasks, while LM-BUTLER is fine-tuned from a full GPT2-medium of 355M parameters and all 3,553 training tasks. Besides, our agent demonstrates constant performance improvements with the 4 rounds in the held-out seen evaluation scenarios and outperforms LM-BUTLER (Table [14] in Appendix [C.1]). For the iterative refinement setting, our agent obtains 100% success by accumulating the decision-making trials of all the 4 trained agents from each round. The accumulated trajectory set accounts for the significance of the performance. Table [3] shows that the success rate of the trajectories composed by the agent on the training tasks improves continually. More details are covered in Appendix [C.1].


2. WebShop


WebShop is an online shopping environment where an agent needs to purchase the most suitable item according to a provided instruction.
The agent should navigate through a sequence of query searches and button clicks on the website to accomplish the task.


WebShop provides a real-valued reward R ∈ [0, 1], with R = 1 as success. The benchmark provides 11,587 tasks for training and validation, and 500 held-out tasks as testing scenarios.


We use 2,700 out of the 11,587 tasks for our experiments, with 2,300 for training and 400 for validation. 20 trajectories are composed for each training task failed by our trained agent in each round. Other training details are listed in Appendix [A].


Baseline methods are still divided by whether or not to perform test-time iterative refinement. For the setting of a single test trial, we compare with ReAct prompting and WebGUM [(Furuta et al., 2024)] by jointly fine-tuning a ViT visual encoder and a Flan-T5-XL.


TABLE 4


TABLE 5 


TABLE 6


Recently, AgentBoard [(Ma et al., 2024)] offers an easy/hard split of the first 251 test tasks in WebShop for better evaluation, and [Liu et al. 2024] report the benchmarked results of xLAM-v0.1 [(Zhang et al., 2024)] with multi-task full finetuning of Mixtral-8x7B-Instructv0.1 [(Jiang et al., 2024)]. We also include xLAM-v0.1 [(Zhang et al., 2024)] as a single-shot baseline and report the performance comparison on AgentBoard. While LUMOS [(Yin et al., 2023)] shares a similar spirit with xLAM-v0.1, the WebShop task is treated as an unseen scenario in their setting. To conduct a fair comparison, we do not compare ours with LUMOS. For the setting that allows test-time iterative refinement, Reflexion has been claimed to be ineffective in [Shinn et al. 2023]. We compare ours with LATS [(Zhou et al., 2023a)], a prompting-based language agent with multiple rounds of self-reflection and tree search.


TABLE 7


Tables [4] and [5] demonstrate the significance of A3T agents. With a single test trial, the A3T agent matches averaged human performance (reward: 73.9 v.s. 75.5; success rate: 49.0% v.s. 50.0%). With 4 shots of test trials, A3T achieves a 54.8% success rate, closing the gap with human expert performance (59.6%). The 1-shot A3T agent also outperforms prompting with GPT-4-32k-0613 in both the easy and the hard split of WebShop from AgentBoard. Table [6] further shows the quality improvement of the accumulated trajectories across multiple rounds of A3T. Case studies for annotated trajectories, as well as the dataset statistics for each round of training are reported in Appendices [B] and [C.2], respectively.


Ablation Studies


In this section, we conduct ablation studies for the self-training techniques proposed in Section [2.2], and fine-tune the proprietary gpt-3.5-turbo-1106 for further comparisons.


1. Variants of the Self-Training Techniques


We conduct ablated experiments on WebShop, as it provides a real-valued reward ranging from 0 to 1. We first study the effect of different reward thresholds for training trajectory filtering. In A3T, only the trajectories with reward R = 1 are used in training. We change the constraint into R ≥ 0.75 and R ≥ 0.5 and compare the performance of the initial round with supervised ReAct fine-tuning. According to Table [7], the best performance is still obtained with R = 1, which agrees with the findings of [Zhou et al. 2023b].


We proceed to ablate the policy gradient technique with binarized rewards and conduct experiments for training in Round 1. The first type to be compared with is supervised fine-tuning. We implement supervised training with the successful trajectories only, namely R(τf) = 0 in Eq. [(2)]. This echoes the practice adopted by [Singh et al. 2023]. We also prepend the trajectories with the label conditions &quot;Success&quot; / &quot;Fail&quot; in the training data and conduct Round-1 supervised training.
For the comparisons with policy gradient Eq. [(2)], we alternatively set R(τf) to be the original reward provided by WebShop, or to be 0.1 following the practice of [Wang et al. 2024]. When using the original reward, we also include another setting by relaxing the reward threshold constraint to be R ≥ 0.75. The comparisons are shown in Table [7]. Conclusions can be drawn that: (1) Policy gradient methods lead to higher promotion in task performance than supervised training methods.
This resembles the effectiveness of RLHF [(Ouyang et al., 2022)] on top of supervised fine-tuning. (2) The binarized rewards (±1) used in A3T lead to a significant improvement of success rate. We leave the incorporation of advanced RL [(Zhou et al., 2024)] and RLAIF [(Lee et al., 2023; Chen et al., 2024; Hosseini et al., 2024; Yuan et al., 2024)] algorithms into A3T as future work.


TABLE 8


TABLE 9 


TABLE 10 


2. Experiments with gpt-3.5-turbo-1106 fintuning


While all of the experiments we previously reported are conducted with Mistral-7B-Instruct-v0.2 and QLoRA finetuning, in this section, we also validate A3T with gpt-3.5-turbo-1106 finetuning, the proprietary service provided by OpenAI. As the initial trajectory set for Round-0 training in A3T is obtained by ReAct prompting with gpt-3.5-turbo-instruct-0914, the starting point for the two base LLMs is the same.


Tables [8] and [9] report the performance comparison of Round-0 supervised training between the open-sourced and the proprietary LLMs. In AlfWorld, the performance of the QLoRA fine-tuned Mistral-7B-Instruct-v0.2 even surpasses that of the proprietary gpt-3.5-turbo-1106 fine-tuning service. In WebShop, the proprietary gpt-3.5-turbo-1106 finetuning performs better in Round-0 supervised training. We then let the two models separately compose diverse trajectories for their self-training. Because of the expense of inferring with the finetuned gpt-3.5-turbo-1106 model [2], we compose 10 trajectories for each failed training task (20 with the open-sourced LLM). Shown in Table [10], the quality of the accumulated trajectories composed by the proprietary LLM is on par with those composed by the open-sourced LLM. After Round-1 self-training, the open-sourced model achieves an even higher test success rate. This is attributed to the proprietary service providing only the supervised fine-tuning option, while also indicating the importance of contrastive fine-tuning in A3T.


Conclusion


In this work, we propose A3T, a framework that enables the autonomous annotation of agent trajectories in the style of ReAct for contrastive self-training. The key factor in the trajectory annotation process is the ActRe prompting agent, which produces the textual rationales given arbitrary external actions. Together with ActRe and environmental feedback, the ReAct-style agent autonomously synthesizes trajectories for self-training. In the contrastive self-training process, we leverage the policy gradient methods with binarized rewards to boost the task success rate. Extensive experiments on AlfWorld and WebShop have demonstrated the superiority of A3T over multiple strong baselines.