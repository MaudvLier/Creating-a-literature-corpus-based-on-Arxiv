
MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge


Introduction


Developing autonomous embodied agents that can attain human-level
performance across a wide spectrum of tasks has been a long-standing
goal for AI research. There has been impressive progress towards this
goal, most notably in games [*REF*, *REF*, *REF*] and robotics [*REF*,
*REF*, *REF*, *REF*, *REF*]. These embodied agents are typically trained
tabula rasa in isolated worlds with limited complexity and
diversity. Although highly performant, they are specialist models that
do not generalize beyond a narrow set of tasks. In contrast, humans
inhabit an infinitely rich reality, continuously learn from and adapt
to a wide variety of open-ended tasks, and are able to leverage large
amount of prior knowledge from their own experiences as well as
others.


We argue that three main pillars are necessary for generalist
embodied agents to emerge. First, the environment in which the agent
acts needs to enable an unlimited variety of open-ended goals
[*REF*, *REF*, *REF*, *REF*]. Natural evolution is able to nurture an
ever-expanding tree of diverse life forms thanks to the infinitely
varied ecological settings that the Earth supports
[*REF*, *REF*]. This process has not
stagnated for billions of years. In contrast, today&apos;s agent training
algorithms cease to make new progress after convergence in narrow
environments [*REF*, *REF*]. Second, a
large-scale database of prior knowledge is necessary to facilitate
learning in open-ended settings. Just as humans frequently learn from
the internet, agents should also be able to harvest practical
knowledge encoded in large amounts of video demos [*REF*, *REF*], multimedia tutorials
[*REF*], and forum discussions [*REF*, *REF*, *REF*]. In a
complex world, it would be extremely inefficient for an agent to learn
everything from scratch through trial and error. Third, the agent&apos;s
architecture needs to be flexible enough to pursue any task in 
openended environments, and scalable enough to convert large-scale
knowledge sources into actionable insights [*REF*,
*REF*]. This motivates the design of an agent that has
a unified observation/action space, conditions on natural language
task prompts, and adopts the Transformer pre-training paradigm
[*REF*, [91], *REF*] to internalize knowledge effectively.


FIGURE 1


In light of these three pillars, we introduce MINEDOJO, a new
framework to help the community develop open-ended, generally-capable
agents. It is built on the popular Minecraft game, where a player
explores a procedurally generated 3D world with diverse types of
terrains to roam, materials to mine, tools to craft, structures to
build, and wonders to discover. Unlike most other games [*REF*, *REF*, *REF*],
Minecraft defines no specific reward to maximize and no fixed
storyline to follow, making it well suited for developing open-ended
environments for embodied AI research. We make the following three
major contributions:


1. Simulation platform with thousands of diverse open-ended tasks.
MINEDOJO provides convenient APIs on top of Minecraft that
standardize task specification, world settings, and agent&apos;s
observation/action spaces. We introduce a benchmark suite that
consists of thousands of natural language-prompted tasks, making it
two orders of magnitude larger than prior Minecraft benchmarks
like the MineRL Challenge [*REF*, *REF*]. The suite includes long-horizon, open-ended
tasks that cannot be easily evaluated through automated procedures,
such as &quot;build an epic modern house with two floors and a swimming
pool&quot;. Inspired by the Inception score [*REF*] and
FID score [*REF*] that are commonly used to assess
AI-generated image quality, we introduce a novel agent evaluation
protocol using a large video-language model pre-trained on Minecraft
YouTube videos. This complements human scoring
[*REF*] that is precise but more expensive. Our
learned evaluation metric has good agreement with human judgment in
a subset of the full task suite considered in the experiments.


2. Internet-scale multimodal Minecraft knowledge base. Minecraft
has more than 100 million active players [*REF*],
who have collectively generated an enormous wealth of data. They
record tutorial videos, stream live play sessions, compile recipes,
and discuss tips and tricks on forums. MINEDOJO features a massive
collection of 730K+ YouTube videos with time-aligned transcripts,
6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia
contents (Fig. [3]). We hope that this enormous
knowledge base can help the agent acquire diverse skills, develop
complex strategies, discover interesting objectives, and learn
actionable representations automatically.


FIGURE 2


3. Novel algorithm for embodied agents with large-scale
pre-training. We develop a new learning algorithm for embodied
agents that makes use of the internet-scale domain knowledge we have
collected from the web. Using the massive volume of YouTube videos
from MINEDOJO, we train a video-text contrastive model in the spirit
of CLIP [*REF*], which associates natural language
subtitles with their time-aligned video segments. We demonstrate
that this learned correlation score can be used effectively as an
open-vocabulary, massively multi-task reward function for RL
training. Our agent solves the majority of 12 tasks in our
experiment using the learned reward model (Fig. [2]).
It achieves competitive performance to agents trained with
meticulously engineered dense-shaping rewards, and in some cases
outperforms them, with up to 73% improvement in success rates. For
open-ended tasks that do not have a simple success criterion, our
agents also perform well without any special modifications.


In summary, this paper proposes an open-ended task suite,
internet-scale domain knowledge, and agent learning with recent
advances on large pre-trained models [*REF*]. We have
open-sourced MINEDOJO&apos;s simulator, knowledge bases, algorithm
implementations, pretrained model checkpoints, and task curation tools
at WEBSITE. We hope that MINEDOJO will serve as an
effective starter framework for the community to develop new
algorithms and advance towards generally capable embodied agents.


MINEDOJO Simulator &amp; Benchmark Suite


MINEDOJO offers a set of simulator APIs help researchers develop
generally capable, open-ended agents in Minecraft. It builds upon the
open-source MineRL codebase [*REF*] and makes the
following upgrades: 1) We provide unified observation and action
spaces across all tasks, facilitating the development of multi-task
and continually learning agents that can constantly adapt to new
scenarios and novel tasks. This deviates from the MineRL Challenge
design that tailors observation and action spaces to individual tasks;
2) Our simulation unlocks all three types of worlds in Minecraft,
including the Overworld, the Nether, and the End, which
substantially expands the possible task space, while MineRL only
supports the Overworld natively; and 3) We provide convenient APIs to
configure initial conditions and world settings to standardize our
tasks.


With this MINEDOJO simulator, we define thousands of benchmarking
tasks, which are divided into two categories: 1) Programmatic tasks
that can be automatically assessed based on the ground-truth simulator
states; and 2) Creative tasks that do not have well-defined or
easily-automated success criteria, which motivates our novel
evaluation protocol using a learned model (Sec. [4]). To
scale up the number of Creative tasks, we mine ideas from YouTube
tutorials and use OpenAI&apos;s GPT-3 [*REF*]
service to generate substantially more task definitions. Compared to
Creative tasks, Programmatic tasks are simpler to get started, but
tend to have restricted scope, limited language variations, and less
open-endedness in general.


Task Suite I: Programmatic Tasks


We formalize each programmatic task as a 5-tuple: FORMULA. 
G is an English description of the task goal, such as
&quot;find material and craft a gold pickaxe&quot;. G is a natural language
guidance that provides helpful hints, recipes, or advice to the agent.
We leverage


OpenAI&apos;s GPT-3-davinci API to automatically generate detailed guidance
for a subset of the tasks. For the example goal &quot;bring a pig into
Nether&quot;, GPT-3 returns: 1) Find a pig in the overworld; 2)
Right-click on the pig with a lead; 3) Right-click on the Nether
Portal with the lead and pig selected; 4) The pig will be pulled
through the portal! I is the initial conditions of the agent and the
world, such as the initial inventory, spawn terrain, and weather.
FORMULA is the success criterion, a deterministic
function that maps the current world state st to a Boolean success
label. FORMULA is an optional dense reward function. We
only provide fR for a small subset of the tasks in MINEDOJO due


to the high costs of meticulously crafting dense rewards. For our
current agent implementation (Sec. [4.1]), we do not use
detailed guidance. Inspired by concurrent works SayCan
[*REF*] and Socratic Models [*REF*], one potential idea is to feed each step in the guidance to our learned
reward model sequentially so that it becomes a stagewise reward
function for a complex multi-stage task.


MINEDOJO provides 4 categories of programmatic tasks with 1,581
template-generated natural language goals to evaluate the agent&apos;s
different capabilities systematically and comprehensively:
1. Survival: surviving for a designated number of days.
2. Harvest: finding, obtaining, cultivating, or manufacturing
hundreds of materials and objects.
3. Tech Tree: crafting and using a hierarchy of tools.
4. Combat: fighting various monsters and creatures that require
fast reflex and martial skills.


Each task template has a number of variations based on the terrain,
initial inventory, quantity, etc., which form a flexible spectrum of
difficulty. In comparison, the NeurIPS MineRL Diamond challenge
[*REF*] is a subset of our programmatic task suite,
defined by the task goal &quot;obtain 1 diamond\&quot; in MINEDOJO.


Task Suite II: Creative Tasks


We define each creative task as a 3-tuple, FORMULA, which
differs from programmatic tasks due to the lack of straightforward
success criteria. Inspired by model-based metrics like the Inception
score [*REF*] and FID score [*REF*] for
image generation, we design a novel task evaluation metric based
on a pre-trained contrastive video-language model (Sec.
[4.1]). In the experiments, we find that the learned metric exhibits a high level of
agreement with human evaluations (see Table [2]).


We brainstorm and author 216 Creative tasks, such as &quot;build a haunted
house with zombie inside&quot; and &quot;race by riding a pig&quot;. Nonetheless,
such a manual approach is not scalable. Therefore, we develop two
systematic approaches to extend the total number of task definitions
to 1,560. This makes our Creative tasks 3 orders of magnitude larger
than Minecraft BASALT challenge [*REF*], which has 4
Creative tasks.


Approach 1. Task Mining from YouTube Tutorial Videos. We identify
our YouTube dataset as a rich source of tasks, as many human players
demonstrate and narrate creative missions in the tutorial playlists.
To collect high-quality tasks and accompanying videos, we design a
3-stage pipeline that makes it easy to find and annotate interesting
tasks (see Sec. [C.2] for details). Through this
pipeline, we extract 1,042 task ideas from the common wisdom of a huge
number of veteran Minecraft gamers, such as &quot;make an automated mining
machine&quot; and &quot;grow cactus up to the sky&quot;.


Approach 2. Task Creation by GPT-3. We leverage GPT-3&apos;s few-shot
capability to generate new task ideas by seeding it with the tasks we
manually author or mine from YouTube. The prompt template is: Here
are some example creative tasks in Minecraft:.


FIGURE 3


Let&apos;s brainstorm more detailed while reasonable creative tasks in
Minecraft. GPT-3 contributes 302 creative tasks after de-duplication,
and demonstrates a surprisingly proficient understanding of Minecraft
terminology.


Collection of Starter Tasks


We curate a set of 64 core tasks for future researchers to get started
more easily. If their agent works well on these tasks, they can more
confidently scale to the full benchmark.
- 32 programmatic tasks: 16 &quot;standard&quot; and 16 &quot;difficult&quot;,
spanning all 4 categories (survival, harvesting, combat, and tech
tree). We rely on our Minecraft knowledge to decide the difficulty
level. &quot;Standard&quot; tasks require fewer steps and lower resource
dependencies to complete.
- 32 creative tasks: 16 &quot;standard&quot; and 16 &quot;difficult&quot;. Similarly,
tasks labeled with &quot;standard&quot; are typically short-horizon tasks.


We recommend that researchers run 100 evaluation episodes for each
task and report the percentage success rate. The programmatic tasks
have ground-truth success, while the creative tasks need our novel
evaluation protocol (Sec. [5]).


Internet-scale Knowledge Base


Two commonly used approaches [*REF*, *REF*, *REF*, *REF*] to
train embodied agents include training agents from scratch using RL
with well-tuned reward functions for each task, or using a large
amount of human-demonstrations to bootstrap agent learning. However,
crafting well-tuned reward functions is challenging or infeasible for
our task suite (Sec. [2.2]), and employing expert gamers to provide large amounts of demonstration data
would also be costly and infeasible [*REF*].


Instead, we turn to the open web as an ever-growing, virtually
unlimited source of learning material for embodied agents. The
internet provides a vast amount of domain knowledge about Minecraft,
which we harvest by extensive web scraping and filtering. We collect
33 years worth of YouTube videos, 6K+ Wiki pages, and millions of
Reddit comment threads. Instead of hiring a handful of human
demonstrators, we capture the collective wisdom of millions of
Minecraft gamers around the world. Furthermore, language is a key and
pervasive component of our database that takes the form of YouTube
transcripts, textual descriptions in Wiki, and Reddit discussions.
Language facilitates open-vocabulary understanding, provides grounding
for image and video modalities, and unlocks the power of large
language models [*REF*, *REF*, *REF*] for embodied agents. To ensure socially
responsible model development, we take special measures to filter out
low-quality and toxic contents [*REF*,
*REF*] from our databases, detailed in the Appendix (Sec. [D]).


FIGURE 4


YouTube Videos and Transcripts. Minecraft is among the most
streamed games on YouTube [*REF*]. Human players have
demonstrated a stunning range of creative activities and sophisticated
missions that take hours to complete (examples in Fig.
[3]). We collect 730K+ narrated Minecraft videos, which
add up to 33 years of duration and 2.2B words in English transcripts.
In comparison, HowTo100M [*REF*] is a large-scale human
instructional video dataset that includes 15 years of experience in
total -- about half of our volume. The time-aligned transcripts enable
the agent to ground free-form natural language in video pixels and
learn the semantics of diverse activities without laborious human
labeling. We operationalize this insight in our pre-trained
video-language model (Sec. [4.1]).


Minecraft Wiki. The Wiki pages cover almost every aspect of the
game mechanics, and supply a rich source of unstructured knowledge in
multimodal tables, recipes, illustrations, and step-by-step tutorials.
We use Selenium [*REF*] to scrape 6,735 pages that
interleave text, images, tables, and diagrams. The pages are highly
unstructured and do not share any common schema, as the Wiki is meant
for human consumption rather than AI training. To preserve the layout
information, we additionally save the screenshots of entire pages and
extract 2.2M bounding boxes of the visual elements (visualization in
Fig. [A.4] and [A.5]). We do not use
Wiki data in our current experiments. Since the Wiki contains detailed
recipes for all crafted objects, they could be provided as input or
training data for hierarchical planning methods and policy sketches
[*REF*]. Another promising future direction is to apply
document understanding models such as LayoutLM [*REF*,
*REF*] and DocFormer [*REF*] to learn
actionable knowledge from these unstructured Wiki data.


Reddit. We scrape 340K+ posts along with 6.6M comments under the
&quot;r/Minecraft&quot; subreddit. These posts ask questions on how to solve
certain tasks, showcase cool architectures and achievements in
image/video snippets, and discuss general tips and tricks for players
of all expertise levels. We do not use Reddit data for training in
Sec. [5], but a potential idea is to finetune large language models [[27], *REF*] on our
Reddit corpus to generate instructions and execution plans that are
better grounded in the Minecraft domain. Concurrent works
[[3], [56], *REF*] have explored similar ideas and showed excellent results on robot learning,
which is encouraging for more future research in MINEDOJO.


Agent Learning with Large-scale Pre-training


One of the grand challenges of embodied AI is to build a single agent
that can complete a wide range of open-world tasks. The MINEDOJO
framework aims to facilitate new techniques towards this goal by
providing an open-ended task suite (Sec. [2]) and large-scale internet
knowledge base (Sec. [3]). Here we
take an initial step towards this goal by developing a proof of
concept that demonstrates how a single language-prompted agent can be
trained in MINEDOJO to complete several complex Minecraft tasks. To
this end, we propose a novel agent learning algorithm that takes
advantage of the massive YouTube data offered by MINEDOJO. We note
that this is only one of the numerous possible
ways to use MINEDOJO&apos;s internet database --- the Wiki and Reddit
corpus also hold great potential to drive new algorithm discoveries
for the community in future works.


TABLE


In this paper, we consider a multi-task reinforcement learning (RL)
setting, where an agent is tasked with completing a collection of
MINEDOJO tasks specified by language instructions (Sec.
[2]). Solving these tasks often requires the agent to interact with the Minecraft world in a prolonged
fashion. Agents developed in popular RL benchmarks
[*REF*, *REF*] often rely on meticulously crafted dense and task-specific reward functions to guide
random explorations. However, these rewards are hard or even
infeasible to define for our diverse and open-ended tasks in MINEDOJO.
To address this challenge, our key insight is to learn a dense,
language-conditioned reward function from in-the-wild YouTube videos
and their transcripts. Therefore, we introduce MINECLIP, a
contrastive video-language model that learns to correlate video
snippets and natural language descriptions (Fig. [4]).
MINECLIP is multi-task by design, as it is trained on open-vocabulary
and diverse English transcripts.


During RL training, MINECLIP provides a high-quality reward signal
without any domain adaptation techniques, despite the domain gap
between noisy YouTube videos and clean simulator-rendered frames.
MINECLIP eliminates the need to manually engineer reward functions for
each and every MINEDOJO task. For Creative tasks that lack a simple
success criterion (Sec. [2.2]), MINECLIP also serves the dual purpose of an automatic evaluation
metric that agrees well with human judgement on a subset of tasks we
investigate (Sec. [4.2],(Table [2]). Because the learned reward model incurs a
non-trivial computational overhead, we introduce several techniques to
significantly improve RL training efficiency, making MINECLIP a
practical module for open-ended agent learning in Minecraft (Sec. [4.2]).


Pre-Training MINECLIP on Large-scale Videos


Formally, the learned reward function can be defined as FORMULA that maps a language goal
G and a video snippet V to a scalar reward. An ideal FORMULA should
return a high reward if the behavior
depicted in the video faithfully follows the language description, and
a low reward otherwise. This can be achieved by optimizing the InfoNCE objective
[*REF*, *REF*, *REF*], which learns to correlate positive video and text pairs
[[118], [6], [78], [4], *REF*.]


Similar to the image-text CLIP model [*REF*], MINECLIP
is composed of a separate text encoder SYMBOL that embeds a language
goal and a video encoder SYMBOL that embeds a moving window of 16
consecutive frames with 160 × 256 resolution (Fig.
[4]). Our neural architecture has a similar design as
CLIP4Clip [*REF*], where SYMBOL reuses OpenAI CLIP&apos;s pretrained
text encoder, and SYMBOL is factorized into a frame-wise
image encoder SYMBOL and a temporal aggregator SYMBOL that summarizes
the sequence of 16 image features into a single video embedding.
Unlike CLIP4Clip, we insert two extra layers of residual CLIP Adapter
[*REF*] after the aggregator SYMBOL to produce a better
video feature, and finetune only the last two layers of the
pretrained SYMBOL and SYMBOL.


From the MINEDOJO YouTube database, we follow the procedure in
VideoCLIP [*REF*] to sample 640K pairs of 16-second
video snippets and time-aligned English transcripts, after applying a
keyword filter. We train two MINECLIP variants with different types of
aggregator SYMBOL: (1) MINECLIP[avg] does simple average pooling,
which is fast but agnostic to the temporal ordering; (2)
MINECLIP[attn] encodes the sequence by two transformer layers, which
is relatively slower but captures more temporal information, and thus
produces a better reward signal in general. Details of data
preprocessing, architecture, and hyperparameters are listed in the
Appendix (Sec. [E]).


TABLE


RL with MINECLIP Reward


We train a language-conditioned policy network that takes as input raw
pixels and predicts discrete control. The policy is trained with PPO
[*REF*] on the MINECLIP rewards. In each episode, the
agent is prompted with a language goal and takes a sequence of actions
to fulfill this goal. When calculating the MINECLIP rewards, we
concatenate the agent&apos;s latest 16 egocentric RGB frames in a temporal
window to form a video snippet. MINECLIP handles all task prompts
zero-shot without any further finetuning. In our experiments (Sec.
[5]), we show that MINECLIP provides effective dense
rewards out of the box, despite the domain shift between in-the-wild
YouTube frames and simulator frames. Besides regular video data
augmentation, we do not employ any special domain adaptation methods
during pre-training. Our finding is consistent with CLIP&apos;s strong
zero-shot performances on robustness benchmarks in object recognition
[*REF*].


Compared to hard-coded reward functions in popular benchmarks
[*REF*, *REF*, *REF*], the MINECLIP model has 150M parameters and is thus much more expensive
to query. We make several design choices to greatly accelerate RL
training with MINECLIP in the loop:
1. The language goal G is fixed for a specific task, so the text
features SYMBOL can be precomputed to avoid invoking the text encoder repeatedly.
2. Our agent&apos;s RGB encoder reuses the pre-trained weights of SYMBOL
from MINECLIP. We do not finetune SYMBOL during RL training, which
saves computation and endows the agent with good visual
representations from the beginning.
3. MINECLIP&apos;s video encoder SYMBOL is factorized into an image encoder
SYMBOL and a light-weight aggregator SYMBOL. This design choice
enables efficient image feature caching. Consider two
overlapping video sequences of 8 frames, V[0:8\] and V[1:9\]. We
can cache the image features of the 7 overlapping frames FORMULA 
to maximize compute savings. If SYMBOL is a monolithic model
like S3D [*REF*] in VideoCLIP
[*REF*], then the video features from every sliding
window must be recomputed, which would incur a much higher cost per
time step.
4. We leverage Self-Imitation Learning [*REF*] to
store the trajectories with high MINECLIP reward values in a buffer,
and alternate between PPO and self-imitation gradient steps. It
further improves sample efficiency as shown in the Appendix (Fig.
[A.8]).


Experiments


We evaluate our agent-learning approach (Section [4]) on
8 Programmatic tasks and 4 Creative tasks from the MINEDOJO
benchmarking suite. We select these 12 tasks due to the diversity of
skills required to solve them (e.g., harvesting, combat, building,
navigation) and domain-specific entities (e.g., animals, resources,
monsters, terrains, and structures). We split the tasks into 3 groups
and train one multi-task agent for each group: Animal-Zoo (4
Programmatic tasks on hunting or
harvesting resource from animals), Mob-Combat (Programmatic, fight 4
types of hostile monsters), and Creative (4 tasks).


TABLE 3


In the experiments, we empirically check the quality of MINECLIP
against manually written reward functions, and quantify how different
variants of our learned model affect the RL performance. Table
*REF* presents our main results, and Fig. *REF*
visualizes our learned agent behavior in 4 of the considered tasks.
Policy networks of all methods share the same architecture and are
trained by PPO + Self-Imitation (Sec. [4.2],
training details in the Appendix, Sec. [F]).
We compare the following methods:
- Ours (Attn): our agent trained with the MINECLIP[attn] reward
model. For Programmatic tasks, we also add the final success
condition as a binary reward. For Creative tasks, MINECLIP is the
only source of reward.
- Ours (Avg): the average-pooling variant of our method.
- Manual Reward: hand-engineered dense reward using ground-truth
simulator states.
- Sparse-only: the final binary success as a single sparse reward.
Note that neither sparse-only nor manual reward is available for
Creative tasks.
- CLIPOpenAI: pre-trained OpenAI CLIP model that has not
been finetuned on any MINEDOJO videos.


MINECLIP is competitive with manual reward. For Programmatic tasks
(first 8 rows), RL agents guided by MINECLIP achieve competitive
performance as those trained by manual reward. In three of the tasks,
they even outperform the hand-engineered reward functions, which
rely on privileged simulator states unavailable to MINECLIP. For a
more statistically sound analysis, we conduct the Paired Student&apos;s
t-test to compare the mean success rate of each task (pairing column
3 &quot;Ours (Attn)&quot; and column 5 &quot;Manual Reward&quot; in Table *REF*). The
test yields p-value 0.3991 » 0.05, which
indicates that the difference between our method and manual reward is
not considered statistically significant, and therefore they are comparable with each other.
Because all tasks require nontrivial exploration, our approach also
dominates the Sparse-only baseline. Note that the original OpenAI CLIP
model fails to achieve any success. We hypothesize that the creatures
in Minecraft look dramatically different from their real-world
counterparts, which causes CLIP to produce misleading signals worse
than no shaping reward at all. It implies the importance of finetuning
on MINEDOJO&apos;s YouTube data.


MINECLIP provides automated evaluation. For Creative tasks (last 4
rows), there are no programmatic success criteria available. We
convert MINECLIP into a binary success classifier by thresholding the
reward value it outputs for an episode. To test the quality of
MINECLIP as an automatic evaluation metric, we ask human judges to
curate a dataset of 100 successful and 100 failed trajectories for
each task. We then run both MINECLIP variants and CLIPOpenAI on the
dataset and report the binary F1 score of their judgement against
human ground-truth in Table [2.] The results demonstrate
that both MINECLIP[attn] and MINECLIP[avg] attain a very high
degree of agreement with human evaluation results on this subset of
the Creative task suite that we investigate. CLIPOpenAI baseline
also achieves nontrivial agreement on Find Ocean and Dig Hole tasks,
likely because real-world oceans and holes have similar texture. We
use the attn variant as an automated success criterion to score the 4
Creative task results in Table [1.] Our proposed method
consistently learns better than CLIPOpenAI-guided agents. It shows
that MINECLIP is an effective approach to solving open-ended tasks
when no straightforward reward signal is available. We provide further
analysis beyond these 4 tasks in the Appendix (Sec. [G.4]).


TABLES 4 and 5


MINECLIP shows good zero-shot generalization to significant visual
distribution shift. We evaluate the learned policy without
finetuning on a combination of unseen weather, lighting conditions,
and terrains --- 27 scenarios in total. For the baseline, we train
agents with the original CLIPOpenAI image encoder (not trained on
our YouTube videos) by imitation learning. The robustness against
visual shift can be quantitatively measured by the relative
performance degradation on novel test scenarios for each task. Table
*REF* shows that while all methods incur performance
drops, agents with MINECLIP encoder is more robust to visual changes
than the baseline across all tasks. Pre-training on diverse
in-the-wild YouTube videos is important to boosting zero-shot visual
generalization capability, a finding consistent with literature
[[92], *REF*.]


Learning a Single Agent for All 12 Tasks We have also trained a
single agent for all 12 tasks. To reduce the computational burden
without loss of generality, the agent is trained by self-imitating
from successful trajectories generated from the self-imitation
learning pipeline (Section [F].3). We summarize the
result in Table [4.] Similar to our main experiments,
all numbers represent percentage success rates averaged over 3
training seeds, each tested for 200 episodes. Compared to the original
agents, the new 12-multitask agent sees a performance boost in 6
tasks, degradation in 4 tasks, and roughly the same success rates in 2
tasks. This result suggests that there are both positive and negative
task transfers happening. To improve the multi-task performance, more
advanced algorithms [[141], *REF*] can
be employed to mitigate the negative transfer effects.


We also perform Paired Student&apos;s t-test to statistically compare the
performance of the 12-multitask agent and those separately trained on
each task group. We obtain a p-value of 0.3720 » 0.05, which
suggests that the difference between the two training settings is not
statistically significant.


Generalize to Novel Tasks To test the ability to generalize to new
open-vocabulary commands, we evaluate the agent on two novel tasks:
&quot;harvest spider string&quot; and &quot;hunt pig&quot;. Table *REF* shows
that the agent struggles in the zero-shot setting because it has not
interacted with pigs or spider strings during training, and thus does
not know how to interact with them effectively. However, the
performance improves substantially by finetuning with the MINECLIP
reward. Here the baseline methods are trained from scratch using RL
with the MINECLIP encoders and reward. Therefore, the only difference
is whether the policy has been pre-trained on the 12 tasks or not.
Given the same environment sampling budget (only around 5% of total samples), it
significantly outperforms baselines. It suggests that the multitask
agent has learned transferable knowledge on hunting and resource
collection, which enables it to quickly adapt to novel tasks.


Related work


Open-ended Environments for Decision-making Agents. There are many
environments developed with the goal of open-ended agent learning.
Prior works include maze-style worlds [*REF*, *REF*, *REF*], purely text-based game
[*REF*], grid worlds [*REF*, *REF*], browser/GUI-based environments
[*REF*, *REF*], and indoor simulators for robotics [*REF*, *REF*,
*REF*, *REF*, *REF*, *REF*, *REF*]. Minecraft offers an
exciting alternative for open-ended agent learning. It is a 3D visual
world with procedurally generated landscapes and extremely flexible
game mechanics that support an enormous variety of activities. Prior
methods in open-ended agent learning [[30], [57], [130], [63],
*REF*] do not make use of external knowledge, but our
approach leverages internet-scale database to learn open-vocabulary
reward models, thanks to Minecraft&apos;s abundance of gameplay data
online.


Minecraft for AI Research. The Malmo platform
[*REF*] is the first comprehensive release of a
Gym-style agent API [*REF*] for Minecraft. Based on
Malmo, MineRL [*REF*] provides a codebase and human
play trajectories for the annual Diamond Challenge at NeurIPS [*REF*, *REF*, *REF*].
MINEDOJO&apos;s simulator builds upon the pioneering work of MineRL, but
greatly expands the API and benchmarking task suite. Other Minecraft
benchmarks exist with different focuses. For example, CraftAssist
[*REF*] and IGLU [*REF*] study agents
with interactive dialogues. BASALT [*REF*] applies
human evaluation to 4 open-ended tasks. EvoCraft
[*REF*] is designed for structure building, and Crafter
[*REF*] optimizes for fast experimentation. Unlike
prior works, MINEDOJO&apos;s core mission is to facilitate the development
of generally capable embodied agents using internet-scale knowledge.
We include a feature comparison table of different Minecraft platforms
for AI research in Table [A.1.]


Internet-scale Multimodal Knowledge Bases. Big dataset such as
Common Crawl [*REF*], the Pile [*REF*], LAION [*REF*], YouTube-8M [*REF*] and
HowTo100M [*REF*] have been fueling the success oflarge pretrained language models [*REF*,
*REF*, *REF*] and multimodal models [*REF*, *REF*, *REF*,
*REF*, *REF*, *REF*, *REF*]. While generally useful for learning
representations, these datasets are not specifically targeted at
embodied agents. To provide agent-centric training data, RoboNet
[*REF*] collects video frames from 7 robot platforms,
and Ego4D [*REF*] recruits volunteers to record
egocentric videos of household activities. In comparison, MINEDOJO&apos;s
knowledge base is constructed without human curation efforts, much
larger in volume, more diverse in data modalities, and comprehensively
covers all aspects of the Minecraft environment.


Embodied Agents with Large-scale Pre-training. Inspired by the
success in NLP, embodied agent research [[29], [11], [94], *REF*] has seen
a surge in adoption of the large-scale pre-training paradigm. The
recent advances can be roughly divided into 4 categories. 1) Novel
agent architecture: Decision Transformer [*REF*,
*REF*, *REF*] applies the powerful
self-attention models to sequential decision making. GATO
[*REF*] and Unified-IO [*REF*] learn a
single model to solve various decision-making tasks with different
control interfaces. VIMA [*REF*] unifies a wide range
of robot manipulation tasks with multimodal prompting. 2)
Pre-training for better representations: R3M
[*REF*] trains a general-purpose visual encoder for
robot perception on Ego4D videos [*REF*]. CLIPort
[*REF*] leverages the pre-trained CLIP model
[*REF*] to enable free-form language instructions for
robot manipulation.


3) Pre-training for better policies: AlphaStar
[*REF*] achieves champion-level performance on
StarCraft by imitating from numerous human demos. SayCan
[*REF*] leverages large language models (LMs) to ground
value functions in the physical world. [*REF*] and
[*REF*] directly reuse pre-trained LMs as policy
backbone. VPT [*REF*] is a concurrent work that learns
an inverse dynamics model from human contractors to pseudo-label
YouTube videos for behavior cloning. VPT is complementary to our
approach, and can be finetuned to solve language-conditioned
open-ended tasks with our learned reward model. 4) Data-driven
reward functions: Concept2Robot [*REF*] and DVD
[*REF*] learn a binary classifier to score behaviors
from in-the-wild videos [*REF*]. LOReL
[*REF*] crowd-sources humans labels to train
language-conditioned reward function for offline RL. AVID
[*REF*] and XIRL [*REF*] extract
reward signals via cycle consistency. MINEDOJO&apos;s task benchmark and
internet knowledge base are generally useful for developing new
algorithms in all the above categories. In Sec. [4], we
also propose an open-vocabulary, multi-task reward model using
MINEDOJO YouTube videos.


Conclusion


In this work, we introduce the MINEDOJO framework for developing
generally capable embodied agents. MINEDOJO features a benchmarking
suite of thousands of Programmatic and Creative tasks, and an
internet-scale multimodal knowledge base of videos, wiki, and forum
discussions. As an example of the novel research possibilities enabled
by MINEDOJO, we propose MINECLIP as an effective language-conditioned
reward function trained with in-the-wild YouTube videos. MINECLIP
achieves strong performance empirically and agrees well with human
evaluation results, making it a good automatic metric for Creative
tasks. We look forward to seeing how MINEDOJO empowers the community
to make progress on the important challenge of open-ended agent
learning.