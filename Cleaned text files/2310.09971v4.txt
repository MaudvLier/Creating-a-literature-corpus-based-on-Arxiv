AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents


Introduction


Reinforcement Learning (RL) research has created effective methods for training specialist decision-making agents that maximize one objective in a single environment through trial-and-error [*REF*; *REF*; *REF*; *REF*; *REF*].
New efforts are shifting towards developing generalist agents that can adapt to a variety of environments that require long-term memory and reasoning under uncertainty [*REF*; *REF*; *REF*; *REF*].
One of the most promising approaches to generalist RL is a family of techniques that leverage sequence models to actively adapt to new situations by learning from experience at test-time. These &quot;in-context&quot; agents use memory to recall information from previous timesteps and update their understanding of the current environment [*REF*; *REF*]. In-context RL&apos;s advantage is its simplicity: it reduces partial observability, generalization, and meta-learning to a single problem in which agents equipped with memory train in a collection of related environments [*REF*; *REF*; *REF*]. The ability to explore, identify, and adapt to new environments arises implicitly as agents learn to maximize returns in deployments that may span multiple trials [*REF*; *REF*; *REF*].


Effective in-context agents need to be able to scale across two axes: 1) the length of their planning horizon, and 2) the length of their effective memory. The earliest versions of this framework [*REF*; *REF*; *REF*] use recurrent networks and on-policy learning updates that do not scale well across either axis. Transformers [*REF*] improve memory by removing the need to selectively write information many timesteps in advance and turning recall into a retrieval problem [*REF*]. Efforts to replace recurrent policies with Transformers were successful [*REF*; *REF*], but long-term adaptation remained challenging, and the in-context approach was relegated to an unstable baseline for other meta-RL methods [*REF*]. However, it has been shown that pure in-context RL with recurrent networks can be a competitive baseline when the original on-policy gradient updates are replaced by techniques from a long line of work in stable off-policy RL [*REF*; *REF*]. A clear next step is to reintroduce the memory benefits of Transformers. Unfortunately, training Transformers over long sequences with off-policy RL combines two of the most implementation-dependent areas in the field, and many of the established best practices do not scale (Section [3]).
The difficulty of this challenge is perhaps best highlighted by the fact that many popular applications of Transformers in off-policy (or offline) RL devise ways to avoid RL altogether and reformulate the problem as supervised learning [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].


Our work introduces AMAGO ([A]daptive [M]emory [A]gent for achieving [GO]als) -- a new learning algorithm that makes two important contributions to in-context RL. First, we redesign the off-policy actor-critic update from scratch to support long-sequence Transformers that learn from entire rollouts in parallel (Figure [1]). AMAGO breaks off-policy in-context RL&apos;s biggest bottlenecks by enabling memory lengths, model sizes, and planning horizons that were previously impractical or unstable. Our agent is open-source and specifically designed to be efficient, stable, and applicable to new environments with little tuning. We empirically demonstrate its power and flexibility in existing meta-RL and memory benchmarks, including state-of-the-art results in the POPGym suite [*REF*], where Transformers&apos; recall dramatically improves performance. Our second contribution takes in-context RL in a new direction with fresh benchmarks. AMAGO&apos;s use of off-policy data -- as well as its focus on long horizons and sparse rewards -- makes it uniquely capable of extending the in-context RL framework to goal-conditioned problems [*REF*; *REF*] with hard exploration. We add a new hindsight relabeling scheme for multi-step goals that generates effective exploration plans in multi-task domains. AMAGO can then learn to complete many possible instructions while using its memory to adapt to unfamiliar environments.
We first study AMAGO in two new benchmarks in this area before applying it to instruction-following tasks in the procedurally generated worlds of Crafter [*REF*].


FIGURE


Related Work 


RL Generalization and Meta-Learning.


General RL agents should be able to make decisions across similar environments with different layouts, physics, and visuals [*REF*; *REF*; *REF*; *REF*].
Many techniques are based on learning invariant policies that behave similarly despite these changes [*REF*]. Meta-RL takes a more active approach and studies methods for rapid adaptation to new RL problems [*REF*]. Meta-RL is a crowded field with a complex taxonomy [*REF*]; this paper uses the informal term &quot;in-context RL&quot; to refer to a specific subset of implicit context-based methods that treat meta-learning, zero-shot generalization, and partial observability as a single problem. In practice, these methods are variants of RL *MATH* [*REF*; *REF*], where we train a sequence model with standard RL and let memory and meta-reasoning arise implicitly as a byproduct of maximizing returns [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
There are many other ways to use historical context to adapt to an environment. However, in-context RL makes so few assumptions that alternatives mainly serve as ways to take advantage of problems that do not need its flexibility. Examples include domains where each environment is fully observed, can be labeled with ground-truth parameters indicating its unique characteristics, or does not require meta-adaptation within trials [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].


The core AMAGO agent is conceptually similar to AdA [*REF*] -- a recent work on off-policy meta-RL with Transformers. AdA trains in-context RL agents in a closed-source domain that evaluates task diversity at scale, while our work is more focused on long-term recall and open-source RL engineering details.


Goal-Conditioned RL.


Goal-conditioned RL (GCRL) [*REF*; *REF*] trains multi-task agents that generalize across tasks by making the current goal an input to the policy. GCRL often learns from sparse (binary) rewards based solely on task completion, reducing the need for reward engineering. GCRL methods commonly use hindsight experience replay (HER) [*REF*; *REF*; *REF*; *REF*] to ease the challenges of sparse-reward exploration. HER randomly relabels low-quality data with alternative goals that lead to higher rewards, and this technique is mainly compatible with off-policy methods where the same trajectory can be recycled many times with different goals. Goal conditioned supervised learning (GCSL) [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*] uses HER to relabel every trajectory and directly imitates behavior conditioned on future outcomes. GCSL has proven to be an effective way to train Transformers for decision-making [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*] but has a growing number of theoretical and practical issues [*REF*; *REF*; *REF*; *REF*].
AMAGO avoids these limitations by providing a stable and effective way to train long-sequence Transformers with pure RL.


Background and Problem Formulation 


Solving Goal-Conditioned CMDPs with In-Context RL.


Partially observed Markov decision processes (POMDPs) create long-term memory problems where the true environment state *MATH* may need to be inferred from a history of limited observations *MATH* [*REF*]. Contextual MDPs (CMDPs) [*REF*; *REF*; *REF*] extend POMDPs to create a distribution of related decision-making problems where everything that makes an environment unique can be identified by a context parameter. More formally, an environment&apos;s context parameter, *MATH*, conditions its reward, transition, observation emission function, and initial state distribution. CMDPs create zero-shot generalization problems when *MATH* is sampled at the end of every episode [*REF*] and meta-RL problems when held constant for multiple trials [*REF*]. We slightly adapt the usual CMDP definition by decoupling an optional goal parameter *MATH* from the environment parameter *MATH* [*REF*]. The goal *MATH* only contributes to the reward function *MATH*, and lets us provide a task description without assuming we know anything else about the environment.


Knowledge of *MATH* lets a policy adapt to any given CMDP, but its value is not observed as part of the state (and may be impossible to represent), creating an &quot;implicit POMDP&quot; [*REF*; *REF*]. From this perspective, the only meaningful difference between POMDPs and CMDPs is the need to reason about transition dynamics (*MATH*), rewards (*MATH*), and resets (*MATH*) based on full trajectories of environment interaction *MATH*.
In-context RL agents embrace this similarity by treating CMDPs as POMDPs with additional inputs and using their memory to update state and environment estimates at every timestep. Memory-equipped policies *MATH* take trajectory sequences as input and output actions that maximize returns over a finite horizon (*MATH*) that may span multiple trials: *MATH*.


Challenges in Off-Policy Model-Free In-Context RL.


In-context RL is a simple and powerful idea but is often outperformed by more complex methods [*REF*; *REF*; *REF*]. However, deep RL is a field where implementation details are critical [*REF*], and recent work has highlighted how the right combination of off-policy techniques can close this performance gap [*REF*]. Off-policy in-context agents collect training data in a replay buffer from which they sample trajectory sequences *MATH* up to a fixed maximum context length of *MATH*. Training on shorter sequences than used at test-time is possible but creates out-of-distribution behavior [*REF*; *REF*; *REF*].
Therefore, an agent&apos;s context length is its most important bottleneck because it establishes an upper bound on memory and in-context adaptation. Model-free RL agents control their planning horizon with a value learning discount factor *MATH*. Despite context-based learning&apos;s focus on long-term adaptation, learning instability restricts nearly all methods to *MATH* and effective planning horizons of around *MATH* timesteps. Trade-offs between context length, planning horizon, and stability create an unpredictable hyperparameter landscape [*REF*] that requires extensive tuning.


Agents that are compatible with both discrete and continuous actions use *MATH* to update actor (*MATH*) and critic (*MATH*) networks with two loss functions optimized in alternating steps [*REF*; *REF*] that make it difficult to share sequence model parameters [*REF*; *REF*; *REF*]. It is also standard to use multiple critic networks to reduce overestimation [*REF*; *REF*] and maintain extra target networks to stabilize optimization [*REF*]. The end result is several sequence model forward/backward passes per training step -- limiting model size and context length [*REF*; *REF*] (Figure [10]). This problem is compounded by the use of recurrent networks that process each timestep sequentially [*REF*]. Transformers parallelize sequence computation and improve recall but can be unstable in general [*REF*; *REF*], and the differences between supervised learning and RL create additional challenges. RL training objectives do not always follow clear scaling laws, and the correct network size can be unpredictable. Datasets also grow and shift over time as determined by update-to-data ratios that impact performance but are difficult to tune [*REF*; *REF*]. Finally, the rate of policy improvement changes unpredictably and makes it difficult to prevent model collapse with learning rate cooldowns [*REF*; *REF*]. Taken together, these challenges make us likely to optimize a large policy network for far longer than our small and shifting dataset can support [*REF*; *REF*]. Addressing these problems requires careful architectural changes [*REF*; *REF*] and Transformers are known to be an inconsistent baseline in our setting [*REF*; *REF*; *REF*].


Method


We aim to extend the limits of off-policy in-context RL&apos;s three main barriers: 1) the memory limit or context length *MATH*, 2) the value discount factor *MATH*, and 3) the size and recall of our sequence model. Transformers are a strong solution to the last challenge and may be able to address all three barriers if we were able to learn from long context lengths *MATH* and select actions for *MATH* at test time. In principle, this would allow agents to remember and plan for long adaptation windows in order to scale to more challenging problems while removing the need to tune trade-offs between stability and context length. AMAGO overcomes several challenges to make this possible. This section summarizes the most important techniques that enable AMAGO&apos;s performance and flexibility. More details can be found in Appendix [7] and in our open-source code release.


A high-level overview of AMAGO is illustrated in Figure [1]. The observations of every environment are augmented to a unified goal-conditioned CMDP format, even when some of the extra input information is unnecessary. This unified format allows AMAGO to be equally applicable to generalization, meta-RL, long-term memory, and multi-task problems without changes. AMAGO is an off-policy method that takes advantage of large and diverse datasets; trajectory data is loaded from disk and can be relabeled with alternative goals and rewards. The shape and format of trajectories vary across domains, but we use a timestep encoder network to map each timestep to a fixed-size representation. AMAGO allows the timestep encoder to be the only architecture change across experiments. A single Transformer trajectory encoder processes sequences of timestep representations. AMAGO uses these representations as the inputs to small feed-forward actor and critic networks, which are optimized simultaneously across every timestep of the causal sequence. Like all implicit context-based methods (Sec. [2]), this architecture encourages the trajectory encoder outputs to be latent estimates of the current state and environment *MATH* (Sec. [3]). In short, AMAGO&apos;s learning update looks more like supervised sequence modeling than an off-policy actor-critic, where each training step involves exactly one forward pass of one Transformer model with two output heads. This end result is simple and scalable but is only made possible by important technical details.


Sharing One Sequence Model.


The accepted best practice in off-policy RL is to optimize separate sequence models for the actor and critic(s) -- using additional target models to compute the critic loss. Sharing actor and critic sequence models is possible but has repeatedly been shown to be unstable [*REF*; *REF*; *REF*]. AMAGO addresses this instability and restructures the learning update to train its actor and critics on top of the same sequence representation; there are no target sequence models, and every parameter is updated simultaneously without alternating steps or conflicting learning rates.
We combine the actor and critic objectives into a single loss trained by one optimizer. However, we must put each term on a predictable scale to prevent their weights from needing to be tuned across every experiment [*REF*; *REF*]. The key detail enabling the simultaneous update to work in AMAGO -- without separating the Transformer from the actor loss [*REF*; *REF*] -- is to carefully detach the critic from the actor loss (Appendix [7.1]). We compute loss terms with a custom variant of REDQ [*REF*] for discrete and continuous actions that removes unintuitive hyperparameters wherever possible.


Stable Long-Context Transformers in Off-Policy RL.


Transformers bring additional optimization questions that can be more challenging in RL than supervised learning (Sec.
[2]). The shared off-policy update may amplify these issues, as we were unable to successfully apply existing architectural changes for Transformers in on-policy RL [*REF*; *REF*] to our setting. We find that attention entropy collapse [*REF*] is an important problem in long-sequence RL.
Language modeling and other successful applications of Transformers involve learning a variety of temporal patterns. In contrast, RL agents can converge on precise memory strategies that consistently recall specific timesteps of long sequences (Figure [15]).
Optimizing these policies encourages large dot products between a small set of queries and keys that can destabilize attention. We modify the Transformer architecture based on Normformer [*REF*] and *MATH* Reparam [*REF*], and replace saturating activations with Leaky ReLUs to preserve plasticity [*REF*; *REF*] as discussed in Appendix [7.3]. This architecture effectively stabilizes training and reduces tuning by letting us pick model sizes that are safely too large for the problem.


Long Horizons and Multi-Gamma Learning.


While long-sequence Transformers improve recall, adaptation over extended rollouts creates a challenging credit assignment and planning problem. An important barrier in increasing adaption horizons is finding stable ways to increase the discount factor *MATH*. Time information in the trajectory improves stability [*REF*], but we also use a &quot;multi-gamma&quot; update that jointly optimizes many different values of *MATH* in parallel. Each *MATH* creates its own *MATH* -value surface, making the shared Transformer more likely to have a strong actor-critic learning signal for some *MATH* throughout training. The relative cost of the multi-gamma update becomes low as the size of the sequence model increases. AMAGO can select the action corresponding to any *MATH* during rollouts, and we use *MATH* unless otherwise noted. A discrete-action version of the multi-gamma update has previously been studied as a byproduct of hyperbolic discounting in (memory-free) MDPs with shared visual representations [*REF*]. As a fail-safe, we include a filtered behavior cloning (BC) term [*REF*; *REF*], which performs supervised learning when actions are estimated to have positive advantage (*MATH*) but does not directly depend on the scale of the *MATH* function.


Hindsight Instruction Relabeling.


AMAGO&apos;s use of off-policy data and high discount factors allows us to relabel long trajectories in hindsight. This ability lets us expand the in-context RL framework to include goal-conditioned problems with sparse rewards that are too difficult for on-policy agents with short planning horizons to explore. AMAGO introduces a variant of HER (Sec.
[3]) for multi-step goals where *MATH*. Besides adding the flexibility to create more complex multi-stage tasks, our experiments show that relabeling multi-step goals effectively creates automatic exploration plans in open-world domains like Crafter [*REF*].


FIGURE


Our relabeling scheme works in two steps. First, we do not restrict goals to represent target states and allow them to be an abstract set of tokens or strings from a closed vocabulary. During rollouts, we can evaluate the goal tokens we were instructed to achieve and record alternatives that can be used for relabeling. The second step extends the HER relabeling scheme to &quot;instructions&quot;, or sequences of up to *MATH* subgoals. If an agent is tasked with *MATH* goals but only completes the first *MATH* steps of the instruction, we sample *MATH* timesteps that provide alternative goals, and then sample from all the goals achieved at those timesteps. We merge the *MATH* alternative goals into the original instruction in chronological order and replay the trajectory from the beginning, recomputing the reward for the new instruction, which leads to a return of *MATH* when rewards are binary. Figure [2] illustrates the technique with a maze-navigation example. Our implementation also considers importance-sampling variants that weight goal selection based on rarity and improve sample efficiency. Details are discussed in Appendix [8].


Experiments


Our experiments are divided into two parts. First, we evaluate our agent in a variety of existing long-term memory, generalization, and meta-learning environments. We then explore the combination of AMAGO&apos;s adaptive memory and hindsight instruction relabeling in multi-task domains with procedurally generated environments. Additional results, details, and discussion for each of our experiments can be found in Appendix [9]. Unless otherwise noted, AMAGO uses a context length *MATH* equal to the entire rollout horizon *MATH*. These memory lengths are not always necessary on current benchmarks, but we are interested in evaluating the performance of RL on long sequences so that AMAGO can serve as a strong baseline in developing new benchmarks that require more adaptation.


Long-Term Memory, Generalization, and Meta-Learning 


FIGURE


POPGym [*REF*] is a recent benchmark for evaluating long-term memory and generalization in a range of CMDPs. Performance in POPGym is far from saturated, and the challenges of sequence-based learning often prevent prior on-policy results from meaningfully outperforming memory-free policies. We tune AMAGO on one environment to meet the sample limit and architecture constraints of the original benchmark before copying these settings across *MATH* additional environments.
Figure [3] summarizes results across the suite. We compare against the best existing baseline (a recurrent GRU-based agent [*REF*]) and the most comparable architecture (an efficient Transformer variant [*REF*]). We also report the aggregated results of the best baseline in each environment -- equivalent to an exhaustive grid search over *MATH* alternative sequence models trained by on-policy RL. If we standardize the return in each environment to *MATH* based on the highest and lowest baseline, as done in [*REF*], AMAGO achieves an average score of *MATH* across the suite, making it a remarkably strong default for sequence-based RL. We perform an ablation that naively applies the shared Transformer learning update without AMAGO&apos;s other details -- such as our Transformer architecture and multi-gamma update. The naive baseline performs significantly worse, and the metrics in Figure [3] mask its collapse in *MATH* environments; AMAGO maintains stability in all of its *MATH* trials despite using model sizes and update-to-data ratios that are not tuned.
Learning curves and additional experiments are listed in Appendix [9.1].


FIGURE


Much of AMAGO&apos;s performance gain in POPGym appears to be due to its ability to unlock Transformers&apos; memory capabilities in off-policy RL: there are *MATH* recall-intensive environments where the GRU baseline achieves a normalized score of *MATH* compared to AMAGO&apos;s *MATH*. Figure [4] ablates the impact of AMAGO&apos;s trajectory encoder from the rest of its off-policy details by replacing its Transformer with a GRU-based RNN in a sample of these recall-intensive environments.


We test the limits of AMAGO&apos;s recall using a &quot;Passive T-Maze&quot; environment from concurrent work [*REF*], which creates a way to isolate memory capabilities at any sequence length. Solving this task requires accurate recall of the first timestep at the last timestep *MATH*. AMAGO trains a single RL Transformer to recover the optimal policy until we run out of GPU memory at context length *MATH* (Figure [5] top right). One concern is that AMAGO&apos;s maximum context lengths, large policies, and long horizons may hinder performance on easier problems where they are unnecessary, leading to the kind of hyperparameter tuning we would like to avoid. We demonstrate AMAGO on two continuous-action meta-RL problems from related work with dense (Fig. [5] left) and sparse (Fig. [5] bottom right) rewards where performance is already saturated, with strong and stable results.


FIGURE


Like several recent approaches that reformulate RL as supervised learning (Sec. [2]), AMAGO scales with the size and sequence length of a single Transformer. However, it creates a stable way to optimize its Transformer on the true RL objective rather than a sequence modeling loss. This can be an important advantage when the supervised objective becomes misaligned with our true goal. For example, we replicate Algorithm Distillation (AD) [*REF*] on a version of its Dark Key-To-Door meta-learning task. AMAGO&apos;s ability to successfully optimize the return over any horizon *MATH* lets us control sample efficiency at test-time (Fig. [5] center), while AD&apos;s adaptation rate is limited by its dataset and converges several thousand timesteps later in this case.


FIGURE


Figure [6] (left) breaks from our default context length *MATH* and evaluates varying lengths on a sample of Meta-World ML-1 robotics environments [*REF*]. Meta-World creates meta-RL tasks out of goal-conditioned environments by masking the goal information, which can be inferred from short context lengths of dense reward signals. While the maximum meta-train performance of every context length is nearly identical, the meta-test success rates decrease slightly over long sequences. Extended contexts may encourage overfitting on these smaller environment distributions. Figure [6] (right) measures the scalability of AMAGO relative to off-policy agents with a separated RNN architecture as well as the more efficient (but previously unstable) shared model. AMAGO lets us train larger models faster than RNNs with equivalent memory capacity. More importantly, Transformers make optimizing these super-long sequences more realistic.


Goal-Conditioned Environment Adaptation


We now turn our focus towards generalization over procedurally generated environments *MATH* and multi-step instructions *MATH* of up to *MATH* goals (Sec. [3]).
A successful policy is able to adapt to a new environment in order to achieve sparse rewards of *MATH* for completing each step of its instruction, creating a general instruction-following agent. Learning to explore from sparse rewards is difficult, but AMAGO&apos;s long-horizon off-policy learning update lets us relabel trajectories with alternative instructions (Figure [2]). Before we scale up to a more complex domain, we introduce two easily-simulated benchmarks. &quot;Package Delivery&quot; is a toy problem where agents navigate a sequence of forks in a road to deliver items to a list of addresses. This task is both sparse and memory-intensive, as achieving any reward requires recall of previous attempts. Appendix [9.3] provides a full environment description, and Figure [7] compares key ablations of AMAGO.
Relabeling, memory, and multi-gamma learning are essential to performance and highlight the importance of AMAGO&apos;s technical details.


FIGURE


&quot;MazeRunner&quot; is a zero-shot maze navigation problem modeled after the example in Figure [2] and loosely based on Memory Maze [*REF*]. The agent finds itself in a familiar spawn location in an otherwise unfamiliar maze and needs to navigate to a sequence of *MATH* locations. Although the underlying maze map is generated as an *MATH* gridworld, the agent&apos;s observations are continuous Lidar-like depth sensors. When *MATH*, the problem is sparse but solvable without relabeling thanks to our implementation&apos;s low-level improvements (Figure [8] left). However, *MATH* is impossibly sparse, and relabeling is crucial to success (Fig.
[8] right). We run a similar experiment where action dynamics are randomly reset every episode in Appendix [9.4]. While this variant is more challenging, our method outperforms other zero-shot meta-RL agents and nearly recovers the original performance while adapting to the new action space.


Crafter [*REF*] is a research-friendly simplification of Minecraft designed to evaluate multi-task capabilities in procedurally generated worlds. Agents explore their surroundings to gather food and resources while progressing through a tech tree of advanced tools and avoiding dangerous enemies. We create an instruction-conditioned version where the agent is only successful if it completes the specified task. Our instructions are formed from the *MATH* original Crafter achievements with added goals for traveling to a grid of world coordinates and placing blocks in specific locations.
Goals are represented as strings like &quot;make stone pickaxe&quot;, which are tokenized at the word level with the motivation of enabling knowledge transfer between similar goals. Any sequence of *MATH* goal strings forms a valid instruction that we expect our agent to solve in any randomly generated Crafter environment.


FIGURE


Figure [9] shows the success of AMAGO on the entire goal distribution along with several highlighted tasks.
Resource-acquisition and tool-making tasks that require test-time exploration and adaptation to a new world benefit from Transformer policies and long-term memory. As the steps in an instruction become less likely to be achieved by random exploration, relabeling becomes essential to success. However, Crafter reveals a more interesting capability of our multi-step hindsight relabeling. If we ablate our relabeling scheme to single-goal HER and evaluate on instructions with length *MATH* (Fig. [9] &quot;w/o Multi-Goal\...&quot;), we find AMAGO loses the ability to complete advanced goals. Crafter&apos;s achievement system locks skills behind a progression of prerequisite steps. For example, we can only make tools after we have collected the necessary materials. Relabeling with instructions lets us follow randomly generated sequences of goals we have mastered until we reach the &quot;frontier&quot; of skills we have discovered, where new goals have a realistic chance of occurring by random exploration. The agent eventually learns to complete new discoveries as part of other instructions, creating more opportunities for exploration. However, this effect only occurs when we provide the entire task in advance instead of revealing the instruction one step at a time (Fig. [9] &quot;w/o Full Instruction\...&quot;).
We continue this investigation in detail with additional Crafter experiments in Appendix [9.5].


Conclusion


We have introduced AMAGO, an in-context RL agent for generalization, long-term memory, and meta-learning. Our work makes important technical contributions by finding a stable and high-performance way to train off-policy RL agents on top of one long-context Transformer. With AMAGO, we can train policies to adapt over more distant planning horizons with longer effective memories. We also show that the benefits of an off-policy in-context method can go beyond stability and sample efficiency, as AMAGO lets us relabel multi-step instructions to discover sparse rewards while adapting to unfamiliar environments. Our agent&apos;s efficiency on long input sequences creates an exciting direction for future work, as very few academic-scale meta-RL or RL generalization benchmarks genuinely require adaptation over thousands of timesteps.
AMAGO is open-source, and the combination of its performance and flexibility should let it serve as a strong baseline in the development of new benchmarks in this area. The core AMAGO framework is also compatible with sequence models besides Transformers and creates an extensible way to research more experimental long-term memory architectures that can push adaptation horizons even further.