Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptatio


Introduction


With the increasing prevalence of LLMs such as ChatGPT, researchers have progressively shifted their focus towards LLM-centered principles, building embodied agents that interact with humans to tackle open-ended tasks [*REF*; *REF*]. To achieve this target, we need to resolve the challenge of developing AI agents with the ability to continuously learn new skills, which is related to a domain commonly referred to as open-ended learning that is broadly categorized into two main factions: (1) pre-training LLMs to translate human-instructions into sub-tasks, for open-ended planning [*REF*; *REF*], and (2) curriculum RL for open-ended control [*REF*; *REF*].


For pre-trained LLMs, particularly those with closed source architectures, focus on resolving planning with general knowledge acquired during the pre-training stage [*REF*]. However, they share shortcomings like relying on task-oriented and hand-crafted prompting, struggling to comprehend interactions in special contexts such as games and be incompetent for high real-time requirements due to inefficient model computation. In contrast, curriculum RL conducts open-ended learning in an end-to-end manner, developing in diverse methodologies such as population-based RL [*REF*], goal-conditioned RL (GCRL) [*REF*] and etc. Despite RL excels in learning novel skills compared to rule-based control, it lacks the direct capability for interaction with humans.


FIGURE


To leverage advantages from both sides, i.e., being capable of interacting human and AI in solving real-time tasks towards open-endedness, an ideal implementation is to integrating LLM for planning and RL policy for decision making. However, existing studies in this domain have focused on improving training efficiency or reducing interaction costs by either independently training the RL policy or LLM [*REF*; *REF*] to adapt the other, resulting in overfitting and failing to explore novel skills in specific environments that necessitate specialized knowledge and falling short of achieving true open-endedness. Despite previous work resolve this issue with LLM-based re-planning [*REF*; *REF*], it is inefficient for high-dimensional tasks and the re-planning is still in the existing range of strength.


To address the above challenge, we propose a co-training framework, OpenPAL, strunctured as a two-stage learning process to implemente bi-directional adaptation. This design enables the RL policy continuously explore novel skills whiling align the LLM and the policy towards achieving instruction open-endedness. In the first stage, we separately train a ChatGLM-6B [*REF*] as a planner (or goal generator) *MATH* and policy *MATH*, where *MATH* generates goals with given instructions and environment context, and *MATH* learns to execute goals. To achieve that, we propose multi-step fine-tuning a pre-trained LLM with GPT-4-generated instructions and goals, and open-ended goal generation to learn a goal-conditioned policy. In the second stage, we implement co-training to align *MATH* (planning) and *MATH* (decision-making), as illustrated in FIGURE. This aims to achieve instruction open-endedness, aligning the instruction space with the open-ended goal space that the agent achieved. Specifically, we implement the co-training as an interleaved execution of (1) Reinforcement Learning with Agent Feedback (RLAF) for *MATH* and (2) GCRL for *MATH* with goals generated by *MATH*, where RLAF centers around rewarding *MATH* with agent feedback and goal execution. This two-staged approach optimizes the LLM for comprehending environment context under the consideration of decision-making, while concurrently enhancing decision-making for goals aligned with human instructions. For evaluation, we employ Contra, an open-ended FPS game. The results demonstrate that OpenPAL achieves a high goal completion ratio for open-ended human-AI interaction.


Background


Goal-conditioned Reinforcement Learning. 


Formally, GCRL could be formulated as a goal-augmented Markov Decision Process *MATH*  [*REF*]. Denoting *MATH* a tuple *MATH*, where *MATH*, *MATH*, *MATH* the state, action and goal spaces, respectively. In general, *MATH* is a projection of *MATH*, i.e., *MATH*.
*MATH* defines the state transition probabilities, i.e., *MATH*, where *MATH* a distribution.
*MATH* defines the reward function *MATH*. At the beginning of an trajectory *MATH*, a goal *MATH* is sampled from a distribution *MATH*, which generally defines a task for the agent to execute. As for decision making, *MATH* denotes a policy as *MATH*, which is a distribution over the action space. To solve a *MATH*, or achieve open-endedness in other words, an agent with policy *MATH* needs to maximize its accumulative reward over the goal space as *MATH*, where *MATH* discounts a reward at each time step to ensure the convergence. Normally, *MATH* is binary as *MATH* *MATH*. *MATH* To approximate *MATH*, GRL suggests using the Universal Value Function Approximator (UVFA) *MATH*. As for the solving of open-endedness, there are critical challenges that need to be resolved, including but not limited to: (1) *MATH* is agnostic to an agent, and (2) *MATH* is uncountable and continuous. To resolve these issues, existing research is centered on curriculum RL [*REF*], i.e., automatically discovering novel goals from past learning. Hindsight goal relabelling [*REF*; *REF*; *REF*; *REF*] implicitly implements curriculum learning by introducing a prioritized replay mechanism and performs high data efficiency. Despite numerous curriculum approaches, the sample inefficiency due to the setting of binary reward signals hinders policy learning. As a solution, existing research leverages reward shaping [*REF*; *REF*; *REF*; *REF*] which is a straightforward and efficient idea.


LLMs and Human-AI Interaction.


LLMs [*REF*; *REF*] a class of neural networks that execute in auto-regressive for text generation. Given a sequence of text tokens with length *MATH* as *MATH*, the generation of a next token *MATH* could be formulated as sampling from a probabilistic model *MATH*. As for the training of LLMs, the target is equivalently to find a parameter set *MATH* which satisfies the optimal generation, i.e., *MATH*.
Beyond the research of LLMs, it is attractive to leverage LLMs as an interface to interact human with agents [*REF*; *REF*]. We can roughly reformulate the generation as *MATH* for human-AI interaction, where *MATH* the language instruction as a prompt, *MATH* the context. For the cases have low real-time requirements, *MATH* is a control signal for decision making [*REF*; *REF*; *REF*]. While for the cases have high real-time requirements, *MATH* is a goal and will be fed to a controller to guide the decision making [*REF*; *REF*]. Our study falls within the latter situation, building open-endeded embodied agents in the cases with high real-time requirements.


The Contra: A Battle Royale FPS Game


Contra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elements inherent in first-person shooting games [*REF*]. It unfolds with multiple hostile teams, necessitating players to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena. The agent&apos;s objectives encompass individual survival and the elimination of encountered enemies. An agent in Contra mandates a sequential acquisition of skills, starting from fundamental abilities like running and item collection. As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles and coordinating tactics with teammates, an open-ended learning process. The primary rationale behind choosing Contra as our testbed lies in its possession of proprietary knowledge not encompassed in general knowledge repositories. Consequently, we employ Reinforcement Learning (RL) for knowledge discovery, and co-training to align the Language Model (LLM) and RL in comprehending the environment.


A Co-training Framework: OpenPAL


Considering the training in the context of open-ended learning is extensive data-thirsty, we first introduce two critical engineering designs to enhance training efficiency. Specifically, OpenPAL incorporates a distributed RL framework inspired by AlphaStar [*REF*] with modifications, resulting in the formation of the Actor-League-Learner architecture. In this architecture, the League is responsible for distributing rollout tasks to a cluster of Actors (CPU nodes) for data collection and evaluation, while optimization tasks are delegated to the Learner (GPU node) for policy updates. This distributed approach significantly enhances rollout throughput, thereby improving overall training efficiency. Another efficiency challenge stems from the iterative development of Contra.
During the period of our research, Contra&apos;s environmental attributes continuously change as the programming development. Thus, policy retraining would be necessary if there is no explicit intervention. To reduce such an extra computation burden, we employ surgery [*REF*] to retain learned skills at the lowest training cost, enabling adaptation to a changing observation/goal space while ensuring compatibility with network inputs. Detailed information on the distributed RL framework can be found in [12], and version changes are listed in TABLE. In the following content, we will introduce OpenPAL in two stages, including the independent training at stage I ([4.1] *MATH* [4.3]) and the co-training at stage II ([4.4]).


Exploring Basic Skills via Non-goal RL 


In the realm of GCRL, the prevalent approach involves curriculum learning a goal-conditioned policy from scratch, learning goal execution while exploring goals. However, it maybe inefficient for an agent to explore the whole goal space when there is a lack of prior knowledge of the goal space. Thus, we opt for leveraging non-goal RL for basic skill learning before goal-conditioned learning. For the implementation, we employ Proximal Policy Optimization (PPO) [*REF*] with fine-grained reward shaping as *MATH* *MATH*, *MATH* where *MATH* for skill learning which focuses on targeting the agent towards wining and surviving as long as possible, is a linear combination of diverse behavior factors, *MATH* encourages the agent to avoid obstacles due to the agent is not sensitive to obstacles when navigating, and *MATH*, *MATH* the factors to weight the contribution of each item. The details of the reward construction are included in [14], [7]. Then, the value function for estimating *MATH* is implemented as a multi-head network and shares the backbone of policy, i.e., *MATH*, where *MATH* and *MATH* approximate *MATH* and *MATH*, respectively.


Learning a Goal-conditioned Policy 


We construct the goal space using various state attributes which can be determined and effected by interaction. In detail, they are (1) agent private states that can be directly changed by the agent or other players, such as firing, walking, etc. (2) enemies states that can be effected through the agent interactions, such as knock down an enemies; and (3) teammates states that can be effected by the interaction between the agent and its teammates. We summarize them in TABLE. With the above consideration, we further model each attribute as a sub-goal space *MATH* with multiple candidates that can be expressed as a set of normalized indices *MATH*, where 0 an invalid attribute value indicates the corresponding attribute not be selected. For the goal space, there are 68 sub-goal spaces that shape it as *MATH*. Obviously, *MATH* which comprises of more valid sub-goals, the more difficult to complete for the policy.


Open-ended Goal Generation.


Among existing GCRL research, hindsight goal relabelling and generation [*REF*; *REF*] are effective goal-conditioned learning methods that advantage from a free of goal prior, compared to explicit curriculum. However, there is a limitation of in-distribution goal exploration [*REF*], i.e., policy learning and goal exploration shares the same training dataset, which is inefficient in exploration as the range of goals are limited by the scale of samples. Comparatively, if we can model the goal distribution, we can not only achieve data efficiency akin to that of hindsight goal generation, but also progressively attain an open-ended goal space by adjusting the goal distribution. Therefore, we train a neural-based goal generator *MATH* over a dataset of trajectory segments *MATH* explored by the well-trained policy *MATH* from [4.1] as it is trained for exploring basic skills. We assume that a goal *MATH* corresponding to a given initial state *MATH* can be represented by a 3-tuple *MATH*, where *MATH* the time slot required to achieve *MATH* starting from *MATH*, and *MATH* a vector of state values from *MATH* to *MATH* with the consideration of representation. As a result, we train the goal generator *MATH* to take input in the form of *MATH*, thereby allowing variations in *MATH* and *MATH* to yield different goals for a given state *MATH*. For an implementation, we firstly construct a dataset *MATH* from *MATH*, where each item in *MATH* satisfies: *MATH* *MATH* *MATH* *MATH* the first 150 states of *MATH*, *MATH* the last 20 states. Then we train *MATH* with a MSE loss as *MATH*.
While varying *MATH* and *MATH* produces diverse goals, it remains challenging to comprehensively cover the entire goal space corresponding to a given state. As a supplement, we propose integrating goal generation with a uniform sampler, denoted as *MATH*, which randomly samples goals from the goal space *MATH* using *MATH*. This results in a goal generation *MATH*.


Intrinsic Reward Shaping.


As introduced in the aforementioned, a critical challenge hinders the goal completion is sparse rewarding. To mitigate this, we extend the reward function in EQUATION with an intrinsic reward *MATH* that evaluates the degree of goal completion.
EQUATION shows the calculation of *MATH* as the Euclidean norm difference between two consecutive states and a goal as *MATH* *MATH*, *MATH* where *MATH* indicates the *MATH* -norm. This reward provides a denser reward signal at each time step to the agent about its proximity to the goal, offering more nuanced information than a binary signal indicating whether it has reached the goal or not. In our current implementation, we set *MATH*. Thus, the reward function for GCRL is formulated as *MATH* *MATH*, *MATH* where *MATH* comes from EQUATION. And for the value function corresponds to EQUATION, we extend the multi-head *MATH* with a new value head *MATH* as *MATH*, where *MATH* approximates *MATH*.


Avoiding Policy Degeneration.


Let *MATH* denote the well-trained policy from the non-goal reinforcement learning step. However, we have observed a performance degeneration on basic skill execution when continuing the training of the goal-conditioned policy *MATH* starting from *MATH*. This is attributed to two aspects: (1) catastrophic forgetting on the basic skills as the goal-conditioned learning continues; (2) a change in the input of the policy network from *MATH* to *MATH*, where *MATH* introduces interference in decision-making, as the policy has not encountered goal inputs during non-goal-conditioned training. To address thes issues, we propose a modification to the goal-conditioned policy learning objective by introducing a KL-divergence regularizer, and introduce 20% workers for non-goal policy learning to avoid catastrophic forgetting. This regularizer quantifies the distance between *MATH* and *MATH* when *MATH* conditioned on *MATH* as it is equivalently to non-goal policy: *MATH* *MATH*. *MATH* *MATH* the policy loss in PPO, and *MATH* indicates that the KL-divergence term is only activated when an empty goal input for *MATH*.
ALGORITHM, [14] summarizes the learning process.
Furthermore, we observed that occasionally sampling experience from *MATH* to train *MATH* can also relieve the degeneration.


Fine-tuning a LLM-based Goal Generator 


Let *MATH* represent the set of natural language instructions, and *MATH* the set of abstracted environment states in text. Our objective is to fine-tune a pre-trained LLM as a goal generator, denoted as *MATH*, which means *MATH* generates a goal relevant to a given instruction with the consideration of current environment context, i.e., a state abstraction.


Dataset Construction.


To achieve that, we first construct *MATH* using states collected by the *MATH*. Each abstraction *MATH* encapsulates essential state features of its corresponding state *MATH*, and the extraction rules are outlined in [8]. For the creation of *MATH*, we leverage various instruction generation to ensure its diversity and scalability, aligning with our overarching goal of achieving open-endedness in the instruction space. Specifically, *MATH* is derived from four types. Most of these are formulated through a tuple of an initial state and a target state/trajectory collected by *MATH*, which aims to align *MATH* and *MATH* at environmental comprehension. Then, we leverage this data and GPT-4 [*REF*] to generate appropriate instruction. This instruction aims to direct from the specified initial state to the intended target state, and CoT [*REF*] is deployed to enhance performance. Specifically, the four types of instruction generation are (1) *MATH* (Human Instructions, HI): human-annotated instructions; (2) *MATH* (State Instructions, SI): GPT-4-generated instructions by giving a tuple of states *MATH* where the *MATH* the initial state that sampled from agent trajectories and *MATH* the target state that is manually constructed by modifying features of the *MATH*; (3) *MATH* (Agent Instructions, AI): GPT-4-generated instructions by giving a pair of *MATH* where *MATH* the initial state, *MATH* the agent trajectory; and (4) *MATH* (Random Instructions, RI): a mixture of the above three instruction sets to form a supplementary dataset. By accompanying *MATH* with *MATH*, we further construct *MATH*.
Subsequently, we employ GPT-4 to generate appropriate goals *MATH* using *MATH* as labeled data for training *MATH*, resulting in a dataset *MATH*.
To ensure that the goals generated by GPT-4 conform to the format we want, a comprehensive prompt engineering endeavor was conducted to establish a set of predetermined rules for GPT-4. The rule-based prompts that guide GPT-4&apos;s responses are documented in [16], with examples of prompts for generation provided in [38].


Multi-step Fine-tuning.


We fine-tune ChatGLM-6B with LoRA [*REF*] in three steps, as illustrated in [5]. The steps include (1) CoT-assisted fine-tuning (CoFT): we split the CoT steps of building *MATH* into independent training data, aiming to expand the volume of training data as well as enhance the goal generator&apos;s reasoning and understanding to *MATH*; (2) Supervised Fine-tuning (SFT): strictly formatting the LLM-generated goals and further improving the accuracy; and (3) Ensemble Fine-tuning (EFT): multiple checkpoints of *MATH* are utilized to generate goal candidates for each *MATH*, then sub-goals with highest counts are reconstructed as a ground goal to fine-tune the model to enhance the generation.


Collaborative Training 


After completing the above training steps, we obtained a well-trained goal generator *MATH* and goal-conditioned policy *MATH* that satisfactorily adhere to their respective goal distributions. However, an inconsistency persists between *MATH* and *MATH* stemming from their independent training objectives, where *MATH* aimed to generate goals that satisfy given instructions, and *MATH* focused on exploring goals. Therefore, we introduce co-training to address the aforementioned issue ensuring that the goals generated by *MATH* are not only linguistically sound but also aligned with the capabilities of *MATH*. We formulate the co-training as follows: *MATH* *MATH* *MATH* where *MATH* the goal distribution conditioned by *MATH*, *MATH* denotes an approximate evaluation for *MATH* or *MATH*, in general, a state value function.
It is noteworthy that our co-training framework is close to a hierarchical reinforcement learning framework (HRL) [*REF*], where the Manager (comparable to *MATH*) plans goals for the learning of the Worker (comparable to *MATH*), with RL being performed for each. Inspired by HRL, we implement co-training by integrating the goal-conditioned training of *MATH* and Reinforcement Learning with Agent Feedback (RLAF) for *MATH*. RLAF is built upon PPO, with a reward shaping that considers (1) *MATH* the evaluation of goal completion, where a high reward indicates that a goal is completed or the reachable probability from current state; (2) *MATH* the evaluation of crucial sub-goal completion, which involves examining cases by pairing instructions in a batch with a set of essential sub-goals; (3) *MATH* the evaluation of outputting the proper goal format, with the LLM being penalized based on edit distance.
Then, we can express the reward function as *MATH* and [17] includes more details. We observed the training will lead *MATH* and *MATH* compromise to a local optimal, i.e., *MATH* comforts a high completion ratio for *MATH* but neglect consistency with instructions, and *MATH* simultaneously rewards *MATH* with a high completion ratio. Furthermore, as the policy training continuing, the evaluation for goal generation is out-date. To fix this issue, we propose a periodic reset for the RLAF, i.e., the parameters of the *MATH* will be reset for every set number of steps to avoid being trapped in a local convergence, achieving enhanced goal completion, and keeping goals consistent with human instructions.
Considering the training efficiency, we conduct LoRA [*REF*] to update the model weights for *MATH*.
[5] illustrates the whole training process, and ALGORITHM summarizes the corresponding pseudo-code.


ALGORITHM


Experiment


We conduct empirical experiments to evaluate the efficacy of both stages of our proposed OpenPAL. To make the Contra satisfy the learning requirements, we give well-designed spaces and reward functions as follows.


Observation Space.


The observation space encompasses many factors, such as unit features detailing the agent states, those of other players and environmental features capturing interaction events. Additionally, an agent-centric RGB bird&apos;s-eye-view (BEV) of the local environment is considered.
TABLE includes detailed information.


Action Space.


The action space is implemented on top of Contra&apos;s micro-operation API, comprising a collection of multi-grained actions. These actions range from fine-grained movements, such as six-degrees-of-freedom movement and weapon usage, to compound actions in coarse-grained categories, such as firing at a target, and each action is executed over a duration of 200ms, hence the control frequency is 5Hz. The total size of the action space is 54. Further details in TABLE.


Reward Functions.


A comprehensive representation is employed for the reward function, considering various factors contributing to goal-conditioned policy learning. These factors are organized as a linear combination to formulate the reward function. Furthermore, we determine the weights for the combination with a two-fold principle: (1) assigning weights to reward items based on their scales and emphasizing important factors; (2) dynamically adjusting weights in response to learning feedback, such as decreasing or increasing the weights of corresponding factors.
Additional information is available in [7].


Evaluating Goal-conditioned RL


We evaluate the *MATH* of stage I from three distinct perspectives to verify the open-endedness achieved on *MATH*: (1) the completion ratio, (2) generalization capability concerning unseen goals, and (3) robustness when integrating goal-conditioned learning atop non-goal learning. Given that GCRL in OpenPAL comprises random and hindsight stages, our evaluation involves a comparative analysis with a baseline, [HER], i.e., training the RL agent with hindsight goal generation.
FIGURE presents a comparison of the goal completion ratio across different methods on a validation dataset where goals are generated using *MATH* and *MATH*. As depicted in FIGURE, our method surpasses HER by *MATH*.
FIGURE evaluates the generalization on unseen goals, addressing the second aspect mentioned earlier. It is noteworthy that the unseen goals are re-combinations of goals obtained with HER and *MATH*. As indicated in FIGURE, our method excels over the baseline in terms of completion ratio.
FIGURE answers the third point by comparing the use of KL-divergence regularizer for policy learning, considering changes in overall performance and the ability to eliminate enemies. Three metrics are designed for evaluation: (1) Mean basic reward per step, which indicates whether the current policy degenerates in performing basic skills per step against a well-trained non-goal policy, and intentional to emphasize the agent&apos;s immediate responsiveness over final results; (2) Enemies killed, representing the average number of enemies killed by the agent per episode; and (3) Enemies knocked down, representing the average number of enemies knocked down by the agent per episode.


FIGURE


Evaluating LLM-based Goal Generation


We conducted evaluation of *MATH* through two comparative experiments on GPT-4-generated instruction datasets, aiming to investigate the impact of different instruction datasets and fine-tuning paradigms. The evaluation metrics employed encompass precision, recall, and F1 score.
It&apos;s worth noting that a potential issue in determining the precision of generating sub-goals that are close in semantics. For instance, associating the sub-goal &quot;moving speed&quot; values &quot;very fast&quot; versus &quot;fast&quot; may be perceived as a negative instance under precision measurement.
Consequently, we argue that the generation of such sub-goals should weigh more in choosing sub-goal than determining values. Thus, we further propose three choice-based metrics: precision (choice), recall (choice), and F1 (choice).


TABLE provides a comparison of five types of instruction datasets used in the multi-step fine-tuning process for *MATH*. The comparison reveals that utilizing a mixture significantly outperforms individual base datasets, which indicates a mixture aids *MATH* in capturing human preferences and understanding the implications of each abstracted state, thereby enhancing goal generation.


TABLE compares four kinds of fine-tuning with the proposed multi-step fine-tuning, including (1) SFT: only use the target prompt without CoT data to supervised fine-tuning, which can be regarded as a baseline for a naive SFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF *MATH* SFT: further SFT target prompt after CoTF; (4) CoTF *MATH* SFT *MATH* EFT: further ensemble fine-tuning target prompt after CoTF. With the comparison, we conclude that CoTF and SFT can improve each other and achieve better performance. Furthermore, ensemble fine-tuning significantly enhances precision while marginally decreasing recall, making it more suitable for generating accurate concise goals.


FIGURE


TABLE


Evaluating Co-training


We conduct an analysis of the completion ratio corresponding to the number of valid sub-goals during the co-training process. Though the dimension size of goal space achieves 68, the number of sub-goals for valid goals predominantly falls within the range of 1 to 7. This is rational as completing a goal with an excessive number of sub-goals is exceedingly challenging for a policy, even impossibility for human to achieve. Furthermore, FIGURE shows that the improvements mainly lies in *MATH*, because *MATH* is too easy while *MATH* is too hard to complete.
FIGURE shows a case of *MATH* that co-training indeed improves the completion ratio as the green curve. It is noteworthy that the performance suddenly downgrades at each reset.
This phenomenon is attributed to the reset of *MATH* breaks the adaptation with *MATH*, avoiding being trapped in local optimal.
Meanwhile, the performance tends to converge, which indicates the successor loops produce a better adaptation between LLM and policy than before. Additionally, we investigated the change in the generation probability of sub-goals TABLE illustrates changes within a training loop, while TABLE indicates changes across loops. As training progresses, the probabilities associated with each *MATH* undergo gradual modifications. For instance, sub-goals with growing probabilities are central to the agent private states due to their relatively attainable nature and influence in agent interaction.
Conversely, sub-goals with falling probabilities are central to other players&apos; states, as they are not directly changed by agent actions, and *MATH* tends to generate outputs for these sub-goals only when absolutely necessary. To investigate the impact of co-training to *MATH*, we have also identified the changes of goal-generation for an instruction, as shown in [13]. Evidently, after co-training, *MATH* demonstrates its capacity to eliminate contradictory and irrational elements within the initial objectives and exhibits the ability to introduce new sub-goals, thereby rendering the overall goal more attainable, all while retaining its exceptional semantic comprehension capabilities.


Conclusion


In this paper, we propose OpenPAL experts on learning open-ended embodied agents for human-AI interaction, excelling in achieving instruction open-endedness through a two-stage learning process. The empirical results on Contra represent that OpenPAL shows the potential as a practical solution for human-AI interaction in complex situations.
Despite the positive results, we admit there are still some limitations to our work that would be expected to be researched in the future---for instance, a truly open-ended goal description instead of the handcrafted goal space in the current version; supporting multi-modality input/output to free from expensive feature engineering.