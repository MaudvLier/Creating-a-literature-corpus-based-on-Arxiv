Self-supervised Visual Reinforcement Learning with Object-centric Representations


Introduction


Reinforcement learning (RL) includes a promising class of algorithms
that have shown capability to solve challenging tasks when those tasks
are well specified by suitable reward functions. However, in the real
world, people are rarely given a well-defined reward function. Indeed,
humans are excellent at setting their own abstract goals and achieving
them. Agents that exist persistently in the world should likewise
prepare themselves to solve diverse tasks by first constructing
plausible goal spaces, setting their own goals within these spaces, and
then trying to achieve them. In this way, they can learn about the world
around them.


In principle, the goal space for an autonomous agent could be any
arbitrary function of the state space. However, when the state space is
high-dimensional and unstructured, such as only images, it is desirable
to have goal spaces which allow efficient exploration and learning,
where the factors of variation in the environment are well disentangled.
Recently, unsupervised representation learning has been proposed to
learn such goal spaces [*REF*; *REF*; *REF*]. All
existing methods in this context use variational autoencoders (VAEs) to
map observations into a low-dimensional latent space that can later be
used for sampling goals and reward shaping.


However, for complex compositional scenes consisting of multiple
objects, the inductive bias of VAEs could be harmful. In contrast,
representing perceptual observations in terms of entities has been shown
to improve data efficiency and transfer performance on a wide range of
tasks [*REF*]. Recent research has proposed a range of
methods for unsupervised scene and video
decomposition [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
These methods learn object representations and scene decomposition
jointly. The majority of them are in part motivated by the fact that the
learned representations are useful for downstream tasks such as image
classification, object detection, or semantic segmentation. In this
work, we show that such learned representations are also beneficial for
autonomous control and reinforcement learning.


We propose to combine these object-centric unsupervised representation
methods that represent the scene as a set of potentially structured
vectors with goal-conditional visual RL. In our method (illustrated in
Figure [1]), dubbed SMORL (for self-supervised multi-object RL), a representation of
raw sensory inputs is learned by a compositional latent variable model
based on the SCALOR architecture [*REF*]. We show that
using object-centric representations simplifies the goal space learning.
Autonomous agents can use those representations to learn how to achieve
different goals with a reward function that utilizes the structure of
the learned goal space. Our main contributions are as follows:
-  We show that structured object-centric representations learned with
generative world models can significantly improve the performance of
the self-supervised visual RL agent.
-  We develop SMORL, an algorithm that uses learned representations to
autonomously discover and learn useful skills in compositional
environments with several objects using only images as inputs.
-  We show that even with fully disentangled ground-truth
representation there is a large benefit from using SMORL in
environments with complex compositional tasks such as rearranging
many objects.


FIGURE


Related work


Our work lies in the intersection of several actively evolving topics:
visual reinforcement learning for control and robotics, and
self-supervised learning. Vision-based RL for robotics is able to
efficiently learn a variety of behaviors such as grasping, pushing and
navigation [*REF*; *REF*; *REF*; *REF*]
using only images and rewards as input signals. Self-supervised
learning is a form of unsupervised learning where the data provides the
supervision. It was successfully used to learn powerful representations
for downstream tasks in natural language
processing [*REF*; *REF*] and computer
vision [*REF*; *REF*]. In the context of RL,
self-supervision refers to the agent constructing its own reward signal
and using it to solve self-proposed
goals [*REF*; *REF*; *REF*; *REF*].
This is especially relevant for visual RL, where a reward signal is
usually not naturally available. These methods can potentially acquire a
diverse repertoire of general-purpose robotic skills that can be reused
and combined during test time. Such self-supervised approaches are
crucial for scaling learning from narrow single-task learning to more
general agents that explore the environment on their own to prepare for
solving many different tasks in the future. Next, we will cover the two
most related lines of research in more detail.


Self-supervised visual RL [*REF*; *REF*; *REF*; *REF*; *REF*]
tackles multi-task RL problems from images without any external reward
signal. However, all previous methods assume that the environment
observation can be encoded into a single vector, e.g. using VAE
representations. With multiple objects being present, this assumption
may result in object encodings overlapping in the representation, which
is known as the binding problem [*REF*]. In addition, as the
reward is also constructed based on this vector, the agent is
incentivized to solve tasks that are incompatible, for instance
simultaneously moving all objects to goal positions. In contrast, we
suggest to learn object-centric representations and use them for reward
shaping. This way, the agent can learn to solve tasks independently and
then combine these skills during evaluation.


Learning object-centric representations in RL [*REF*; *REF*; *REF*; *REF*]
has been suggested to approach tasks with combinatorial and
compositional elements such as the manipulation of multiple objects.
However, the previous work has assumed a fixed, single task and a given
reward signal, whereas we are using the learned object-representations
to construct a reward signal that helps to learn useful skills that can
be used to solve multiple tasks. In addition, these methods use
scene-mixture models such as MONET [*REF*] and
IODINE [*REF*], which do not explicitly contain features like
position and scale. These features can be used by the agent for more
efficient sampling from the goal space and thus the explicit modeling of
these features helps to create additional biases useful for manipulation
tasks. However, we expect that other object-centric representations
could also be successfully applied as suitable representations for RL
tasks.


Background


Our method combines goal-conditional RL with unsupervised
object-oriented representation learning for multi-object environments.
Before we describe each technique in detail, we briefly state some RL
preliminaries. We consider a Markov decision process defined by
*MATH*, where *MATH* and *MATH* are the continuous state and action spaces,
*MATH* is an unknown probability density representing the probability of
transitioning to state *MATH* from state
*MATH* given action *MATH*, and *MATH* is a function computing the
reward for reaching state *MATH*. The agent&apos;s objective is to
maximize the expected return *MATH* 
over the horizon *MATH*, where *MATH* is the state marginal
distribution induced by the agent&apos;s policy *MATH*.


Goal-conditional Reinforcement Learning


In the standard RL setting described before, the agent only learns to
solve a single task, specified by the reward function. If we are
interested in an agent that can solve multiple tasks (each with a
different reward function) in an environment, we can train the agent on
those tasks by telling the agent which distinct task to solve at each
time step. But how can we describe a task to the agent? A simple, yet
not too restrictive way is to let each task correspond to an environment
state the agent has to reach, denoted as the goal state *MATH*. The task is
then given to the agent by conditioning its policy
*MATH* on the goal *MATH*, and the agent&apos;s objective turns
to maximize the expected goal-conditional return: *MATH*.


where *MATH* is some distribution over the space of goals
*MATH* the agent receives for training. The
reward function can, for example, be the negative distance of the
current state to the goal: *MATH*. Often, we are only
interested in reaching a partial state configuration, e.g. moving an
object to a target position, and want to avoid using the full
environment state as the goal. In this case, we have to provide a
mapping *MATH* of states to the
desired goal space; the mapping is then used to compute the reward
function, i.e. *MATH*.


As the reward is computed within the goal space, it is clear that the
choice of goal space plays a crucial role in determining the difficulty
of the learning task. If the goal space is low-dimensional and
structured, e.g. in terms of ground truth positions of objects, rewards
provide a meaningful signal towards reaching goals. However, if we only
have access to high-dimensional, unstructured observations, e.g. camera
images, and we naively choose this space as the goal space, optimization
becomes hard as there is little correspondence between the reward and
the distance of the underlying world states [*REF*].


One option to deal with such difficult observation spaces is to learn a
goal space in which the RL task becomes easier. For instance, we can
try to find a low-dimensional latent space *MATH* and use it both
as the input space to our policy and the space in which we specify
goals. If the environment is composed of independent parts that we
intend to control separately, intuitively, learning to control is
easiest if the latent space is also structured in terms of those
independent components. Previous
research [*REF*; *REF*] relied on the disentangling
properties of representation learning models such as the
*MATH* -VAE [*REF*] for this purpose. However, these
models become insufficient when faced with multi-object scenarios due to
the increasing combinatorial complexity of the scene, as we show in
Sec. [5.2] and in App. [7.2]. Instead, we use a model explicitly
geared towards inferring object-structured representations, which we
introduce in the next section.


Structured representation learning with SCALOR


SCALOR [*REF*] is a probabilistic generative world model
for learning object-oriented representations of a video or stream of
high-dimensional environment observations. SCALOR assumes that the
environment observation *MATH* at step *MATH* is generated by the
background latent variable *MATH* and the foreground
latent variable *MATH*. The foreground is further
factorized into a set of object representations
*MATH*, where *MATH* is the set of recognised object indices. To combine the
information from previous time steps, a propagation-discovery model is
used [*REF*]. In SCALOR, an object is represented by
*MATH*. The scalar *MATH* defines if the object is present in
the scene, whereas the vector *MATH* encodes
object appearance. The component *MATH* is
further decomposed into the object&apos;s center position
*MATH*, scale *MATH*, and depth *MATH*. With this, the generative process
of SCALOR can be written as: *MATH* where *MATH*,
*MATH* contains latent variables of objects discovered in
the present step, and *MATH* contains latent variables of
objects propagated from the previous step. Due to the intractability of
the true posterior distribution *MATH*, SCALOR
is trained using variational inference with the following posterior
approximation: *MATH* by maximizing the following evidence lower bound
*MATH* where *MATH* denotes the Kullback-Leibler divergence. As we
are using SCALOR in an active setting, we additionally condition the
next step posterior predictions on the actions *MATH* taken by the
agent. For more details and hyperparameters used to train SCALOR, we
refer to App. [10.3]. In the next section, we describe how
the structured representations learned by SCALOR can be used in
downstream RL tasks such as goal-conditional visual RL.


Self-Supervised Multi-Object Reinforcement Learning


Learning from flexible representations obtained from unsupervised scene
decomposition methods such as SCALOR creates several challenges for RL
agents. In particular, these representations consist of sets of vectors,
whereas standard policy architectures assume fixed-length state vectors
as input. We propose to use a goal-conditioned attention policy that
can handle sets as inputs and flexibly learns to attend to those parts
of the representation needed to achieve the goal at hand.


In the setting we consider, the agent is not given any reward signal or
goals from the environment at the training stage. Thus, to discover
useful skills that can be used during evaluation tasks, the agent needs
to rely on self-supervision in the form of an internally constructed
reward signal and self-proposed goals. Previous VAE-based methods used
latent distances to the goal state as the reward signal. However, for
compositional goals, this means that the agent needs to master the
simultaneous manipulation of all objects. In our experiments in
Sec. [5.1], we show that even with fully
disentangled, ground-truth representations of the scene, this is a
challenging setting for state-of-the-art model-free RL agents. Instead,
we propose to use the discovered structure of the learned goal and state
spaces twofold: the structure within each representation, namely object
position and appearance, to construct a reward signal, and the set-based
structure between representations to construct sub-goals that correspond
to manipulating individual objects.


Policy with Goal-conditioned Attention 


We use the multi-head attention mechanism [*REF*] as the
first stage of our policy *MATH* to deal with the challenge of
set-based input representations. As the policy needs to flexibly vary
its behavior based on the goal at hand, it appears sensible to steer the
attention using a goal-dependent query *MATH*.
Each object is allowed to match with the query via an object-dependent
key *MATH* and contribute to the attention&apos;s
output through the value *MATH*, which is
weighted by the similarity between *MATH* and *MATH*. As
inputs, we concatenate the representations for object *MATH* to vectors
*MATH*, and similarly the goal representation to
*MATH*. The attention head *MATH* is computed as *MATH* where *MATH* is a packed matrix of all *MATH* &apos;s,
*MATH*, *MATH*, *MATH* constitute learned linear transformations and *MATH* 
is the common key, value and query dimensionality. The final attention
output *MATH* is a concatenation of all the attention heads
*MATH*. In general, we expect it to be beneficial for
the policy to not only attend to entities conditional on the goal; we
thus let some heads attend based on a set of input independent, learned
queries, which are not conditioned on the goal. We go into more details
about the attention mechanism in App. [10.1] and ablate the impact of
different choices in App. [8].


The second stage of our policy is a fully-connected neural network *MATH* 
that takes as inputs *MATH* and the goal representation *MATH* and
outputs an action *MATH*. The full policy *MATH* can thus
be described by *MATH*.


Self-Supervised Training 


In principle, our policy can be trained with any goal-conditional
model-free RL algorithm. For our experiments, we picked soft-actor
critic (SAC) [*REF*] as a state-of-the-art method for
continuous action spaces, using hindsight experience replay
(HER) [*REF*] as a standard way to improve
sample-efficiency in the goal-conditional setting.


The training algorithm is summarized in
Alg. We first train SCALOR on data collected from a
random policy and fit a distribution *MATH* to
representations *MATH* of collected data. Each
rollout, we generate a new goal for the agent by picking a random
*MATH* from the initial observation *MATH* and
sampling a new *MATH* from the fitted distribution
*MATH*. The policy is then rolled out using this
goal. During off-policy training, we are relabeling goals with HER, and,
similar to RIG [*REF*], also with &quot;imagined goals&quot; produced in
the same way as the rollout goals.


A challenge with compositional representations is how to measure the
progress of the agent towards achieving the chosen goal. As the goal
always corresponds to a single object, we have to extract the state of
this object in the current observation in order to compute a reward. One
way is to rely on the tracking of objects, as was shown possible e.g.
by SCALOR [*REF*]. However, as the agent learns, we noticed
that it would discover some flaws of the tracking and exploit them to
get a maximal reward that is not connected with environment changes, but
rather with internal vision and tracking flaws (details in App. [11]).


We follow an alternative approach, namely to use the
*MATH* component of discovered objects and match them
with the current goal representation *MATH*. As the
*MATH* space encodes the appearance of objects, two
detections corresponding to the same object should be close in this
space (we verify that this hypothesis holds in
App. [7.1]). Thus, it is easy to
find the object corresponding to the current goal object using the
distance *MATH*. In
case of failure to discover a close representation, i.e. when all
*MATH* have a distance larger than some threshold
*MATH* to the goal representation *MATH*, we use
a fixed negative reward *MATH* to incentivise the agent
to avoid this situation.


Our reward signal is thus *MATH*.


ALGORITHM


Composing Independent Sub-Goals during Evaluation


At evaluation time, the agent receives a goal image from the environment
showing the state to achieve. The goal image is processed by SCALOR to
yield a set of goal vectors. For our experiments, we assume that these
sub-goals are independent of each other and that the agent can thus
sequentially achieve them by cycling through them until all of them are
solved. The evaluation algorithm is summarized in
Alg., with more details added in
App. [10.2].


Experiments


FIGURE


We have done computational experiments to address the following
questions:
-  How well does our method scale to challenging tasks with a large
number of objects in case when ground-truth representations are
provided?
-  How does our method perform compared to prior visual
goal-conditioned RL methods on image-based, multi-object continuous
control tasks?
-  How suitable are the representations learned by the compositional
generative world model for discovering and solving RL tasks?


To answer these questions, we constructed the Multi-Object Visual Push
and Multi-Object Visual Rearrange environments. Both environments are
based on MuJoCo [*REF*] and the Multiworld package for
image-based continuous control tasks introduced by  *REF*, and
contain a 7-dof Sawyer arm where the agent needs to be controlled to
manipulate a variable number of small picks on a table. In the first
environment, the objects are located on fixed positions in front of the
robot arm that the arm must push to random target positions. We included
this environment as it largely corresponds to the Visual Pusher
environments of  *REF*. In the second environment, the task is
to rearrange the objects from random starting positions to random target
positions. This task is more challenging for RL algorithms due to the
randomness of initial object positions. For both environments, we
measure the performance of the algorithms as the average distance of all
pucks to their goal positions on the last step of the episode. Our code,
as well as the multi-objects environments will be made public after the
paper publication.


SMORL with ground-truth (GT) state representation 


We first compared SMORL with ground-truth representation with Soft
Actor-Critic (SAC) [*REF*] with Hindsight Experience
Replay (HER) relabeling [*REF*] that takes an
unstructured vector of all objects coordinates as input. We are using a
one-hot encoding for object identities *MATH* and
object and arm coordinates as *MATH* components. With
such a representation, the matching task becomes trivial, so our main
focus in this experiment is on the benefits of the goal-conditioned
attention policy and the sequential solving of independent sub-tasks. We
show the results in Fig. [2]. While for *MATH* objects, SAC+HER
is performing similarly, for *MATH* and *MATH* objects, SAC+HER fails to
rearrange any of the objects. In contrast, SMORL equipped with
ground-truth representation is still able to rearrange *MATH* and *MATH* 
objects, and it can solve the more simple sub-tasks of moving each
object independently. This shows that provided with good
representations, SMORL can use them for constructing useful sub-tasks
and learn how to solve them.


FIGURE


Visual RL Methods Comparison 


FIGURE


We compare the performance of our algorithm with two other
self-supervised, multi-task visual RL algorithms on our two
environments, with one and two objects. The first one,
RIG [*REF*], uses the VAE latent space to sample goals
and to estimate the reward signal. The second one,
Skew-Fit [*REF*], also uses the VAE latent space, however, is
additionally biased on rare observations that were not modeled well by
the VAE on previously collected data. In terms of computational
complexity, both our method and RIG need to train a generative model
before RL training. We note that training SCALOR is more costly than
training RIG&apos;s VAE due to the sequence processing utilized by SCALOR.
However, once trained, SCALOR only adds little overhead compared to
RIG&apos;s VAE during RL training, and compared to Skew-Fit, our method is
still faster to train as Skew-Fit needs to continuously retrain its VAE.


We show the results in Fig. [3]. For the simpler
Multi-Object Visual Push environment, the performance of SMORL is
comparable to the best performing baseline, while for the more
challenging Multi-Object Visual Rearrange environment, SMORL is
significantly better then both RIG and Skew-Fit. This shows that
learning of object-oriented representations brings benefits for goal
sampling and self-supervised learning of useful skills. However, our
method is still significantly worse than SAC with ground-truth
representations. We hypothesize that one reason for this could be that
SCALOR right now does not properly deal with occluded objects, which
makes the environment partially observable from the point of view of the
agent. On top of this, we suspect noise in the representations,
misdetections and an imperfect matching signal to slow down training and
ultimately hurt performance. Thus, we expect that adding recurrence to
the policy or improving SCALOR itself could help close the gap to an
agent with perfect information.


Out-of-Distribution Generalization for different number of objects


FIGURE


One important advantage of structured policies is that they could
potentially still be applicable for observations that are from
different, but related distributions. Standard visual RL algorithms were
shown to be sensitive to small changes unrelated to the current
task [*REF*]. To see how our algorithm can generalize to a
changing environment, we tested our SMORL agent trained on observations
of the Rearrange environment with 2 objects on the environment with 1
object. As can be seen from Fig.,
the performance of such an agent increases during training up to a
performance comparable to a SMORL agent that was trained on the 1 object
environment.


Conclusion and Future Work


In this work, we have shown that discovering structure in the
observations of the environment with a compositional generative world
models and using it for controlling different parts of the environment
is crucial for solving tasks in compositional environments. Learning to
manipulate different parts of object-centric representations is a
powerful way to acquire useful skills such as object manipulation. Our
SMORL agent learns how to control different entities in the environment
and can then combine the learned skills to achieve more complex
compositional goals such as rearranging several objects using only the
final image of the arrangement.


Given the results presented so far, there are a number of interesting
directions to take this work. First, one can combine learned sub-tasks
with a planning algorithm to achieve a particular goal. Currently, the
agent is simply sequentially cycling through all discovered sub-tasks,
so we expect that a more complex planning algorithm as e.g. described
by  *REF* could allow solving more challenging tasks and
improve the overall performance of the policy. To this end, considering
interactions between objects in the manner of  *REF* 
or  *REF* could help to lift the assumption of
independence of sub-tasks. Second, prioritizing certain sub-tasks during
learning, similar to  *REF*, could accelerate the
training of the agent. Finally, an active training of SCALOR to combine
the object-oriented bias of SCALOR with a bias towards independently
controllable objects [*REF*] is an interesting
direction for future research.