A Machine with Short-Term, Episodic, and Semantic Memory Systems


Introduction 


The cognitive science theory suggests that humans have an explicit
memory system, which is composed of semantic and episodic memory
systems [*REF*; *REF*; *REF*].
Semantic memory deals with general world knowledge, while episodic deals
with one&apos;s personal memory. For example, when asked: &quot;Where are laptops
usually located?&quot;, you might be able to answer: &quot;On a desk&quot;, if you
have successfully encoded and stored a relevant memory. However, you are
unlikely able to recall when and where such memory (or knowledge) got to
be stored in your memory system. Nonetheless, you were able to retrieve
it. This is because this type of memory is semantic. Semantic memory
contains (general) factual knowledge without momentary or time-bound
information (i.e., where and when the event occurred). Let&apos;s consider
another question: &quot;Where is Alice&apos;s laptop?&quot; and let us assume that you
have observed where Alice&apos;s laptop was. To answer this question, one
revisits when and where this memory was encoded and stored. Retrieval of
such a memory is the reconstruction process of the event. This type of
memory is called episodic. These are more individual to you than
semantic memories, include information about personal experiences, and
often have an affective connotation.


Nowadays, machines have become reasonably good at answering factual
knowledge questions. Search engines and virtual assistants can retrieve
relevant information (often harvested from the Internet) and answer
factual and commonsense questions (i.e., those probing for semantic
knowledge). However, these systems will not remember what they did
yesterday (i.e., episodic memory). This is because they were not
designed to answer such questions.


Consider, for example, a robot deployed to a nursery home whose job is
to remind the elderly to take their pills every day. If such robot can
successfully remember when and where it has seen them taking the pills,
by observing the environment, it is a great help to them. However, it is
not feasible for an agent to encode and store every observation as an
episodic memory, as memory capacity is bounded. If the information is no
longer stored, the agent can use commonsense knowledge (semantic memory)
and say &quot;You left your pills in the cabinet&quot;.


Motivated by this type of examples, we have modeled an agent that has
both semantic and episodic memory systems. The agent interacts with the
environment and answers questions to maximize its reward. To make it
more realistic, our agent can only partially observe the environment and
hence it needs a memory system to be able to answer. Our hypothesis is
that if it has two explicit memory systems, rather than one, it can
answer the questions more successfully.


Knowledge graphs (KG) model data in the form of a graph, where entities
are linked with directed relations. This way of representing data is not
only readable / writable by both humans and machines, but also deductive
and inductive reasoning can be performed. Moreover, once we convert them
into knowledge graph embeddings (KGE), we can take advantage of neural
networks to solve various problems [*REF*].


Although KGs can be built by experts with their domain knowledge, they
are more powerful when they are open and crowdsourced, since they can
take advantage of collective human knowledge. Wikidata [*REF*] and ConceptNet [*REF*] are
some of the examples. ConceptNet tries to capture the meanings of the
words, resulting in capturing common sense knowledge (e.g.,
&apos;(laptop, AtLocation, desk)&apos;). We used part of their knowledge graph for
our experiments.


Reinforcement learning (RL) has become increasingly popular over the
past few years [*REF*]. Deep Q-learning [*REF*], an RL that uses deep
learning, has showed us that an RL agent can learn how to play Atari
games. It finds an optimal policy by finding the optimal action-value
function. RL is especially a powerful method, when it is infeasible for
us humans to label every best state-action pair that the agent can take.


The contributions of this paper are as follows.


INSPIRATION


The rest of this paper is organized as follows. In
Section [2], we propose our method, which allows an
agent to learn how to encode observations, and to store and retrieve
memories. In Section [3], we outline how we carried out our
experiments and show the results in Section [4].
In Section [5], we compare our work with other works, and show
how ours differs from theirs. Finally, in
Section [6], we conclude this paper and mention some
possible future research directions.


Methodology 


FIGURE


To perform our experiments, we have to design an agent which mimics the
memory systems. Besides, we need an environment in which the agent
operates, as well as define how the agent observes the environment.


The Room Environment 


We model our environment with a discrete-event simulation (DES). In this
simulation, there is a room with *MATH* humans, *MATH* 
different types of objects, and *MATH* possible object
locations. At each time step, every human can place an object at one of
the possible object locations, within their capacity limit. For example,
if the capacity of a desk is four, then only four objects can be placed
on the desk at once. Each change in the room state is called an event.
For example, let us assume that Alice was the only person in the room.
If at *MATH*, Alice&apos;s laptop was on the desk, but at *MATH*,
it is on her lap, then the event at *MATH* is the change of the
state.


The humans in the simulation do not move other humans&apos; objects, but only
theirs. They also have their own personal fixed routine. For example,
Alice has her laptop on the desk two days in a row, and then the next
three days she has it on her lap. She repeats this routine. The actual
routines of the humans in our experiments are longer and involve more
objects and object locations than this example.


The object locations on which each human places their object is not
uniformly random. With a given probability *MATH*, they place
their objects on a commonsense location (e.g., a bowl on the cupboard),
and with *MATH* chance, they place them on a
non-commonsense location (e.g., a bowl in the wardrobe). As
ConceptNet [*REF*] can have multiple commonsense
object locations per object, we chose commonsense location of an
object as the one with the highest weight.


We make our environment compatible with OpenAI-Gym by creating a wrapper
around the DES, where an agent only partially observes the entire state.
The agent goes around the room and observes one human at a time (e.g.,
At *MATH*, the agent observes that Alice has her laptop on her
lap). After each observation, the agent is asked a question about the
current state of the DES (e.g., Where is Alice&apos;s laptop?). If it answers
correctly, it gets a reward of +1 and if not, it gets a reward of 0.


The observations made are given as quadruples, *MATH*.
Here *MATH*, *MATH*, and *MATH* stand for head, relation, and tail, respectively.
This means that the agent observes that the entity *MATH* has the relation
*MATH* to the entity *MATH* at the given timestamp (e.g.,
&apos;(Bob’s laptop, AtLocation, desk, 42)&apos;). The question is given as a
triple *MATH*, such as &apos;(Bob’s laptop, AtLocation, ?)&apos;.


Human-Like Memory Systems 


FIGURE


We model our agent with both a short-term and a long-term memory system.
The long-term memory is split in two parts, the episodic and semantic
memory. Every observation is initially stored in the short-term memory
system. When that gets full, the agent must decide what to do with the
oldest observation in short-term memory. It can take one of the three
actions:


INPARANEUM


A diagram of this memory structure can be found in Figure [1].


There are constraints on what type of information can be stored in each
memory system; we closely follow the cognitive science theories behind
them [*REF*; *REF*; *REF*]. Both the short-term and episodic 
memory have the same quadruple format as the observation.


The facts in the semantic memory are also quadruples
*MATH*. Here, *MATH*, *MATH*, and *MATH* are the same as above,
but rather than storing the timestamp, it has a *MATH* term, which
indicates how strong this memory is. An example could be
&apos;(laptop, AtLocation, desk, 3)&apos;, where 3 stands for the number of times
that the agent has decided to store this memory in its semantic memory
system. This memory system has to do with the general world knowledge,
and thus *MATH* is not individual-specific (e.g., &apos;laptop&apos; instead of
&apos;Bob’s laptop&apos;). Figure [2] shows an example of memories that
could be stored.


Memory Storage 


ALGORITHM


All three memory systems (i.e., short-term, episodic, and semantic) are
bounded in size. Each of them is a knowledge graph where the number of
quadruples is limited. We use the cognitive science theory to manage the
episodic and semantic memory systems. That is, if the episodic memory
system is full, we &quot;forget&quot; the oldest memory stored. If the semantic
memory system is full, we &quot;forget&quot; the weakest memory.


Dealing with the short-term memory system when it is full is more
interesting and challenging. Although the cognitive science theory
suggests that the memories in this system will be either moved to the
long-term memory systems or forgotten, there is no clear explanation on
how this choice is made. Besides, this gets complicated by the
ever-changing environment the agent is in. Therefore, we decided to
learn the best behavior. We use a deep Q-learning based agent that
learns to do this by maximizing its
return [*REF*].


Memory Retrieval 


In our scenario, each time an agent is asked a question, a relevant
memory (i.e., quadruple) has to be retrieved from the knowledge graph to
answer the question. For example, when the question
&apos;(Bob’s laptop, AtLocation, ?)&apos; is asked at *MATH*, the episodic
memory &apos;(Bob’s laptop, AtLocation, desk, 42)&apos; might be relevant.
However, when the agent has a more recent episodic memory
&apos;(Bob’s laptop, AtLocation, cupboard, 43)&apos;, it is likely that this
memory provides a more accurate answer to the question. Therefore, when
there is more than one relevant memory in the episodic memory system,
we&apos;ll have the agent choose the most recent one.


The episodic memory system alone might not contain the answer to every
question. For example, if there are no relevant memories in the episodic
memory system to answer the above question, the agent can look up its
semantic memory system where it might find something like
&apos;(laptop, AtLocation, cupboard, 2)&apos;. Using this general world knowledge,
the agent might be able to answer the question. However, there might
also be another relevant memory in the system such as
&apos;(laptop, AtLocation, desk, 3)&apos;. Since this memory is stronger than the
other, it is likely that this memory provides the correct answer to the
question. These rules form an algorithm for memory retrieval with
pseudocode presented in Algorithm.


ALGORITHM


FIGURE


The Deep Q-learning Reinforcement Learning Agent 


As mentioned in Section [2.2.1], we learn the task of choosing what to
do with the oldest short-term memory with an RL approach. That is, the
memory storage policy is learned by RL, while the memory retrieval
policy is not learned but done by Algorithm. We have chosen deep Q-learning as our
RL algorithm, because of its proven track record on other problems.


We define the state *MATH* as *MATH*,
where the elements stand for the short-term, episodic, and semantic
memory systems, respectively. The size of the discrete action space
*MATH* is three: forget the oldest short-term memory in
*MATH* completely, move it to the episodic memory *MATH*, or
move it to the semantic memory *MATH*. Every time the agent takes
an action, it also answers a given question, and a corresponding reward
is given immediately. This reward is 1 if the answer is correct, 0
otherwise.


For every entity and relation in each of the memory systems (i.e.,
knowledge graphs), we maintain a learnable embedding vector in the
lookup table *MATH*. Now, in order to use a memory
system as an input for the Q-network, we need to create an embedding
(i.e., a numeric representation) for the complete memory system itself.
We do this by ordering the quadruples of the memory system by strength
for semantic memories and timestamp for short-term and episodic. The
embedding of the memory system is then the sequence of embeddings of
quadruple in the same order. To get the embedding of a quadruple, we
retrieve and concatenate the embeddings of its head, relation, and tail.
Algorithm outlines the procedure. Note that for the
short-term and episodic, we also compute embeddings for the human
actors, so that the agent can learn their individual characteristics.
Figure [4] gives an example.


There are many sophisticated neural networks. For our Q-network we use
an LSTM-based [*REF*] neural network because this has shown to
work well in many cases. The Q-network is written out in
Equation and the diagram of it is visualized in
Figure [3]. The output dimension of *MATH* is three, and each of them represents the
state-action value (Q-value) of the *MATH* function.


FIGURE


*MATH* where *MATH* denotes the vector concatenation operator.


FIGURE


Experimental Setup 


The Environment and Agent Parameters 


In our experimental environment, we set *MATH*, *MATH*, and *MATH*. We determined
these values to be sufficiently challenging. As for *MATH*,
we set it as *MATH*, to avoid bias by giving equal weights to commonsense
and non-commonsense probabilities, and thus avoiding bias.


The environment terminates after 128 steps, meaning that the agent will
make 128 observations and be asked a total of 128 questions. This makes
the minimum and maximum total episode rewards 0 and 128, respectively.


We further simplify the agent&apos;s observation, *MATH* 
because we only use &apos;AtLocation&apos; for the relation *MATH*. Since this would
be the same for all the memory systems, we set the relation embedding to 0.


The agent observes the humans in a fixed order (e.g., Alice → Bob →
Alice → Bob), this ensures that it observes all of them roughly equally
often. The questions about object locations that the agent has observed
are sampled uniformly at random. This means that an agent with infinite
and perfect memory would be able to always answer these questions
perfectly.


We train RL agents with different memory capacities. The short-term
memory capacity is fixed at 1, while we try episodic and semantic
capacity from 1, 2, 4, 8, 16, and to 32. For example, the agent with the
biggest memory capacity is *MATH*, *MATH*, and
*MATH*, where *MATH*, *MATH*, and *MATH* 
stand for the short-term, episodic, and semantic memory systems,
respectively. In the experiments, we give an equal capacity for the
episodic and semantic memory systems. We will refer to the agent&apos;s total
memory capacity as *MATH*.


We train two variants of our agents: one variant starts with an empty
semantic memory, while the other variant starts with general world
knowledge from ConceptNet in its semantic memory system. The agent with
its semantic memory system prefilled can be considered as a &quot;pretrained&quot;
agent. With these variants, we want to investigate whether agents that
already know about the world would learn differently from those that
start from scratch.


At test time, the policy of a trained agent given a state is to choose
the action that has the highest Q-value (i.e., the one the agent
considers to be most &quot;valuable&quot;). We compare our two trained agents with
the three baselines. The first one always moves the short-term memory
contents to episodic memory. The second, on the other hand, always
chooses to move to semantic memory. The third one is a random baseline
that chooses one of the three actions (i.e., including dropping the
memory) uniform at random.


The Deep Q-learning and Other Training Parameters 


Our training objective follows that of the original deep Q-learning: to
minimize the temporal difference loss (See
Equation). Some of the few differences are that we use the
Huber loss [*REF*], instead of the squared error
loss, and that we use Adam [*REF*] as an optimizer. We found
these to achieve slightly better performance.


*MATH* where *MATH*, *MATH*, *MATH*, *MATH*, *MATH*, and *MATH* denote
state, action, next state, next action, reward, and discount factor,
respectively. *MATH* is the Q-network parameters from *MATH* 
steps before.


We use PyTorch Lighting as our deep learning
framework [*REF*]. See Appendix [7] for the complete list of the
hyperparameters.


FIGURE


Results 


Deep Q-learning Performance by Agent 


We know that the agent observes 16 different kinds of objects in the
environment. Therefore, we first look at the agents with a memory
capacity of 32. Ideally, these agents will use their semantic memory
capacity of 16 to learn the general world knowledge and the other 16 to
remember individual-specific episodic memories. The episodic-only and
semantic-only baseline agents also have a memory capacity of 32, which
is either all episodic or all semantic memory. To account for the
stochasticity of the questions that the environment asks, after each
training epoch (i.e., episode) we run it on a validation environment.
After 16 epochs of training, we choose the model that has the highest
total rewards per episode on validation. This model is then run again on
a test environment to report the final numbers. We ran each experiment 5
times. Figure shows the results.


We observe that the RL agents with episodic and semantic memory systems
outperform all the three baselines. It is also interesting to see the
performance gap between the two RL agents. The one with its semantic
memory system pre-filled not only learns faster, but also converges to a
better optimum (116 vs. 109 for average total test rewards). This is
comparable to transfer learning in neural networks [*REF*], except
that the transferred knowledge in our setting is a symbolic knowledge
graph, not neural network parameters.


Next, we investigate the varying size of the memory capacity. That is,
we want to see how the agents perform if it can store less or more than
32 memory triples, while all other parameters are kept the same.
Figure shows the results. As the memory
capacity grows, the episodic-only agent can answer more questions,
resulting in higher total rewards. If it has high enough memory capacity
(e.g., 64), it can indeed answer all 128 questions. As for the agent
with only semantic memory system, it performs better than the others in
the low memory capacity settings. This suggests that it is better to
focus only on the general world knowledge rather than remembering
individuals, when the memory capacity is too limited. However, this
semantic-only agent&apos;s performance plateaus after a certain memory
capacity. For example, the total test rewards for the memory capacity of
32 and 64 are both about 92. This is because 32 general world knowledge
triples were already enough to answer the general knowledge related
questions, and having more memories has only a minimal impact.


Understanding What the Agent Learns 


FIGURE


We display the Q-values during the test time in
Figure [5]. We can see that the semantic-scratch
agent starts both of its episodic and semantic memory systems from
scratch, while the semantic memory system of the semantic-pretrained
agent is already prefilled with the general world knowledge from the
beginning. There is one same strategy between the two agents. Both of
them think it is most &quot;valuable&quot; to store a not-so-common short-term
memory (e.g., &apos;(Frank’s train, AtLocation, circus, 86)&apos;) in the episodic
memory system. This is understandable, since such an observation is more
individual-specific than generalizable.


But we observe one fundamental difference between them. Whenever the
semantic-scratch agent observes something that is generalizable (e.g.,
&apos;(Ann’s bowl, AtLocation, cupboard, 2)&apos;), it stores it in the semantic
memory system. However, the semantic-prefilled agent never decides to
store any short-term memory in the semantic memory system. It always
forgets generalizable short-term memories completely. This makes its
semantic memory system untouched. As it is filled in the beginning, it
stays the same until the end. What we observe is that it retains the
general world knowledge it was given. Then, it either stores short-term
memory in the episodic one or forgets it completely. In our experiments,
this strategy indeed results in receiving higher rewards, since we
prefilled its semantic memory system with the perfect world knowledge.
As for the semantic-scratch agent, although it manages to learn most of
the general world knowledge, it also makes mistakes. For example, it
thinks that &apos;(train, AtLocation, kitchen)&apos; is generalizable, while
actually it should have been &apos;(train, AtLocation, zoo)&apos;. This kind of
mistakes results in answering some questions incorrectly, leading to
lower total rewards per episode.


Related Work 


Our work stands somewhere between cognitive architecture and machine
learning with memory systems. Cognitive architectures focus on the
theory of the human mind, while machine learning with memory systems try
to take advantage of memory systems to solve problems such as partial
observability and catastrophic forgetting. Here we list the works that
are the most relevant to us.


ACT-R [*REF*] and Soar [*REF*] study the human memory systems comprehensively
and provide opportunities for AI agents. However, they lack in
computational methods, such as training an RL agent with memory systems.


Gorski and Laird [*REF*] have applied RL within the Soar
cognitive architecture. While our work focuses on learning a memory
storage policy, they have focused on learning episodic memory retrieval.
Li [*REF*] has extended their work by applying it to bigger
data.


The Tensor Brain [*REF*] focuses on the
computational theory of an agent&apos;s perception, episodic memory, semantic
memory. They introduced the model named bilayer tensor network (BTN)
where perception, episodic memory, and semantic memory are realized.
They used the VRD dataset [*REF*] to carry out their
experiments. Although this dataset includes image modality, it is
similar to ours in that it is a form of triples that can form a
knowledge graph. In their experiments, they used supervised learning to
predict the relationships between the entities.


Hemmer and Steyvers [*REF*] have studied how episodic
and semantic memory systems play a role in recall of objects, but their
experiments were human-based empirical results, which does not scale as
well as our computational method.


Memory-based RL [*REF*; *REF*] adds
memory components to their classical RL problems to address the partial
observability problem and to achieve sample efficiency, respectively.
Episodic Memory Deep Q-Networks [*REF*] uses the term
&quot;episodic memory&quot; that is used to regularize their deep Q-network. They
use the term episodic memory to refer to the state-action pairs that the
agent has taken along the training episode.


Episodic Memory Reader [*REF*] trains an RL agent that
learns what to remember in their memory system, and they also use
question-answering to evaluate their method. Their work differs from
ours in that they have only one memory system and that it is not made of
a knowledge graph of triples, but rather numeric embeddings, which are
harder to interpret.


When a deep neural network trained to solve task A is retrained to solve
task B, it often forgets what it has learned previously, which is called
catastrophic forgetting. Continual learning (also known as lifelong
learning) is concerned with training a neural network on a sequence of
tasks without forgetting them. Continual learning with an episodic
memory system [*REF*; *REF*] addresses this issue by storing some previous task examples in their
episodic memory system. Adjusting the neural network weights for the
current task also accounts for the previous task examples, to avoid
forgetting them.


Similar to our work, CLS-ER [*REF*] also takes advantage of
the dual memory system (i.e., episodic and semantic) for their continual
learning problem. Their episodic memory system stores data samples,
while the semantic stores previous model weights.


Conclusions 


Current machine learning agents with memory systems cannot answer all
types of questions. They are especially weak at answering personal
questions. To tackle this problem, we investigated an agent with
human-like memory systems, inspired by the cognitive science theory. Our
agent has short-term, episodic, and semantic memory systems, each of
which is modeled with a knowledge graph.


To experiment with this agent, we have designed and released our own
environment, &quot;the Room&quot;, compatible with OpenAI Gym. We successfully
trained an agent that has interpretable memory systems, with a deep
Q-learning based reinforcement learning algorithm. This experiment
confirms that a machine with human-like memory systems outperforms the
ones without this structure, when both general world knowledge and
individual-specific questions are asked.


In the future, we would like to make our environment not only more
complex (e.g., include more relations in its triples), but also
multimodal (e.g., images). The questions given by the environment can
extend to natural language with ambiguity, not just triples. We would
also like to add other kinds of human-like memory systems to our agent
(e.g., working memory, implicit memory, etc.). It would also be
interesting to include real humans in the loop, so that the agent can
ask them a question when it is uncertain of the actions that it can
take.


Hyperparameters 


Table lists all the hyperparameters used.
Since every episode terminates after 128 steps (questions) and we are
training for 16 epochs, the total number of training steps is
128 *MATH*. One epoch corresponds to one
episode (i.e., 128 steps), where every training step includes computing
one temporal difference loss from one batch of 1024 data samples. And
then Adam [*REF*] makes one weight update.


We set epoch length, replay size, and warm start size as
1024 *MATH*. This corresponds to the total number
of data samples that the optimizer sees in one epoch. The epsilon value
of DQN decays linearly. It starts with 1.0 and ends with 0. The time
step when it reaches 0 is 128 *MATH*, which
corresponds to the last (16th) epoch. We tried different values for
gamma (discount factor), and we found 0.65 to be the best. *MATH* 
is synced with *MATH* every 10 steps.


As for the Q-network, the embedding dimension was set to 32, and the
LSTM hidden size dimension was set to 64. The input feature size of the
linear layers was also set to 64, so that last hidden state of the LSTM
can be directly fed into them. ReLU [*REF*] was used for
the non-linearity between the linear layers in the MLP. The number of
LSTM layers used is 2.


Since our neural network is relatively small (i.e., the number of
parameters is 265,000), we just used one CPU (i.e., Intel® Core™
i7-10875H) with 64 GB of RAM, instead of using GPUs or ASICs. It took
about 3 hours to train one semantic-scratch agent with the memory
capacity of 32.


For every validation and test, we ran the model 10 times to get the
average of total rewards per episode. We did this because the
environment is stochastic, and we wanted to get the best estimate. This
means that the final reported total rewards of the test environment are
the average of 10 *MATH* numbers.


Total Test Rewards for All Capacities 


Table shows the average total test rewards and their standard 
deviations per episode for all capacities.