RL-GPT: Integrating Reinforcement Learning and Code-as-Policy


Introduction 


Building agents to master tasks in open-world environments has been a long-standing goal in AI research [*REF*; *REF*; *REF*]. The emergence of Large Language Models (LLMs) has revitalized this pursuit, leveraging their expansive world knowledge and adept compositional reasoning capabilities [*REF*; *REF*; *REF*].
LLMs agents showcase proficiency in utilizing computer tools [*REF*; *REF*], navigating search engines [*REF*; *REF*], and even operating systems or applications [*REF*; *REF*]. However, their performance remains constrained in open-world embodied environments [*REF*; *REF*]. Despite possessing &quot;world knowledge&quot; akin to a human professor, LLMs fall short when pitted against a child in a video game. The inherent limitation lies in LLMs&apos; adeptness at absorbing information but their inability to practice skills within an environment. Proficiency in activities such as playing a video game demands extensive practice, a facet not easily addressed by in-context learning, which exhibits a relatively low upper bound [*REF*; *REF*; *REF*]. Consequently, existing LLMs necessitate human intervention to define low-level skills or tools [*REF*; *REF*].


FIGURE


Reinforcement Learning (RL), proven as an effective method for learning from interaction, holds promise in facilitating LLMs to &quot;practise&quot;. One line of works grounds LLMs for open-world control through RL fine-tuning [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Nevertheless, this approach necessitates a substantial volume of domain-specific data, expert demonstrations, and access to LLMs&apos; parameters, rendering it slow and resource-intensive in most scenarios.
Given the modest learning efficiency, the majority of methods continue to operate within the realm of &quot;word games&quot; such as tone adjustment rather than tackling intricate embodied tasks.


Addressing this challenge, we propose to integrate LLMs and RL in a novel approach: Empower LLMs agents to use an RL training pipeline as a tool. We introduce RL-GPT, a framework designed to enhance LLMs with trainable modules for learning interaction tasks within an environment.
As shown in Fig. , RL-GPT comprises an agent pipeline featuring multiple LLMs, wherein the neural network is conceptualized as a tool for training the RL pipeline. Illustrated in Fig. [1], unlike conventional approaches where LLMs agents and RL optimize coded actions and networks separately, RL-GPT unifies this optimization process. The line chart in Fig. [1] illustrates that RL-GPT outperforms alternative approaches on the &quot;harvest a log&quot; task in MineDojo [*REF*].


We further point out that the pivotal issue in using RL is to decide: Which actions should be learned with RL? To tackle this, RL-GPT is meticulously designed to assign different actions to RL and Code-as-policy, respectively. Our agent pipeline entails two fundamental steps. Firstly, LLMs should determine &quot;which actions&quot; to code, involving task decomposition into distinct sub-actions and deciding which actions can be effectively coded. Actions falling outside this realm will be learned through RL. Secondly, LLMs are tasked with writing accurate codes for the &quot;coded actions&quot; and test them in the environment.


We employ a two-level hierarchical framework to realize the two steps, as depicted in Fig. . Allocating these steps to two independent agents proves highly effective, as it narrows down the scope of each LLM&apos;s task. Coded actions with explicit starting conditions are executed sequentially, while other coded actions are integrated into the RL action space. This strategic insertion into the action space empowers LLMs to make pivotal decisions during the learning process. Illustrated in Fig. [2], this integration enhances the efficiency of learning tasks, exemplified by our ability to more effectively learn how to break a tree.


For intricate tasks such as the ObtainDiamond task in the Minecraft game, devising a strategy with a single neural network proves challenging due to limited computing resources. In response, we incorporate a task planner to facilitate task decomposition. Our RL-GPT framework demonstrates remarkable efficiency in tackling complex embodied tasks. Specifically, within the MineDojo environment, it attains state-of-the-art performance on the majority of selected tasks and adeptly obtains diamonds within a single day, utilizing only an RTX3090 GPU.


Our contributions are summarized as follows:


- Introduction of an LLMs agent utilizing an RL training pipeline as a tool.
- Development of a two-level hierarchical framework capable of determining which actions in a task should be learned with RL.
- Pioneering work as the first to incorporate high-level GPT-coded actions into the RL action space, enhancing the sample efficiency for RL.


FIGURE


Related Works 


Agents in Minecraft


Minecraft, a widely popular open-world sandbox game, stands as a formidable benchmark for constructing efficient and generalized agents.
Previous endeavors resort to hierarchical reinforcement learning, often relying on human demonstrations to facilitate the training of low-level policies [*REF*; *REF*]. Efforts such as MineAgent [*REF*], Steve-1 [*REF*], and VPT [*REF*] leverage large-scale pre-training via YouTube videos to enhance policy training efficiency. However, MineAgent and Steve-1 are limited to completing only a few short-term tasks, and others [*REF*; *REF*] still require a substantial number of steps for finetuning on long-horizon tasks.
DreamerV3 [*REF*] utilizes a world model to expedite exploration but still demands a substantial number of interactions to acquire diamonds. These existing approaches either necessitate extensive expert datasets for training or exhibit low sample efficiency when addressing long-horizon tasks in the Minecraft environment.


An alternative research direction employs Large Language Models (LLMs) for task decomposition and high-level planning to address intricate challenges. Certain works [*REF*] leverage few-shot prompting with Codex [*REF*] to generate executable policies.
DEPS [*REF*] and GITM [*REF*] investigate the use of LLMs as high-level planners in the Minecraft context. Some works [*REF*; *REF*; *REF*] further explore LLMs for high-level planning, code generation, lifelong exploration, and creative tasks. Other studies [*REF*; *REF*] delve into grounding smaller language models for control with domain-specific finetuning.
Nevertheless, these methods often rely on manually designed controllers or code interfaces, sidestepping the challenge of learning low-level policies.


Plan4MC [*REF*] integrates LLM-based planning and RL-based policy learning but requires defining and pre-training all the policies with manually specified environments. Our RL-GPT extends LLMs&apos; ability in low-level control by equipping it with RL, achieving automatic and efficient task learning in Minecraft.


LLMs Agents


Several works leverage LLMs to generate subgoals for robot planning [*REF*; *REF*]. Works like Inner Monologue [*REF*] incorporate environmental feedback into robot planning with LLMs. Code-as-Policies [*REF*] and ProgPrompt [*REF*] directly utilize LLMs to formulate executable robot policies. VIMA [*REF*] and PaLM-E [*REF*] involve fine-tuning pre-trained LLMs to support multimodal prompts. Besides, Chameleon [*REF*] effectively executes sub-task decomposition and generates sequential programs.
ReAct [*REF*] utilizes chain-of-thought prompting to generate task-specific actions. AutoGPT [*REF*] automates NLP tasks by integrating reasoning and acting loops. DERA [*REF*] introduces dialogues between GPT-4 [*REF*] agents. Generative Agents [*REF*] simulate human behaviors by storing experiences as memories.


Compared with existing works, RL-GPT equips the LLM agent with RL, extending its capability in intricate low-level control in open-world tasks.


Integrating LLMs and RL


Since LLMs and RL possess complementary abilities in providing prior knowledge and exploring unknown information, it is promising to integrate them for efficient task learning.


Most work studies improve RL with the domain knowledge in LLMs.
SayCan [*REF*] and Plan4MC [*REF*] decompose and plan subtasks with LLMs, thereby RL can learn easier subtasks to solve the whole task. Recent works [*REF*; *REF*; *REF*; *REF*] studies generating reward functions with LLMs to improve the sample efficiency for RL. Another line of research [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*] finetunes LLMs with RL to acquire the lacked ability of LLMs in low-level control. However, these approaches usually require a lot of samples and can harm the LLMs&apos; abilities in other tasks. Our study is the first to overcome the inabilities of LLMs in low-level control by equipping them with RL as a tool. The acquired knowledge is stored in context, thereby continually improving the LLMs skills and maintaining its capability.


FIGURE


Methods 


RL-GPT incorporates three distinct components, each contributing to its innovative design: (1) a slow agent tasked with decomposing a given task into several sub-actions and determining which actions can be directly coded, (2) a fast agent responsible for writing code and instantiating RL configuration, and (3) an iteration mechanism that facilitates an iterative process refining both the slow agent and the fast agent. This iterative process enhances the overall efficacy of the RL-GPT across successive iterations. For complex long-horizon tasks requiring multiple neural networks, we employ a GPT-4 as a planner to initially decompose the task.


As discussed in concurrent works [*REF*; *REF*], segregating high-level planning and low-level actions into distinct agents has proven to be beneficial. The dual-agent system effectively narrows down the specific task of each agent, enabling optimization for specific targets. Moreover, *REF* highlighted the Degeneration-of-Thought (DoT) problem, where an LLM becomes overly confident in its responses and lacks the ability for self-correction through self-reflection. Empirical evidence indicates that agents with different roles and perspectives can foster divergent thinking, mitigating the DoT problem. External feedback from other agents guides the LLM, making it less susceptible to DoT and promoting accurate reasoning.


RL Interface


As previously mentioned, we view the RL training pipeline as a tool accessible to LLMs agents, akin to other tools with callable interfaces.
Summarizing the interfaces of an RL training pipeline, we identify the following components: 1) Learning task; 2) Environment reset; 3) Observation space; 4) Action space; 5) Reward function. Specifically, our focus lies on studying interfaces 1) and 4) to demonstrate the potential for integrating RL and Code-as-policy.


In the case of the action space interface, we enable LLMs to design high-level actions and integrate them into the action space. A dedicated token is allocated for this purpose, allowing the neural network to learn when to utilize this action based on observations.


Slow Agent: Action Planning


Assume the task *MATH* needs to be learned by a single network within constrained computing resources. We employ a GPT-4 [*REF*] as a slow agent *MATH*. *MATH* is tasked with decomposing *MATH* into sub-actions *MATH*, where *MATH*, determining if each *MATH* in *MATH* can be directly addressed through code implementation. This approach optimally allocates computational resources to address more challenging sub-tasks using RL. Importantly, *MATH* is not required to perform any low-level coding tasks; it solely provides high-level textual instructions regarding sub-actions *MATH*. These instructions are then transmitted to the fast agent *MATH* for further processing. The iterative process of the slow agent involves systematically probing the limits of coding capabilities.


For instance, in Fig. , consider the specific action of crafting a wooden pickaxe. Although *MATH* is aware that players need to harvest a log, writing code for this task with a high success rate can be challenging. The limitation arises from the insufficient information available through APIs for *MATH* to accurately locate and navigate to a tree. To overcome this hurdle, an RL implementation becomes necessary.
RL aids *MATH* in completing tasks by processing complex visual information and interacting with the environment through trial and error. In contrast, some simple, straightforward actions like crafting something with a crafting table can be directly coded and executed.


It is crucial to instruct *MATH* to identify sub-actions that are too challenging for rule-based code implementation. As shown in Table [1], the prompt for *MATH* incorporates role description, the given task *MATH*, reference documents, environment knowledge , planning heuristics , and programming examples .
To align *MATH* with our goals, we include the heuristic in the planning tips. This heuristic encourages *MATH* to further break down an action when coding proves challenging. This incremental segmentation aids *MATH* in discerning what aspects can be coded. Further details are available in Appendix.


Fast Agent: Code-as-Policy and RL


The fast agent *MATH* is also implemented using GPT-4. The primary task is to translate the instructions from the slow agent *MATH* into Python codes for the sub-actions *MATH*. *MATH* undergoes a debug iteration where it runs the generated sub-action code and endeavors to self-correct through feedback from the environment. Sub-actions that can be addressed completely with code implementation are directly executed, as depicted in the. For challenging sub-actions lacking clear starting conditions, the code is integrated into the RL implementation using the temporal abstraction technique [*REF*; *REF*], as illustrated in Fig. [2]. This involves inserting the high-level action into the RL action space, akin to the [orange] segments in Fig. *MATH* iteratively corrects itself based on the feedback received from the environment.


TABLE


TABLE


Two-loop Iteration


In Fig. [3], we devise the two-loop iteration mechanism to optimize the proposed two agents, namely the fast agent *MATH* and the slow agent *MATH*. To facilitate it, a critic agent *MATH* is introduced, which could be implemented using GPT-3.5 or GPT-4.


The optimization for the fast agent, as shown in Fig. [3], aligns with established methods for code-as-policy agents. Here, the fast agent receives a sub-action, environment documents *MATH* (observation and action space), and examples *MATH* as input, generating Python code. It then iteratively refines the code based on environmental feedback. The objective is to produce error-free Python-coded sub-actions that align with the targets set by the slow agent. Feedback, which includes execution errors and critiques from *MATH*, plays a crucial role in this process. *MATH* evaluates the coded action&apos;s success by considering observations before and after the action&apos;s execution, offering insights for improvement.


Within Fig. [3], the iteration of the slow agent *MATH* encompasses the aforementioned fast agent *MATH* iteration as a step. In each step of *MATH*, *MATH* must complete an iteration loop. Given a task *MATH*, *MATH*, and *MATH*, *MATH* decomposes *MATH* into sub-actions *MATH* and refines itself based on *MATH* &apos;s outputs. Specifically, it receives a sequence of outputs *MATH* from *MATH* about each *MATH* to assess the effectiveness of action planning. If certain actions cannot be coded by the fast agent, the slow agent adjusts the action planning accordingly.


Task Planner


Our primary pipeline is tailored for tasks that can be learned using a neural network within limited computational resources. However, for intricate tasks such as ObtainDiamond, where it is more effective to train multiple neural networks like DEPS [*REF*] and Plan4MC [*REF*], we introduce a task planner reminiscent of DEPS, implemented using GPT-4. This task planner iteratively reasons what needs to be learned and organizes sub-tasks for our RL-GPT to accomplish.


TABLE


Experiments 


Environment


MineDojo


MineDojo [*REF*] stands out as a pioneering framework developed within the renowned Minecraft game, tailored specifically for research involving embodied agents. This innovative framework comprises a simulation suite featuring thousands of tasks, blending both open-ended challenges and those prompted by language. To validate the effectiveness of our approach, we selected certain long-horizon tasks from MineDojo, mirroring the strategy employed in Plan4MC [*REF*]. These tasks include harvesting and crafting activities. For instance, Crafting one wooden pickaxe requires the agent to harvest a log, craft planks, craft sticks, craft tables, and craft the pickaxe with the table. Similarly, tasks like milking a cow involve the construction of a bucket, approaching the cow, and using the bucket to obtain milk.


ObtainDiamond Challenge


It represents a classic challenge for RL agents. The task of obtaining a diamond demands the agent to complete the comprehensive process of harvesting a diamond from the beginning. This constitutes a long-horizon task, involving actions such as harvesting logs, harvesting stones, crafting items, digging to find iron, smelting iron, locating a diamond, and so on.


Implementation Details


LLM Prompt


We choose GPT-4 as our LLMs API. For the slow agents and fast agents, we design special templates, responding formats, and examples. We design some special prompts such as &quot;assume you are an experienced RL researcher that is designing the RL training job for Minecraft&quot;. Details can be found in the Appendix . In addition, we encourage the slow agent to explore more strategies because the RL task requires more exploring. We encourage the slow agent to further decompose the action into sub-actions which may be easier to code.


PPO Details


Similar to MineAgent [*REF*], we employ Proximal Policy Optimization (PPO) [*REF*] as the RL baseline. This approach alternates between sampling data through interactions with the environment and optimizing a \&quot;surrogate\&quot; objective function using stochastic gradient ascent. PPO is constrained to a limited set of skills. When applying PPO with sparse rewards, specific tasks such as &quot;milk a cow\&quot; and &quot;shear a sheep\&quot; present challenges due to the small size of the target object relative to the scene, and the low probability of random encounters. To address this, we introduce basic dense rewards to enhance learning efficacy in these tasks. It includes the CLIP Reward, which encourages the agent to exhibit behaviors that align with the prompt [*REF*]. Additionally, we incorporate a Distance Reward that provides dense reward signals to reach the target items [*REF*]. Further details can be found in the appendix.


Main Results


FIGURE


MineDojo Benchmark


Table  presents a comparative analysis between our RL-GPT and several baselines on selected MineDojo tasks. Notably, RL-GPT achieves the highest success rate among all baselines. All baselines underwent training with 10 million samples, and the checkpoint with the highest success rate was chosen for testing.


MineAgent, as proposed in [*REF*], combines PPO with Clip Reward. However, naive PPO encounters difficulties in learning long-horizon tasks, such as crafting a bucket and obtaining milk from a cow, resulting in an almost 0% success rate for MineAgent across all tasks. Another baseline, MineAgent with autocraft, as suggested in Plan4MC [*REF*], incorporates crafting actions manually coded by humans. This alternative baseline achieves a 46% success rate on the milking task, demonstrating the importance of code-as-policy. Our approach demonstrates superiority in coding actions beyond crafting, enabling us to achieve higher overall performance compared to baselines that focus primarily on crafting actions.


Plan4MC [*REF*] breaks down the problem into two essential components: acquiring fundamental skills and planning based on these skills. While some skills are acquired through Reinforcement Learning (RL), Plan4MC outperforms MineAgent due to its reliance on an oracle task decomposition from the GPT planner. However, it cannot modify the action space of an RL training pipeline or flexibly decompose sub-actions. It is restricted to only three types of human-designed coded actions. Consequently, our method holds a distinct advantage in this context.


In tasks involving IMAGE and IMAGE, the agent is tasked with crafting a stick from scratch, necessitating the harvesting of a log. Our RL-GPT adeptly codes three actions for this: 1) Navigate to find a tree; 2) Attack 20 times; 3) Craft items. Notably, Action 2) can be seamlessly inserted into the action space. In contrast, Plan4MC is limited to coding craft actions only. This key distinction contributes to our method achieving higher scores in these tasks.


To arrive at the optimal code planning solution, RL-GPT undergoes a minimum of three iterations. As illustrated in Fig. , in the initial iteration, RL-GPT attempts to code every action involved in harvesting a log, yielding a 0% success rate. After the first iteration, it decides to code navigation, aiming at the tree, and attacking 20 times. However, aiming at the tree proves too challenging for LLMs. As mentioned before, the agent will be instructed to further decompose the actions and give up difficult actions. By the third iteration, the agent correctly converges to the optimal solution -- coding navigation and attacking, while leaving the rest to RL, resulting in higher performance.


In tasks involving crafting a wooden pickaxe IMAGE and crafting a bed IMAGE, in addition to the previously mentioned actions, the agent needs to utilize the crafting table. While Plan4MC must learn this process, our method can directly code actions to place the crafting table on the ground, use it, and recycle it. Code-as-policy contributes to our method achieving a higher success rate in these tasks.


In tasks involving crafting a furnace IMAGE and a stone pickaxe IMAGE, in addition to the previously mentioned actions, the agent is further required to harvest stones. Plan4MC needs to learn an RL network to acquire the skill of attacking stones. RL-GPT proposes two potential solutions for coding additional actions. First, it can code to continuously attack a stone and insert this action into the action space. Second, since LLMs understand that stones are underground, the agent might choose to dig deep for several levels to obtain stones instead of navigating on the ground to find stones.


In tasks involving crafting a milk bucket IMAGE and crafting wool IMAGE, the primary challenge lies in crafting a bucket or shears. Since both RL-GPT and Plan4MC can code actions to craft without a crafting table, their performance is similar and comparable.


In tasks involving obtaining beef IMAGE and IMAGE, the only actions that can be further coded are navigating to find the target. Given that both RL-GPT and Plan4MC can code actions to navigate, their performance in these tasks is similar.


TABLE 


TABLE


ObtainDiamond Challenge


As shown in Tab. [3], we compare our method with existing competitive methods on the challenging ObtainDiamond task.


DreamerV3 [*REF*] leverages a world model to accelerate exploration but still requires a significant number of interactions.
Despite the considerable expense of over 100 million samples for learning, it only achieves a 2% success rate on the Diamond task from scratch.


VPT [*REF*] employs large-scale pre-training using YouTube videos to improve policy training efficiency. This strong baseline is trained on 80 GPUs for 6 days, achieving a 20% success rate in obtaining a diamond and a 2.5% success rate in crafting a diamond pickaxe.


DEPS [*REF*] suggests generating training data using a combination of GPT and human handcrafted code for planning and imitation learning. It attains a 0.6% success rate on this task. Moreover, an oracle version, which directly executes human-written codes, achieves a 60% success rate.


Plan4MC [*REF*] primarily focuses on crafting the stone pickaxe. Even with the inclusion of all human-designed actions from DEPS, it requires more than 7 million samples for training.


Our RL-GPT attains an over 8% success rate in the ObtainDiamond challenge by generating Python code and training a PPO RL neural network. Despite requiring some human-written code examples, our approach uses considerably fewer than DEPS. The final coded actions involve navigating on the ground, crafting items, digging to a specific level, and exploring the underground horizontally.


Ablation Study


We present ablation studies on our core designs in Tab. [4], Tab. [5], and Tab. [6], covering the framework structure, two-loop iteration, and RL interface.


Framework Structure


In Tab. [4], we analyze the impact of the framework structure in RL-GPT, specifically examining different task assignments for various agents. Assigning all tasks to a single agent results in confusion due to the multitude of requirements, leading to a mere 0.34% success rate in crafting a table. Additionally, comparing the 3rd and 4th rows emphasizes the crucial role of a critic agent in our pipeline.
Properly assigning tasks to the fast, slow, and critic agents can improve the performance to 0.65%. The slow agent faces difficulty in independently judging the suitability of actions based solely on environmental feedback and observation. Incorporating a critic agent facilitates more informed decision-making, especially when dealing with complex, context-dependent information.


Two-loop Iteration


In Tab. [5], we ablate the importance of our two-loop iteration. Our iteration is to balance RL and code-as-policy to explore the bound of GPT&apos;s coding ability. We can see that pure RL and pure code-as-policy only achieve a low success rate on these chosen tasks.
Our method can improve the results although there is no iteration (zero-shot). In these three iterations, it shows that the successful rate increases. It proves that the two-loop iteration is a reasonable optimization choice. Qualitative results can be found in Fig. . Besides, we also compare the results with and without special prompts (SP) to encourage the LLMs to further decompose actions when facing coding difficulty. It shows that suitable prompts are also essential for optimization.


RL Interface


Recent works [*REF*; *REF*] explore the use of LLMs for RL reward design, presenting an alternative approach to combining RL and code-as-policy. With slight modifications, our fast agent can also generate code to design the reward function. However, as previously analyzed, reconstructing the action space proves more efficient than designing the reward function, assuming LLMs understand the necessary actions. Tab. [6] compares our method with the reward design approach, revealing that our method achieves a higher average success rate and lower dead loop ratio on our selected MineDojo tasks.


Conclusion


In conclusion, we propose RL-GPT, a novel approach that integrates Large Language Models (LLMs) and Reinforcement Learning (RL) to empower LLMs agents on challenging tasks within complex, embodied environments. Our two-level hierarchical framework divides the task into high-level coding and low-level RL-based actions, leveraging the strengths of both approaches. RL-GPT exhibits superior efficiency compared to traditional RL methods and existing GPT agents, achieving remarkable performance in challenging Minecraft tasks.