Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards


Introduction


Deep reinforcement learning (DRL) algorithms with parameterized policy
and value function have achieved remarkable success in various complex
domains [*REF*; *REF*; *REF*].
However, tasks that require reasoning over long horizons with sparse
rewards remain exceedingly challenging for the parametric approaches. In
these tasks, a positive reward could only be received after a long
sequence of appropriate actions. The gradient-based updates of
parameters are incremental and slow and have a global impact on all
parameters, which may cause catastrophic forgetting and performance
degradation. Many parametric approaches rely on recent samples and do
not explore the state space systematically. They might forget the
positive-reward trajectories unless the good trajectories are frequently
collected.


FIGURE


Recently, non-parametric memory from past experiences is employed in DRL
algorithms to improve policy learning and sample efficiency. Prioritized
experience replay [*REF*] proposes to learn from past
experiences by prioritizing them based on temporal-difference error.
Episodic reinforcement learning [*REF*; *REF*; *REF*], self-imitation
learning [*REF*; *REF*], and memory-augmented
policy optimization [*REF*] build a memory to store past good
experience and thus can rapidly latch onto past successful policies when
encountering with states similar to past experiences. However, the
exploitation of good experiences within limited directions might hurt
performance in some cases. For example on Montezuma&apos;s Revenge
(Fig.), if the agent exploits the past good
trajectories around the yellow path, it would receive the small positive
rewards quickly but it loses the chance to achieve a higher score in the
long term. Therefore, in order to find the optimal path (red), it is
better to consider past experiences in diverse directions, instead of
focusing only on the good trajectories which lead to myopic behaviors.
Inspired by recent work on memory-augmented generative models [*REF*; *REF*], we note that
generating a new sequence by editing prototypes in external memory is
easier than generating one from scratch. In an RL setting, we aim to
generate new trajectories visiting novel states by editing or augmenting
the trajectories stored in the memory from past experiences. We propose
a novel trajectory-conditioned policy where a full sequence of states is
given as the condition. Then a sequence-to-sequence model with an
attention mechanism learns to &apos;translate&apos; the demonstration trajectory
to a sequence of actions and generate a new trajectory in the
environment with stochasticity. The single policy could take diverse
trajectories as the condition, imitate the demonstrations to reach
diverse regions in the state space, and allow for flexibility in the
action choices to discover novel states.


Our main contributions are summarized as follows. (1) We propose a novel
architecture for a trajectory-conditioned policy that can flexibly
imitate diverse demonstration trajectories. (2) We show the importance
of exploiting diverse past experiences in the memory to indirectly drive
exploration, by comparing with existing approaches on various
sparse-reward RL tasks with stochasticity in the environments. (3) We
achieve a performance superior to the state-of-the-art under 5 billion
number of frames, on hard-exploration Atari games of Montezuma&apos;s Revenge
and Pitfall, without using expert demonstrations or resetting to
arbitrary states. We also demonstrate the effectiveness of our method on
other benchmarks.


Method


Background and Notation for DTSIL


In the standard RL setting, at each time step *MATH*, an agent observes a
state *MATH*, selects an action *MATH*, and receives a
reward *MATH* when transitioning to a next state
*MATH*, where *MATH* and *MATH* is a set
of states and actions respectively. The goal is to find a policy
*MATH* parameterized by *MATH* that maximizes the expected return
*MATH*, where *MATH* is a discount factor. In our work, instead of
directly maximizing expected return, we propose a novel way to find best
demonstrations *MATH* with (near-)optimal return and train the policy
*MATH* to imitate any trajectory *MATH* in the
buffer, including *MATH*. We assume a state *MATH* includes
the observation *MATH* (e.g., raw pixel image) and a high-level
abstract state embedding *MATH* (e.g., the agent&apos;s location in the
abstract space). The embedding *MATH* may be available as a part of
*MATH* (e.g., the physical features in the robotics domain) or may be
learnable from *MATH* (e.g., [*REF*; *REF*] could localize the agent
in Atari games, as discussed in Sec. [5]). A trajectory-conditioned policy
*MATH* (which can be viewed as a goal-conditioned policy and denoted as *MATH*) takes a
sequence of state embeddings *MATH* as input for a
demonstration, where *MATH* is the length of the trajectory *MATH*. A
sequence of the agent&apos;s past state embeddings *MATH* is provided to determine which
part of the demonstration has been followed by the agent. Together with
the current observation *MATH*, it helps to determine the correct action
*MATH* to imitate the demonstration. Our goal is to find a set of
optimal state embedding sequence(s) *MATH* and the policy
*MATH* to maximize the return: *MATH*.
We approximately solve this joint optimization problem via the
sampling-based search for *MATH* over the space of *MATH* realizable by
the (trajectory-conditioned) policy *MATH* and gradient-based
local search for *MATH*. For robustness, we may want to find
multiple trajectories with high returns and a trajectory-conditioned
policy executing them. We name our method as Diverse
Trajectory-conditioned Self-Imitation Learning (DTSIL).


Overview of DTSIL


FIGURE


Organizing Trajectory Buffer


As shown in Fig. (a), we maintain a trajectory buffer *MATH* of diverse past trajectories.
*MATH* is the best trajectory ending with a state with embedding
*MATH*. *MATH* is the number of times the cluster represented by
the embedding *MATH* has been visited during training. To maintain a
compact buffer, similar state embeddings within the tolerance threshold
*MATH* are clustered together, and an existing entry is replaced if an
improved trajectory *MATH* ending with a near-identical state is
found. In the buffer, we keep a single representative state embedding
for each cluster. If a state embedding *MATH* observed in the
current episode is close to a representative state embedding
*MATH*, we increase visitation count *MATH* of the
*MATH* -th cluster. If the sub-trajectory *MATH* of the
current episode up to step *MATH* is better than *MATH*,
*MATH* is replaced by *MATH*. Pseudocode for
organizing clusters is in the appendix.


Sampling States for Exploitation or Exploration


In RL algorithms, the agent needs to exploit what it already knows to
maximize reward and explore new behaviors to find a potentially better
policy. For exploitation, we aim at reaching the states with the highest
total rewards, which probably means a good behavior of receiving high
total rewards. For exploration, we would like to look around the rarely
visited states, which helps discover novel states with higher total
rewards. With probability *MATH*, in exploitation mode, we sample the
states in the buffer with the highest cumulative rewards. With
probability *MATH*, in exploration mode, we sample each state *MATH* 
with the probability proportional to *MATH*, as
inspired by count-based exploration [*REF*; *REF*] and rank-based
prioritization [*REF*; *REF*]. To balance
between exploration and exploitation, we decrease the hyper-parameter
*MATH* of taking the exploration mode. The pseudo-code algorithm of
sampling the states is in the appendix.


Imitating Trajectory to State of Interest


In stochastic environments, in order to reach diverse states *MATH* 
we sampled, the agent would need to learn a goal-conditioned policy
[*REF*; *REF*; *REF*; *REF*]. But it is difficult to learn 
the goal-conditioned policy only with the
final goal state because the goal state might be far from the agent&apos;s
initial states and the agent might have few experiences reaching it.
Therefore, we provide the agent with the full trajectory leading to
the goal state. So the agent benefits from richer intermediate
information and denser rewards. We call this trajectory-conditioned
policy *MATH* where *MATH*, and introduce how to train the policy in detail in
Sec. [2.3].


Updating Buffer with New Trajectory


With trajectory-conditioned policy, the agent takes actions to imitate
the sampled demonstration trajectory. As shown in
Fig. (c), because there could be stochasticity in the
environment and our method does not require the agent to exactly follow
the demonstration step by step, the agent&apos;s new trajectory could be
different from the demonstration and thus visit novel states. In a new
trajectory *MATH* *MATH*, if *MATH* is nearly identical to a state
embedding *MATH* in the buffer and the partial episode
*MATH* is better than (i.e. higher return or shorter
trajectory) the stored trajectory *MATH*, we replace the existing
entry *MATH* by *MATH*. If *MATH* is not sufficiently similar
to any state embedding in the buffer, a new entry
*MATH* is pushed into the buffer, as shown in
Fig. (d). Therefore we gradually increase the
diversity of trajectories in the buffer. The detailed algorithm is
described in the supplementary material.


Learning Trajectory-Conditioned Policy 


Policy Architecture


For imitation learning with diverse demonstrations, we design a
trajectory-conditioned policy *MATH* that
should flexibly imitate any given trajectory *MATH*. Inspired by neural
machine translation methods [*REF*; *REF*], one can view the
demonstration as the source sequence and view the incomplete trajectory
of the agent&apos;s state representations as the target sequence. We apply a
recurrent neural network (RNN) and an attention mechanism
*REF* to the sequence data to predict actions that would
make the agent follow the demonstration. As illustrated in
Fig., RNN computes the hidden features *MATH* for
each state embedding *MATH* (*MATH*) in the demonstration
and derives the hidden features *MATH* for the agent&apos;s state
representation *MATH*. Then the attention weight *MATH* is computed
by comparing the current agent&apos;s hidden features *MATH* with the
demonstration&apos;s hidden features *MATH* (*MATH*). The
attention readout *MATH* is computed as an attention-weighted summation
of the demonstration&apos;s hidden features to capture the relevant
information in the demonstration and to predict the action *MATH*.
Training is performed by combining RL and supervised objectives as
follows.


Reinforcement Learning Objective


Given a demonstration trajectory *MATH*, we provide rewards for
imitating *MATH* and train the policy to maximize rewards. For each
episode, we record *MATH* to denote the index of state in the given
demonstration that is lastly visited by the agent. At the beginning of
an episode, the index *MATH* of the lastly visited state embedding in the
demonstration is initialized as *MATH*, which means no state in the
demonstration has been visited. At each step *MATH*, if the agent&apos;s new
state *MATH* has an embedding *MATH* and it is the similar enough
to any of the next *MATH* state embeddings starting from the last
visited state embedding *MATH* in the demonstration (i.e.,
*MATH* where *MATH*), then the index of the last visited state embedding in the demonstration
is updated as *MATH* and the agent receives environment reward and
positive imitation reward *MATH*,
where *MATH* is a monotonically increasing function
(e.g., clipping [*REF*]) and *MATH* is the imitation
reward with a value of 0.1 in our experiments. Otherwise, the reward
*MATH* is 0 (see appendix for an illustration example). This
encourages the agent to visit states in the demonstration in a
soft-order so that it can edit or augment the demonstration when
executing a new trajectory. The demonstration plays a role to guide the
agent to the region of interest in the state embedding space. After
visiting the last (non-terminal) state in the demonstration, the agent
performs random exploration (because *MATH*)
around and beyond the last state until the episode terminates, to push
the frontier of exploration. With *MATH*, the
trajectory-conditioned policy *MATH* can be trained with a policy
gradient algorithm [*REF*]: *MATH* where *MATH* indicates the empirical average over a
finite batch of on-policy samples and *MATH* denotes the number of rollout
steps taken in each iteration. We use Proximal Policy Optimization
[*REF*] as an actor-critic policy gradient algorithm for
our experiments.


Supervised Learning Objective


To improve trajectory-conditioned imitation learning and to better
leverage the past trajectories, we propose a supervised learning
objective. We leverage the actions in demonstrations, similarly to
behavior cloning, to help RL for imitation of diverse trajectories. We
sample a trajectory *MATH*, formulate the demonstration *MATH* and
assume the agent&apos;s incomplete trajectory is the partial trajectory
*MATH* for any *MATH*. Then *MATH* is the &apos;correct&apos; action at step *MATH* for
the agent to imitate the demonstration. Our supervised learning
objective is to maximize the log probability of taking such actions: *MATH*.
Extensions of DTSIL for Improved Robustness and Generalization


DTSIL can be easily extended for more challenging scenarios. Without
hand-crafted high-level state embeddings, we can combine DTSIL with
state representation learning approaches (Sec. [5.1]). In highly stochastic environments,
we modify DTSIL to construct and select proper demonstrations
(Sec. [5.2]). In addition, DTSIL can be
extended with hierarchical reinforcement learning for generalization
over multiple tasks (Sec. [5.3]). See individual sections for
more details.


Related Work


Imitation Learning


The goal of imitation learning is to train a policy to mimic a given
demonstration. Many previous works achieve good results on
hard-exploration Atari games by imitating human demonstrations [*REF*; *REF*]. *REF* learn
embeddings from a variety of demonstration videos and proposes the
one-shot imitation learning reward, which inspires the design of rewards
in our method. All these successful attempts rely on the availability of
human demonstrations. In contrast, our method treats the agent&apos;s past
trajectories as demonstrations.


Memory Based RL


An external memory buffer enables the storage and usage of past
experiences to improve RL algorithms. Episodic reinforcement learning
methods [*REF*; *REF*; *REF*] typically store and update a look-up table to memorize the best episodic
experiences and retrieve the episodic memory in the agent&apos;s
decision-making process. *REF* and *REF* train a
parameterized policy to imitate only the high-reward trajectories with
the SIL or GAIL objective. Unlike the previous work focusing on
high-reward trajectories, we store the past trajectories ending with
diverse states in the buffer, because trajectories with low reward in
the short term might lead to high reward in the long term.
*REF* train a range of directed exploratory policies based on
episodic memory. *REF* propose to learn multiple diverse
policies in a SIL framework but their exploration can be limited by the
number of policies learned simultaneously and the exploration
performance of every single policy, as shown in the supplementary
material.


Learning Diverse Policies


Previous works [*REF*; *REF*; *REF*] seek a
diversity of policies by maximizing state coverage, the entropy of
mixture skill policies, or the entropy of goal state distribution.
*REF* learns a variety of policies, each performing novel
action sequences, where the novelty is measured by a learned
autoencoder. However, these methods focus more on tasks with relatively
simple state space and dense rewards while DTSIL shows experimental
results performing well on long-horizon, sparse-reward environments with
a rich observation space like Atari games.


Exploration


Many exploration methods [*REF*; *REF*; *REF*; *REF*]
in RL tend to award a bonus to encourage an agent to visit novel states.
Recently this idea was scaled up to large state spaces
[*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Intrinsic curiosity uses the prediction error or pseudo count as
intrinsic reward signals to incentivize visiting novel states. We
propose that instead of directly taking a quantification of novelty as
an intrinsic reward, one can encourage exploration by rewarding the
agent when it successfully imitates demonstrations that would lead to
novel states. *REF* also shows the benefit of exploration by
returning to promising states. Our method can be viewed in general as an
extension of [*REF*], though we do not need to rely on the
assumption that the environment is resettable to arbitrary states.
Similar to previous off-policy methods, we use experience replay to
enhance exploration. Many off-policy methods [*REF*; *REF*; *REF*]
tend to discard old experiences with low rewards and hence may
prematurely converge to sub-optimal behaviors, but DTSIL using these
diverse experiences has a better chance of finding higher rewards in the
long term. Contemporaneous works [*REF*; *REF*] as
off-policy methods also achieved strong results on Atari games. NGU
[*REF*] constructs an episodic memory-based intrinsic reward
using k-nearest neighbors over the agent&apos;s recent experience to train
the directed exploratory policies. Agent57 [*REF*]
parameterizes a family of policies ranging from very exploratory to
purely exploitative and proposes an adaptive mechanism to choose which
policy to prioritize throughout the training process. While these
methods require a large number of interactions, ours perform
competitively well on the hard-exploration Atari games with less than
one-tenth of samples. Model-based reinforcement learning
[*REF*; *REF*; *REF*] generally improves the efficiency of policy learning. However, in the
long-horizon, sparse-reward tasks, it is rare to collect precious
transitions with non-zero rewards and thus it is difficult to learn a
model correctly predicting the dynamics of getting positive rewards. We
instead perform efficient policy learning in the hard-exploration tasks
because of efficient exploration with the trajectory-conditioned policy.


Experiments


In the experiments, we aim to answer the following questions: (1) How well does the
trajectory-conditioned policy imitate the diverse demonstration
trajectories? (2) Does the imitation of the past diverse experience
enable the agent to further explore more diverse directions and guide
the exploration to find the trajectory with a near-optimal total reward?
(3) Is our method helpful for avoiding myopic behaviors and converging
to near-optimal solutions?


We compare our method with the following baselines: (1) PPO
[*REF*]; (2) PPO+EXP: PPO with reward *MATH*, where *MATH* is the
count-based exploration bonus, *MATH* is the number of times the cluster
which the state representation *MATH* belongs to was visited during
training and *MATH* is the hyper-parameter controlling the weight of
exploration term; (3) PPO+SIL: PPO with Self-Imitation Learning
[*REF*]; (4) DTRA (&quot;Diverse Trajectory-conditioned Repeat
Actions&quot;): we keep a buffer of diverse trajectories and sample the
demonstrations as DTSIL, but we simply repeat the action sequence in the
demonstration trajectory and then perform random exploration until the
episode terminates. More details about the implementation can be found
in the appendix.


FIGURE


Apple-Gold Domain


The Apple-Gold domain (Fig. [3]) is a grid-world environment with misleading rewards that can lead the
agent to local optima. At the start of each episode, the agent is placed
randomly in the left bottom part of the maze. An observation consists of
the agent&apos;s location *MATH* and binary variables showing whether
the agent has gotten the apples or the gold. A state is represented as
the agent&apos;s location and the cumulative positive reward indicating the
collected objects, i.e. *MATH*.


In Fig. [4], PPO+EXP achieves the average reward of 4.
PPO+EXP agent can explore the environment and occasionally gather the
gold to achieve the best episode reward around 8.5. However, it rarely
encounters this optimal reward. Thus, this parametric approach might
forget the good experience and fails to replicate the best past
trajectory to achieve the optimal total reward.
Fig. [4] shows that PPO+SIL agent is stuck with the
sub-optimal policy of collecting the two apples with a total reward of 2
on average. Fig. [5] visualizes how the trajectories in the memory
buffer evolve during the learning process. Obviously, PPO+SIL agent
quickly exploits good experiences of collecting the apples and the
buffer is filled with the trajectories in the nearby region. Therefore,
the agent only adopts the myopic behavior and fails on this task. In the
environment with the random initial location of the agent, repeating the
previous action sequences is not sufficient to reach the goal states.
The DTRA agent has a difficulty in exploring the environment and
achieving good performance.


Unlike the baseline methods, DTSIL is able to obtain the near-optimal
total reward of 8.5. Fig. [5] verifies that DTSIL can generate new
trajectories visiting novel states, gradually expand the explored region
in the environment, and discover the optimal behavior. A visualization
of attention weight in Fig. [6] investigates which states in the
demonstration are considered more important when generating the new
trajectory. Even though the agent&apos;s random initial location is different
from the demonstration, we can see a soft-alignment between the source
sequence and the target sequence. The agent tends to pay more attention
to states which are several steps away from its current state in the
demonstration. Therefore, it is guided by these future states to
determine the proper actions to imitate the demonstration.


Atari Games


We evaluate our method on the hard-exploration games in the Arcade
Learning Environment [*REF*; *REF*]. The
environment setting is the same as [*REF*]. There is a
sticky action [*REF*] resulting in stochasticity in the
dynamics. The observation is a frame of raw pixel images, and the state
representation *MATH* consists
of the agent&apos;s ground truth location (obtained from RAM) and the
accumulated positive environment reward, which implicitly indicates the
objects the agent has collected. It is worth noting that even with the
ground-truth location of the agent, on the two infamously difficult
games Montezuma&apos;s Revenge and Pitfall, it is highly non-trivial to
explore efficiently and avoid local optima without relying on expert
demonstrations or being able to reset to arbitrary states. Many
complicated elements such as moving entities, traps, and the agent&apos;s
inventory should be considered in decision-making process. Empirically,
as summarized in Tab., the previous SOTA baselines using the
agent&apos;s ground truth location information even fail to achieve high
scores.


TABLE


Using the state representation *MATH*, we introduce a variant &apos;DTSIL+EXP&apos;
that adds a count-based exploration bonus *MATH* to
*MATH* for faster exploration. [^1] DTSIL discovers novel
states mostly by random exploration after the agent finishes imitating
the demonstration. The pseudo-count bonus brings improvement over random
exploration by explicitly encouraging the agent to visit novel states
with less count. For a fair comparison, we also include count-based
exploration bonus in DTRA. However, with stochasticity in the dynamics,
it cannot avoid the dangerous obstacles and fails to reach the goal by
just repeating the stored action sequence. Therefore, the performance of
DTRA+EXP (Fig.) is poor compared to other methods.


On Venture (Tab.), it is relatively easy to explore and gather
positive environment rewards. DTSIL performs only slightly better than
the baselines. On Montezuma&apos;s Revenge (Fig.), in the early stage, the average episode
reward of DTSIL+EXP is worse than PPO+EXP because our policy is trained
to imitate diverse demonstrations rather than directly maximize the
environment reward. Contrary to PPO+EXP, DTSIL is not eager to follow
the myopic path (Fig.). As training continues, DTSIL+EXP
successfully discovers trajectories to pass the first level with a total
reward more than 20,000. As we sample the best trajectories in the
buffer as demonstrations, the average episode reward increases to
surpass 20,000 in Fig. On Pitfall, positive rewards are much
sparser and most of the actions yield small negative rewards (time step
penalty) that would discourage getting a high total reward in the long
term. However, DTSIL+EXP stores trajectories with negative rewards,
encourages the agent to visit these novel regions, discovers good paths
with positive rewards and eventually attains an average episode reward
over 0. In Fig., different performances under different
random seeds are due to huge positive rewards in some states on
Montezuma&apos;s Revenge and Pitfall. Once the agent luckily finds these
states in some runs, DTSIL can exploit them and perform much better than
other runs.


Continuous Control Tasks


When the initial condition is highly random, previous works imitating expert demonstrations (e.g.
[*REF*]) would also struggle. We slightly modify DTSIL to
handle the highly random initial states: in each episode, from buffer
*MATH*, we sample the demonstration trajectory with a start state
similar to the current episode. The detailed algorithm is described in
the supplementary material.


FIGURE


Navigation


We focus on a more realistic environment, a distant visual navigation
task designed on Gibson dataset [*REF*]. To make the task more
challenging, the agent is randomly placed in the environment (red
rectangle in Fig.), a positive reward 10 is given only when it
reaches a fixed target location (green point in
Fig.) which is significantly further away. The agent
receives no information about the target (such as the target location or
image) in advance. The observation is a first-person view RGB image and
the state embedding is the agent&apos;s location and orientation (which is
usually available in robotics navigation tasks) and the cumulative
reward. This experiment setting is similar to the navigation task with a
static goal defined in [*REF*]. Apart from the baseline
PPO+EXP, we also compare with Nav A3C+D1D2L [*REF*],
which uses the agent&apos;s location and RGB and depth image. This method
performs well in navigation tasks on DeepMind Lab where apples with
small positive rewards are randomly scattered to encourage exploration,
but on our indoor navigation task, it fails to discover the distant goal
without the exploration bonus. Fig. shows that Nav A3C+D1D2L can never reach the
target. PPO+EXP, as a parametric approach, is sample-inefficient and
fails to quickly exploit the previous successful experiences. However,
DTSIL agent can successfully explore to find the target and gradually
imitate the best trajectories of reward 10 to replicate the good
behavior.


Manipulation


Bin picking is one of the hardest tasks in Surreal Robotics Suite
[*REF*]. Fig. shows the bin picking task with a single
object, where the goal is to pick up the cereal and place it into the
left bottom bin. With carefully designed dense rewards (i.e. positive
rewards at each step when the robot arm moving near the object, touching
it, lifting it, hovering it over the correct bin, or successfully
placing it), the PPO agent can pick up, move and drop the object
[*REF*]. We instead consider a more challenging scenario with
sparse rewards. The reward is 0.5 at the single step of picking up the
object, -0.5 if the object is dropped in the wrong place, 1 at each step
when the object keeps in the correct bin. The observation is the
physical state of the robot arm and the object. The state embedding
consists of the position of the object and gripper, a variable about
whether the gripper is open, and cumulative environment reward. Each
episode terminates at 1000 steps. In Fig., PPO+EXP agent never discovers
a successful trajectory with episode reward over 0, because the agent
has difficulty in lifting the cereal and easily drops it by mistake. In
contrast, DTSIL imitates the trajectories lifting the object, explores
to move the cereal over the bins, finds trajectories successfully
placing the object, exploits the high-rewarding trajectories, and
obtains a higher average reward than the baseline (Fig.).


Other Domains: Deep Sea and Mujoco Maze


In the supplementary material, we present additional details of the
experimental results and also the experiments on other interesting
domains. On Deep Sea [*REF*], we show that the advantage
of DTSIL becomes more obvious when the state space becomes larger and
rewards become sparser. On Mujoco Maze [*REF*; *REF*], we show that DTSIL
helps avoid sub-optimal behavior in continuous action space.


Discussions: Robustness and Generalization of DTSIL 


Robustness of DTSIL with Learned State Representations 


Learning a good state representation is an important open question and
extremely challenging especially for long-horizon, sparse-reward
environments, but it is not the main focus of this work. However, we
find that DTSIL can be combined with existing approaches of state
representation learning if the high-level state embedding is not
available. When the quality of the learned state representation is not
satisfactory (e.g., on Montezuma&apos;s Revenge, [*REF*] fails
to differentiate the dark rooms at the last floor), the
trajectory-conditioned policy might be negatively influenced by the
inaccuracy in *MATH* or *MATH*. Thus, we modify DTSIL to handle this
difficulty by feeding sequences of observations (instead of sequences of
learned state embeddings) into the trajectory-conditioned policy. The
learned state embeddings are merely used to cluster states when counting
state visitation or determining whether to provide imitation reward
*MATH*. Then DTSIL becomes more robust to possible errors in the
learned embeddings. With the learned state representation from
[*REF*], on Montezuma&apos;s Revenge, DTSIL+EXP reaches the
second level of the maze with a reward \&gt;20,000 (Fig).


Robustness of DTSIL in Stochastic Environments 


In the single-task RL problem, a Markov Decision Process (MDP) is
defined by a state set *MATH*, an action set *MATH*, an
initial state distribution *MATH*, a state transition dynamics model
*MATH*, a reward function *MATH* and a discount
factor *MATH*. So the environment stochasticity falls into three
categories: stochasticity in the initial state distribution,
stochasticity in the transition function, and stochasticity in the
reward function. For sparse-reward, long-horizon tasks, if the precious
reward signals are unstable, the problem would be extremely difficult to
solve. Thus, in this paper, we mainly focus on the other two categories
of stochasticity. In Sections, we show the efficiency and robustness of
DTSIL in the environment with sticky action (i.e. stochasticity in
*MATH*) or highly random initial states (i.e. stochasticity in *MATH*).


Generalization Ability of DTSIL 


While many previous works about exploration focus on the single-task RL
problem with a single MDP [*REF*; *REF*; *REF*], we step
further to extend DTSIL for the multiple MDPs, where every single task
is in a stochastic environment with local optima. For example, in the
Apple-Gold domain, we design 12 different structures of the maze as a
training set (Fig.). In each episode, the structure of maze
is randomly sampled and the location of agent and gold is randomized in
a small region. If the structure in the demonstration is different from
the current episode, DTSIL agent might fail to recover the state of
interest by roughly following the demonstration. Thus, using the buffer
of diverse trajectories, we alternatively learn a hierarchical policy,
which can behave with higher flexibility in the random mazes to reach
the sampled states. We design the rewards so that the high-level policy
is encouraged to propose the appropriate sub-goals (i.e., agent&apos;s
locations) sequentially to maximize the environment reward and
goal-achieving bonus (i.e. positive reward when the low-level policy
successfully reaches the long-term goal sampled from the buffer). The
low-level policy learns to visit sub-goals given the current observation
(i.e. RGB image of the maze). The diverse trajectories in the buffer are
also used with a supervised learning objective to improve policy
learning. Fig. shows that the hierarchical policy
outperforms PPO+EXP during training. When evaluated on 6 unseen mazes in
the test set, it can generalize the good behavior to some unseen
environments (Fig.). More details of the algorithm
and experiments are in the supplementary material. Solving multi-task RL
is a challenging open problem [*REF*; *REF*; *REF*]. Here we
verified this variant of DTSIL is promising and the high-level idea of
DTSIL to leverage and augment diverse past trajectories can help
exploration in this scenario. We leave the study of improving DTSIL
furthermore as future work.


FIGURE


Conclusion


This paper proposes to learn diverse policies by imitating diverse
trajectory-level demonstrations through count-based exploration over
these trajectories. Imitation of diverse past trajectories can guide the
agent to rarely visited states and encourages further exploration of
novel states. We show that in a variety of stochastic environments with
local optima, our method significantly improves count-based exploration
method and self-imitation learning. It avoids prematurely converging to
a myopic solution and learns a near-optimal behavior to achieve a high
total reward.


Broader Impact 


DTSIL is likely to be useful in real-world RL applications, such as
robotics-related tasks. Compared with previous exploration methods,
DTSIL shows obvious advantages when the task requires reasoning over
long-horizon and the feedback from environment is sparse. We believe RL
researchers and practitioners can benefit from DTSIL to solve RL
application problems requiring efficient exploration. Especially, DTSIL
helps avoid the cost of collecting human demonstration and the manual
engineering burden of designing complicated reward functions. Also, as
we discussed in Sec. [5], when deployed for more problems in the
future, DTSIL has a good potential to perform robustly and avoid local
optima in various stochastic environments when combined with other state
representation learning approaches.


DTSIL in its current form is applied to robotics tasks in the simulated
environments. And it likely contributes to real robots in solving
hard-exploration tasks in the future. Advanced techniques in robotics
make it possible to eliminate repetitive, time-consuming, or dangerous
tasks for human workers and might bring positive societal impacts. For
example, the advancement in household robots will help reduce the cost
for home care and benefit people with disability or older adults who
needs personalized care for a long time. However, it might cause
negative consequences such as large-scale job disruptions at the same
time. Thus, proper public policy is required to reduce the social
friction.


On the other hand, RL method without much reward shaping runs the risk
of taking a step that is harmful for the environments. This generic
issue faced by most RL methods is also applicable to DTSIL. To mitigate
this issue, given any specific domain, one simple solution is to apply a
constraint on the state space that we are interested to reach during
exploration. DTSIL is complementary to the mechanisms to restrict the
state space or action space. More principled way to ensure safety during
exploration is a future work. In addition to AI safety, another common
concern for most RL algorithms is the memory and computational cost. In
the supplementary material we discuss how to control the size of the
memory for DTSIL and report the cost. Empirically DTSIL provides ideas
for solving various hard-exploration tasks with a reasonable computation
cost.