Sub-goal Distillation: A Method to Improve Small Language Agents


[Introduction]


Recently, Large Language Models (LLMs) have found applications in
various fields, including multi-task learning, decision making,
answering questions, summarizing documents, translating languages,
completing sentences, and serving as search assistants. They showcase
a remarkable ability to make predictions based on input, enabling
their use in generative AI applications to produce content based on
input prompts (*REF*).


The promising advantage of LLMs is attributed to their training on
extensive text datasets, resulting in impressive capabilities. This
prior knowledge can be leveraged for action planning to solve tasks in
robotics and reinforcement learning (*REF*). Recent works have utilized in-context learning
with LLMs to provide actions in autonomous decision-making agents and
interactive environments (*REF*).


However, the extreme size of LLMs makes them computationally
unaffordable for many applications. Moreover, closed-source models
like ChatGPT (*REF*, *REF*) and GPT-4
(*REF*, *REF*) limit accessibility and
reproducibility. Consequently, there is an increasing demand to find
approaches that are less computationally intensive while still
capitalizing on the knowledge embedded in LLMs. One prevalent
technique is the use of Knowledge Distillation (KD) (*REF*, *REF*; 
*REF*, *REF*), wherein a smaller model is
trained with guidance from a larger model.


Through this approach, we can leverage the knowledge in an LLM to
train a more compact model with a reduced number of parameters.


Distilling knowledge from LLMs offers significant advantages,
allowing for the training of specialized local models rather than
depending on an LLM as a general model. This approach not only
enhances privacy, particularly for systems with security-sensitive
considerations like co-pilot models, but also provides greater
flexibility in tailoring models for specific tasks. Additionally, the
use of a smaller model offers the advantage of versatility across diverse applications without
size constraints, including device models and mobile apps. Another
challenge with LLMs is their susceptibility to hallucinations. This
tendency poses a hindrance to their effective execution of long-tail
planning, especially in interactive decision-making scenarios.


TASK DESCRIPTION


In our research, we leverage the knowledge of LLMs to train an
autonomous agent for effective decision-making in complex
interactive text environments, utilizing small language models as our
policy. Knowledge Distillation facilitates the training of smaller
policies, allowing seamless integration of LLM knowledge. To address
the challenges at hand, adopting a two-level planning approach proves
beneficial for reducing hallucination -- one for high-level reasoning
to formulate subgoals and another for low-level action planning to
execute each sub-goal.


Figure *REF* illustrates this concept in the task of
freezing water from ScienceWorld (*REF*,
*REF*). The agent&apos;s subtasks involve navigating to
the kitchen, finding a thermometer and a metal pot, pouring water
into the pot, placing it in the freezer, and continuously monitoring
its temperature until frozen. These constitute sub-goals generated by
a high-level model, with each sub-goal subsequently executed by a low-
level model. The generation of sub-goals empowers an autonomous
agent to expedite learning for the current task and reuse similar
sub-goals in various tasks to have more generalization.


The contributions in this work are:
- We employ Knowledge Distillation from an LLM to train a high-level
policy capable of generating sub-goals without making assumptions
about the specific set of sub-goals. Notably, these sub-goals remain
flexible, accommodating various sequences of actions.
- We demonstrate that employing Knowledge Distillation with
hierarchical policies surpasses the performance achieved by both
standalone imitation learning and its combination with in-context
learning.
- We illustrate that this approach is more cost-effective in terms of
the number of calls to an LLM compared to other methods utilizing
in-context learning.
- We introduce an effective approach instead of using computational
requirements of LLM and their restricted number of calls for using
in interactive decision making tasks.


FIGURE


[Related Work]


Using LLMs for Action Planning Recent works have demonstrated the
ability of LLMs to perform action planning for interactive decision
making process without any additional training (*REF*, *REF*). ReAct (*REF*, *REF*) proposes a way of prompting
an LLM with interleave reasoning step and action taking step. That led
the resolution of a variety of language-based reasoning and
decision-making tasks. This approach empowers the model to construct
high-level plans for effective action. Reflexion (*REF*, *REF*) draws inspiration from
reinforcement learning, employing a framework to reinforce language
agents through linguistic feedback. At the end of each trial, it uses
selfreflection to determine what went wrong with the task and keeps
it in a memory. Then it leverages this information for the next trial.


Some works use a programmatic LLM prompt structure with available
actions and objects in an environment to translate natural language
commands into robot policy code via few-shot examples (*REF*, *REF*; *REF*, *REF*). *REF* 
(*REF*) introduced a decomposed prompting approach
wherein a task is broken down into simpler sub-tasks, allowing for
recursive handling. Subsequently, these sub-tasks are assigned to
sub-task-specific LLMs, with both the decomposer and the sub-task LLMs
with their own few-shot prompts. *REF* 
(*REF*) uses three steps, action mining, plan
formulation, and plan execution to decompose a question into a
sequence of actions by few-shot prompting of LLMs. In *REF* (*REF*) tasks are decomposed
explicitly by a separate LLM through prompting when an executor is
unable to execute a given sub-task.


Imitation learning Some works employ imitation learning to train a
language model as the agent&apos;s policy, as seen in offline decision
transformers (*REF*, *REF*). The inputs consist of states, actions, and reward-to-go values, which
are fed into a transformer. This transformer then predicts actions in
an autoregressive manner, utilizing a causal self-attention mask
(*REF*, *REF*). Contextual Action Language Model (CALM) (*REF*,
*REF*) is another work which uses a fine-tuned language
model with oracle data to generate a set of candidate actions which
are then passed to a policy network to select the best one. In *REF* (*REF*), the authors fine-tune
GPT-3 to address long-form questions within a web-browsing context.
Human feedback is employed as a direct optimization measure for
enhancing the quality of answers generated by the model.


Knowledge Distillation: Knowledge Distillation (KD) typically
falls into two categories: black-box KD and whitebox KD. In
black-box KD, only the teacher&apos;s predictions are available for
guidance, while in white-box KD, we have access to the teacher&apos;s
parameters (*REF*, *REF*). Recently, black-box KD has gained widespread use for finetuning
original models using self-instruct techniques, as proposed by [Wang
et al.](_bookmark55) (*REF*), or for smaller models
(*REF* *REF*, *REF*; *REF*, *REF*) through the utilization of
prompt-response pairs generated by LLMs. *REF* 
(*REF*) introduces symbolic KD from text rather than
logits. This process involves the transfer of knowledge from a large,
general model to a more compact commonsense model, facilitated by a
commonsense corpus, yielding a commonsense knowledge graph and model.
The work by *REF* (*REF*) trains a smaller model that outperform LLM using reasoning steps called
rationales. They incorporated rationales as informative supervision to
train smaller models with less training data.


Complex interactive text environments In text-based games, an
agent interacts with the environment by reading and writing text while
aiming towards the end game or solving a given task. Out of the recent
frameworks that deals with generating and interfacing text-based games
(*REF*, *REF*; *REF*, *REF*; *REF*, *REF*; *REF*, *REF*), 
we use ScienceWorld (*REF*, *REF*) which is very
complicated by having a large set of objects, actions, and tasks.


[Model]


In this paper, we propose to train a hierarchical policy by combining
KD from an LLM and imitation learning from expert trajectories. This
section describes both modules in detail and we refer the reader to
Figure *REF* for a schematic view. We first formulate the
problem as a POMDP (Section [3.1]). Next, we
describe what knowledge we are distilling from an LLM to guide the
agent in accomplishing tasks (Section [3.2]). Then, we detail how both
the high-level and low-level policies of the hierarchical policy are trained (Section [3.3]).


1. [Problem Formulation]


ScienceWorld (*REF*, *REF*) can
be defined as a partially observable Markov decision process (POMDP),
where observations provide information solely on environmental changes
induced by the current action. ScienceWorld is
an interactive text environment meaning all task instructions,
observations and actions are expressed in textual form. Importantly,
both observations and rewards in this environment are conditioned by
the ongoing task.


FIGURE


Given a language vocabulary V and an arbitrary maximum number of
tokens N, an observation is defined such as o ∈ Ω ⊂ V, a
reward such as r ∈ R and an action as a ∈ A ⊂ V. Finally, a
task or goal description is shown by g ∈ G ⊂ V.


We formalize the problem as a goal-augmented POMDP M = (S, V, A,
Ω, G, T, R, O, γ) with S the state space, A ⊂ V the action
space, Ω ⊂ V the goal space,
T: S × A × G → S the goal-conditioned transition function, 
R: S × A × G → R the goal-conditioned reward function, O: S → V
an (unknown) observation function mapping a state to a textual
description and γ the discount factor. We assume γ = 1 in our experiments.


2. [Distilling Knowledge from an LLM]


The initial step in training our policies is creating a dataset. This
dataset should include sub-goals along with their corresponding
aligned sequences of actions for each task.


To generate sub-goals along with their corresponding aligned sequences
of actions we do the following steps. We assume access to a collection
of expert trajectories. Then we prompt an LLM with two in-context
examples. Each example is composed of a task description, a similar
task as the one we wish to annotate, and its expert trajectory. The
example also contains a set of sub-goals, with the sequences of
actions linked to each sub-goal.


Given the two in-context examples and a new task description with its
expert trajectory, the LLM is then instructed to generate a response.
The response is a set of sub-goals with their associated list of
actions. The generated list of actions is used to determine each
sub-goal corresponds to which segment of the expert trajectory. It is
important to note that these responses are collected only for the
training tasks for which we assume having access to expert
trajectories. Also, it is important to point out that the LLM is not
generating any novel trajectories.


Figure *REF* illustrates the prompt examples for task 1 − 1 
which is boiling a given substance. To ensure more uniform sub-goals
that can generalize across tasks, we opted for a format that looks
like function calls. Since that format was shown in the in-context
examples, the LLM-generated sub-goals mimic this format as well making
them easier to parse.


Since the expert trajectories for some tasks can be long (+100
actions), the generated sub-sequence of actions corresponding to
each sub-goal may not align exactly with the expert trajectory.
Sometimes, it might miss certain actions, while in other instances, it
might include additional actions, especially when there are repeated
actions in the trajectory. To address this, we use a trajectory
alignment process that finds the minimal set of edits to go from the
generated trajectory to the expert trajectory according to the
Levenshtein distance. For each &quot;remove&quot; edit, i.e. the generated
trajectory has superfluous actions, we simply remove those from the
generated trajectory. On the other hand, for &quot;add&quot; edit, i.e. the
generated trajectory is missing some actions, we prompt the LLM to
generate a new sub-goal for those. An example is shown in Figure *REF*.


FIGURE


In the resulting annotated dataset, each data point follows the same
format as used by *REF* (*REF*)
but with the added mention of completed sub-goals and the current
sub-goal. Precisely, it corresponds to:
- Input: task description, number of steps, current score,
completed sub-goal, current sub-goal, a history of 10
recent actions-observations, current items in the room, inventory, and
the visited rooms.
- Target: next action, next sub-goal.


1. [Hierarchical Imitation Learning]


With the dataset obtained from distilling knowledge from an LLM, we
can now focus on training the policies.


Low-level policy: The low-level policy is a language model (LM)
which is trained through imitation learning using the annotated
dataset. The goal is to have a model much smaller than an LLM so it
can fit on a single machine and run faster, ideally below a billion of
parameters. This policy learns to predict the next action given the
current task description, the 10 previous observation-action pairs,
the previous completed sub-goals, and the current sub-goal. We refer
to this policy as the action generator.


High-level policy: The high-level policy is another LM with a
reasonable size. It is trained using the annotated dataset to generate
the next sub-goal given the previous sub-goals and a short history,
i.e. the last 10 actions and observations. So the high-level policy
generates sub-goals while the low-level policy generate actions.
Moreover, this policy conditions on the same input information as for
the action generator. We call this policy the sub-goal generator.


Hierarchical policy: During inference, we first leverage the
high-level policy to generate a sub-goal. This generated sub-goal is
then fed into the action generator, allowing it to produce the next
action aligned with the provided sub-goal. This sequential approach
serves as a guiding cue for the action generator, particularly when
the trajectory to achieve the goal is complex or long. Moreover, it
serves to prevent the action generator from generating actions that
might deviate the agent from the correct path, thereby improving the
precision and relevance of the actions being generated.


FIGURE


[Experiments]


1. [Environment]


We chose ScienceWorld (*REF*, *REF*) as the environment due to its complexity and
the diverse range of tasks it encompasses. This environment is an
interactive multi-task text-based game where the agent conducts
elementary science experiments in a simulated environment. Each
experiment is designed as a separate task. For example, &quot;Your task is
to boil water. For compounds without a boiling point, combusting the
substance is also acceptable. First, focus on the substance. Then,
take actions that will cause it to change its state of matter&quot;. To
complete a task, the agent must perform multiple actions and receives
the result of each action as an observation and a score. The
observations and actions are in text format. An observation describes
the changes in the environment, and the score is a numerical value
ranging from 0% to 100%, indicating the degree of completion of the
current task through the current action.


Furthermore, ScienceWorld is a benchmark with 30 distinct tasks
spanning 10 science domains which are widely different (Appendix
[A.4]). For instance, in the &quot;Changes of State&quot;
task, the agent is required to locate and use heating/freezing
sources to alter the state of a substance (e.g., ice or chocolate).
Conversely, in a task such as &quot;Mendelian Genetics,&quot; the agent is
tasked with determining whether a specified trait (e.g., white flower
color) is dominant or recessive in a plant. These examples illustrate
the substantial diversity across the domains, ranging from physical
transformations to genetic analyses, underscoring the broad spectrum
of challenges within ScienceWorld.


On top of that, ScienceWorld has 10 different locations, more than 200
object types, and 25 action templates which makes the search space
very larger for the agent. Each type of task has different variations
in which the task objects, the agent&apos;s initial location, and random
contents of each room are altered.


2. [Experimental Setup]


The environment has separate sets of variations for train and test. In
the test variations, the combinations of objects and conditions are
not seen in the train set. Following the experimental setup in (*REF*), if the number of
variations is more than 10, we consider only the first 10 variations.


Our base models for both policies is a pre-trained FLAN-T5-LARGE
(*REF*, *REF*) with 700M parameters. For the both polices, we used greedy decoding at
inference. We also conduct an ablation study over different model
sizes (Figure *REF*). For fine-tuning the policies, we
use all the training tasks and their variations (3600 games in total)
from ScienceWorld. We vary the number of training epochs in function
of the size of the models (see Appendix [A.3]).


TABLE


The denotes scores reported from (*REF*, *REF*) which all use ChatGPT (GPT-3.5).


3. [Baseline Agents]


We compare our approach with other works that leverage LLMs. Some rely
only on prompting such as SayCan, ReAct, and Reflexion, but SwiftSage
also do imitation learning. Here is a brief description of each
method.


SayCan: the LLM initially offers a set of actions along with their
respective ranks. Then, a value-based method is employed to re-rank
these actions in order to determine the most rewarding action for
execution (*REF*, *REF*).


ReAct: the LLM generates actions by incorporating the provided
prompt and the history of generated texts. It employs reasoning traces
as intermediate thought steps during the action generation to refine a
plan for the upcoming steps (*REF*, *REF*).


Reflexion: the language agent reflects the task feedback at each
trial in the form of text and retains this information within an
episodic memory. During the subsequent trial, it leverages the stored
memory text to enhance its decisionmaking process (*REF*, *REF*).


SwiftSage: this method comprises two components: Swift, a
fine-tuned LM to predict actions, and Sage, a module that queries an
LLM for planning when the performance of Swift is inadequate (as
determined by some handcrafted rules) (*REF*, *REF*).


Swift-only: this is the Swift part of the SwiftSage method which
only has the fine-tuned LM to predict the actions. We consider this
method as a strong baseline and the most comparable to our approach as
it relies on imitation learning without the need for querying an LLM
during inference.


Note that all baselines use ChatGPT (GPT-3.5) as their LLM.


4. [Results and Analysis]


Main Results: Table *REF* compares the performance of
the baselines with our approach in the ScienceWorld. The score for
each task type is the average score (in percent) obtained for 10 test
variations. Our approach demonstrates an overall performance of
65.43%, surpassing Swift-only by 16.71% (33.9% relative increase),
and showing a slight improvement over SwiftSage of 3.3% (5.3%
relative). Interestingly, our method is able to solve all test
variations (i.e., gets an average score of 100%) for 11 out of the 30
task types. In contrast, SwiftSage solves them only for 2 task types,
and Swift-only, only for 4 task types.


Additionally, we measured the performance of the agents with respect
to the length of the tasks (a proxy for task complexity). The length
of a task is determined by how many actions was needed by the expert
to solve it. *REF* Following *REF* (*REF*), we group the tasks into three categories: 
Short when the length is less than 20 actions, Medium when it
falls between 20 and 50 (inclusively), and Long if above 50. As
shown in Table *REF*, our approach outperforms
other methods on short and medium tasks. On long tasks, we outperform
all methods except SwiftSage, which has a substantial advantage here: 
The longer the task, the higher the chance it triggers one of the
rules for Sage to take over.


As part of the comparison, there are other approaches that do not use
a LLM including DRRN (*REF*, *REF*), KG-A2C (*REF*, *REF*), 
CALM (*REF*, *REF*), BC (*REF*, *REF*), TDT (*REF*, *REF*). 
The results from (*REF*, *REF*) show these approaches
perform poorly, below 17%, in ScienceWorld. For this reason, we did
not include them here and only focus on approaches comparable with us.


A key motivation for our approach is cost-effectiveness in terms of
LLM queries. During training, we make one query to ChatGPT per task to
identify the sub-goals within an expert trajectory. Sometimes
mismatches occur between the expert trajectory and the actions
assigned to each sub-goal by ChatGPT. When that is the case, we employ
dynamic programming, with a maximum of 10 attempts per task. This
contrasts with other baseline methods, where LLM is queried for each
action, incurring considerably higher costs.


Why is it failing on some task types? The performance of our
algorithm in some tasks are low, (see Table *REF*). In
Table *REF*, the scores of two tasks are presented. One
contributing factor is the variations in the test are very different
from those in the training. For instance, the objects might be very
different or the path to complete the task is very different and
longer. The main culprit is the sub-goal generator which is not able
to generate good sub-goals.


As a concrete example (Table *REF*), in the test
variations for task 3-3, the agent needs to go to kitchen and then
fill a jug with water. When looking at the transcript, we see the
agent is able to go to kitchen but then when it arrives, the sub-goal
generator issues a sub-goal which is not relevant, FocusOn(fountain).
The agent attempts to focus on the fountain which is a wrong action
and the game terminates with a score of 0.


Another example is task 1-1 (Table *REF*) in which the
agent should boil a substance. It should first find the substance but
since the substance is in a totally different location than those seen
during training, the sub-goal generator is not able to generate a good
sub-goal for this step. Consequently the agent will do other actions
and exhaust all the allocated time steps.


TABLE


The impact of scale: We conduct a comparison across various sizes
of language models such as FLAN-T5-XL, FLANT5-BASE, and
FLAN-T5-SMALL. Additionally, we evaluate T5-3B and T5-LARGE to
determine the effectiveness of FLAN-T5 versus T5. The results are
illustrated in Figure *REF*. In our initial findings, we
observed that FLANT5 outperforms T5 significantly. Moreover, our
results reveal a positive correlation between the LM size and its
performance -- larger models generally yield better results.
Intriguingly, we observe that for smaller models (FLANT5-SMALL and
FLAN-T5-BASE), not conditioning on sub-goals works slightly better
than including them. This might be indicative that the sub-goal
generator is not expressive enough to generate meaningful and
effective sub-goals which in turn impacts the action generator policy
and leads to lower scores.


The impact of sub-goals: To study the impact of the sub-goal
generator&apos;s size on the overall performance, we try pairing different
sizes of sub-goal generator while limiting the action generator to be
small. In Figure *REF*, the average scores exhibit an
upward trajectory. This can be attributed to the larger sub-goal
generators producing more accurate and relevant sub-goals,
subsequently empowering the action generator to generate more correct
actions. See Table *REF* for a complete breakdown of the
score per task type and per model size.


FIGURE


TABLE


To further demonstrate the importance of the sub-goal, we generated
random sub-goals and then fed them to the action generator. That yield
an average score of 6.4%, indicating that the action generator do
condition on the sub-goals, subsequently, it cannot solve the tasks
effectively. We conducted an additional experiment by altering the
arguments of the sub-goals, as they have a functional format. If the
argument corresponds to a location, we replaced it with another from
the environment, and if it is an object, we replaced it with a
randomly chosen object available at that step of the game. We named
this approach semi-random sub-goals. The result for this experiment
is 14.2%, showing an increase in performance compared to the
random sub-goals. Table *REF* shows the average scores and
Table *REF* shows the score for each task.


Recovery from noisy sub-goals: We also assess the performance when
both the action and sub-goal generators have been exposed to noisy
sub-goals. More specifically, we consider two settings: applying
noise 1) only at the first step, or 2) every 10 steps. In the first
setting, the first sub-goal is (semi-)randomly selected, while the
subsequent sub-goals are generated using the FLAN-T5-LARGE sub-goal
generator. In the second experiment, a sub-goal is (semi-)randomly
selected every 10 steps instead of using the sub-goal generator for
all steps. Table *REF* shows the overall scores for both
settings and a breakdown per task types is presented in Table *REF*.


In both scenarios, semi-random selection (53.1% and 43.3%)
yields better results, as it closely resembles the subgoals
generated by the sub-goal generator. Some tasks achieve a score of
100, indicating successful recovery from noisy sub-goals. While
overall scores are lower compared to using the FLAN-T5-LARGE sub-goal
generator, it is still higher than using Swift only in the first
setting and closely approaching it in the second setting (Appendix [A.10]).


Generalization on heldout task types: We select one or two task
types from each science domain (see highlighted ones in Table
*REF*) to train the action and sub-goal models. Then, we
assessed their performance on the rest of the task types. We compared
our algorithm against the Swift-only baseline. The average total
scores are 40.63% with subgoals vs. 36.56% for Swift-only.
For unseen tasks, the scores are 27.72% with sub-goals vs.
15.25% for Swift-only. This suggests that using sub-goals helps
improve generalization across unseen tasks. The scores for each task
are presented in Table *REF*.


[Discussion and Limitation]


In contrast to SwiftSage, which relies on interactive usage of the
ChatGPT API to handle planning, our approach makes use of a trained
sub-goal generator to guide the action generator. Moreover, our
framework empowers the agent to retrieve a nearly optimal trajectory
by supplying the appropriate sub-goal. Nevertheless our framework has
significantly reduced the frequency of API calls, which are both
expensive and not universally accessible. ReAct, Reflexion, and
SwiftSage require human annotations to correct sub-goals and predict a
reasonable action. However in our approach, we do not need human help
to predict sub-goals or provide precise prompts.


Generalization: In this work, our focus is on optimizing
performance within the environment, and there might be a potential
limitations when transitioning to entirely different scenarios. If we
test it in a distinct environment, the performance may not be optimal,
given the fine-tuning with data specific to the ScienceWorld
environment. It&apos;s acknowledged that for generalization across diverse
scenarios, an LLM may perform better, given its capacity to handle a
broader range of inputs and contexts.


Goal Modification: When the agent encounters challenges in solving
the current sub-goal, it will often find itself cycling through the
same sub-goal for several steps. Consequently, the action generator
repeats a sequence of actions mirroring recent ones. Sometimes the
sub-goal generator will adjust the sub-goal slightly based on the
input and that can be enough to get unstuck.


Ideally, we would like to avoid being stuck for several steps and
learn to modify the sub-goal in the right way. One strategy involves
online learning, where the controller is updated based on the reward
from the environment. However, this approach carries the risk of
catastrophic forgetting, necessitating additional measures such as
loss modification and regularization to mitigate this risk. Another
approach could involve incorporating an LLM alongside the controller.
If the controller fails to produce effective actions, the LLM can
suggest alternative sub-goals. This might have the risk of poor
sub-goals and hallucinations which rewards might help but it is still
challenging in such a sparse environment.


[Conclusion]


We introduce a straightforward yet highly effective approach for
tackling complex text-based environments. Our framework leverages the
knowledge of an LLM to extract sub-goals. A hierarchical policy of two
LMs proposed: a high-level policy predicts a sub-goal, and a low-level
policy, by using the predicted sub-goal, generates elementary actions.
Through extensive experiments across 30 task types in ScienceWorld,
our approach demonstrates increase performance compared to
state-of-the-art baselines, including standard imitation learning and
SwiftSage.


As future directions for this work, we aim to delve into further
exploration of goal modification strategies when the agent encounters
challenges in solving the current sub-goal. This could involve
breaking down or transforming a sub-goal into a more achievable form.
Another venue for future research involves extending this approach to
a multimodule environment. In such scenarios, the sub-goal generator
could leverage each module as an independent source to generate
diverse and context-specific sub-goals. Exploring strategies for goal
modification and online learning is another avenue we are keen to
pursue.