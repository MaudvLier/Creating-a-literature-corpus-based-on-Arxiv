ScreenAgent: A Vision Language Model-driven Computer Control Agent


Introduction


We have constructed a realistic computer-controlled environment and
designed a control pipeline for the agent. The VLM agent retrieves
instruction prompts and real computer states from the environment, then
runs its internal control flow, going through the planning, acting, and
reflecting phases. It outputs the next action operation, utilizes
function calls to perform actions, induces changes in the computer
environment, and achieves genuine real-time interaction between the agent and the environment. *REF*.


Large language models (LLM), such as ChatGPT and GPT-4, have recently
demonstrated exceptional performance in natural language processing
tasks like generation, understanding, and dialogue. They have also
significantly revolutionized research in other artificial intelligence
fields. In particular, the development of these technologies paves the
way for the study of intelligent LLM agents, which are capable of
performing complex tasks  *REF*. A LLM agent is an AI
entity with a large language model as its core computational engine. It
possesses capabilities like Perception, Cognition, Memory, and Action,
enabling the agent to perform highly proactive autonomous behaviors
*REF*. In LLM agent-related research, how to enable agents
to learn to effectively use tools for expanding their action space has drawn extensive attention.


With the growing prevalence of electronic devices, such as personal
computers, smartphones, tablets, and smart electronic instruments, our
lives are becoming more intertwined with the digital world. Daily
activities often require frequent interaction with electronic device
screens. If an agent can free people from manual operations, and
seamlessly navigate these devices by controlling screens according to
user needs, it would mark a significant step towards achieving more
general and autonomous intelligence  *REF*. Indeed, a
screen interaction agent must possess powerful visual information
processing capabilities, and the ability to execute computer control
instructions as shown in *REF*. Therefore, to achieve such a goal, it is
necessary to first create a real interactive environment for the VLM
agent, then guide the model and environment to form a continuous
interactive pipeline, and train the agent to improve its performance.
However, it&apos;s highly challenging to implement these functions within a
unified framework and achieve satisfactory results from both the project
engineering and theoretical research perspectives.


Despite the recent progress achieved by several works, some aspects
still need to be further explored. For instance,
CogAgent  *REF* specializes in GUI understanding and
planning, showcasing remarkable proficiency in addressing diverse
cross-modal challenges. However, CogAgent lacks the capability of a
complete thinking chain, preventing it from forming a comprehensive tool
invocation similar to ChatGLM [*REF*; *REF* -130B].
Later, AppAgent  *REF* focuses on smartphone operations,
learning navigation, and acquiring new application usage through
autonomous exploration or by observing human demonstrations. While
AppAgent lacks a planning process and sacrifices the freedom of
operation. It guides the agent to click by labeling each element,
thereby limiting the method of tapping. As a result, current Vision
Language Models (VLM agents) are typically unable to interact with real
computer or mobile environments to generate and execute continuous manipulative commands.


To address the above-mentioned issues, we propose ScreenAgent, an
entirely automated agent designed to handle continuous screen
operations. This agent is primarily achieved through three components,
namely planning, execution, and reflection. In particular, the
reflecting module is inspired by Kolb&apos;s renowned experiential Learning
Cycle theory  *REF*, which enables the agent to perform
reflective behaviors, making the entire pipeline more comprehensive and
aligned with human action and thought processes. It autonomously
assesses the execution status of the current action, providing feedback
based on the ongoing state. This capability enhances its performance for
subsequent actions, enabling our agent to possess the capability of a
continuous thinking chain. Consequently, our agent can understand the
next steps and engage in complete tool invocation to execute a series of
continuous manipulative commands. The major contributions are summarized as follows:


We present a Reinforcement Learning (RL) environment that enables
the VLM agent to directly interact with a real computer screen via
VNC protocol. By observing the screenshot, our agent can interact
with the GUI through basic mouse and keyboard operations.


We develop an automated pipeline that encompasses the planning
phase, acting phase, and reflecting phase. This integrated pipeline
facilitates the agent&apos;s continuous interaction with the environment, distinguishing our agent from others.


We propose the ScreenAgent dataset, which includes action sequences
for completing generic tasks on Linux and Windows desktops.
Moreover, we provide a fine-grained scoring metric to
comprehensively evaluate the various capabilities that are necessary
for a VLM agent in computer-controlling tasks.


We test GPT-4V and two state-of-the-art open-source VLMs on our test
set. The results demonstrate that GPT-4V is capable of controlling
computers, but it lacks precise positioning capabilities. We thus
trained a ScreenAgent to enable precise positioning and achieved
comparable results to GPT-4V in all aspects. Our work can facilitate
further research on building a generalist agent.


Related Work


Multimodal Large Language Models


LLMs have demonstrated powerful contextual understanding and text
generation capabilities, enabling the implementation of complex
multi-turn question-answering systems. LLaMA  *REF* is a
series of foundational language models spanning from 7 billion to 65
billion parameters, with Vicuna-13B  *REF*, an open-source
chatbot, being refined through fine-tuning on the LLaMA architecture.
GPT-4 is an advancement by OpenAI following the success of GPT-3 and it introduces several noteworthy improvements.
GPT-4V(ision) [*REF* ], building upon GPT-4, has added
multimodal capabilities. LLaVA  *REF* and LLaVA-1.5  *REF* connect the pre-trained
CLIP  *REF* visual encoder with the Vicuna, achieving
multimodal capabilities. Fuyu-8B does not use an image encoder but
opts for a pure decoder Transformer architecture.
CogVLM  *REF* is a powerful open-source Visual Language
Model that supports image understanding at a resolution of 490 × 490 and
multi-turn dialogues. In addition, Monkey  *REF* 
introduces an efficient training method that enhances input resolution capability.


FIGURE


Computer Control Environment &amp; Dataset


In simulated environments, agents can be trained to emulate clicking and
typing. WebNav  *REF* creates a navigation environment with
links, testing the agent&apos;s sequential decision-making ability. MiniWoB++
*REF* provides a lot of simplified ATARI-like
web-browser-based tasks as a Reinforcement Learning (RL) environment.
WebShop  *REF* offers tasks for controlling the browser to
complete the purchase process. SWDE  *REF* preserves
webpage HTML files to train information extraction models.
WebSRC  *REF* is a QA-style dataset that
contains a large number of questions and answers about webpage
screenshots. Mind2Web  *REF* introduces a
dataset for developing generalist web agents. Seq2act  *REF* 
integrates three datasets for Android, AndroidHowTo, Rico-SCA, and
PixelHelp. Screen2Words  *REF* is a large-scale screen
summarization dataset for Android UI screens. META-GUI  *REF* is
a dataset for training a multi-modal conversational agent on mobile GUI.
*REF* provides a dataset of unknown command feasibility on Android.


Large Language Model-Driven Agents


With the advancement of large language models, the capabilities of
intelligent agents have also been enhanced. WebGPT  *REF* 
conducts fine-tuning on GPT-3 to address extended questions within a
text-based web-browsing environment, enabling the model to explore and
navigate the web for answers. ToolFormer  *REF* 
integrates an assortment of utilities, featuring a calculator, Q&amp;A
system, and search engine, among others. Voyager  *REF* 
stands as the inaugural embodiment of a Large Language Model-powered,
lifelong learning agent within the Minecraft environment.
RecAgent  *REF* proposes that agents can generate
high-level thoughts through the operation of memory reflection.
ProAgent  *REF* introduces a novel paradigm in process
automation that seamlessly integrates agents powered by Large Language
Models. CogAgent  *REF*, an 18-billion-parameter visual
language model, is meticulously designed for GUI comprehension and
navigation. AppAgent  *REF* proposes a multi-modal agent
framework for learning operations on smartphone applications.


Framework


In this section, we introduce our Reinforcement Learning (RL)
environment and the autonomous control flow within the Agent. Through
this environment, a VLM agent can interact with a real computer screen,
observe screen images, select actions, and autonomously complete specific tasks.


Computer Control Environment


We construct a computer control environment to assess the capabilities
of VLM agents. This environment connects to a desktop operating system
through remote desktop (VNC) protocol and allows the sending of mouse
and keyboard events to the controlled desktop. The formal definitions of
this environment are defined as follows:


*MATH* - Action Space. We define an action as a form of a function
call. If the agent outputs an action in the required JSON-style
format, the action will be parsed and executed by the environment.
All action types and corresponding action attributes are defined in *REF.


*MATH* - State Space. The screenshot image is utilized as the state
space of the environment. The environment will collect screenshots
*MATH* and *MATH*, denoting the state before and after each action, respectively.


*MATH* - Reward Function. Due to the highly open-ended nature of the
task, the reward function in this environment is flexibly opened to
different interfaces, which can integrate different existing or future reward models.


Through remote control, the agent can perform arbitrary tasks on the
screen, which creates a highly challenging open environment having a large state and action space.


Control Pipeline


To guide the agent to continually interact with the environment and
complete multi-step complex tasks. We designed a control pipeline
including the Planning, Acting, and Reflecting phases. The whole
pipeline is depicted in *REF*. The pipeline will ask the agent to
disassemble the complex task, execute subtasks, and evaluate execution
results. The agent will have the opportunity to retry some subtasks or
adjust previously established plans to accommodate the current
occurrences. The details are depicted as follows:


Planning Phase. In the planning phase, based on the current
screenshot, the agent needs to decompose the complex task relying on its
own common-sense knowledge and computer knowledge.


Acting Phase. In the acting phase, based on the current screenshot,
the agent generates low-level mouse or keyboard actions in JSON-style
function calls. The environment will attempt to parse the function calls
from the agent&apos;s response, and convert them to device actions defined in
the VNC protocol. Then our environment will send actions to the
controlled computer. The environment will capture the after-action
screen as input for the next execution phase.


Reflecting Phase. The reflecting stage requires the agent to assess
the current situation based on the after-action screen. The agent
determines whether needs to retry the current sub-task, go on to the
next sub-task, or make some adjustments to the plan list. This phase is
crucial within the control pipeline, providing some flexibility to
handle a variety of unpredictable circumstances.


All prompts and templates in the pipeline are provided in *REF*.


We invoke GPT-4V to generate an original response, and annotators correct this response as the golden labeled
response. The environment parses executable actions from the text and
sends them for real computer execution. The original response and the
golden labeled response form a pair, which can be utilized for training
in future Reinforcement Learning from Human Feedback (RLHF) processes.


FIGURE


TABLE


ScreenAgent Dataset &amp; CC-Score


Existing computer-controlled datasets typically have a narrow range of
applicability scenarios. For instance, building upon the foundational
premise that it is easy to obtain UI element metadata through HTML or
developer modes, WebNav  *REF*, Mind2Web  *REF*, and
SWDE  *REF* mainly focused on web browsing, while
Seq2act  *REF* and Screen2Words  *REF* are
tailored for Android. However, the mouse and keyboard are also common
and universal interfaces to control a computer. To fill the gap in this
type of control method, we build an interactive annotation process shown
in *REF* to construct the ScreenAgent Dataset which is collected from Linux and Windows operating
systems for completing specific tasks. This dataset endeavors to cover a
wide range of daily computer usage scenarios, including daily office,
booking, information retrieval, card games, entertainment, programming,
system operations, and so on. As illustrated in
*REF*, the ScreenAgent Dataset
encompasses 39 sub-task categories across 6 themes. The dataset has 273
complete task sessions, with 203 sessions (3005 screenshots) for
training and 70 sessions (898 screenshots) for testing. *REF* shows important statistical
information about the dataset. More statistical information and samples are provided in *REF*.


To assess an agent&apos;s capability in the computer control task, we have
designed a fine-grained evaluation metric Vision Language Computer
Control Score (CC-Score) for assessing the similarity of control action
sequences. This metric takes into account both the sequential order and
actions&apos; attribution. We developed specific similarity metrics for every
action type. For mouse actions, the metrics include four aspects of
consistency: action type, mouse operation type, mouse button, and
whether the click coordinates are within the annotated feasible bounding
box. For text and keyboard actions, the metrics involve two aspects:
action type consistency and the BLEU score of the input text, single
key, or keyboard shortcut combination. For the entire action sequence,
we employ an alignment algorithm that identifies the maximum matching
pairs of predicted action and labeled action, while maintaining the
sequence order. This approach maximizes the overall score, which is used
as the measure of sequence similarity. Ultimately, the CC-Score
encompasses the normalized scores of predicted and labeled sequences,
the F1 values for each action attribute classification, and the unigram
similarity values for text types. The implementation details of CC-Score are provided in *REF*.


Experiment


In the experimental phase, we assessed OpenAI GPT-4V performance on the
ScreenAgent test set, along with evaluations of three open-source VLMs.
Furthermore, one of these models underwent fine-tuning to potentially
augment its proficiency in screen control tasks. Subsequently, we
conducted a thorough analysis of the outcomes and identified several
typical cases to elucidate the inherent challenges of our task.


Evaluation Results on ScreenAgent Test-Set


Apart from GPT-4V, we selected several recently released SoTA VLMs for
testing, including LLaVA-1.5  *REF* and CogAgent  *REF*. LLaVA-1.5 is a 13B-parameter multimodal
model, unfortunately, it only supports up to 336 × 336px image size
inputs. CogAgent is an 18B-parameter visual language model designed for
GUI comprehension and navigation. Leveraging dual image encoders for
both low-resolution and high-resolution inputs, CogAgent demonstrates
proficiency at a resolution of 1120 × 1120px, allowing it to discern minute elements and text.


FIGURE


We test the models&apos; capabilities from two aspects: The ability to follow
instructions to output the correct function call format, shown in
*REF*, and the ability to complete specific tasks assigned by the user, shown in *REF*.


Following instructions and executing correct function calls is the most
fundamental skill for an LLM agent when learning to use external tools.
*REF* presents the success rate of these function calls for each attribute key. This assessment focuses on
whether the model can accurately execute various functions encompassing
the attribute items expected by manual action annotations. Note that,
this evaluation does not consider the consistency of the attribute
values with the golden labeling; it solely examines if the model&apos;s
output includes the necessary attribute keys. From the table, GPT-4V and
LLaVA-1.5 achieved higher scores, while CogAgent and its upstream model
CogAgent-VQA underperformed. CogAgent-VQA and CogAgent-Chat almost
entirely disregarded the JSON format action definitions in our prompts,
resulting in a very low score on successful function calls. Therefore,
rendering them completely incapable of interacting with our environment.
To ensure fairness in comparison, we utilize OpenAI GPT-3.5 to extract
action into JSON-style function calls from the original CogAgent-Chat
responses, indicated as &quot;CogAgent-Chat (helped by GPT-3.5)&quot;. Even so,
its scores are significantly lower than those of LLaVA-1.5 and GPT-4V,
although CogAgent has been trained on Mind2Web web browsing simulation datasets.


FIGURE


*REF* displays the fine-grained scores of
predicted attribute values for each action within the successfully
parsed function calls. As can be seen, GPT-4V remains the best
performer, with action type prediction F1 score of 0.98. This implies
that it can accurately select appropriate mouse or keyboard actions.
Additionally, it can precisely choose the mouse action type, typing
text, or pressing keys consistent with the golden label actions.


The ability for precise positioning is crucial in computer-controlling
tasks. As indicated by the &quot;Mouse Position&quot; column in
*REF*, current VLMs have not yet achieved
the capability for precise positioning required for computer
manipulation. GPT-4V refuses to give precise coordinate results in its
answers, and two open-source models also fail to output the correct
coordinates with our pipeline prompt template.


Another significant challenge for all models is the reflection phase. In
this phase, the agent is required to determine whether the subtask has
been completed in the current state, and decide whether to proceed
further or make some adjustments. This is crucial for constructing a
continuous interactive process. Regrettably, all models show
insufficient accuracy in this determination, with GPT-4V achieving only
a 0.60 F1 score. This implies that human intervention is still necessary during task execution.


FIGURE


Fine-tuning Training


To demonstrate the potential for ongoing research on the task, we
continue to fine-tune the CogAgent-Chat model on our ScreenAgent
training set to enhance its function call ability. Similar to the
approach adopted in recent VLM works  *REF*, we mix data
from multiple datasets and construct four distinct training phases,
which is illustrated in *REF*. We reformulated two objective
detection datasets, COCO  *REF* and Widget Captions
*REF*, into mouse-click tasks to enhance the model&apos;s
localization ability. For Mind2Web, we implemented a series of complex
data augmentations to align with our task objectives. The details of the data construction are outlined in *REF*.


After vision fine-tuning, ScreenAgent achieved the same level of
following instructions and making function calls as GPT-4V on our
dataset, as shown in *REF*. In *REF*, ScreenAgent also reached a
comparable level to GPT-4V. Notably, our ScreenAgent far surpasses
existing models in the precision of mouse clicking. This indicates that
vision fine-tuning effectively enhances the model&apos;s precise positioning
capabilities. Additionally, we observed that ScreenAgent has a
significant gap compared to GPT-4V in terms of task planning,
highlighting GPT-4V&apos;s common-sense knowledge and task-planning abilities.


Case Study


To evaluate our ScreenAgent model on computer control tasks, we provide
two cases. In *REF*, we present a case illustrating the
workflow of ScreenAgent executing a chain of actions. In
*REF*, we compare different agents in
executing the details of the three phases in the pipeline.
*REF* (a) shows the planning process of all
the agents, where we find that our ScreenAgent produces the most concise
and effective plan. *REF* (b) presents four different click
action tasks, each representing a step in a specific task. Results show
that LLaVA clicks on the bottom-left corner on all screens, cogAgent may
fail to generate click positions, and in the fourth task, only our agent
can correctly click on the position.
*REF* (c) shows that our agent can recognize
whether an action needs to be re-tried after reflection and successfully
execute the action following a failure.


Conclusion


In this paper, we build a new environment for the screen control task.
VLM agents can manipulate a real computer through direct mouse and
keyboard control commands. To encourage the agent to continuously
interact with the environment and accomplish complex multi-step tasks,
we designed a control pipeline that includes planning, acting, and
reflecting phases. Furthermore, we unveil a new dataset that covers a
wide range of everyday digital works. We propose a fine-grained metric
to assess the agent&apos;s computer-controlling capabilities with both
action-level and task-level evaluation. We tested OpenAI GPT-4V and two
state-of-the-art VLM models on the test set. The results indicate that
GPT-4V has the potential to act as a computer-controlling agent, but it
lacks precise positioning capabilities. Finally, we trained the
ScreenAgent model, inherited from CogAgent, to achieve comparable scores
with GPT-4V but more accuracy in UI element localization. It is hoped
that our work will inspire further research in building more powerful
and generalized agents. In terms of technical limitations, due to the
input restrictions of VLM, our model can only process single-frame
images, not videos or multi-frame images. The VLM&apos;s language capability
is limited by the abilities of its foundation language model. We also
found that even GPT-4V has limited support for non-English text on the screen.


Ethical Statement 


The rational use of automated agents with autonomous decision-making
capabilities brings significant societal benefits, including improving
the accessibility of computers, reducing duplication of human effort on
digital work, and aiding in computer education. However, the potential
negative impacts of these agents, such as employment impact, fraud and
abuse, privacy issues, and the risk of misoperation, cannot be
overlooked. The method could potentially be used to bypass the CAPTCHA
test for automatic account registration, spreading misinformation, or
conducting illegal activities. We are focused on and committed to the responsible use of AI technology.