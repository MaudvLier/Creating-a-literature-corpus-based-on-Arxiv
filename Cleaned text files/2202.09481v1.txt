TransDreamer: Reinforcement Learning with Transformer World Models


Introduction


Model-based reinforcement learning (MBRL) [*REF*] provides a solution
for many problems of current reinforcement learning. Its
imagination-based training provides sample efficiency by fully
leveraging the interaction experience via the world
model [*REF*]. The world model can be considered a form of
task-agnostic general knowledge enabling reusability of the knowledge
about the environment in many downstream tasks [*REF*], and
finally, the dynamics model makes planning possible [*REF*; *REF*]
for accurate and safe decisions [*REF*; *REF*; *REF*].


Among the recent advances in MBRL, a particularly notable one is the
Dreamer agent [*REF*; *REF*]. Learning a world model in latent
representation space, Dreamer is the first visual MBRL model that
achieves performance and sample efficiency better than model-free
approaches such as Rainbow [*REF*] and IQN [*REF*].


To deal with partial observability [*REF*], the dynamics models in MBRL
have been implemented using recurrent neural networks
(RNNs) [*REF*; *REF*; *REF*; *REF*]. However,
Transformers [*REF*; *REF*] have shown to be more effective than
RNNs in many domains requiring long-term dependency and direct access to
memory for a form of memory-based reasoning [*REF*; *REF*].
Also, it has been shown that training complex policy networks based on
transformers using only rewards is difficult [*REF*], so learning a
transformer-based world model where the training signal is more diverse
may facilitate learning. Therefore, it is important to investigate how
to make an MBRL agent using a transformer-based world model and to
analyze the benefits and challenges in doing so.


In this paper, we propose a transformer-based MBRL agent, called
TransDreamer. As implied by the name, the proposed model inherits from
the Dreamer framework, but aims to bring the benefits of transformers
into it. Seemingly, it may seem like a simple plug-in task to replace an
RNN with a transformer. However, there are a few critical challenges to
make it work. First of all, we need to develop a new transformer-based
world model that supports effective stochastic action-conditioned
transitions in the latent space to implement the prior and posterior of
the transition model. At the same time, this model should also preserve
the parallel trainability of transformers for computational efficiency.
To the best of our knowledge, there is no such model yet. Also, as shown
in [*REF*], finding an architecture, hyperparameters, and other design
choices to make a working model is particularly challenging for
Transformer-based RL models.


The main contribution of this paper is the first transformer-based MBRL
agent. We introduce the Transformer State-Space Model (TSSM) as the
first transformer-based stochastic world model. Using this world model
in the Dreamer framework, we propose TransDreamer, a fully
transformer-based MBRL framework. In experiments, we show that
TransDreamer outperforms Dreamer on tasks that require long-term and
complex memory interactions, and the world model of TransDreamer is
better than Dreamer at predicting rewards and future frames for
imagination. Furthermore, we also show that the performance of
TransDreamer is comparable to Dreamer on a few simple DMC [*REF*]
and Atari [*REF*] tasks that do not require long-term memory.


Preliminaries


Our model builds on top of Dreamer [*REF*; *REF*], a model-based
reinforcement learning framework [*REF*] for visual control in a
partially observable Markov decision process (POMDP) [*REF*]. Dreamer
consists of three main steps: (1) world model learning, (2) policy
learning, and (3) environment interaction. These steps are cycled until
convergence. Specifically, a dynamics model of the environment (i.e.,
world model) and a reward function are learned to fit the data in an
experience replay buffer. Then, an actor-critic policy is trained on
imagined experience, i.e., hypothetical trajectories generated by
simulating the learned world model. Lastly, to provide fresh experiences
to the replay buffer, the agent collects trajectory data by executing
the trained policy in the real environment.


World Model in Dreamer


The backbone of the world model used in Dreamer is a stochastic
recurrent neural network, called the Recurrent State-Space Model (RSSM)
[*REF*]. Depicted in Figure [1], the RSSM represents a latent state *MATH* by the
concatenation of a stochastic state *MATH* and a deterministic state
*MATH* which are updated by *MATH* and
*MATH*, respectively. Here, *MATH* is an action
and the deterministic update *MATH* is the state update rule of a recurrent
neural network (RNN) such as an LSTM [*REF*] or GRU [*REF*]. While the
deterministic path helps to model the temporal dependency in the world
model, it also inherits the limitations of RNNs, particularly when
compared to the benefits of transformers. The stochastic state makes it
possible to capture the stochastic nature of the world, e.g., for
imagining multiple hypothetical future trajectories. Crucially, using
the above models, rollouts can be executed efficiently in a compact
latent space without the need to generate observation images.


Learning the RSSM is via the maximization of evidence lower
bound [*REF*]. The representation model
*MATH* infers the stochastic state given an
observation *MATH*. Whenever a new observation is provided, the current
state is updated by the representation model. The observation model
*MATH* and the reward model *MATH* are then used for the
reconstruction of observation *MATH* and reward *MATH* from the latent
state. All component models are listed in Table [1].


Policy Learning in Dreamer 


After updating the world model for a number of iterations, Dreamer
updates its policy *MATH*. The policy learning is done
without interaction with the actual environment; it uses imagined
trajectories obtained by simulating the learned world model.
Specifically, from each state *MATH* obtained from a batch sampled from
the replay buffer, it generates a future trajectory of length *MATH* using
the RSSM world model and the current policy as the behavior policy for
the imagination. Then, for each state in the trajectory, the rewards
*MATH* and the values *MATH* are estimated. This allows
us to compute the value estimate *MATH*, e.g., by the discounted sum
of the predicted rewards and the bootstrapped value *MATH* at the
end of the trajectory. See [*REF*] for more details and other options
about the value estimation.


Learning the policy in Dreamer means updating two models, the policy
*MATH* and the value model *MATH*. For updating the
policy, Dreamer uses the sum of the value estimates of the simulated
trajectories, *MATH*, to construct the
objective function. While we can compute the gradient of this objective
w.r.t. the parameters of the policy *MATH* via a policy gradient method
such as REINFORCE [*REF*], Dreamer also takes advantage of the
differentiability of the learned world model by directly backpropagating
from the value function to the world model, and to the parameters of the
policy network. This provides gradients of lower variance than that of
REINFORCE. Updating the value model parameter *MATH* is done via
temporal difference learning with the value estimate *MATH* as the
target.


TransDreamer


Transformers have been shown to outperform RNNs in many tasks in both
NLP and computer vision. In particular, their ability to directly access
historical states and to learn complex interactions among them, has been
shown to excel in tasks that require complex long-term temporal
dependencies such as memory-based reasoning [*REF*; *REF*].
Furthermore, they have been shown to be effective for temporal
generation in both language and visual domains. Observing that both of
these abilities are important and desirable properties for a robust
world model, we hypothesize that a model-based agent based on
transformers can outperform the RNN-based Dreamer agent for tasks
requiring complex and long-term memory dependency.


FIGURE


Transformer State Space Model (TSSM)


In the design of a transformer-based world model, we aim to achieve the
following desiderata: (i) to directly access past states, (ii) to update
the states of each time step in parallel during training, (iii) to be
able to roll out sequentially for trajectory imagination at test time,
and (iv) to be a stochastic latent variable model. To our knowledge, no
such model is available. The RSSM does not satisfy (i) and (ii).
Although we can consider a simple modification of the RSSM, a
memory-augmented RSSM, by introducing direct attention to the past
states in the RNN state update [*REF*] and make (i) satisfied as
well, it still does not satisfy (ii) and thus remains computationally
inefficient. Traditional transformers are deterministic and thus do not
satisfy (iv). This motivates us to introduce the Transformer State-Space
Model (TSSM).


The TSSM is a stochastic transformer-based state-space model.
Figure [3] illustrates the architectures of TSSM in
comparison to RSSM. In the RSSM, the main source of the
sequentially-dependent computation is the RNN-based state update
*MATH*, depicted in red arrows in Figure [1]. This
means that all component models of the RSSM are computed sequentially
because they all take the hidden state as input. To remove this
sequential computation and enable direct access to and complex
interaction of the historical states, we propose employing a transformer
as a replacement for the RNN. Unlike the RSSM which accesses the past
indirectly only via a compression *MATH*, the transformer is allowed
to directly access the sequence of stochastic states and actions of the
past at every time step, i.e., *MATH*. If all *MATH* can
be computed in parallel in this way, then all components taking *MATH* as
input can also be computed in parallel.


Myopic Representation Model. This hope, however, is broken due to
the representation model *MATH*. This is because unlike other
component models listed in Table [1], the output of the representation model is
used as the input to the transformer. Since a transformer should not use
an output also as an input to achieve parallel training, the
representation model should not be conditioned on *MATH*. That is, the
purple arrows in Figure [3] should be removed. To this end, we propose
approximating the posterior representation model by *MATH*,
removing *MATH*. Since the posterior can now be computed independently
for each time step, we can compute all of the inputs *MATH* 
simultaneously. Then, with a single forward pass through the
transformer, we can obtain *MATH*.


TABLE


One may argue that removing *MATH* and thereby ignoring all the history
*MATH* in the representation model may result in a poor
approximation. This can be true if we use only *MATH* as the full state
of our model. However, like the RSSM, the full state of our model is a
concatenation of the stochastic state *MATH* and the deterministic state
*MATH*. Therefore, we hypothesize that encoding temporal information in
the stochastic states may not be essential for model performance because
that information is provided by the deterministic states. Furthermore,
the trajectory imagination does not require the representation model but
only requires the stochastic state model *MATH* that can still use
*MATH*. We observe that if this hypothesis is correct, a modified Dreamer
using *MATH* instead of *MATH* would perform similarly
to the original Dreamer and the plot on the right confirms this. Another
possible yet more complex choice for the representation model is to
condition the posterior representation directly on all the past
observations using another transformer layer, i.e.,
*MATH*. However, due to its increased complexity, we do not consider this model.


Imagination. During imagination, we use the prior stochastic state
*MATH* as the input to the transformer to
autoregressively generate future states as shown in
Figure [7] in the Appendix. This allows the agent to imagine future states
completely in the latent space but with more direct access to the
historical states than the RSSM. In this way, the TSSM achieves all the
desiderata discussed above. Table [1]
highlights the key differences between the RSSM and the TSSM.


The loss function is almost the same as that of the RSSM. The
difference is that we approximate the representation posterior
*MATH* by *MATH* instead of *MATH* 
used in the RSSM. The loss function can be found in Appendix
[7.2] with the derivation of the ELBO.


Policy Learning and Implementation Details


Policy Learning. The policy learning in TransDreamer inherits the
general framework of Dreamer described in
Section [2.2]. The main difference is that we replace
the RSSM with the TSSM. The component models are thus based on the
states from the TSSM which can capture the long-term and complex
temporal dependency better. Since the TSSM is fully differentiable, we
can similarly use both REINFORCE and dynamics backpropagation to train
the policy. The TSSM parameters are held fixed when training the agent.


Training Stability. Transformers have notably had stability issues
when used in RL settings, especially in cases where the rewards are
sparse. GTrXL [*REF*], in particular, adds GRU gating layers to try to
alleviate this problem. In TransDreamer, however, since the transformer
parameters are held fixed during agent training and only trained to
predict images, rewards, and discounts, we find that we do not encounter
similar stability issues even without any additional gating.


Prioritized Replay. Since the training signal for agent training is
based on only the rewards the agent receives, the reward prediction in
the world model is especially important for learning a good agent.
Learning a good reward predictor, on the other hand, relies on the agent
having a good enough policy so that it can collect trajectories with
reward, especially in environments with sparse rewards. To facilitate
this, we optionally weight the replay buffer so that trajectories with
higher rewards are more likely to be sampled. In particular, we sample
only from nonzero-reward trajectories *MATH* -percentage of the time
where *MATH*. The remaining trajectories are sampled
uniformly across the replay buffer.


Number of Imagination Trajectories. Due to the increased memory
requirements of transformers compared with RNNs, we find that it is not
feasible to generate imagined trajectories from every state sampled from
the replay buffer, as is done in Dreamer. Instead, we randomly choose a
smaller subset of starting states of size *MATH* to generate imagined
trajectories from. While this effectively reduces the number of
trajectories the agent can learn from in any given iteration, we find
that we are still able to achieve performance comparable or better than
Dreamer.


Related Works


Transformers in RL. Transformers have been used successfully in
diverse domains including NLP
[*REF*; *REF*; *REF*; *REF*; *REF*], computer vision
[*REF*; *REF*; *REF*], and video generation
[*REF*; *REF*]. *REF* address the problem of using transformers in
RL and showed that adding gating layers on top of the transformers
layers can stabilize training. Subsequent works addressed the increased
computational load of using a transformer for an agent&apos;s policy
[*REF*; *REF*]. [*REF*; *REF*] take a different approach by
modeling the RL problem as a sequence modeling problem and use a
transformer to predict actions without additional networks for an actor
or critic. Several recent works also explore long-term video generation
with transformers which is related to building world models using
transformer-based architectures [*REF*; *REF*].


Stochastic Transformers. Stochasticity has been added to several
transformer-based architectures in the context of response generation
[*REF*], sign language translation [*REF*],
story completion [*REF*], and layout generation [*REF*].
[*REF*] introduce the Sequential Monte Carlo Transformer which adds
stochastic hidden states to the network architecture and outputs a
distribution of predictions allowing for uncertainty quantification. To
our knowledge, no previous work investigates stochastic transformers in
the context of world models and MBRL.


Model-based RL. Dyna [*REF*] introduced a general framework for MBRL
that our model is based on. SimPle [*REF*] adopts this framework by
making predictions at the pixel level and training a PPO agent on that
model. Our work mainly builds off of the Dreamer [*REF*; *REF*]
framework. The RSSM is first introduced in PlaNet [*REF*] where it is
used for planning in the latent space. [*REF*] use a VAE with an
RNN as the world model and learns a policy with an evolution strategy.
MuZero [*REF*] uses task-specific rewards to build a model and
Monte-Carlo Tree Search to solve RL tasks.


Experiments


In this section, we compare TransDreamer and Dreamer on a variety of
tasks, from tasks that require long-term memory and reasoning to tasks
that can be solved with only short-term memory. We try to answer the
following three questions: 1) How do TransDreamer and Dreamer perform in
tasks that require long-term memory and reasoning? 2) How do the learned
world models of TransDreamer and Dreamer compare? 3) Can TransDreamer
also work comparably to Dreamer in environments that require short-term
memory?


FIGURE


To answer the first question, we created a new set of tasks called
Hidden Order Discovery that is inspired by the Numpad task
[*REF*; *REF*]. We create both 2D and 3D versions of this
task. The 2D environment, built with the Minigrid [*REF*]
framework, provides a top-down view of the agent navigating a room while
the 3D environment, built with Unity [*REF*], provides a more partially
observable and visually rich first-person view of the agent. These tasks
require long-term memory and reasoning to solve. To answer the second
question, we thoroughly analyze the quality of the world model that is
learned in solving these tasks both quantitatively and qualitatively.
Lastly, to answer the third question, we compared TransDreamer and
Dreamer on some tasks in DeepMind Control Suite (DMC) [*REF*] and
Atari [*REF*]. These tasks are almost fully observable and do not
require long-term memory and reasoning to solve.


Hidden Order Discovery in 2D Object Room


To evaluate our model on the tasks that require long-term memory, we
created a new task called Hidden Order Discovery inspired by the NumPad
task [*REF*; *REF*]. In this task, there are several color
balls in a 2D grid, as shown in Fig. [4]. The agent
(illustrated by the red triangle) can only see the highlighted area in
front of it, so this is a partially observable environment. For each
episode, there is a hidden order of balls and the agent must collect the
balls in the correct order. If the agent fails, the map is reset, and
the agent needs to start from the first ball. Note that when the map is
reset, the agent position and the hidden order are not changed but all
the balls are reset to their initial positions. Therefore, to find the
hidden ball order efficiently, the agent can benefit from remembering
what it has tried in the past in the current episode. When a ball is
collected in the correct order, a reward of +3 is given, but if the map
is reset due to the agent collecting the incorrect ball, the rewards for
balls visited in previous tries are 0. This prevents the agent from
collecting a high reward from just constantly revisiting the first ball
in the sequence. When the agent successfully collects every ball in the
hidden order, the map and rewards are reset. The hidden order and ball
positions are randomized per episode.


We evaluate in environments with 4, 5, and 6 balls where the grid size
is 8x8 cells, and the agent is given 100 time steps to collect as much
reward as possible. The results are shown in Figure. We see that TransDreamer outperforms
Dreamer in all of these configurations. Since the reward for correctly
collecting one ball is +3, an average reward of 3 means that on average
the agent collects the first ball correctly, and an average reward of 6
means that on average the agent collects the first two balls correctly,
and so on. For the 4-Ball configuration, TransDreamer reaches an episode
reward of around 7 while Dreamer&apos;s episode reward is around 4. This
means that in the 4-Balls setting, TransDreamer averages collecting over
two balls in the correct order, whereas Dreamer only collects one ball
in the correct order.


To obtain further understanding beyond the averaged behavior, we also
measure the success rate of each agent, defined as the percentage of
trajectories where an agent collects all balls in the correct order at
least once. For the 4-Ball configuration, we find that TransDreamer has
a success ratio of 23% and Dreamer has a success ratio of only 7%,
providing further evidence that TransDreamer can better solve this task
than Dreamer. A full comparison of the success ratio is reported in
Appendix Table [2]. The difficulty of this task increases
as the number of balls increases since with more balls, there are more
combinations for the agent to try before determining the hidden order.
Thus, we see the performance for both degrade as the number of balls
increases.


We emphasize the difficulty of this task. Because the hidden order is
randomized in each episode, in the worst-case scenario, discovering the
first ball in the given order would require searching through all 4
balls. Then, determining the second ball in the sequence would require
searching among the remaining 3 balls, while always remembering what has
happened before in order not to waste time by visiting balls already
known to be incorrect. The higher scores for TransDreamer provides some
evidence that the transformer-based architecture is effective in tasks
that require this long-term memory and reasoning.


FIGURE


Hidden Order Discovery in 3D Object Room


To evaluate this task in a more realistic environment, we also
implemented a 3D version of Hidden Order Discovery in Unity [*REF*].
The reward structure is the same as the 2D task, but the agent view is a
3D first-person view. Figure [5] shows an overview view of the different
configurations and Figure [6] shows the agent&apos;s first-person view.
Compared to the 2D environment, since the environment is larger, it
takes more steps to navigate to the balls, especially in the sparse
setting. Therefore, with the 3D environment, we can evaluate how
TransDreamer can handle long-term dependency and complex reasoning more
clearly.


We implemented 3 settings for this task. The 4-Ball and 5-Ball Dense
environments have 4 and 5 balls, separated by at least one ball-length
each. The 4-Ball Sparse environment tests longer-term memory by
increasing the distance between balls to three ball-lengths so the agent
needs to navigate a longer distance between balls. The results are shown
in Figure. Even if the 3D environment provides more
severe partial observability and longer-term dependency than the 2D
environment due to its degree of freedom in exploring a larger
environment with a first-person view, we see that TransDreamer obtains
similar outperforming results as we obtained in the 2D Object Room.
Next, we compare the quality of the trajectories imagined by the TSSM
and the RSSM by measuring the generation performance quantitatively and
qualitatively.


World Model - Quantitative Results


We measure the Mean Squared Error (MSE) of the predicted images and the
reward prediction accuracy during the action-conditioned generation in
the 3D 5-Ball Dense configuration. Even though the image is not directly
used for policy training, the quality of the predicted image can serve
as a proxy for measuring latent state prediction accuracy. Reward
prediction accuracy, on the other hand, is directly related to policy
training, and may provide some insights into why TransDreamer performs
better than Dreamer in the above environments.


Image Generation. For a fair comparison, we separately trained the
TSSM and the RSSM with the same set of trajectories without any policy
training. Given a trajectory of 100 time steps, we measure the
generation quality for several different context lengths and measure the
per image MSE in the remaining generated steps. This allows us to
measure the generation quality given different amounts of historical
context and analyze how the different models utilize this context. The
reported MSE is for the foreground objects (i.e., the balls), since that
is where the most important information for this task is and more than
60% of the gap in overall MSE can be attributed to the foreground,
despite the balls only occupying a small portion of the image most of
the time (see Appendix Table for overall MSE results). The results
are shown in Table. We see that TransDreamer generally achieves lower
or comparable MSE when compared with Dreamer. As expected, more steps
given in the context results in lower MSE since the agents have more
opportunity to see the entire environment before making predictions. In
the 4-Ball Sparse setting, the MSE between TransDreamer and Dreamer are
very comparable. This may be because when the environment is sparse, the
agent sees the foreground objects less frequently.


Reward Prediction. Since the reward is zero for most time steps and
both models can predict zero reward time steps well (see Appendix Table), we focus on the reward
prediction accuracy for the nonzero +3 reward time steps. Since the
reward is a continuous value, in order to obtain an accuracy, we
classify rewards as positive if they are predicted in the range
*MATH*. The results are shown in Table. We once again see that TransDreamer
generally achieves more accurate reward prediction than Dreamer, with
longer contexts resulting in higher accuracy. For the 5-Ball Dense
environments, however, Dreamer&apos;s reward prediction does not improve much
as the context increases. This can indicate that Dreamer is not fully
taking advantage of the additional context when predicting rewards in
these more complex settings. TransDreamer, in contrast, does continue to
improve when given more context, showing that the transformer
architecture can take advantage of the increased context in making more
accurate predictions. A full version including zero-reward prediction
accuracy is reported in Appendix Table.


SUBTABLES


FIGURE


World Model - Qualitative Comparison


In Figure, we show the imagined trajectories from
TransDreamer and Dreamer in the 5-Ball Dense environment. We provide
context timesteps from each agent&apos;s trained policy up to when all the
balls are collected for the first time. After this, the agent and balls
reset to their original positions (frame 48 for TransDreamer and frame
58 for Dreamer). We then imagine the rest of the trajectory up to 100
total steps for each agent. Since the context timesteps contain
information about the correct order of balls, ideally the agent would
revisit the balls in this order during the imagination timesteps and
correctly predict the rewards when the balls are collected. Note that
the context frames for Dreamer and TransDreamer are different since they
are based on trajectories from their own policies. This is necessary
because the world model is trained from the trajectories of each agent&apos;s
policy. Providing context that is not from the agent&apos;s policy would not
necessarily be in the training distribution of the respective world
model. See Figure in the Appendix for an example of when
the same context is given to both agents. Despite this, however, we can
still see some clear differences between the quality of the imagination
steps as well as the reward predictions.


In particular, TransDreamer is able to predict the appearance of the
environment correctly as well as the collection and subsequent
disappearance of the balls in frames 54, 62, 73, 90, and 98. It also
accurately predicts the reward at these timesteps of around +3
(highlighted in red). For timesteps where there is no reward, it
correctly predicts a reward around 0. Dreamer, on the other hand,
predicts images that are blurrier than TransDreamer. Furthermore, the
imagined trajectories are incorrect. While it does predict the
collection of the red ball and the reward in frame 67, this color is
incorrect since the first ball should be purple. When it subsequently
predicts the collection of the purple ball in frame 78, it is again the
wrong color and no reward is predicted. This error seems to compound as
the dark green ball it predicts at the end of the trajectory is not even
one of the possible colors in the environment. This shows that the
quality of the world model is better in TransDreamer than Dreamer,
especially in the later steps of imagination where the long-term memory
is more important. This can be a reason why TransDreamer outperforms
Dreamer in these tasks.


Short-Term Memory Tasks in DMC and Atari 


As the final validation, we perform a sanity check by testing the
proposed model on a few simple DMC and Atari tasks. We note that it is
expected that these tasks may favor Dreamer over TransDreamer, because
solving these tasks does not require long-term and complex memory
interactions, but modeling the dynamics of just the last few steps can
suffice[^1]. Specifically, we expect that RNN-based models can learn
faster than transformer-based models because the former has the
specific inductive bias to focus on the near-term history. Nevertheless,
TransDreamer is supposed to converge eventually to an accuracy similar
to that of Dreamer, and it is an important step to see whether these
expectations are met.


We follow the configurations used by the authors in Dreamer [*REF*]
for DMC and DreamerV2 [*REF*] for Atari. The only difference is
that Dreamer uses the imagined trajectory from all states sampled from
the replay buffer, whereas TransDreamer uses a few randomly selected
states. Configurations for Transformer are described in Appendix
[7.3]. As shown in Fig., Dreamer and TransDreamer eventually reach
comparable performance as expected, but TransDreamer is slower to
saturate in general than Dreamer except for a few tasks. Interestingly,
TransDreamer shows slightly better performance and faster convergence
for the DMC Cheetah Run.


FIGURE


Conclusion


We proposed TransDreamer, a transformer-based MBRL agent, and the
Transformer State-Space Model (TSSM), the first transformer-based
stochastic world model. TransDreamer shows comparable performance with
Dreamer on DMC and Atari tasks that do not require long-term memory, and
outperforms Dreamer on Hidden Order Discovery tasks that require
long-term complex memory interactions. We also show that image
generation and reward prediction of TSSM is better than Dreamer
qualitatively and quantitatively. Future work may involve validating our
model on more complex tasks such as Crafter [*REF*].