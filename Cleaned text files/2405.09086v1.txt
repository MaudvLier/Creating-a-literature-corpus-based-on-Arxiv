Chaos-based reinforcement learning with TD3


Introduction


Neural networks (NNs) have been studied for many years, partially
inspired by findings in neuroscience research
[*REF*]. In recent years, the development of deep
learning techniques used to successfully train deep NNs has produced
remarkable results in various machine learning fields. The recent
progress in this field began with outstanding achievements in the study
of image recognition with NNs
[*REF*; *REF*; *REF*]. Recurrent
neural networks (RNNs) also performed well in time-series processing,
such as speech recognition and natural language processing
[*REF* _Speech; *REF*; *REF*; *REF*].
Vaswani et al. proposed the transformer, which is an innovative model
for natural language processing [*REF*]. The transformer has
provided a breakthrough in the capabilities of artificial intelligence
(AI) as the underlying technology for Large Language Models (LLMs)
[*REF*]. It has also demonstrated high performance in other areas
such as image processing and speech recognition
[*REF*; *REF*]. AI capabilities based on deep learning
techniques have developed quickly over the past decade, leading to
innovations in diverse fields.


In recent years, AI has demonstrated even high-quality creative
abilities [*REF*], and the creativity of AI has been attracting much
attention [*REF*; *REF*]. The
outputs of LLMs can be made more conservative or more creative by
modifying a probability distribution parameter called &quot;temperature\&quot;
[*REF*]. Historically, J. McCarthy et al. discussed the
difference between creative thinking and unimaginative competent
thinking in a proposal for the Dartmouth Workshop in 1956
[*REF*]. They raised the importance of randomness in AI and the
need to pursue how randomness can be effectively injected into machines,
just as the brain does. In reinforcement learning, a learning agent
performs exploratory action driven by random numbers to an environment
and improves its policy based on feedback from the environment.
Reinforcement learning is inspired by psychological findings and the
principle of reinforcement, which states that agents learn from their
actions and their consequences. Various studies on reinforcement
learning have been conducted over long years
[*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
In recent years, research on deep reinforcement learning, which
incorporates deep learning techniques, has become popular, and this
approach has made it possible to learn even more difficult tasks
[*REF*; *REF*; *REF*; *REF*; *REF*].
Deep reinforcement learning can also provide effective learning
performance in tasks such as Go and video games, where it is difficult
to specify the desired behavior designed by human hands
[*REF*; *REF*; *REF*]. Randomness enables AI systems to generate creative outputs and perform
exploratory learning.


This study focuses on the spontaneous and autonomous exploration of
organisms. Organisms can act spontaneously and autonomously in
environments with diverse dynamics, and can adapt to the environment
based on the experience gained through these behavior [*REF*].
Reinforcement learning agents stochastically explore the environment by
introducing random numbers to select actions that deviate from the
optimal action determined by the current policy. On the other hand, it
remains a fascinating unsolved problem to understand how the biological
brain behaves in various ways and realizes exploratory learning. One
hypothesis for the essential property of the source of exploration is to
utilize fluctuations within the brain. Various studies have shown that
fluctuations caused by spontaneous background activity in the neural
populations vary their responses [*REF*]. Briggman et
al. demonstrated that decision-making of escape behavior in leeches
depends on the activity of specific neurons that fluctuate under the
influence of neuronal population dynamics [*REF*]. Fox et
al. measured activity in the human motor cortex with fMRI while
performing a button-pressing task, and they found a correlation between
fluctuations in the BOLD signals and behavioral variability
[*REF*]. These studies suggest that fluctuations in neural
activity influence decision-making and produce a variety of behaviors.


Freeman pointed out the possibility that the brain uses chaotic dynamics
for exploratory learning [*REF*]. Skarda and Freeman observed the
EEG of the rabbit olfactory system and showed that there are many
chaotic attractors in the dynamics of the olfactory bulb that are
attracted when the rabbit is exposed to the known olfactory conditioned
stimulus [*REF*]. This study has also confirmed that when the rabbit
is exposed to an unknown odorant, chaotic dynamics that itinerate
between existing attractors emerge, followed by the reorganization of
existing attractors and the emergence of new attractors corresponding to
new stimuli. Freeman argues that the chaotic dynamics of the brain
continually generate new patterns of activity necessary to generate new
structures and that this underlies the ability of trial and error
problem solving [*REF*]. Aihara et al. discovered the chaotic
dynamics in the squid giant axon and constructed chaotic neural network
models based [*REF*; *REF*; *REF*],
and proposed Chaotic Simulated Annealing effective for combinatorial
optimization problems [*REF*]. Hoerzer et al. showed that a
reservoir network, which fixes the recurrent and input weights, can
acquire various functions using an exploratory learning algorithm based
on random noise [*REF*]. This study examined how generic neural
circuits that resemble a cortical column can acquire and maintain
computational functions, including memory attractors, through
biologically plausible learning rules rather than supervised learning.
The result suggests that stochasticity (trial-to-trial variability) of
neuronal response plays a crucial role in the learning process. It has
also been shown that the system&apos;s own chaotic dynamics can drive
exploration in this learning process
[*REF*; *REF*]. These studies have
implications for understanding how the brain achieves exploratory
learning and utilizes chaotic dynamics in the process.


Shibata et al. have proposed chaos-based reinforcement learning (CBRL),
which exploits the internal chaotic dynamics of the system for
exploration [*REF*]. This method uses an RNN as an agent system
and its internal chaotic dynamics as a source of exploration components.
Shibata et al. hypothesize that the system can acquire transient
dynamics with higher functions by developing its dynamics purposive
through exploratory learning. Various studies have viewed brain activity
as transitive or transient dynamics
[*REF*; *REF*; *REF*]. CBRL provides insights from a reinforcement learning perspective on
studies of the exploratory nature and the transient dynamics of the
brain. CBRL agents also have the advantage that their exploration can
autonomously subside or resume depending on the situation. It has been
suggested that CBRL agents can acquire higher exploration strategies
that reflect their learned behavior [*REF*]. CBRL offers a new
perspective on both understanding the brain&apos;s exploratory learning
mechanisms and novel learning algorithms for AI.


The algorithms used in CBRL require treating deterministic and
continuous action to eliminate stochastic exploration and human-designed
action selection. Due to the necessity of a reinforcement learning
algorithm that can handle deterministic and continuous actions without
requiring random noise, Shibata et al. proposed causality trace learning
and used it to train CBRL agents
[*REF*; *REF*; *REF*; *REF*]. However, this method
has limited performance, and the CBRL algorithm is not yet well
established. This study introduces the Twin Delayed Deep Deterministic
Policy Gradient (TD3) algorithm, which is one of the state-of-the-art
deep reinforcement learning algorithms designed for handling
deterministic and continuous actions. The ability of the agent is
evaluated by learning a simple goal task. We also examine how CBRL
agents trained using TD3 respond to environmental changes. Furthermore,
we investigate how the behavior of the agents changes depending on the
levels of chaoticity in the model&apos;s dynamics and how this affects their
learning performance and ability to adapt to environmental changes.


This paper is organized as follows. Section 2 summarizes chaos-based
reinforcement learning. Section 3 describes the experimental method.
Section 4 presents the results of the experiments. Section 5 summarizes
the conclusions of this study and discusses future research directions.


FIGURE


Chaos-based reinforcement learning


Exploration driven by internal chaotic dynamics


In the Chaos-based reinforcement learning method, the agent explores
using internal chaotic dynamics. Figure [1] shows an overview of regular reinforcement learning and chaos-based
reinforcement learning (CBRL). In general, exploration in reinforcement
learning is performed probabilistically based on external random
numbers. The *MATH* -greedy method selects a random action with a
probability of *MATH*. The softmax policy selects an action
stochastically using random numbers from a Boltzmann distribution of the
state action values. Some algorithms for continuous action spaces
explore by adding random noise to the action outputs for exploration
purpose [*REF*]. On the other hand, in CBRL, exploration is driven by
chaotic dynamics within the agent system rather than relying on external
random numbers.


A structure in which internal dynamics serves as the basis for
exploration resources is plausible as a model of the biological brain
and offers several learning benefits. As shown in Fig.
[1](b), the source of exploration (i.e.,
chaotic dynamics) is a property of the agent system itself in CBRL. The
properties of the system dynamics can be changed by learning. Therefore,
the agent has the potential to optimize its exploration strategy through
training. Previous studies have shown that CBRL agents can autonomously
switch from an exploration mode to an exploitation mode as learning
progresses [*REF*; *REF*]. In the initial training
episodes, when learning has not progressed sufficiently, the agent
behaves chaotically and exploratively in the environment. As learning
progresses, the system&apos;s dynamics becomes more ordered, and the agent&apos;s
exploratory behavior subsides autonomously. Additionally, the agent can
autonomously resume exploration when the environment changes its rules
and the previously learned behavior is no longer rewarding.


Expectation for CBRL


Shibata et al. hypothesized that the key to creating systems with higher
exploratory behavior and human-like intelligence is having a dynamical
system with rich and spontaneous activity [*REF*]. The dynamics
of brain activity can be considered as a chaotic itinerancy
[*REF*] or as a process that transits through a series of
saddle points [*REF*]. The hypothesis expects the
system to reconstruct its exploratory chaotic dynamics into transient
dynamics that is purposeful to maximize rewards.


Since internal dynamics drives exploration, CBRL agents are also
expected to be able to optimize exploration itself through learning.
Goto et al. demonstrated that CBRL agents can change motor noise-like
exploration into more sophisticated exploration that selects routes to
avoid obstacles using an obstacle avoidance task [*REF*; *REF*].
It is expected that CBRL agents can acquire more advanced exploration
capabilities that effectively utilize previously acquired behaviors by
constructing the agent&apos;s internal state and decision-making processes as
transitive dynamics such as chaotic itinerancy.


Issue of learning algorithm for CBRL


To ensure freedom and autonomy in learning, CBRL agents have used
algorithms that deal with deterministic and continuous action spaces
rather than stochastic and discrete ones in which action selections are
defined heteronomously. In previous studies, the learning algorithm for
CBRL agents has been an Actor-Critic, which can handle deterministic and
continuous action spaces [*REF*]. However, a classic Actor-Critic
method that trains an actor using a correlation between the external
exploration noise and the resulting change in value cannot be employed
for CBRL, which does not use random number exploration. Therefore,
causality trace learning, which is similar to Hebbian learning, has been
proposed and employed as the training method for the actor-network
[*REF*; *REF*; *REF*; *REF*]. This method generates
the teacher signal for the actor-network by multiplying the TD error by
the input values stored in a tracing mechanism that changes the input
tracing rate according to changes in the neuron&apos;s output. However, this
method has many problems, such as the inability to introduce the
backpropagation method and the difficulty of learning complex tasks.
Therefore, the learning algorithm for CBRL has not yet been sufficiently
well-established.


Method


TD3


This study introduces Twin Delayed Deep Deterministic Policy Gradients
(TD3) [*REF*], a deep reinforcement learning algorithm that can handle
deterministic policy and continuous action spaces, to CBRL. TD3 is an
improved algorithm based on the model-free deep reinforcement learning
method called &quot;Deep Deterministic Policy Gradients\&quot; (DDPG) [*REF*], and
is one of the state-of-the-art reinforcement learning algorithms[*REF*].


In the following part of this subsection, we first describe the DDPG and
then the improvements introduced in TD3. In the DDPG, the agent model
consists of an actor-network *MATH* and a critic network
*MATH*. Here, *MATH* and *MATH* are the weight
value parameters of each network. *MATH* determines the
action output *MATH* based on the agent&apos;s state *MATH* and *MATH* 
estimates the state action value from *MATH* and *MATH*. Target networks
*MATH* and *MATH* are generated for
each network with their weights copied and are used to generate the
teacher signals to stabilize learning.


The agent acts in state *MATH* at each time *MATH* according to the
following action output, to which the external exploration noise
*MATH* is added as follows: *MATH*. As a result of the action, the agent receives the reward
*MATH*, and the state transitions to *MATH*. The experience
*MATH* is stored in the replay buffer *MATH*.


Training is performed using *MATH* minibatch data of
*MATH* randomly sampled from *MATH* every
step. Note that *MATH* indicates the index number of the samples in the *MATH* 
minibatch data. The teacher signal for the Critic network *MATH* for the
input data *MATH* and *MATH* is estimated as follows: *MATH* where the discount rate *MATH* is a
hyperparameter that determines the present value of future rewards. We
then update *MATH* to minimize the loss function such that
*MATH*. The actor-network *MATH* learns with deterministic policy
gradient [*REF*] estimated based on the sampled data: *MATH* where *MATH*. Note that
*MATH* is fixed and only *MATH* is updated for training
based on Equation. The target network weights *MATH* and
*MATH* are updated as follows: *MATH*  where *MATH* is the constant parameter that
determines the update speed of the target network.


TD3 introduces three methods to the DDPG algorithm: Clipped Double
Q-learning, Delayed Policy Updates, and Target Policy Smoothing. Clipped
Double Q-learning is a method to suppress overestimaton of the value by
preparing two Critic networks *MATH* and *MATH* and adopting the smaller
output value when generating teacher signals. Target Policy Smoothing is
a technique to suppress the overfitting of *MATH* to inaccurate value
estimations by adding noise limited between *MATH* and *MATH* to the output
of *MATH* during the generation of the teacher signal. With these
techniques, the teacher signal for *MATH* is given by *MATH*. Note that the learning of
*MATH* by equation always uses *MATH*. Therefore, the training is
based on the following gradient *MATH*. Delayed Policy Updates stabilizes learning by limiting
updates of *MATH*, *MATH* and *MATH* to once every *MATH* steps.


Reservoir network


CBRL requires the agent system to have chaotic dynamics. RNNs are
dynamical systems and appropriate models for CBRL agents. However,
training RNNs with CBRL presents a challenge in balancing the formation
of convergent dynamics beneficial for completing the task while
maintaining the chaotic dynamics required for exploration. To avoid this
problem, we use a reservoir network (RN) that can be trained without
modifying the parameters that determine the dynamical properties of the
recurrent layer. Maass and Jaeger have proposed RNs as a Liquid State
Machine (LSM) and an Echo State Network (ESN), respectively
[*REF*; *REF*]. In this study, we use an ESN composed
of rate model neurons. The ESN consists of a recurrent layer called the
&quot;reservoir\&quot; and an output layer called the &quot;readout.\&quot; The connection
weights in the reservoir are randomly initialized and fixed, and only
the weights from the reservoir to the readout are trained. The reservoir
receives the time-series input and generates an output that nonlinearly
reflects the spatio-temporal context of the input series. The readout
generates the network output by performing a linear combination of the
reservoir output and input. The dynamical properties of the ESN can be
modified with a single fixed parameter. Therefore, it is easy to tune
the chaoticity of the system during learning with chaos-based
exploration [*REF*]. Note that, in this study, the ESN is
not used to process time series data but rather to generate chaotic
dynamics in the agent system.


FIGURE


The structure of the ESN is shown in Fig. [2]. The reservoir has
*MATH* neurons that are recurrently connected with
*MATH* with probability of *MATH*. The reservoir receives *MATH* -dimensional
inputs through the weight matrix
*MATH*. Then, the reservoir output is computed by the following equation: *MATH* 
where *MATH* is a scaling parameter that scales the strength of the
recurrent connections and *MATH* is the
input vector. *MATH* is the activation function applied
element-wise. Typically, *MATH* is a sparse and fixed
weight matrix initialized with a spectral radius of 1 by the following
procedure. A random *MATH* matrix *MATH* is
generated (from a uniform distribution in this study), and then the
elements of *MATH* are set to *MATH* with probability of *MATH*. The
matrix is normalized by its spectral radius *MATH*. Thus, the
*MATH* is initialized as follows
*MATH*. The arbitrary constant
*MATH* can rescale the spectral radius of *MATH*. In general,
*MATH* is usually set to *MATH* to fulfill the Echo State Property that
requires the reservoir state to depend on past input series, but the
influence of these inputs fades over finite time. A small *MATH* tends to
cause rapid decay of input context, while a large *MATH* tends to cause a
slow decay of it. However, *MATH* sometimes achieves better performance,
and it is argued that the best value for learning is realized when the
reservoir dynamics is around the edge of chaos
[*REF*; *REF*]. When *MATH* is further increased beyond
*MATH*, and the reservoir dynamics crosses the edge of chaos, the reservoir
state exhibits chaotic and spontaneous behavior. In this study, *MATH* is
set larger than *MATH* to induce chaotic dynamics, which allows the CBRL
agent to explore.


The network output *MATH* is calculated
as *MATH* where *MATH* 
is the output weight matrix, *MATH* is the output dimension, and
*MATH* denotes the concatenation of two column vectors.
*MATH* is often fitted using algorithms such as ridge
regression in reservoir computing. This study uses a reservoir network
as an actor-network for CBRL agents. Since *MATH* is
trained by a slightly modified version of the TD3 algorithm as described
in the folloing subsection, *MATH* is updated by using
gradient descent with the Adam optimizer [*REF*].


FIGURE


TD3-CBRL


The following modifications are made to the TD3 algorithm to adapt to
the CBRL approach. We eliminated the random number exploration noise
*MATH* and *MATH*. Instead of exploration by random numbers,
we rely on exploration driven spontaneously by chaotic dynamics and use
an ESN with a larger spectral radius, as shown in Fig. [3],
for the *MATH* network. Here, *MATH* unless otherwise mentioned. We also
add the reservoir output to the experience stored in the replay buffer.
That is, *MATH* is stored into the replay buffer *MATH*. The agent learns without
the Back Propagation Through Time method using the stored *MATH* and
*MATH* as state *MATH*. This method has been employed in several
studies and efficiently trains deep reinforcement learning agents using
ESN [*REF*; *REF*]. TD3 uses random numbers to randomly
sample past experiences from the replay buffer. Since this sampling has
an exploration-like effect with the random numbers, the replay buffer
and batch sizes are set to the same value of *MATH* to eliminate this
effect. Therefore, the training is based on the memory of the past *MATH* 
steps of experience. The variant of CBRL that introduces the above
modified TD3 algorithm is called TD3-CBRL in this study. In the
experiment, we compare the three cases shown in Fig.[3](a-c) to confirm 
that the ESN dynamics contributes to the exploration during learning.


FIGURE


Goal task


This study uses a goal task to estimate the agent&apos;s learning ability.
Figure [4] shows the task outline. The task field is
inside the range *MATH* to *MATH* on the x-y plane. There are *MATH* initial
positions *MATH* in the environment.
At the beginning of an episode, the agent is randomly placed at one of
the initial positions, with Gaussian noise *MATH* added to
its position in the field. The agent obtains an input *MATH* based on
its position *MATH* and uses it to determine
its action outputs *MATH* and *MATH*. As a
result of the action, the agent&apos;s position is updated according to the
following equation *MATH* where the agent cannot leave the task field and its
range of movement is limited to *MATH* and
*MATH*. When the agent is at *MATH*, the input *MATH* from the
environment is given by *MATH* where *MATH* is the Euclidean distance between
the center of the agent and the goal, and *MATH* is the length of the
diagonal of the task field. This task is episodic, and an episode ends
when the agent either enters the goal circle of radius *MATH* (i.e.,
*MATH*) or fails to reach the goal within *MATH* steps. The
agent receives a reward of *MATH* for reaching the goal, *MATH* for
colliding with the wall surrounding the field, and *MATH* for any other step.


We also examine a goal change task to estimate the ability to resume
exploration and re-learn when the environment changes. This task
initially places the goal at the initial position
*MATH*, but changes the position to
*MATH* when the number of steps reaches
*MATH*. After the goal change, the agent can no longer receive
rewards for the behavior it has learned previously, and it needs to
resume exploration and learn to reach the new goal to receive rewards
again.


Experiment


Conditions


The reservoir size was set to *MATH* and the recurrent connection
probability was set to *MATH*. The input weight matrix *MATH* 
was sampled from a uniform distribution over *MATH*. The critic
network is a fully connected Multilayer Perceptron (MLP) with two hidden
layers consisting of *MATH* ReLU nodes, and the output neuron is a linear
node. The initial weights were set using the default settings in PyTorch
(version 1.13.1 in this study). Both the critic *MATH* and the actor *MATH* 
are trained using the Adam optimizer, with a learning rate of *MATH*.
We paused training every *MATH* steps and verified the agent&apos;s behavior
starting from 8 initial positions *MATH*, *MATH*, *MATH*,
*MATH*, *MATH*, *MATH*, *MATH*, *MATH* and slightly
different positions (shifted *MATH* in the *MATH* and *MATH* axes,
respectively). The replay buffer size and batch size were both set to
64. The discount rate *MATH* was set to 0.95. The time constant *MATH* 
of the target network was set to 0.05. We set
*MATH*, and the Delayed Policy Updates step to *MATH*.


FIGURE


Learning result


The TD3-CBRL agent learned the goal task in *MATH* steps. Figure
[5](a) shows the learning curve resulting from the test conducted every *MATH* step.
This figure shows that the average number of steps required by the agent
to reach the goal decreases as the learning progresses. This result
indicates that TD3-CBRL can successfully learn the goal task. Figure
[6](a) shows the trajectories of the agent&apos;s movement in the environment for each test
trial. This figure shows that in the initial stage of learning (*MATH* 
step), the agent acts exploratively in the environment. On the other
hand, in the trajectories after *MATH* steps, the agent is moving toward
the goal from each initial position, although it takes detours in some
cases. We also see that the exploratory behavior subsides as the
learning progresses. It is important to note that no external random
noises for exploration were added to the agent&apos;s action output during
the learning process. This result indicates that the internal dynamics
of the reservoir caused spontaneous exploratory behavior, and as
learning progressed, such variability in the output was autonomously
suppressed.


FIGURE


As seen in the purple trajectory where the agent starts from the initial
position *MATH* in Fig. [6](a), the agent wandered to the left around the goal
before reaching it at the *MATH* step, even though the agent had
already learned to reach the goal in a straight path by the *MATH* 
step. This result seems to be caused by the chaoticity of the agent&apos;s
dynamics, which causes its behavior to change sensitively depending on
the initial position and parameter changes. Figure
[7](a) shows the results of investigating the sensitivity of the agent&apos;s
behavior to variations of the initial positions in this case. This
figure shows the trajectories of agents starting from slightly different
initial positions. At the *MATH* step, the agent, starting from its
original initial position, continues toward the wall and ends the
episode without reaching the goal. On the other hand, although the agent
starting from the slightly shifted position behaves like the agent
starting from the original initial position initially, it leaves the
original trajectory after a while and eventually reaches the goal. This
result indicates that in the early phases of learning, a slight
difference in the initial position can significantly change the agent&apos;s
behavior and even determine the task&apos;s success or failure. At the *MATH* 
step, the two agents eventually reach the goal, but their paths diverge
from the middle of the episode. The agent, starting from the original
initial position, explores both the left and right sides before reaching
the goal. On the other hand, the agent starting from a slightly
different point did not explore the right side of the goal; instead, it
went to the goal. Thus, the sensitivity to the initial condition of the
TD3-CBRL agent allows it to exhibit both exploration driven by the
chaoticity and exploitation of its previously learned behavior.


FIGURE


Effects by presence or absence of exploration component


To confirm that the chaoticity of the model contributes to the agent&apos;s
exploration, we evaluated the learning performance when the model does
not have chaotic dynamics. Specifically, instead of an actor-network
with a chaotic reservoir, we used a Multilayer Perceptron (MLP) with one
hidden layer consisting of 256 *MATH* neurons, as shown in Fig.
[3](b). In this case, the learning rate was readjusted to *MATH*. Figure
[5](b) shows the learning curve for the MLP case without exploration random noise. This
figure shows that the number of steps to the goal did not decrease and
that the agent failed to learn the goal task. Figure
[6](b) and Fig.[7](b) show the agent trajectory during the test phase and the sensitivity
test under this condition. These figures show that the agent lacked
exploratory behavior and failed to learn. This result appears to be due
to the fact that the learning model did not have the dynamics to
generate spontaneous activity, and thus, the exploration did not occur
without random noises.


To clarify that the absence of exploration random noise is the direct
cause of the learning failure, we verified the case where the MLP is
trained with random noise for exploration in the same way as in the
regular TD3, as shown in Fig. [3](c). Figure [5](c) shows the learning curve under this condition.
This figure shows that the number of steps to reach the goal decreased
and that the agent succeeded in learning the task. Figure
[6](c) shows the trajectories of the agent&apos;s behavior in this validation. This figure
shows that the agent can learn the behavior of moving toward the goal
due to exploration with random numbers during the learning process. Note
that during the test phase, adding random numbers to the action outputs
is stopped. These results indicate that the presence or absence of
exploration by random numbers has a significant influence on the success
or failure of learning and that the TD3-CBRL agent successfully learns
the goal task through exploration driven by the chaotic dynamics of the
reservoir.


Comparing (a) and (c) in Fig. [6], the regular TD3 agent can go to the goal in a
straighter path than the TD3-CBRL agent. However, this is due to an
external intervention that removes the random noise during the test
phase. Figure [6](d) shows the result when the agent in (c) acts with the exploration random
noise during the test phase. If the exploration random noise is not
eliminated, the agent cannot reach the goal in a straight path. On the
other hand, the TD3-CBRL agent can autonomously reduce the variability
driven by its chaoticity as its learning progresses, although there are
still cases where it takes detours depending on its initial position.
This is a beneficial property and a limitation of the CBRL agent.


FIGURE


Goal change task. 


Previous studies have shown that when the environment changes and the
agent cannot be rewarded with previously learned behaviors, CBRL agents
can autonomously resume their exploration and learn to adapt to the new
environment [*REF*]. These results suggest that CBRL agents have
the flexibility to adapt to dynamic and uncertain environments. Here, we
observed the agent&apos;s response to changing the goal position to test
whether TD3-CBRL agents can re-learn when the task rule changes.


In the goal change task, the goal was initially placed at
*MATH*, and learning was performed for *MATH* 
steps. Then, at the *MATH* step, the goal position was
changed to *MATH*, and learning continued in the new
environment for another *MATH* steps. The test is conducted every
*MATH* step. The learning curve under these conditions is shown in
Fig. [8](a). This figure shows that when the goal
position is changed, the number of steps required to reach the goal,
which had decreased during learning in the initial environment,
temporarily increases. However, as learning in the new environment
progresses, the number of steps decreases again. This result indicates
that the agent is adapting to the new environment.


The trajectories of the agent in the environment under these conditions
are shown in Fig. [9](a). This figure shows that at the
*MATH* steps, the agent autonomously resumes its exploration with
internal chaotic dynamics due to the change in the goal position. After
that, the agent gradually adapts its behavior to move toward the new
goal.


FIGURE


Learning performance and chaoticity


We investigated the effect of the chaoticity of the system on learning
performance. The chaoticity of the reservoir network can be tuned by
changing the spectral radius of the reservoir&apos;s recurrent weight matrix
using the parameter *MATH*. We changed the value of *MATH* from *MATH* to *MATH* in
*MATH* increments and conducted trials with 100 different random number
seeds under each condition to obtain the learning success probability
and the average number of steps to reach the goal. Here, success is
defined as an event when the agent reaches the goal from all *MATH* 
initial positions during the final test. The results are shown in Fig.
[10](a). This figure shows that learning performance begins to improve as *MATH* 
exceeds around *MATH*, and learning becomes successful in most cases when
*MATH* exceeds approximately *MATH*. This result indicates that the chaoticity
of the system is essential for successful exploration and learning. The
results also indicate that the probability of success remains high even
when the value of *MATH* is further increased. In general, there is an
appropriate range for the parameter *MATH* for time series processing in
reservoir computing. It can be considered that since this study does not
target tasks that require memory and the task is simple, the success
probability of learning by TD3-CBRL remains stable even when *MATH* is
larger than the typical critical value.


FIGURE


We examined how the flexibility of switching between exploration and
exploitation was affected by the value of *MATH*. The value of *MATH* was
changed from *MATH* to *MATH* in *MATH* increments, and a goal change task was
learned under *MATH* different random number seeds to ensure statistical
validity. The task settings are the same as in Section
[4.4] except for *MATH*. The results are shown
in Fig. [10](b). This figure shows that performance on the
goal change task decreases with an extremely large *MATH*. This result
suggests that when *MATH* is extremely large, the balance between
exploration and exploitation is lost, and the flexibility to deal with
environmental changes is reduced. These results indicate that choosing
an appropriate *MATH* value is still essential in CBRL using reservoir
networks.


We experimented with long-term re-learning to investigate whether a
larger *MATH* decreases the model&apos;s re-learning ability or increases the
number of steps required for re-learning. Specifically, after changing
the goal position at the *MATH* step, the agent learned
for *MATH* steps in the new environment. The results are shown in Fig.
[10](c). This figure shows that long-term learning mitigated the decrease in
learning performance when *MATH* is large. This result indicates that a
model with a larger *MATH* and stronger chaoticity requires more
re-learning steps, consequently making re-learning more difficult.


Readout weights and chaoticity


To clarify the reason for the increase in steps required for re-learning
when *MATH* is extremely large, we investigated how the acquired readout
weights vary with the spectral radius *MATH*, which adjusts the chaoticity
of the reservoir. The inputs for the readout are the reservoir output
and the agent&apos;s state in the environment. They are received through 256
and 5 weights, respectively. Figure [11] shows the readout weights after
training for spectral radius *MATH* of *MATH* and *MATH*. The blue dots show
the weights from the reservoir, and the red dots show the bypass
weights. Comparing these figures, we can see that the bypass weights are
larger for the case of *MATH* than for the case of *MATH*, and the
weights from the reservoir are relatively smaller than the bypass
weights. Figure [12] shows how the average of the absolute
values of the weights from the reservoir and the bypass weights change
when the spectral radius is varied. This figure shows that the bypass
weights increase as the spectral radius increases. This result suggests
that the agent ignores the reservoir outputs and focuses more on the
state inputs directly given from the environment.


FIGURE


FIGURE


FIGURE


We have validated the goal change task with *MATH*. Figure
[8](b) shows the results of this validation.
Comparing this with the result under the setting of *MATH* shown in
Fig. [8](a), re-learning convergence was slower
when *MATH*. Figure [9](b) shows the agent&apos;s behavior in the
environment at *MATH*. Comparing Fig. [9](a) and (b), we can see that in the case
of *MATH*, the agent continues its exploratory behavior for more steps
and takes more steps to shift to an exploitation mode. This slow
re-learning seems to be because more updates are required to modify the
larger bypass weights to adapt them to the new environment when the
spectral radius is large. To clarify this, we analyzed the change in the
bypass weights in the re-learning task, and Fig. [13] shows the results. Comparing the two
results, the bypass weight change in the case of *MATH* is significantly
larger than in the case of *MATH*. In the case of *MATH*, the bypass
weights increased to attend to state inputs, ignoring the reservoir&apos;s
chaoticity, and thus, more steps are required. These results confirm
that the inflated bypass weights inhibit flexible switching between
exploration and exploitation. These results indicate that an excessive
spectral radius and agent chaoticity impair the ability of TD3-CBRL
agents to properly balance exploration and exploitation properly.


Exploration with random number layer


The above verification confirmed that learning proceeds to ignore the
reservoir&apos;s output if the reservoir is too chaotic. To verify this
result further, we investigated learning when a random vector of
independent and identically distributed random numbers replaces the
reservoir. Specifically, we replaced the reservoir with a uniform random
vector sampled from *MATH* and conducted learning under this
condition. In this setting, the random vector serves as the exploration
component but is worthless as information about the agent&apos;s state.
Figure [14] shows the results of this
experiment. This figure shows that the agent succeeded in learning the
task even when a random number vector replaces the reservoir, although
the number of steps required for learning tends to increase. The
learning results for the readout weights under this situation show that
the bypass weights are significantly larger than the weights from the
random layer. This result seems to be because the random layer&apos;s output
is worthless for accomplishing the task and suggests that a similar
phenomenon occurs when the spectral radius of the reservoir is too high.


We experimented with a goal change task to verify whether the
exploration with a random layer is flexible enough to adapt to
environmental changes. The task settings are the same as in Section
[4.4]. Figure [15] shows the
experimental results under these conditions. This figure shows that the
random layer model failed to learn the goal change task. This result
indicates that the model with the random layer cannot flexibly switch
between exploration and exploitation.


FIGURE 


FIGURE


Conclusion


This study introduced TD3 as a learning algorithm for chaos-based
reinforcement learning (CBRL) and revealed several characteristics.


It was confirmed that the TD3-CBRL agent can learn the goal task. This
suggests that TD3 is a good candidate as a learning algorithm for CBRL,
which had not been well-established in previous studies. Although a
regular TD3 agent succeeded in learning the goal task, the agent failed
to learn it without external exploration noise, despite the simplicity
of the task. On the other hand, the TD3-CBRL agent, whose actor model
has chaotic dynamics, can learn without exploration with random noises.
This comparison indicates that the internal chaotic dynamics of the
reservoir contributes to the agent&apos;s exploratory behavior.


We also observed the agent&apos;s adaptation to the environmental change of
altering the goal position during the learning process and found that
the agent resumed chaotic exploration and successfully re-learned.
Furthermore, our results indicate that there is an appropriate range for
the strength of internal chaos for the TD3-CBRL agent to have the
flexibility to autonomously switch between exploration and exploitation
modes. In particular, it was found that when the spectral radius is too
large, the weights from the bypass become excessively large. Then, the
inflated weights negatively affect the flexibility of switching between
exploration and exploitation. These results indicate that the
appropriate choice of spectral radius is essential in designing
reservoir networks in CBRL to achieve a proper balance between
exploration and exploitation.


We tested the TD3-CBRL agent when the reservoir network, the source of
the exploration dynamics, was replaced by a random vector. The results
showed that the agent could learn the goal task, but the bypass weights
became significantly large, and the agent failed to learn the goal
change task. These results indicate that the reservoir, which generates
the dynamics in CBRL, does not act just as a mere noise source. While
the random vectors do not reflect any external input, the reservoir
creates a variety of background activities in the system incorporating
the input&apos;s influence. This activity seems to provide the variability
necessary for exploration and to be a resource that drives new
exploratory behavior when encountering unknown experiences.


Various studies have shown that neural populations operate in a critical
state [*REF*; *REF*; *REF*; *REF*].
Reservoir network performance has also been shown to be optimized at the
edges of chaos [*REF*; *REF*]. In addition, a study in
which ESNs performed chaos-based exploration learning also showed
results suggesting that exploration and exploitation are balanced at the
edges of chaos [*REF*]. In this study, the spectral
radius was not optimal around *MATH*, where the reservoir dynamics is
typically on the edge of chaos. One hypothesis for explaining this
result is that the spectral radius of the subsystem, the reservoir,
needed to be larger to place the entire system&apos;s dynamics, including the
environment, at the edge of chaos. It is important to verify this by
focusing on the entire system&apos;s behavior, including the interaction
between the CBRL agent and the environment.


This study introduced TD3 to CBRL and validated its ability with a
simple task to investigate learning and re-learning in detail. In
previous studies and this one, CBRL has yet to learn difficult benchmark
tasks used for deep reinforcement learning research, such as Atari game
tasks and nonlinear control tasks using MUJOCO. In future works,
improving the model to learn more difficult tasks is necessary. A wider
selection of tasks will allow us to examine CBRL from newer
perspectives.


Introducing the new reservoir structure proposed to extend its
performance is useful to improve the learning performance of CBRL.
Reservoirs do not perform well with high-dimensional input such as
images. Several studies have proposed methods that use untrained CNNs
for feature extraction [*REF*; *REF*].
Introducing these methods can enable CBRL agents to learn tasks with
high-dimensional input, such as raw images. Structural improvement that
constructs reservoirs in multiple layers
[*REF*; *REF*; *REF*] and methods that
reduce the model size by using multi-step reservoir outputs as input to
readouts [*REF*] have been proposed. It is worth verifying
the use of these new reservoir techniques to improve the performance of
CBRL agents.


Improvements in learning algorithms are also worth considering.
Sensitivity adjustment learning (SAL), which adjusts the chaoticity of
neurons based on \&quot;sensitivity,\&quot; has been proposed [*REF*].
This method can modify the recurrent weights while keeping the
chaoticity of the RNN. Using SAL to maintain chaoticity and learning
with Backpropagation Through Time may allow CBRL agents to learn more
difficult tasks. Self-modulated reservoir computing (SM-RC) that extends
the reservoir network&apos;s ability by dynamically changing the
characteristics of the reservoir and attention to the input through a
self-modulated function has been proposed [*REF*]. Since
SM-RC can adjust its spectral radius, it is also expected that CBRL
agents with SM-RC can learn to change their chaoticity and switch
between exploration and exploitation states more dynamically.