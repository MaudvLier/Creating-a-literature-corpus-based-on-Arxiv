Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study


Introduction 


Large Language Models (LLMs) [*REF*; *REF*] have showcased exceptional prowess across various domains. Notably, LLMs find applications in furnishing information for tasks like robot planning [*REF*], machine translation [*REF*] and medicine [*REF*]. In parallel, RL has demonstrated remarkable capabilities in various domains, including achieving human-level performance in games such as the game of Go [*REF*] and multiplayer poker [*REF*].
LLMs have been increasingly incorporated to enhance the performance of Reinforcement Learning (RL) [*REF*; *REF*]. Likewise, RL has also been employed to augment the capabilities of LLMs, furthering their effectiveness [*REF*]. Nevertheless, the effective harnessing of LLMs&apos; latent potential in solving complex tasks, through the synergistic integration with powerful RL frameworks [*REF*], remains a formidable challenge.


The research most related to our study includes the works of Carta et al. [*REF*] and Tran et al. [*REF*]. In the work of Carta et al. [*REF*], they employ LLMs as RL policies to acquire task-solving capabilities while learning new knowledge through interactive experiences. Their experimental findings suggest that their method outperforms baseline approaches. However, a potential limitation of their work is the absence of instruction feedback from RL models, which may impact the overall effectiveness of their method. In the work of Tran et al. [*REF*], they deploy RL to train a conversational agent using a simulator and an initial text generated by a generative chat agent. Subsequently, they input the data from the RL-trained agent to the generative chat agent.
Although their experiment results indicate that their method performs better than baselines, a concern of this approach is the potential time consumption associated with RL training for multi-turn conversations, as each conversation may necessitate RL training requests. Additionally, achieving self-online learning for task execution could be challenging in this framework.


In this study, to address the above challenge, we propose a teacher-student learning framework in a cooperative game, where the integration of RL models (students) and LLMs (teachers) with bi-directional feedback [*REF*] may be an effective solution. The two models cooperatively to carry out complex tasks, which can be considered a win-win collaboration, where the RL model and the LLM act as two agents, cooperating to complement, assist, and provide feedback to each other, ultimately solving the problem together.


Method 


In this section, we introduce a teacher-student learning framework with bi-directional feedback, wherein a synergistic partnership between an LLM and an RL model is employed to tackle tasks collaboratively. As illustrated in Figure [1], these two models operate in tandem, with mutual support, ultimately enabling successful task completion.


LLMs (teachers) help RL models (students): While it is often challenging for LLMs to provide instructions encompassing perfect and comprehensive environmental information, LLMs can supply RL models with approximated information. Providing such rudimentary guidance by LLMs serves the purpose of streamlining the exploration process undertaken by RL models. Consequently, this streamlined exploration process yields a discernible reduction in the exploration space and the time required for RL models to ascertain and establish an optimal policy. This phenomenon underscores the potential utility of LLMs in mitigating the challenges associated with imperfect instructional input, thereby contributing to the enhanced efficiency of RL models in their quest to identify optimal policies.


RL models (students) help LLMs (teachers): During the execution of a policy within the RL framework, RL models benefit from the support provided by LLMs. In this collaborative process, RL models are not only recipients but also evaluators of the output generated by LLMs. This reciprocal interaction allows RL models to offer constructive feedback to LLMs, thereby facilitating an iterative refinement of LLMs&apos; performance. LLMs progressively acquire a more nuanced understanding of the underlying environments with the progression of iterations.
Consequently, they become increasingly adept at furnishing improved output, which aids RL models and LLMs in executing complex tasks with greater efficacy. This iterative and symbiotic relationship between RL models and LLMs emphasizes the potential for continuous improvement and optimization in their collaborative endeavors. The corresponding practical algorithm is provided in Algorithm.


FIGURE


ALGORITHM


Experiments 


Our experimental investigation is conducted within the context of the BabyAI benchmark [*REF*]. To facilitate our experimentation, we leverage the Lamorel framework [*REF*]. Within the Lamorel framework, we augment LLMs with RL instruction feedback, thereby establishing a feedback loop for the enhancement of LLM performance.
Specifically, we proceed to conduct experiments focusing on the task of GoToRedBallNoDists-v0. The experiments are executed over two cases: one comprising 40 iteration steps and another spanning 2100 iteration steps.
In the evaluation, we draw a comparative analysis between our proposed method and the baseline. In our method, we consider RL model feedback to LLMs and LLMs&apos;s information to RL models. However, the state-of-the-art baseline represented by the original Lamorel method that lacks such instruction feedback to LLMs from RL models.


It is noteworthy that for our experiments, we employ the &quot;google/flan-t5-small&quot; model [*REF*] as the LLM, characterized by a parameter count of 80 million. The experimental results are presented in Figure [2]. These findings clearly illustrate the superior performance of our method, as quantified by the performance value metric (where higher values indicate better performance). Furthermore, our method demonstrates notably expedited convergence (one-shot/few-shot learning) when compared to the Lamorel baseline. This empirical evidence highlights the effectiveness of our approach in harnessing bi-directional feedback between RL models and LLMs for improving performance in the context of the BabyAI benchmark.


FIGURE


Conclusion 


In this study, we developed a teacher-student learning framework for unlocking LLMs&apos; powerful capabilities by leveraging an RL model with bi-directional feedback mechanisms in a cooperative game setting. To empirically assess the effectiveness of our method, we conducted experiments using the BabyAI benchmark as an assessment platform. The results of these experiments demonstrate the superior performance of our approach in comparison to the state-of-the-art baseline, underscoring its potential for substantially enhancing learning outcomes.
Importantly, our approach could exhibit promise in cultivating safe and robust learning systems [*REF*], particularly when confronted with the inherent challenges of imperfect information environments.
Furthermore, we hope our insights inspire novel avenues of research in the realms of LLMs and RL for future investigations.