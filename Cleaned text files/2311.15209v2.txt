See and Think: Embodied Agent in Virtual Environment


Introduction


Designing agents that demonstrate intelligent behavior andadaptability in open-world settings has been a longstanding andsignificant challenge in the field of artificial intelligence [*REF*, *REF*]. However, recent progressin the development of large language models (LLMs) [*REF*, *REF*] has exhibited theirpotential as versatile, general-purpose assistants. Recentinnovations in agent design [*REF*, *REF*, *REF*, *REF*] have effectively harnessedthese advanced LLMs, tapping into their extensive world knowledge andreasoning abilities. This development has paved the way for agents,that are autonomously driven and equipped, to formulate and implementstrategies and actions across a diverse array of skills and tasks inopen-world environments.


In many open-world settings, like Minecraft, contemporary agentspredominantly use LLMs for their textual interactions. However, this reliance on text forcommunication poses considerable limitations in their interactionswithin these worlds. Minecraft, with its expansive and interactivesandbox environment [*REF*, *REF*],demands a variety of skills from agents, ranging from crafting basicitems to executing complex tasks. Yet, agents driven by LLMs oftengenerate unpredictable outputs. The effectiveness of theirinteractions is largely contingent on meticulously crafted prompts [*REF*], designed to align the LLM&apos;s understanding withthe environmental context and the intended objectives. This process ofprompt engineering is not only laborious but also fails to meet thegoal of fostering autonomous, selfdirected agents. Furthermore,textual communication has its limitations in naturally and intuitivelyconveying certain concepts of the world, like crafting recipes, whichare often more effectively communicated through visual means.


Players have the distinct capability to assimilate and conveyinformation using both visual and textual channels, significantlyenhancing our interactions with the world around us. Yet, theintegration of LLM-based agents with multimodal inputs in open-endedenvironments remains an under-explored area.


STEVE, named after the protagonist of the game Minecraft, is ourproposed framework aims to build an embodied agent based on thevision model and LLMs within an open world, as illustrated in Figure [1]. STEVE harnesses a vision model to visually perceiveits surroundings, coupled with an LLM to strategize and planactions. This model represents a leap forward in agent design, combining these two input modes, vision and text, to offer a more nuancedand comprehensive understanding of the environment, along withpractical and executable skills.


Our key contributions are outlined as follows:


- We propose STEVE, an embodied agent in virtual environment,consists of vision perception, language instruction, and codeaction, achieving 1.5× faster unlocking of key tech trees and is2.3× quicker in block search tasks compared to previousstate-of-the-art methods.
- We present STEVE-7B/13B, a series of large language model obtainedby fine-tuning with Minecraft knowledge question-answering pairsfrom Llama-2-7B/13B.
- We collect STEVE-21K dataset, including 600+ visionenvironmentpairs, 20K knowledge question-answering pairs, and 200+ skill-codepairs, for justifying the effective performance of STEVE.


Related Works


Intelligent Agent in Minecraft


As an open-ended sandbox game, Minecraft has always been an idealsetting for testing the performance of intelligent agents [*REF*, *REF*]. The agents are required toautonomously perform various tasks in Minecraft, such as chopping trees, crafting tools, and mining diamonds. At the beginning,much of the works focus on exploring reinforcement learning [*REF*, *REF*, *REF*, *REF*] or imitation learning [*REF*, *REF*], without satisfactoryperformance. VPT [*REF*] and MineDojo [*REF*] collect internet-scale datasets for their modelpre-training. More specifically, VPT offers the exciting possibilityof directly learning to act during video pre-training and using theselearned behavioral priors as extremely effective exploration priorsfor reinforcement learning. Yet, recent works found that thepre-trained LLMs could serve as a strong &quot;mind&quot; that provides planningability to the agents. Voyager [*REF*] leveragesGPT-4 [*REF*] as both a high-level planner and alow-level action code generator. Plan4MC [*REF*]proposes a skill graph pre-generated by the LLMs. DEPS [*REF*], an interactive planning method based on LLMs,addresses multi-step reasoning issue in open-world planning. GITM [*REF*] develops a set of structured actions andleverages LLMs to generate action plans for the agents to execute,achieving impressive results in various tasks.


Embodied Multimodal Model


Embodied agent operates within its environment by synthesizingsensory perceptions and physical actions supported by computationalintelligence. This synthesis enables the agent to undertake avariety of tasks, achieving specific objectives. Its key areas ofapplication are diverse, including Navigation [*REF*, *REF*, *REF*, *REF*, *REF*, *REF*], Embodied Question Answering [*REF*, *REF*, *REF*], Active Visual Tracking [*REF*, *REF*, *REF*, *REF*], and Visual Exploration [*REF*, *REF*, *REF*]. The field is evolvingrapidly with the development of Large Language Models (LLMs) [*REF*] and Multimodal LLMs (MLLMs) [*REF*, *REF*, *REF*, *REF*, *REF* -- *REF*, *REF*, *REF*, *REF*, *REF*, *REF*, *REF*, *REF*], integrating multiple modalities for more effective processing. Aprime example of this innovation is PaLM-E [*REF*], asophisticated multimodal model with 562B parameters, adept at a broadspectrum of embodied tasks and demonstrating exceptionalcapabilities in visual reasoning.


Large Language Model with Equipped Tools


While Large Language Models (LLMs) demonstrate impressive skill intackling novel tasks via prompt-based instructions, they often facechallenges in areas where simpler models or tools excel, likemathematical calculations or identifying palindromes. However, LLMs&apos;potential is significantly expanded when integrated with othermodalityspecific models, such as those for vision or audio, enabling multi-modal capabilities [*REF*, *REF*]. Innovations like Toolformer [*REF*] demonstrate LLMs&apos; self-learning to utilizetools through finetuning with extensive API call samples. VisualChatGPT [*REF*] extends this by integrating various visual foundation models, facilitating interactive user experienceswith ChatGPT. Similarly, HuggingGPT [*REF*] presents aframework that harnesses LLMs to link diverse models from Hugging Face for task resolution. AutoGPT [*REF*]is an open-source application that broadens GPT-4&apos;s capabilitieswith internet access, memory management, and plugins. The recentintroduction of MovieChat [*REF*] brings a memorymechanism to MLLM, enhancing its performance in video understandingtasks. Furthermore, LLMs can be adeptly used for goal planning,analogous to language translation [*REF*]. Thisevolving landscape suggests that toolequipped LLMs could forge a newparadigm in AI solution design.


FIGURE 2


Method: STEVE


Overview


We propose STEVE, an autonomous system for embodied agents inMinecraft. It integrates visual perception into the large languagemodel (LLM) instruction and combines them with a skill database toexecute actions. It consists of three components, including:


- Vision Perception (Section [3.2](vision-perception)) We createa vision encoder to interpret visual information of theenvironment, such as blocks and entities.
- Language Instruction (Section [3.3](language-instruction)) Wedevelop STEVE-13B, a powerful language model fine-tuned specificallyfor Minecraft content using LLaMA213B [*REF*]. Thismodel enables adaptive interaction for iterative reasoning andstep-by-step decomposition.
- Code Action (Section [3.4](code-action)) We generate theSkill-Code database. It enables executive skill actions based onguidelines from the Language Instruction.


Vision Perception


The vision perception part aims to convert the vision informationfrom the agent to the tokenizer representation. We utilize the visualbranch of EfficientFormer [*REF*] as the vision encoderto extract features from images or video frames. These features arethen transformed into tokens that encapsulate critical visual information. The encoder is designedto be robust against variability in environmental conditions,ensuring consistent performance regardless of lighting or weatherchanges in the virtual world.


The visual tokens are amalgamated with textual tokens representing theagent&apos;s current state (e.g., health, inventory, etc.) and the taskdescription. This is accomplished using a tokenizer that maps statevariables and task parameters to a tokenized form. The resultantunified token set serves as a comprehensive representation of thecurrent situational context, ready for processing by the languagemodel.


Language Instruction


The Language Instruction, which aims to provide adaptive interactionfor iterative reasoning and step-by-step decomposition, is appliedto the skill retrieval of the Section [3.4](code-action). Itprovides reasoning and planning, it also decomposes from high-levelguidelines to low-level steps. We propose STEVE-13B, a powerfullanguage model derived from LLaMA-2-13B [*REF*],fine-tuned specifically on Minecraft-related content from theSTEVE-20K. This model&apos;s expertise covers a broad spectrum ofgame-specific knowledge areas on worlds, entities, player mechanics,survival, and even game practical experience etc.


By integrating environmental information with this extensiveknowledge base, STEVE-13B iteratively reasons about tasks and devisesa structured plan. It decomposes complex goals into manageableinstructions step-by-step, determining the sequence of actions neededto navigate the world and manipulate objects effectively.


Iterative reasoning. The STEVE-13B receives a stream of tokensthat encode not only the current visual scene but also the agent&apos;sstate and the task&apos;s textual description. STEVE-13B interprets thisrich context to undertake complex reasoning. The model initiates thereasoning process by constructing a series of high-level strategiesthat outline the pathway to task completion. This involves:


- Goal understanding: identifying the goal and understanding therequirements for success.
- Comprehensive consideration: considering the agent&apos;s currentresources, abilities, and constraints.
- Environment assessing: including potential hazards, opportunities, and strategic advantages.
- Strategy formulating: requiring logical and effective sequences of actions.


The reasoning mechanism is akin to an experienced player who canvisualize the end game and chart a course to victory, considering allgameplay elements. This approach ensures that the plans are not justreactive but also proactive, allowing the agent to anticipate andmitigate future challenges. However, most strategies are high-leveland abstract, therefore they often require step-by-step decomposition to derive executable guidelines.


Decomposition. The decomposition process makes the complexstrategies break down into a series of simple, lowlevel guidelinesthat can be directly mapped to actions in the Minecraft world. It issimilar to how a high-level command like &quot;build a shelter&quot; is dividedinto actionable instructions like &quot;collect wood,&quot; &quot;craft planks,&quot; and&quot;place blocks.&quot; The granular steps are structured to provide explicitinstructions that the game engine can readily interpret. Thisrequires:


- High-low conversion: converting high-level strategies into asequence of low-level, actionable guidelines.
- Relevance guarantee: ensuring each step is contextually relevantto the preceding actions and the current state of the game.
- Environmental adaptability: maintaining adaptability, allowingthe agent to respond to dynamic changes in the environment.


This systematic breakdown from high-level reasoning to low-levelactionable steps is the hallmark of the STEVE system, enabling theembodied agent to interact with the Minecraft environment in ameaningful and goal-oriented manner. Through this intricate process ofreasoning and decomposition, STEVE-13B embodies the cognitivecapabilities required for sophisticated task management in virtualenvironments.


Code Action


The code action part is the execution phase, where the STEVE systemconverts planned, decomposed guidelines into concrete actions withinthe Minecraft environment. This process leverages a specialized skilldatabase that pairs code snippets with their descriptions and relevantmetadata, encoded as vectors for efficient retrieval. The transitionfrom high-level language instruction to executable code is achievedthrough query encoding and similarity matching.


Query encoding. Each low-level step derived from the LanguageInstruction phase is encoded into a query. This encoding captures theessence of the action to be performed in a format that the skilldatabase can understand. The database operates like a vast repositoryof code snippets, each tagged with a description and a vector thatabstracts its functionality and context of use. The encoding processinvolves the following:


- Transformation of instructions: The low-level steps aretransformed into structured queries, which reflect the nature ofthe action and the context in which it should be executed.
- Contextual relevance: The query must account for the currentstate of the Minecraft agent, ensuring that the code snippetretrieved is not just theoretically appropriate but also practicallyapplicable given the situation.
- Survival strategies: The instruction is semantically encodedinto a vector that resonates with the encoding scheme of the skilldatabase, allowing for a meaningful comparison.


Retrieval. Once the queries are encoded, the system computessimilarities between the query vectors and the vectors of the codesnippets stored in the database. This step is essential fordetermining which skill best matches the required action.


- Vector similarity: computing the cosine similarity between thequery vector and the database vectors that reflects the closenessof semantic meaning.
- Skill retrieval: retrieving one or multiple code snippets thatbest align with the query, prioritizing those with the highestsimilarity measures.
- Code selection: selecting the most appropriate code snippet(combining them, or choosing one based on additional context orpriorities in cases where multiple snippets are viable).


Execution. After the selection of the appropriate code or skills,the STEVE system executes the action within the game. This involves:


- Action instantiation: The selected code snippet is instantiated as a game action, which interacts with the mineflayerAPI.
- Monitoring and feedback: As actions are executed, the systemmonitors their outcomes for feedback, which may inform subsequentactions or lead to real-time adjustments in strategy.
- Sequential Execution: Actions are executed in sequence,adhering to the planned steps while remaining flexible to adapt toany unexpected changes or results in the game environment.


FIGURE 3


Note that during the collection phase, the language instruction taskis also performed. We simultaneously record and save the chat flowfrom the reasoning and decomposition stages. In the Question-Answeringsection, we obtain information from the Minecraft-Wiki and Redditforums, and use GPT-3.5 to clean the data into Single-round QA pairs.
In the Skill-Code section, we use GPT-3.5 combined with the humanplayer&apos;s code to synthesize code snippets, and then check and revisethem in the game environment.


FIGURE 4


During the Code Action phase, the STEVE system effectivelytranslates its strategic planning into reality, transforming thetheoretical notion of task completion into a sequence of targetedand context-sensitive actions within the virtual landscape ofMinecraft. This crucial phase involves converting abstract,language-based directives into practi-


cal, executable code. This pivotal transformation exemplifies thezenith of STEVE&apos;s embodied cognitive processing, endowing the systemwith the capability to operate autonomously and interact dynamicallywith its environment.


Dataset: STEVE-21K


As shown in Figure [3], STEVE-21K has been meticulouslycompiled, featuring a diverse collection of VisionEnvironment pairs,Question-Answering pairs, and a SkillCode database.


Vision-Environment pairs contain 600 pairs of first-personperspective videos from Minecraft gameplay across six differentterrains (including forest, desert, coastal, etc.), along withcorresponding environmental block entities in the field of vision andcontext information for each timestamp. Additionally, all pairs areoriented around executing the skillrelated task supported bySkill-Code part mentioned in Section [4](dataset-steve-21k). Weemploy the STEVE-13B model to enable robots to autonomously plan andexecute actions based on tasks defined by human supervisors. We recordthe video of the robot operation, the environment information and allthe corresponding chatflow. Note that we use rayTracing [*REF*] to ensure that the environmental informationobtained is the blocks and entities seen in the field of vision.


Question-Answering pairs contain 20K questionanswering pairsfrom the Minecraft-Wiki and Reddit corpus across six data types partlysourced from [*REF*].


TABLE 1


The pairs are organized into instruction, input, and output tripletsand used to train STEVE-13B. The GPT-3.5 [*REF*] isemployed to derive meaningful single-round questionanswer pairs, andLoRA [*REF*] is incorporated during the fine-tuningprocess for efficient resource allocation.


Skill-Code pairs contain 210 skill execution scripts withdescriptions, covering 8 skill types including collecting, crafting,exploration etc. The code part is collected by manual coding. Weuse GPT-3.5 to describe all codes and utilize langchain vectordb togive all pairs a database vector.


Experiments


Experimental Setup


We select LLaMA-2-13B [*REF*] as the base model andthen conduct finetuning with the STEVE-21K dataset (theQuestion-Answering part for warm-up, the chatflow content ofVision-Environment part for practical experience), as shown in Section [4](dataset-steve-21k). In the text part, we set all temperaturesto 0 except for the task proposal, which uses 0.9 to encourage taskdiversity. The vision unit is based on EfficientFormerV2-S0 [*REF*], which is trained on the Vision-Environmentpart of our STEVE-21K dataset. Our simulation environment is builton top of MineDojo [*REF*] and leverages Mineflayer [*REF*] APIs for motor controls.


Baselines


Currently, no vision-based LLM-driven agents work out of the box forMinecraft, so we carefully selected several representative algorithmsas baselines for our experiment. They rely on extracting informationfrom a system&apos;s backend, presenting a significant divergence from real-world scenarios.


AutoGPT [*REF*] is a widely used software tool forautomating NLP tasks. It breaks down a high-level goal into smallersubgoals. We reinterpret them to be executable in Mine-Dojo andcompatible with our experimental setting. In our experiments, AutoGPTworks on GPT-4 [*REF*] for task decomposition. Weprovide it with agent states, environment feedback, and execution errors as observations for subgoal execution.


TABLE 2


Voyager [*REF*] relies only on textual groundingfor perception. It has a long-term procedural memory that stores ahierarchical library of code-based grounding procedures. Complexskills can use simpler skills as sub-procedures. Voyager is known forits ability to explore areas and master the tech tree. However, itsmain focus is to prompt GPT4 [*REF*] on backgroundtext messages in embodied agents rather than vision perception.


Evaluation Results


Continuous block search. As shown in Table [2], weexperiment on block-searching tasks to assess the agent&apos;s exploratory capabilities and proficiency in locating specified blocks.
Diamond blocks are placed at every 16-block interval across themainland map. The agent&apos;s objective is to identify as many blocks aspossible within the fewest iterations, which indicates the method&apos;sefficiency. As shown in Figure [5], enrichinginformation through visual perception significantly enhances theefficiency of search and exploration tasks, leading to moreeffective world exploration.


Knowledge question and answering. We have created aquestion-answering database to evaluate our model&apos;s ability togenerate accurate responses related to Minecraft. This is done byusing a validation dataset. When each model provides a response,both the generated response and the correct answer are fed intoGPT-4, Claude-2, and human participants for blind rating. Initially,We check the accuracy of the response to determine its correctness. Toensure consistency in the evaluation process, all evaluation methodsconduct a thorough assessment that considers the accuracy, relevance,and level of detail of the response. The outcome of this comprehensiveevaluation is an overall score on a scale of 0 to 10, where a higherscore indicates better overall performance.


As shown in Table [3], we experiment on the instructional capabilities of different LLM models. In the STEVE-21k knowledge instruction pairs, we split the dataset into 18, 622samples for training and 1, 000 for testing. STEVE7B and STEVE-13Bmodels outshine the LLaMA2 models in all evaluated categories.
STEVE-13B has achieved the highest total score of 8.54, manifestingits superior ability to comprehend and address a broad spectrum ofMinecraftrelated inquiries. This superior performance of the STEVEmodels, particularly in the 7B and 13B versions, is likelyattributable to their specific optimization for content related toMinecraft.


TABLE 3


TABLE 4


FIGURE 5


The observed gradual enhancement from STEVE-7B to STEVE-13B indicatesthat an increase in model size, coupled with further fine-tuning,benefits performance in knowledge-intensive tasks. Although GPT-4demonstrates robust capabilities across various categories, it isslightly outperformed by the more specialized STEVE-13B in overallscoring. In brief, custom model fine-tuned on specific knowledgeenhances the precision and relevance of Tech tree mastery. As shown in Table [1], weexperiment on the Minecraft tech tree mastery to test the agent&apos;sability to craft and use a hierarchy of tools. Progressing throughthis tree (wooden tool → stone tool → iron tool → diamondtool) requires the agent to master systematic and compositionalskills. As to the wooden, stone, and iron levels of the tech tree,STEVE achieves remarkable efficiency: 23×, 11.7×, and 8.4×faster than AutoGPT [*REF*], and 1.5×, 1.4×, and 1.3× faster than Voyager [*REF*]. Additionally, STEVE successfully accessesthe diamond level, as documented in Table [1]. While itsperformance slightly trails Voyager [*REF*], which alsorelies on GPT4 [*REF*] for critical inference andpossesses a skill library. However, STEVE operates solely on thefine-tuned LLaMA2-13B-chat-hf [*REF*]. This model ismore cost-effective and starts with a lower initial performance.
Furthermore, STEVE incorporates a vision unit, prioritizing visualdata over background information. While this integration canintroduce certain inaccuracies, it offers distinct advantages. Wealso compare our method using the basic skill database and observe asubstantial decrease in performance as the capacity of the skilldatabase diminished.


Ablation Study


To understand the impact of different components on the performance ofour system, we conducted ablation studies focusing on the tech treemastery task in Minecraft. The results, as shown in Table [4], provide insights into the effectiveness of thevision unit and compare our STEVE model with STEVE GPT-4 version (withthe same vision unit as ours). Note that the w/o vision unit setup isthat the environmental perception encompasses data on blocks withinan 8x8 area surrounding the agent, including the front, back, left, andright directions. The following observations are made:


FIGURE 6 


Vision unit is critical. The omission of the vision unit markedlyaffects the system&apos;s performance, especially in more advanced tasks.
While it successfully crafts Wooden, Stone and Iron Tools, it ischallenged with Diamond Tools. This outcome underscores the vitalimportance of visual information in accomplishing complex tasks.


Comparison with GPT-4. STEVE GPT-4 version exhibits consistentsuccess across all categories, securing a flawless success rate.
Interestingly, the STEVE-13B version excels in simpler tasks likecrafting Wooden and Stone Tools. Additionally, it requires feweriterations than methods without the vision part, underscoring itssuperior efficiency.


The ablation study illustrates the importance of vision perception andSTEVE-13B. Visual perception acquiring more distant information aidsin exploring natural environments and aligns better with knowledgeprimarily obtained by human players through visual perception togameplay. Although it&apos;s impossible to perceive unseen block information, this kind of support for exploration is still beneficial for development-oriented tasks like tech tree mastery. Meanwhile,the much smaller STEVE-13B, fine-tuned on Minecraft knowledge, canprovide more efficient guidance with shorter and more preciseinstructions. This is demonstrated in simpler tasks like WoodenTools and Stone Tools.


Case Study


As shown in Figure [6], we perform an extensive casestudy comparison of GPT-4, LLaMA2-13B, and our method STEVE-13B. Eachmodel maintains the same information and question inputs to comparefeedback under different environmental information. Our STEVE overallachieves the best results, surpassing GPT-4 and showing significantimprovement compared to the original LLaMA. Especially in partsinvolving numerical calculations, such as the leftmost image, STEVEaccurately tracks food values to restore hunger levels.


Limitation and Future Work


Lifelong Learning Tasks. Lifelong learning scenarios and tasksthat exceed the database&apos;s scope still plague us. Despite an extensiveand detailed skill database, STEVE struggles with high-dimensionaltasks like &quot;survival&quot;, revealing a gap between its theoreticalknowledge and practical adaptability. We leave it as our future works.


Knowledge Base Gaps. The model&apos;s training data, while extensive,may not cover all the intricacies and constraints of the gameenvironment. This can lead to scenarios where the Instructor, guidedby incomplete or inaccurate knowledge, proposes tasks that arelogically conceivable but practically inapplicable.


Conclusion


In conclusion, STEVE makes a significant advancement in the realm ofmultimodal learning frameworks and their application in open-endedenvironments. By integrating visual-text interfaces with LLM-basedagents, STEVE paves the way for more nuanced and intelligentinteractions within complex digital worlds like Minecraft. The threefold functions of vision perception, language instruction and codeaction endow STEVE with the ability to understand, predict and actwithin its environment.