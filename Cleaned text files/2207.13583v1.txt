Towards the Neuroevolution of Low-level Artificial General Intelligence


Introduction


Artificial General Intelligence (AGI) or strong Artificial Intelligence
(AI) is commonly discussed among AI researchers. It is often defined as
human-level AI, however, the generality of an AI does not need to be
considered at such a level of complexity. In fact, our current
artificial intelligent systems cannot emulate the learning capabilities
of an animal with a simple nervous system, such as a worm
[*REF*; *REF*]. An alternative approach is to
start the quest for the generality of AI from the simplest tasks that
animals can do, but machines cannot [*REF*]. Moreover, AGI
systems should be tested in tasks that require self-learning on the fly
from sensory feedback, as it is often done in meta-learning and
continual learning [*REF*; *REF*].


We argue that a radical paradigm change is needed in order to reach
general intelligence [*REF*; *REF*]. Our
hypothesis is that such a new paradigm requires learning systems with
self-organizing properties, as discussed by *REF*. In this
work, our goal is to achieve the learning capabilities of a primitive
brain. Therefore, we aim at a low-level AGI, i.e., a system that can
learn a map function through sensory experience. Interpreting and
understanding sensory inputs are achieved through evolution,
particularly supervised evolution [*REF*] of agents
interacting with their environment.


The brain is the organ that interprets the encoded signals from our
sensory organs, thanks to the ability to distinguish between positive
and negative sensory experiences depending on what is considered to be
good or harmful, e.g., pleasure and pain. Respectively, pleasure and
pain serve as reward and penalty mechanisms that affect our behavior.


In this work, we evaluate the Neuroevolution of Artificial General
Intelligence (NAGI) framework [*REF*]. NAGI is a
low-level biologically-inspired AGI framework. NAGI consists of an
evolvable spiking neural network with adaptive synapses and
randomly-initialized weights. The network is evolved by an extension of
the method NeuroEvolution of Augmenting Topologies (NEAT)
[*REF*]. The source code of NAGI is available at WEBSITE.


The evolved spiking neural network controls an agent placed in a mutable
environment. Its chances of reproduction are proportional to how long it
can survive in an environment that is constantly changing, sometimes
abruptly. Evolution optimizes how the neurons are connected in the
network, their type of neurotransmitters (excitatory or inhibitory),
their susceptibility to background electrical current noise (analogous
to bias), and their neuroplasticity. With such degrees of freedom in the
optimization process, we attempt to approximately recapitulate the
evolutionary process of the simplest brains. The mutable environment and
random weight initialization propitiate a benchmark for generality and
adaptivity of the agent.


We test NAGI in three mutable environments. The first one is a simple
food foraging task, in which the agent has one photoreceptor (or light
intensity sensor) used to identify food. The food type (color) is either
black or white. Food can be edible or poisonous and this feature changes
over time. The agent can also taste the food as its sensory feedback for
good and bad actions. The second environment is a logic gate task. The
spiking neural network needs to emulate different logic gates in series
where the only reward and penalty sensory signals are the supporting
mechanisms to identify the correct output. The third environment is a
cart-pole balancing task. In this environment, the goal of the agent is
to control the forces applied to the cart in order to maintain the pole
over it upright. The mutable component of this environment is the pole
length which changes during the lifetime of the agent. Because this
environment has sensory feedback for the agent&apos;s actions, there is no
need to add reward and penalty sensory signals.


The article is organized as follows:
Section [2] explains the theoretical basis for
understanding NAGI. Section [3] discusses the related work to our approach.
Section [4] describes the details of the method and experiments.
Section [5] presents the experimental results.
Section [6] concludes the article including a discussion
of the results and plans for future work.


Background 


The components of the NAGI framework are inspired by the overlapping
research fields of artificial life [*REF*],
evolutionary robotics [*REF*], and computational
neuroscience [*REF*]. In particular, the
controller for the agents is a Spiking Neural Network (SNN)
[*REF*], which is a more biologically-plausible
artificial neural network. The neurons in an SNN communicate through
spikes, i.e., binary values in time series. Therefore, an SNN adds a
temporal dimension to binary data. A neuron propagates such data
depending on whether its membrane potential crossed a threshold value or
not. If the threshold is crossed, the neuron propagates a signal
represented as neurotransmitters to its connected neurons; otherwise,
the action potential is not propagated. When neurotransmitters are
released by a neuron, they can be of two types: excitatory, which
increases the membrane potential and the likelihood of producing an
action potential; or inhibitory, which has the opposite effect by
decreasing the membrane potential. Efficient optimization of an SNN
cannot happen through gradient descent as spike trains are not
differentiable [*REF*]. Instead, spiking neurons have
biologically inspired local learning rules, such as Hebbian learning and
Spike-Timing-Dependent Plasticity (STDP) [*REF*; *REF*]. Those
neuroplasticity rules are unsupervised. Their functionality in the brain
is still not fully understood. However, it is inferred that the
supervision comes from a certain network configuration acquired through
evolution. Therefore, in this work, we use a modification of
NeuroEvolution of Augmenting Topologies (NEAT) [*REF*].
NEAT uses a Genetic Algorithm (GA) [*REF*] to optimize the
weights and the topology of a growing neural network that is initialized
with a minimal and functional size. NEAT is typically used to search for
a network configuration that improves a fitness score while maintaining
population diversity (speciation) and avoiding loss of genes during
crossover (historical marking). The weights in the NAGI framework are
randomly initialized, and they change (adapt) after deployment. The
adaptation is coordinated by a realistic Hebbian learning rule, i.e.,
STDP. This neuroplasticity adjusts the synaptic strength of a neuron&apos;s
dendrites (i.e., input connections) when it fires an action potential
(or spike) that goes through its axon (i.e., output connection). The
weights are modified according to the difference in time between
incoming spikes and the generated action potential.


The body and brain interaction (sensors and actuators vs. controller) is
often described as \&quot;chicken and egg\&quot; problem [*REF*].
The natural evolution of body and brain happens together with the
evolution of the environment. They evolve in cooperation and response to
each other [*REF*]. The application of supervised
evolution of agents interacting with the environment is defined as
embodied evolution [*REF*]. As such, an agent needs a
body to learn from the reaction of its environment. We hypothesize that
low-level general intelligence in nature emerged through the evolution
of a sensory feedback learning method.


Related work 


Neuroevolution with adaptive synapses was introduced in 2003 by
*REF*. Such a method is a version of NEAT where the
synaptic strength of the connections changes with Hebbian local learning
rules. In their work, they used a food foraging task where an agent
moves around a field surrounded by edible and poisonous food. The type
of food did not change over time, but it was initialized differently at
every new run. The agents needed to try the food first before
identifying it. Therefore, the agents possess reward and penalty sensory
signals as in NAGI. *REF* proposed an extended version by
replacing the direct encoding of the network in NEAT with an indirect
encoding.


Additional related methods are Refs. [*REF*] and
[*REF*] where randomly-initialized artificial neural networks
are used. The work of *REF* uses a version of NEAT where each
neuron can have one activation function out of several types. While in
the method of *REF*, the network topology is fixed and each
connection evolves to optimize the parameters of its Hebbian learning
rule.


In a recent review on neuroevolution [*REF*], NEAT and its
extensions are comparable to deep neural networks trained with
gradient-based methods for reinforcement learning tasks. Such methods
allow evolving artificial neural networks with indirect encoding for
scalability, novelty search for diversity, meta-learning for learning
how to learn, and architecture search for deep learning models.
Moreover, neuroevolution is described as a key factor for reaching AGI,
in particular in relation to meta-learning and open-ended evolution
(OOE). Meta-learning encompasses the training of a model with certain
datasets and testing with others. The goal of the model is therefore to
learn any given dataset by itself from experience [*REF*].
OOE is the ability to endlessly generate a variety of solutions of
increasing complexity [*REF*]. In NAGI, meta-learning is an
implicit target in the mutable environments and is implemented as
neuroplasticity in the spiking neural network.


In 2020, *REF* introduced the framework for evolutionary
artificial general intelligence (FEAGI). This method uses an indirect
encoding technique for a spiking neural network that resembles the
growth of the biological brain, which is called \&quot;neuroembryogenesis\&quot;.
As a proof of concept, FEAGI demonstrates successful handwritten digits
classification by learning through association and being able to recall
digits from different image samples in real time.


Neuroevolution of Artificial General Intelligence 


The NAGI framework aims at providing a simplified model of the initial
stages of the evolution of biological general intelligence
[*REF*]. The evolving agents in NAGI consist of
randomly-initialized spiking neural networks. Thus, a genome in NAGI
does not require the definition of synaptic weights of the connections
between neurons, as it is done in NEAT. Therefore, the synaptic weights
in the genome are replaced by an STDP rule and its parameters for each
neuron. Since biological neurons may provide one of the two main
neurotransmitters, NAGI&apos;s genome defines such a feature in the neurons&apos;
genes. As such, a neuron can be either excitatory or inhibitory. To
imitate the function of bias in artificial neural networks, neurons may
be also susceptible to a \&quot;background electrical current noise\&quot;.


The environment changes during the lifetime of the agent. This forces
the agent to learn new environmental conditions. Therefore, the agent is
encouraged to generalize and learn how to learn. The aforementioned
random initialization and mutable environment aim at benchmarking the
basic properties needed for low-level AGI.


Spiking neural network


The spiking neural network has a fixed number of input and output
neurons depending on the task to be solved. The neuroevolution process
defines the number of hidden neurons that will be available. Hidden
neurons can be either excitatory or inhibitory, while input and output
neurons are always excitatory. Self-loops and cycles are permitted while
duplicate connections between two neurons in the same direction are
prohibited. The SNN is stimulated from the input neurons, as such units
are spike generators. The spikes are uniformly generated in an assigned
frequency or firing rate.


As a spiking neuron model, we use a simplification of the leaky
integrate-and-fire model [*REF*]. A neuron&apos;s membrane potential
*MATH* is increased directly by its inputs and decays over time by a factor
*MATH*. We can then express the change in membrane potential
*MATH* with regards to a time step *MATH* by *MATH* 
where *MATH* is the input value *MATH* (no spike) or *MATH* (spike) from the
presynaptic neuron *MATH*, the dendrite for this connection has the
synaptic strength defined as *MATH*, and *MATH* is the total number of
presynaptic neurons that the dendrites are connecting. If the membrane
potential *MATH* is greater than the membrane threshold *MATH*, a spike
is released and the membrane potential returns to the resting membrane
potential *MATH*, which is *MATH*. The time step *MATH* we use in
the experiments is *MATH* ms, and decay factor *MATH* is
*MATH*. An action performed by the SNN is calculated by the
number of spikes in a time window. Such an actuator time window covers
250 ms or 2,500 time steps. In NAGI, the weights of the SNN are randomly
initialized with a normal distribution. The mean is equal to *MATH* and the
standard deviation is equal to *MATH*. The weights are always positive.
As mentioned, the excitation and inhibition of a neuron are defined by
the neurotransmitter of the presynaptic neuron.


Homeostasis


Biological neurons have a plasticity mechanism that maintains a steady
equilibrium of the firing rate, which is called homeostasis
[*REF*; *REF*]. In our method, the spiking neurons
can have non-homogeneous inputs, which could lead to very different
firing rates. It is desirable that all neurons have approximately equal
firing rates [*REF*]. In order to homogenize the firing rates of
the neurons in a network, the membrane threshold *MATH* is given
by *MATH* where *MATH* is the \&quot;resting\&quot; membrane threshold equals to *MATH*, and
*MATH* starts with value *MATH*, is increased every time a neuron fires
by a value of *MATH* and decays exponentially with a rate of
*MATH*. Each neuron has an individual *MATH*. Therefore, a
neuron firing more often will get a larger membrane threshold and as
consequence a lower firing rate. To compensate for a neuron with weak
incoming weights, which causes a low firing rate; we use the sum of the
incoming weights as the threshold instead.


Spike-Timing-Dependent Plasticity


The adjustment of the weights of the connections entering into a neuron
happens on every input and output spike to and from a neuron. This is
performed by STDP and is done by keeping track of the time elapsed since
the last output spike, as well as the time elapsed since each input
spike for each incoming connection within a time frame, called the STDP
time window, which is set to be *MATH* ms. The difference between
presynaptic and postsynaptic spikes, or the relative timing between
them, denoted by *MATH* is given by *MATH* where *MATH* is
the timing of the output spike and *MATH* is the timing of the input
spike.


The synaptic weight change *MATH* is calculated in accordance with
one of the four Hebbian learning rules. The functions for each of the
four learning rules are given by *MATH* where *MATH* is a Difference of Gaussian
function given by *MATH*.


*MATH* and *MATH* are the parameters that affect the height of the
curve, *MATH* and *MATH* are the parameters that affect the
width or steepness of the curve of the Asymmetric Hebbian functions, and
*MATH* and *MATH* are the standard deviations for the
Gaussian functions used in the Symmetric Hebbian functions. It is also
required that *MATH*. We experimentally found fitting
ranges for each of these parameters, which can be seen in
Tab. [1] and Tab. [2]. The STDP curves with the maximum value of
those parameters are illustrated in Fig. [5].


TABLES


FIGURE


Weights can take values in a range *MATH*, and every
neuron has a weight budget *MATH* it must follow. What this means
is that if the sum of a neuron&apos;s incoming weights exceed *MATH* 
after initialization or STDP has been applied, they are normalized to
*MATH*, given by *MATH*. The parameters used during our experiments are *MATH*, *MATH*,
and *MATH*. In the SNN without homeostasis, if a connection *MATH* 
has *MATH*, then *MATH*. Therefore, an action potential
coming from *MATH* will always produce a spike. This is the reason to have
*MATH*.


Genome


The genome in NAGI is rather similar to the one in NEAT. Its node genes
have three types: input, hidden, and output. Depending on the type of
the node gene, there is a different collection of loci. The input
node is a spike generator and provides excitation to the neurons it is
connected to. The gene of an input node is the same as in NEAT. The
hidden and output nodes represent adaptable and mutable spiking neurons.
They have three additional loci: the type of the learning rule, the set
of the learning rule parameters, and a bias. The connection gene in NAGI
has no weight locus as in NEAT. The reason for its removal is that the
weights of the SNN are defined by a normal distribution.


The learning rule is one of the four STDPs. The set of learning rule
parameters consists of four parameters that adjust the intensity of the
weight change. They are different for symmetric and asymmetric learning
rules. The symmetric parameters are *MATH*, *MATH*,
*MATH* and the asymmetric parameters are *MATH*,
*MATH*, *MATH*. The bias is a Boolean value that
determines if the neuron has a constant input of *MATH* being added to
*MATH*, which is analogous to the background noise of the neuron.


The hidden node genes have a unique locus, which is a Boolean value that
determines whether it represents an inhibitory or excitatory neuron.
This locus is not included in the output node genes because they are
always excitatory. As a result of combining all the descriptions of the
genome in NAGI, the genotype and the phenotype are illustrated in
Fig.


FIGURE


The initialization of the additional loci in the node genes can be
conditional and non-uniform. The initialization of the neurotransmitter
type of a neuron follows a similar proportion of excitatory and
inhibitory neurons in the brain [*REF*]. The probability
of a neuron being added as excitatory is 70%. The probability of having
a bias is 20%. Depending on the neurotransmitter, excitatory neurons
have a 70% chance of initializing with Hebbian plasticity, and
inhibitory neurons have the same chance but for anti-Hebbian plasticity.
The learning rule parameters are initialized by sampling from a uniform
distribution within the ranges in Tabs. [1] and [2].


The mutations of the additional loci happen in 10% of chance to switch
the neurotransmitter type, bias, learning rule, and learning rule
parameters. Those parameters have 2% chance of a fully
re-initialization. When the parameters are assigned to be mutated, a
random value sampled from a normal distribution with *MATH* and
*MATH* is added to the parameter *MATH*. The equation of *MATH* 
is *MATH* where *MATH* and *MATH* are
the maximum and minimum values the parameter can have, given by the
ranges in Tabs. [1] and [2]. During the neuroevolution, 10% of the
genotypes with the best fitness scores will be passed to the next
generation unchanged, i.e., elitism.


Mutable environments


The benchmark tasks for NAGI are meant to evaluate the agent&apos;s ability
to generalize and self-adapt. Therefore, they consist of environments
that change during the lifetime of the agent. Two types of tasks are
provided, binary classification (two tasks of this kind are provided)
and control (one task of this kind is provided). The first type (binary
classification) is the simplest one, however, it provides the most
abrupt changes in the environment. The binary classification tasks are
food foraging with one input, and logic gates with two inputs. The
control task in a simulated physical environment is the cart-pole
balancing from OpenAI Gym [*REF*]. The changes are less abrupt in
this last task as they consist in modifying the pole size. The fitness
scores are calculated using the number of time steps *MATH* that the agent
survived in these environments, normalized to the range *MATH* using
the maximum possible lifetime *MATH* and minimum possible lifetime
*MATH*. Therefore, the fitness function *MATH* is given by *MATH*.


In the binary classification tasks, the agents have an initial amount of
health points that is reduced every time step as continuous damage. If a
correct action is chosen, the health point amount is reduced by *MATH* 
health point. Otherwise, it is reduced by *MATH*. The input sample is
given to the agent for 1 second or 10,000 time steps, then it is changed
to a new one. The mutation of the environment condition happens when the
agent has seen four samples. The order of the input samples and the
environment conditions is fixed and cyclic.


We noticed that the number of spikes within the actuator time window can
be the same for the output neurons and therefore allowing for a tie in
many cases. Our solution to avoid spiking neural networks with this
behavior is to include a \&quot;confidence\&quot; factor in the fitness score
calculation. Therefore, the higher the difference between the spike
count, the more confident the action is. If the action is correct and
highly confident, the damage is *MATH* or closer. If the action is
incorrect but highly confident, the damage is *MATH* or closer. The lack
of confidence would make the damage lie between the values *MATH* and
*MATH*. The spike count for the correct action *MATH* and incorrect one
*MATH* are used to calculate the participation of the spikes for deciding
the correct action *MATH* and the participation for the incorrect action
*MATH*. In the iterations without spikes of the output neurons, normally
the initial ones; the agent takes *MATH* as damage. Otherwise, the damage
is calculated by *MATH* where *MATH* is the minimum &apos;target&apos; number of spikes.
The purpose of *MATH* is to avoid assigning a too high or low fitness
to agents that fire few spikes through their outputs. The agent takes
damage at every time step and is given by *MATH*.
Damaging is performed until the agent runs out of health points and
&apos;dies&apos;. Subsequently, the fitness score of the agent is calculated from
the fitness function expressed in equation. The damage to the health points in a correct
action *MATH* is 1, in an incorrect one *MATH* is 2. Therefore, correct
actions result in a longer lifetime. The value for the minimum &apos;target&apos;
number of spikes *MATH* is 3 spikes.


In the control task of cart-pole balancing, the behavior of the mutable
environment is different. A new environment is presented to the agent
either after its failure or after the maximum number of environment
iterations is reached. Moreover, the agents do not have health points.
The fitness score is the normalization of the number of iterations that
the agent survived after all environment conditions were executed.


Food foraging


The agent in the food foraging environment possesses just one light
sensor for identifying the food \&quot;in front of it\&quot;. There are two types
of food: edible and poisonous. As such, food is represented in two
colors: black and white. The environment changes by randomly defining
which food color is edible or poisonous. In this environment, the agent
can act in two ways: eating or avoiding the food. The sample has a
predefined time of exposure to the agent. An action is performed after
the first spike and it continues for every time step in the environment
simulation. After this exposure time, the food is replaced by a new one.
The agent can only discover whether it is exposed to an edible or
poisonous food by interacting with it. An incorrect action is defined as
eating poisonous food, or avoiding edible food, while a correct action
is defined as eating edible food or avoiding poisonous one. If the agent
makes an incorrect action, it receives a penalty signal, from which the
agent should learn over the generations that it represents pain,
revulsion, or hunger. If the agent makes a correct action, it receives a
reward signal, from which it should learn that it represents the
pleasure of eating delicious food or recognizing that the food is
poisonous. In Fig., the food foraging environment is illustrated,
how the environment changes and provides new food samples. In
Tab. [3], the four combinations of edible and poisonous food for the white and black ones
are shown. To evolve the spiking neural network for the food foraging
task, the parameters of the genetic algorithm are the following: the
population size is set to 100 individuals, and the number of generations
is set to 1,000.


FIGURE


Logic gates


In this environment, the mutable environmental state is a two-input
logic gate. The environment provides the agent with two binary inputs,
i.e., 0&apos;s and 1&apos;s. The agent&apos;s task is to predict the correct output for
the current logic gate given the current input. Similar to the food
foraging environment, it receives a reward signal if it is currently
predicting the correct output, and a penalty signal if it is currently
predicting the wrong output.


In order to measure the generalizing properties of agents, we use two
different sets of environments: a training environment, which is used in
calculating the fitness score while running the evolutionary algorithm,
and a test environment which has a fully disjoint set of possible
environmental states. A full overview of the logic gates found in both
the training and the test environments, as well as the truth values for
all input and output combinations, are found in Tabs. [4] and
[5]. The evolution of the spiking neural
network is performed by a population of 100 individuals through 1,000
generations.


TABLES


Cart-pole balancing


The cart-pole balancing is a well-known control task used as a benchmark
problem in reinforcement learning. In this environment, there is a cart
that moves when a force is applied to the left or to the right every
time step. In the middle of the cart, there is a vertical pole connected
to a non-actuated joint. The goal of this environment is to maintain the
pole balanced upright by controlling the forces that move the cart.
Moreover, the cart cannot move beyond the limits of the track. The
observations available to the controller are the cart position, the cart
velocity, the pole angle, and the pole angular velocity.


For training, we use poles of different sizes, which are *MATH* 
(default), *MATH*, and *MATH*. For testing, the sizes are *MATH*, and
*MATH*. Those pole sizes are depicted in Fig. [6]. Each
size can run up to 200 environment iterations and it is repeated three
times during training for promoting stable controllers. If there are no
more environment iterations or the pole falls, the cart-pole environment
restarts with the next pole size while using the same SNN or finishes
when all pole sizes were executed. The fitness score is calculated using
the number of iterations the pole kept balanced. Subsequently, it is
normalized to values between *MATH* and *MATH*. The evolution for this task
occurs with a population size of 256 during 500 generations.


FIGURE


Data representation


The data type in a spiking neural network is a binary time series or a
spike train. Because the agent senses and acts in the environment, such
data must be converted from the sensors and to the actuators. The flow
of spikes over time can be quantified as firing rate, which corresponds
to a frequency, or the number of spikes per second. The firing rate is
the data representation that is converted as inputs and outputs for the
SNN. However, the input firing rate must be within a minimum and a
maximum value. In our experiments, we use the value range *MATH*.
The minimum and maximum value of the firing rate are simplified to a
real number range *MATH*. It is preferable that the data from the
sensors has also a minimum and a maximum value. Otherwise, it will be
necessary to clip sensory values or map the values to a desirable range.


In the binary classification tasks, all inputs and outputs are binary.
Therefore, the minimum and maximum values for the input firing rate
stand for, respectively, *MATH* and *MATH*, or *MATH* and *MATH*. To avoid
having a predefined threshold firing rate for the output neurons, we opt
to have two output neurons for one binary value. The neuron with the
highest firing rate within the actuator time window is the one defining
the binary output value. If these two output neurons have the same
firing rate, then the last one with the highest value is selected. We
also decided to have the same \&quot;two neurons-one binary value\&quot; strategy
with the inputs. Therefore, the inputs are presented as one-hot
encoding, as shown in Tab. [6].


FIGURE


For the cart-pole control task, the inputs are real numbers, and the
left and right actions are represented as two output neurons, similar to
the outputs of the binary classification tasks. In this environment, the
inputs are the cart position, cart velocity, pole angle, and pole
angular velocity. Because we infer that real numbers converted to the
firing rate of one neuron can be difficult to deal with in an adaptive
spiking neural network (as also mentioned in
Ref. [*REF*]), we decided to have three neurons for
each input. The firing rate of the three neurons is similar to the
sensitivity for the light spectrum of the three cone cells in the human
eye [*REF*]. The conversion from observation values to firing
rate is depicted in Fig. [7]. We use the sigmoid function [*REF*] for
neurons 1 and 3 and the a normalized version of the Gaussian function
[*REF*] for neuron 2. The sigmoid equation is
*MATH* where *MATH* is the observation value from the environment, *MATH* is the
weight that adjusts the smoothness of the interval between *MATH* and *MATH*,
*MATH* is the shift coefficient to adjust the function on the horizontal
axis, *MATH* is the highest firing rate possible applied to an input
neuron, and *MATH* is the lowest firing rate possible. The Gaussian
function for converting observation value to firing rate is expressed by
*MATH* where *MATH* is the mean and *MATH* is the standard deviation. We
replace *MATH* in the original Gaussian function
to *MATH* because, in this way, we can define the highest firing rate when
the observation value is the mean.


Neurons 1 and 3 use *MATH*, while neuron 2 uses
*MATH*. For cart position, cart velocity, and pole
angular velocity, the parameters are shown in
Tab. [7]. This is depicted in Fig. For pole angle, the parameters are presented in
Tab. [8]. These three functions are depicted in Fig.


TABLES


FIGURE


Results 


The evolution of the spiking neural networks in NAGI is evaluated with
fitness score, accuracy, and end-of-sample accuracy for the binary
classification tasks, which are food foraging and logic gate. The
accuracy is measured at every time step of the simulation. The
end-of-sample accuracy stands for the accuracy measured in the last time
step of a sample. The assessment performed for the control task with
cart-pole balancing is done with the fitness score. We test the best
performing agent in a task with ten simulations where their details are
also provided.


Fig. [8] shows the evolution history of the food foraging task. The average
fitness score has a slight increase, but the maximum fitness score does
not follow this trend. The accuracy and end-of-sample accuracy have high
variation with their maximum values, but they consist of high
accuracies. Moreover, some early generations register 100% end-of-sample
accuracy. The three measurements do not improve through the generations.
However, good solutions are already found in the first generation.
Therefore, this is an easy task that requires a small SNN. For test
simulations, we select the individual with the highest accuracy, which
is found in generation number 34 and has an accuracy of 89.8%. Its
fitness score is 0.541395 and its end-of-sample accuracy is 100%. Its
topology is shown in Fig. [11]. Paying attention to this topology, the
hidden nodes are not needed. They form a loop that does not connect with
the output nodes. The topology summarizes in one of the one-hot encoded
input nodes (node 1) connecting to the two output nodes. Then, the node
with the penalty signal (node 3) connects only with the node for the
&apos;eat&apos; actuator (node 4). The behavior of the network is illustrated in
Fig. The topology of the network indicates
that the two output neurons have the same data input from node 1, but
the neuron for &apos;avoid&apos; action has a bias, which gives it a small
excitatory current. If &apos;avoid&apos; is the wrong action, the penalty input
signal from node 3 excites the output neuron for the &apos;eat&apos; action. This
is how the spiking neural network decides the actions from
\&quot;understanding\&quot; the feedback of the environment given by the penalty
input signal. The result of the ten test simulations is presented in
Tab. [9].


FIGURE


FIGURE


Fig. [12] shows the training results of the logic gate
task and it includes the test of the maximum individual of the
measurement in every generation. The fitness score, accuracy, and
end-of-sample accuracy maintain average values with high variation.
However, the evolution of the agents in the logic gate task is similar
to the one in the food foraging. The early generations already contain
good spiking neural networks for the task. The best-performing agent is
selected from the accuracy measurement. This individual is in generation
48 and has an accuracy of 85.0%. Its fitness score is 0.4421625 and its
end-of-sample accuracy is 100%. The topology of this spiking neural
network is shown in Fig. [15]. Its behavior is shown in
Fig. Even though we have trained with a
\&quot;confidence\&quot; factor in the fitness function, the spike counts are
still with almost the same values. Tab. [10]
contains the accuracy and end-of-sample accuracy of ten test
simulations, which indicates that the SNN can be general to reproduce
the behavior of logic gates without being trained to them.


FIGURES


Fig. [16] shows the fitness score history through the evolution for the cart-pole
balancing task. This task is the one with the highest difficulty to find
a good genome for the adaptive spiking neural network. It can be noted
that the fitness score improves through the generations. The maximum
fitness score in a generation goes from around 0.16 in the first
generation to 0.99944 in generation number 399. Such an individual is
the one selected for the test simulations. Its topology is illustrated
in Fig. [15] and the spike counts of the actuators for
&apos;left&apos; and &apos;right&apos; actions are shown in Fig. The spiking neural network has no
hidden neurons. Therefore, the SNN works as an input selection for the
output neurons. The result of the ten test simulations is presented in
Tab. When the pole is balanced for more than 100
iterations, the controller is considered successful.


FIGURES


TABLE


Discussion and conclusion 


We successfully solved all three presented tasks with the NAGI
framework. The spiking neural networks found showed generality to the
binary classification tasks, even to unseen conditions in the case of
the emulation of logic gates. The neuroevolution produced rather simple
topologies for the SNNs. We infer that binary classification is easy due
to the binary performance feedback. For further research, multi-class
classification is considered.


The cart-pole balancing task was successfully solved without any hidden
neurons. The conversion of one observation into three input neurons is
used to avoid the requirement of weight fine-tuning due to small
differences in firing rate and also to the assumption that Hebbian
plasticity works better with binary data (active and inactive)
[*REF*]. With such a conversion, the SNN became an
input selection.


The topologies for the three tasks caught our attention because almost
all output excitatory neurons were anti-Hebbian, and the two inhibitory
hidden neurons in the logic gate solution have Hebbian neuroplasticity.
Our initial hypotheses were that excitatory neurons mainly have Hebbian
learning rules, and inhibitory neurons are anti-Hebbian. That was the
reason for having different probabilities for anti-Hebbian and Hebbian
learning rules depending on the type of the neurotransmitter when adding
a new neuron through mutation.


Even though there is elitism, the performance measurements are unstable
through generations. This is a demonstration of the randomness in the
initialization of the weights, and input and environment order. This can
be perceived in the results of the ten test simulations of the three
tasks.


For future work, we plan to attempt more challenging tasks. If there is
a failure in executing the task, the constraints imposed on NAGI can be
eased. A major constraint is that one neuron has one plasticity rule for
all dendrites. Maybe its removal can simplify issues in difficult tasks.
This constraint was intended to reduce the dimensionality of the search
space in the neuroevolution and an assumption that the dendrites in the
same neuron adapt under one learning rule. This modification is also
aligned with the work of *REF*, which has meta-learning
properties for more difficult control tasks than the cart-pole
balancing, such as top-down car racing and quadruped walk. Another
opportunity is the addition of curriculum learning [*REF*; *REF*] for increasing the
complexity of the task while the agent becomes better over the
generations.