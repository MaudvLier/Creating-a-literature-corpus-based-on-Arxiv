Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model


Introduction


Drawing on large language models (LLMs) with human knowledge,
communicative competence, and decision-making capabilities, embodied
agents exhibit human-like intelligence in
simulators [*REF*; *REF*; *REF*], programming [*REF*; *REF*], and robotic
tasks [*REF*; *REF*]. The development of
individual intelligence has led to the creation a new, collaborative
framework where multiple agents [*REF*; *REF*] work together. Agents
in this system specialize in complex tasks, sharing insights to enhance
efficiency and foster a learning environment.


For open-ended environments, the need to solve complex tasks makes
people naturally turn to multi-agent cooperation. However, current
methods [*REF*; *REF*] infers based on a combination
of multiple LLMs and prompts with different functions to achieve
sufficient performance in single-agent tasks. This approach incurs high
reasoning costs and seriously hinders the upper limit of the development
of open-ended multi-agents [*REF*]. Agents require more
prior knowledge to understand complex task descriptions. Creative
agents [*REF*] use images generated by a model to help
construct and understand goals, but this can be costly and limits flexibility.


LLM-based agents interact with text-based environments and are skilled
in planning, reflection, and reward shaping. They achieve this through
their reasoning capability and semantic abstraction. However, LLMs
cannot be utilized in such settings due to the complexity of visual
environments. While some researchers [*REF*; *REF*]
have tried to integrate visual encoders into LLMs, the performance
obtained by training on static data using offline training cannot match
the dynamics of the environment. The challenge is fine-tuning
Multi-modal Language Models (MLMs) as an embodied agent with dynamic
alignment to the visual world. EMMA [*REF*] provides a
solution by training an embodied reflex agent through aligning an MLM
with visual world dynamics and distilling API LLM skills. Using the DPO
loss [*REF*] as a training method is highly effective and
also serves as a knowledge distillation technique. By combining the
teacher model with the extra expert, we can integrate additional expert
knowledge into the model, which is crucial for completing complex,
open-ended tasks. We also observe that the fine-grained
Chain-of-Thoughts (CoT) [*REF*] is necessary for improving the
reasoning accuracy of LLMs on complex tasks. Evoked from Centralized
Training with Decentralized Execution (CTDE) [*REF*; *REF*; *REF*; *REF*]
framework for MARL, we propose Centralized Planning with Decentralized
Execution (CPDE) a hierarchical structure maximizing global oversight
and local autonomy and simulate refined CoT to achieve multi-granular
task division and fine-grained action.


As shown in figure, we propose STEVE-2 a hierarchical
knowledge distillation process employing DPO loss [*REF*]
on Multi-Agent System with Extra Expert module. After distillation,
STEVE-2 can develop efficient embodied agents to accomplish
fine-grained open-ended tasks without expert guidance with one
model. We summarize our contributions as follows:
- We design STEVE-2, hierarchical knowledge distillation
for multi-modal multi-agent training. Multi-agents cooperate in a
hierarchical auto-organizing system for fine-grained Chain-of-Thoughts
and efficient deployment. Each agent is trained hierarchically by
mirrored teachers for simulating dynamics and aligning tasks at multiple
levels of granularity, allowing efficient cooperation using only one MLM.
- We develop an extra expert model based on our distillation
method, which allows us to add extra multi-modal knowledge to parallel
teachers and implicitly transfer knowledge to the inference model. By
keeping the knowledge within the model, there is no need for additional
priori or expert guidance during reasoning, and the model can handle
multi-agent cooperation flexibly and efficiently.
- We achieve state-of-the-art performance on the
asynchronous multi-modal navigation and creation tasks in Minecraft&apos;s
open-ended environment, with *MATH* - *MATH* in performance.


FIGURE


Related Works


Embodied Multimodal Model


Embodied agents integrate sensory perceptions, physical actions, and
computational intelligence to accomplish tasks and goals within their
environment. Key areas are wide-ranging, including
Navigation [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*],
Embodied Question Answering [*REF*; *REF*], Active Visual
Tracking [*REF*; *REF*; *REF*; *REF*],
and Visual Exploration [*REF*; *REF*; *REF*]. The
field is evolving rapidly with the development of Large Language Models
(LLMs) [*REF*] and Multimodal LLMs (MLLMs) [*REF*],
integrating multiple modalities for more effective processing. A prime
example of this innovation is PaLM-E [*REF*], a sophisticated
multimodal model with 562B parameters, adept at a broad spectrum of
embodied tasks and demonstrating exceptional capabilities in visual
reasoning.


LLM-based Multi-Agent Frameworks


Large Language Models (LLMs) are skilled at completing new tasks when
given prompt-based instructions. Autonomous agents based on Large
Language Model-based (LLM-based) models have gained significant interest
in industry and academia [*REF*]. Several
works [*REF*; *REF*; *REF*; *REF*; *REF*]
have augmented the problem-solving abilities of LLMs by incorporating
discussions among multiple agents. Stable-Alignment [*REF*]
generates instruction datasets by reaching a consensus on value
judgments through interactions among LLM agents in a sandbox. Some works
in the field of artificial intelligence focus on studying sociological
phenomena. For instance, Generative Agents [*REF*] creates
a virtual &quot;town&quot; comprising 25 agents to investigate language
interaction, social understanding, and collective memory. The Natural
Language-Based Society of Mind [*REF*] involves agents
with different functions interacting to solve complex tasks through
multiple rounds of Mindstorms. In addition, others [*REF*]
propose a model for cost reduction by combining large models as tool
makers and small models as tool users. Some works emphasize cooperation
and competition related to planning and strategy [*REF*], some
propose LLM-based economies [*REF*], and others propose
LLM-based programming [*REF*]. The current method uses many
language models [*REF*], making it slow and inefficient,
particularly in open-ended environments. Control capabilities for LLM
agents [*REF*] are challenging, but this is what Reinforcement
Learning methods [*REF*; *REF*; *REF*] do
best. However, combining LLM with Reinforcement Learning
models [*REF*] can improve control capabilities but lacks
flexibility. To improve multi-agent cooperation, we aim to reduce the
number of language models and increase efficiency through a hierarchical
organization system and knowledge distillation while ensuring control
capabilities.


STEVE-2: Hierarchical Knowledge Distillation


As shown in figure, the STEVE-2 is a hierarchical MLM-based
multi-agent system denoted as *MATH*, which can manage and
execute complex multi-agent tasks *MATH* on visual  *MATH*,
audio  *MATH*, and object  *MATH* goals with perception on the state list of
vision  *MATH*, audio  *MATH*, and other properties  *MATH* within open-ended
environments by leveraging cognitive and collaborative capabilities of
the multi-modal language model: *MATH* where *MATH* represents the state list
of vision, audio, and other properties of one conductor agent,
*MATH* is the initial task. Then, we get *MATH* as the
action for the conductor agent *MATH*, *MATH* is the number of conductor agents.


FIGURE


The Hierarchical architecture consists of two primary operational
domains: higher-order centralized planning, which is managed by the
manager multi-modal language model (*MATH*), and ground-level
decentralized execution, which is conducted by the conductor model
(*MATH*), then the action *MATH* of the conductor can be
obtained as follows, *MATH* where *MATH* and *MATH* 
represent the multi-modal language models of the conductor and manager
agents. Actor agents *MATH* auto-organized by *MATH* 
are optional for additional actions: *MATH*. 
To achieve this, we draw inspiration from recent advances of large pretrained
MLMs [*REF*; *REF*]. As shown in figure, each teacher agent plays the above three
different MLMs through different prompts in the hierarchical multi-agent
system. After hierarchical knowledge distillation by DPO
loss [*REF*], STEVE-2 acquires the performance of
these three scale multi-modal language models *MATH* for the manager, 
conductor, and actor agents. STEVE-2 is modularized into three components:
- The pretrained Vision Transformer (ViT) acts as the vision
encoder that encodes visual input data (represented as *MATH*) into
visual embeddings.
- The Multi-Layer Perceptron (MLP) layer aligns the embeddings
produced by the ViT with the language space.
- The pretrained Language Model (LLM) is utilized as the language
decoder, taking the concatenation of instruction tokens and the
output of the linear projection layer as input and generates a
textual action *MATH*. This textual action is then used to
retrieve code action *MATH*.


Multi-modal Teacher Model 


The Multi-modal Teacher Model
*MATH*, consisting of three different types of MLM of the manager, conductor and
actor agents. As shown in figure, *MATH*,
which consists of Planner, Describer, Critic, and Skill module for the
manager, conductor and actor agents. They formulate aligned task plans,
condense and translate multi-modal data, refine strategies through
feedback, and assign and direct agent subtasks. Then, they translate
strategic plans into executable actions, orchestrate dynamic group
formations, and distribute tasks across agents, ensuring alignment with
centralized directives and facilitating continuous learning and
adaptation through a curriculum of complex tasks. A multi-modal memory
is maintained to store the long-term memory of the description of
multi-modal information generated by *MATH*, the specific
planning trajectories generated by *MATH* corrected by *MATH*.


Adaptive planning with MLM.


Our approach integrates the environment&apos;s observations and task
directives to plan actions based on the current scenario. We begin by
translating multi-modal observations into textual descriptions,
utilizing a method that avoids direct scene captioning by the MLM.
Instead, we extract keywords for items from the STEVE-21K
dataset [*REF*] and employ GPT-4V(ision) [*REF*] to craft
sentences that articulate these observations. The MLM identifies
relevant condition sentences from textual observations during the
planning phase. It also incorporates additional context, such as biome
types and inventory levels, into text formats via predefined templates.
We generate action plans by re-engaging the MLM&apos;s linguistic component
with the task instructions and these descriptive texts. This methodology
leverages the MLM&apos;s capabilities in a layered manner, yielding more
accurate situational descriptions and plans that significantly reduce
the likelihood of generating unrealistic elements compared to fully
integrated models.


Autonomous error correction and proactive planning.


STEVE-2 enhances its planning through a closed-loop feedback
mechanism, automatically correcting failures by analyzing feedback and
identifying errors using its self-explanation capabilities. Unlike other
agents, it generates improved plans without human input or extra
information. Additionally, STEVE-2 simulates and evaluates each plan
step to identify potential flaws early, reducing the likelihood of
encountering difficult situations due to plan failures. This proactive
approach enables it to foresee issues like insufficient resources, which
could hinder task completion.


Teacher with Extra Expert


Language instructions can be limited in their ability to describe
complicated tasks due to the abstract nature of language. This can lead
to high uncertainty in task completion and may even require the agent to
possess creativity. Although many open-ended
agents [*REF*; *REF*] can follow clear instructions
for specific task goals, none can follow uncertain instructions to
perform complicated tasks. Navigation and creation tasks are
particularly challenging as they are open-ended, and the language
instructions provided may lack information and refer to diverse,
complicated outcomes. This requires the agent to imagine details not
specified in the instructions. A simple instruction like &quot;Find a
Pyramid&quot;, or &quot;Build a Pogda&quot; can refer to a complex task, making
planning and execution difficult. We have taken inspiration from the
work of Creative Agents [*REF*], which uses a generation
model as the prior input to simulate the imagination of a house image,
represented by the imaginator *MATH*. However, restoring other
perspectives and internal structures solely through a 2D image from one
perspective is challenging when creating a building. For navigation
tasks, having only one concrete image from the imaginator will be
difficult for long-distance searches. To address this issue, we have
extended *MATH* to the extra *MATH*, modified
VQ-VAE [*REF*] that generates 3D occupancy for creation
task, and a dynamic map [*REF*] with multi-modal signals
for navigation task. Note that we have integrated this part only with
the teacher model to avoid providing direct prompt information to the
STEVE-2 part, which can lead to cheating. This approach also ensures
the lightweight of the STEVE-2. The teacher with extra expert is
shown below: *MATH* where *MATH* is the multi-modal knowledge of the textual description.
The state is represented by *MATH*. *MATH* converts
the textual object goal *MATH* into *MATH*. This process
provides a detailed task description to the teacher model
*MATH*. This approach allows us to develop more advanced agents
to handle creative tasks by utilizing the extra expert&apos;s ability to
generate diverse outcomes. This provides the agent with richer task
information, reducing uncertainty.


Knowledge Distillation 


Our goal is to develop a system of hierarchical MLM agents, and each
assigned a specific function represented by a student MLM
*MATH*, that can learn and replicate the behavior of
teacher MLMs *MATH* with extra knowledge:
*MATH* and *MATH*. We employ a similar approach to
our system in training the Manager, Conductor, and Actor agents in
STEVE-2 using a hierarchical MLM teacher. The objective function,
which is based on the state distribution induced by
*MATH*, is minimized to accomplish this.
*MATH* where the loss function *MATH* is the knowledge distillation loss.
Specifically, we adopt DPO loss [*REF*] as the
distillation loss in the experiment. It uses the relative log
probability of a response to the non-preferred response, with a dynamic
per-sample weight to prevent model degradation, and is proven better
than cross-entropy for aligning language models with teacher
preferences. Therefore, Equation can be extended to the following formulation:
*MATH* where a logistic function (*MATH*) and a
hyperparameter (*MATH*) are to control the deviation from a reference
agent *MATH*. The reference agent is obtained by
behavior cloning on a dataset produced by a rule-based teacher. The
model is trained using Maximum Likelihood Estimation (MLE) with an
additional regularization term to prevent the agent from deviating too
far from the teacher&apos;s accurate distribution, maintain generation
diversity, and avoid premature convergence to easy tasks. To stabilize
the training process, the MLM agent *MATH* is
initialized to *MATH* as well. However, since we
cannot compute the distribution of *MATH*, we must
use the agent to sample *MATH*. We used
DAgger [*REF*] to converge to the optimal agent
*MATH* and overcome cumulative error and
distribution shift issues instead of Naive behavior. Note that we use
GPT-4V(ision) [*REF*] to generate a sequence of actions,
represented by *MATH* in equation DPO [*REF*]
for a given task. With extra expert, these actions may be closer to optimal.


Experiment


We experiment on two tasks to evaluate multi-agent cooperation and
running efficiency on complex tasks. We discuss evaluation settings,
results, and a trial on long-horizon tasks.


Experimental Setups


Our STEVE-2 is based on STEVE-13b [*REF*] pretrained on
LLaMA2-13b [*REF*], and we select
gpt-4-1106-vision-preview [*REF*] as the teacher model. Our
simulation environment is based on MineDojo [*REF*] and uses
Mineflayer [*REF*] APIs for motor controls. The generative model
is a 3D occupancy generator that aligns text input based on the
VQ-VAE [*REF*]. The dynamic map [*REF*]
is a multi-modal memory updated in real-time with information from all
action agents, providing a strategic overview of the environment. The
maximum number of robots that can be allocated based on this environment
is 8, which is also our experimental robots&apos; upper limit.


Baselines


No pre-built multi-agent robots for Minecraft exist. So, we choose
representative algorithms as baselines for our experiment. These
algorithms extract information from the system&apos;s backend, differing from
real-world scenarios.


Voyager


[*REF*] is a blind, single-robot system relying only on
textual grounding for perception. It has a long-term procedural memory
that stores a hierarchical library of code-based grounding procedures.
Voyager is known for its ability to explore areas and master the tech
tree. However, its main focus is to prompt GPT-4 [*REF*] on
background text messages in embodied agents. We use multiple hosts to
deploy multiple models to work directly on the server. We convert the
input of the image goal into a text task through
GPT-4V(ision) [*REF*], using the same Describer module as our teacher model.


STEVE


[*REF*] is a multi-modal single-robot system that combines the
vision unit with the STEVE-13B with the code database. It focuses on
introducing a visual module to endow the model with visual perception
capabilities in processing visual perception information and handling
task reasoning for skill-related execution. Similarly, using multiple
hosts, we deploy multiple models to work directly on the server.


Creative Agents


[*REF*] is a blind, single-robot system enhanced with a
diffusion model that generates detailed imaginations of task outcomes
conditioned on language instructions for open-ended creative tasks. They
observed that diffusion with GPT-4V(vision) [*REF*] performs
best in the creation task. Hence, we use this version and test replacing
diffusion with our finetuned VQ-VAE [*REF*] to provide image input.


Task: Multi-modal Navigation


We conduct experiments on navigation tasks, including multi-modal goal
search, continuous block search, and map exploration. Our STEVE-2
achieves the best performance, indicating the high efficiency of
multi-agent cooperation in multiple navigation tasks with different
modes of operation. The results are shown
in TABLE. Process efficiency and module reduction are
greatly improved when fewer LLMs are used.


Multi-modal goal search.


Multi-modal goal search technique identifies Image, Object, and Audio
goals. Object labels identify in-game items, Image labels locate objects
using images, and Audio labels detect sounds outside the player&apos;s range.
Our multi-agent system, STEVE-2, decomposes tasks layer by layer and
achieves 5.5times better performance than the state-of-the-art
LLM-based method while maintaining the same success rate.


Continuous block search.


Continuous block search is to assess an agent&apos;s exploratory abilities
and proficiency in locating diamond blocks. The agent will perform a
block searching task, identifying as many blocks as possible in the
fewest iterations. The dynamic map and hierarchical structure will allow
us to find and deploy more blocks. We experiment with the same setting
as detailed in the paper [*REF*].


Map exploration.


Map exploration aims to let the agent update the map as much as
possible. We set up the same status awareness: when in an unreached
area, status information prompts the agent in text. We set each step&apos;s
maximum movement distance not to exceed 50 blocks. STEVE-2 achieved 1.9
times better performance than the state-of-the-art LLM-based method
while increasing efficiency by 3 times.


FIGURE


Task: Multi-modal Creation


To evaluate how well the creation task works, we divided it into two
parts: collecting materials and building. This approach helped us assess
the team&apos;s efficiency and ability to understand complex task
instructions and translate them into precise actions. STEVE-2 has
achieved the best performance, reflecting teamwork efficiency and
understanding abstract task descriptions. For more details of case
study, please refer to case.


Material collection.


Material collection is vital for architecture and multi-agent
cooperation. We&apos;ll give the agent a list of materials and their
quantities to collect efficiently. We test only Voyager, as creative
agents are based on Voyager. STEVE-2 can improve efficiency by 19
times while ensuring accuracy.


Building creation.


Building creation is challenging due to novel and complicated outcomes,
requiring the agent to imagine details unspecified by the instruction.
The finetuned VQ-VAE [*REF*] modified to 3D occupancy
generation is used as the imagination, while multiple perspectives of
occupation are used to input the teacher model and train our model for
execution actions. To maintain the fairness of the experiment, we also
replaced diffusion with our VQ-VAE [*REF*] to provide
image input. We do not use 3D structure as input directly. Instead, we
use distillation to integrate knowledge. Our simple architecture with
one agent and a LLM improves output significantly, surpassing even more
complex structures. STEVE-2 can improve the quality by 3.2 times
in FID score and still rank first in the subjective preference scores of
GPT-4V(vision) [*REF*] and human while ensuring accuracy.


Ablation Study


As shown in table n and table c, we observe that the STEVE-2 model
outperforms the teacher model, GPT-4V(vision) [*REF*],
significantly in system performance, achieving 1.8 times better
navigation efficiency and 4 times higher creation quality. Knowledge
distillation further enhances the model, improving navigation efficiency
by up to 24 times and enabling the production of high-quality
buildings from scratch. For more details of running efficiency, please
refer to table.


Hierarchical knowledge distillation is critical.


After knowledge distillation, the resulting model can be significantly
improved compared to the original, STEVE-13B [*REF*]. Moreover,
in the field of creation, knowledge distillation allows the model to
produce high-quality buildings from scratch. This method of knowledge
distillation enhances multi-agent tasks and provides detailed
comprehension of abstract tasks by sorting them out.


Extra knowledge for training is effective.


With extra expert module, STEVE-2 receives additional prior
knowledge from the teacher model, leading to high-quality training data
and significant task improvement. During testing, we outperform the
teacher model without any prior assistance, demonstrating the module&apos;s
effectiveness in enhancing performance.


Conclusion


STEVE-2 is a novel framework that overcomes limitations of
multi-modal language models (MLMs) in open-ended embodied tasks. It uses
a hierarchical structure for nuanced task division, a mirrored
distillation approach for harnessing parallel simulation data, and an
imagination model to infuse extra contextual knowledge into simulations.
This boosts the autonomy and effectiveness of embodied agents, bridging
the gap between task understanding and execution, and dynamically
adapting to open-ended environments. STEVE-2 is more sophisticated,
flexible, and efficient in complex, real-world applications, advancing
the field of artificial intelligence and embodied systems.