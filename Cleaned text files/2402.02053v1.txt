Affordable Generative Agents


Introduction


Constructing an agent based on large language models (LLMs) for the
simulation of believable human behavior holds substantial promise
[*REF*; *REF*; *REF*]. A believable LLM
agent is characterized by its ability to interact with other agents or
humans, respond to environmental changes, and generate responses and
behaviors that are perceived by humans as authentic, natural, and in
alignment with expectations. Such agents could be utilized for diverse
applications, such as simulating the potential impacts of policies
[*REF*; *REF*] or sociological theories
on humans [*REF*], creating Non-Player Characters (NPCs)
[*REF*; *REF*] in games, and developing embodied
intelligence akin to social robots [*REF*] or domestic
service robots [*REF*; *REF*].


However, while we appreciate the potential and capabilities of these
agents, their cost implications also merit attention. The crux of the
matter is that real-time, long-term invocation of LLMs to handle various
types of agent interactions can result in substantial cost consumption.
Previous studies have highlighted this issue. For instance,
*REF* construct a virtual town, aka the &quot;Stanford Town&quot;,
to simulate social behavior, with the entire study incurring costs of
thousands of dollars. *REF* mimics embodied agents,
fulfilling both emotional and rational needs through interaction, with
simulations of 2 to 3 agents costing several dollars per hour.


The cost of believable agent interaction has hindered its use as a
low-cost solution for simulating policy impacts and sociological
phenomena, especially in long-term, large-scale scenarios. The cost,
which increases linearly with the number of interactions, is
unacceptable, particularly when LLM agents are to be used as NPCs in an
open world environment.


To address the issues above, there has been a significant amount of
research [*REF*; *REF*; *REF*; *REF*; *REF*].
Nevertheless, existing methods primarily focus on specific independent
tasks, solving problems through single-round QA sessions with LLMs
[*REF*; *REF*]. Consequently, there lacks
solutions for continuous multi-round agent interactions and the
coordination infrastructure supporting low-cost implementations. To
expand, the main reasons these methods are unsuitable for believable
behavior simulation include the following aspects: 1) the lack of a
unified and explicit evaluation system for believable behavior in
open-world scenarios, making it unsuitable for task difficulty
segmentation and cost-efficient model inference [*REF*];
2) the need for agents to consider context and multi-round interactions,
requiring large amounts of relevant information for decision-making,
making it unsuitable for combining multiple independent tasks into a
single prompt for single-round inference
[*REF*; *REF*]; and 3) the need to consider the
cost of single interactions as well as the redundancy in large-scale
interactions, which requires optimization from the framework level.


Therefore, in this paper, we focus on the believable behavior
interaction simulation of LLM agents in open-world scenarios and propose
a low-cost framework, termed as Affordable Generative Agents. For the
agent-environment interactions, we propose the Lifestyle Policy to
reduce the redundant costs of generating repeated responses by agents;
On the other hand, for the inter-agent interactions, we propose the
Social Memory to reduce the generation of repeated information in
multi-round interactions. These two techniques are distilled from our
insights on how a human interacts with other entities in the real world,
and can be compiled easily into existing agent simulation environments.
To examine the applicability of our techniques, we have conducted
extensive experiments using well-known environments, including the
&quot;Stanford Town&quot; [*REF*] and the VirtualHome
[*REF*], to demonstrate that, while achieving the same
performance, the consumption of generating believable agent behaviors
can be significantly reduced.


Furthermore, to help understand why our approach works, we delve into
the mechanics of how LLM agents generate believable behavior,
demonstrating that there is an upper limit to the believable behavior
that emerges from LLM agents in a fixed environment. This implies that
all agent-environment interactions can be entirely covered by policies.
Based upon this finding, we also propose a set of optimization
strategies for generating richer believable behavior.


In summary, our contributions are as follows:
- We propose Affordable Generative Agents, a low-cost simulation
framework for the believable behavior simulation of LLM agents in
open-world scenarios, with optimization strategies for both
agent-environment and inter-agent interactions.
- We propose several evaluation methods and conduct extensive
experiments in benchmarking environments to validate the
effectiveness of our framework.
- We analyze the mechanism of believable behavior generation by LLM
agents, demonstrate the upper limit of the richness of emergent
behaviors, and further propose corresponding optimization
strategies.


Related Work


This work explores the development of efficient generative behaviors
leveraging LLMs. We discuss the most related works below.


LLM Agents


The development of LLM agents has sparked a myriad of applications.
*REF* leverages multi-turn dialogues with agents to autonomously
address NLP tasks. *REF*, *REF*, and
*REF* integrate pre-defined APIs with LLMs to enable agents to
navigate web pages effectively. *REF* and *REF* 
deploy LLM agents in text game environments, showing the capacity to
engage in complex, strategic gameplay. The most prominent among these
works is the construction of LLM agents that interact in open
environments to create believable behavior. For instance,
*REF* builds a virtual town simulating the social life of
25 agents, while *REF* utilizes LLM to create a robot capable
of completing household chores through natural language interaction.
*REF* implements an agent capable of playing Minecraft
expertly. Besides, researchers have been focusing on enhancing its
performance. Efforts have been made to optimize various modules such as
reasoning [*REF*; *REF*; *REF*], planning
[*REF*; *REF*], reflection [*REF*], and
memory [*REF*; *REF*], or to create more
realistic interaction environments [*REF*; *REF*].
However, the development of affordable and believable agent interactions
is missing in the literature. In this paper, we focus on achieving
believable agents with the same performance at low cost.


The Cost of LLM Interaction


The cost of using LLM is directly proportional to the number of LLM
invocations, the length of the prompt, and the model&apos;s performance. The
cost issue has always been a key focus in the LLM-related field.
Numerous methods have been proposed to reduce the cost of using LLM,
such as Cascaded Invocation of LLM
[*REF*; *REF*; *REF*], summary [*REF*; *REF*; *REF*], and
batch [*REF*]. These methods primarily optimize the cost
of single invocation of LLM for QA tasks. *REF* has
built a cost-efficient multi-agent framework, but it still solves QA
tasks on fixed datasets. The simulation of believable behavior in the
open world requires uninterrupted calls to the LLM for interaction.
Moreover, multiple modules need to combine a large amount of contextual
information for LLM reasoning and planning, making cost a more prominent
issue. Simply optimizing for single QA invocations cannot effectively
reduce costs. Furthermore, the open world does not have a fixed
evaluation method, and the difficulty of tasks is hard to categorize,
rendering many existing methods unsuitable
[*REF*; *REF*]. *REF* proposes
generative agents with low-cost interactions, but their effectiveness is
highly dependent on the tailored scenarios. Hence, there is a pressing
need for a low-cost solution that can effectively handle the challenges
of believable interaction simulation in various open environments using
LLMs. This paper aims to address this gap by proposing a cost-efficient
scheme for simulating believable behavior with LLMs in open
environments.


Method


FIGURE


To implement the AGA framework, we abstract the interaction patterns of
the LLM agent into two types: agent-environment interactions and
inter-agent interactions. Figure provides an overview of the AGA framework. In
the following sub-sections, we will describe the optimization strategies
for these two categories of interactions in detail.


Agent-environment interactions


The interaction between an agent and its environment can be modeled as a
policy function mapping observations to actions. This policy can be
implemented through reinforcement learning [*REF*]
or rule-based finite state machines [*REF*], behavior trees
[*REF*], etc. LLMs trained on a vast amount of data
show immense potential in policy formulation [*REF*]. With
carefully designed prompts, LLMs can generate logically consistent and
believable decisions. The decision-making capabilities of LLMs have been
applied to a wide variety of applications, but the cost associated with
each interaction has been overlooked. On the other hand, behavior trees
operate without additional costs, but they require pre-written scripts
and can not adapt dynamically to environment changes.


To combine the advantages of these two methods, we propose a new method
-- Lifestyle Policy, which aims to retain the flexible interaction
capabilities of the LLMs while achieving low-cost deployment. The
functionality of Lifestyle Policy is to minimize costs by reusing the
similar inference processes of LLM. To this end, we devise
agent-environment interactions containing two stages: Plan Decomposition
and Policy Reuse, expanded as follows.


Plan Decomposition


The stage occurs when the agent encounters situations it has never seen
before. Due to their inherent zero-shot capabilities [*REF*],
LLM agents can respond to unseen tasks in a believable way. Given the
description of the environment and agent&apos;s information, a rough plan or
task is generated by LLM. The Plan Decomposition breaks the plan down
into sub-plans and then converts sub-plans into specific actions, just
like other LLM multi-agent framework [*REF*; *REF*; *REF*]. An additional
step involves generating an executable condition based on the action
sequence. The determination of execution condition is highly correlated
with the environment. For instance, in the case of a domestic robot
[*REF*], the condition of a task include the existence of the
related interactive items and the consistency of their states. If the
plan can be successfully executed, then the whole graph from plan to
actions and corresponding condition will be updated to Lifestyle Policy.


Policy Reuse


The stage occurs when the agent encounters previously seen situations.
Given a plan, we first search the Lifestyle Policy for any existing
plans that match or closely resemble the new one, based on cosine
similarity of their embedding features. When the similarity between two
plans exceeds a certain threshold, we consider them to have same
meaning, albeit with minor differences in their descriptions.
Subsequently, we assess whether the current environment meets the
execution condition associated with the retrieved plan. If the condition
is met, we proceed with the actions specified in that plan. It is
important to note that a single plan may have multiple conditions
tailored for different scenarios. We iterate through all the conditions
until we find one that the current environment satisfies.


Figure (a) illustrates the complete interaction process.
Upon receiving an observation that includes environmental data and the
agent&apos;s personal profile, the LLM agent formulates a plan. Initially,
the Lifestyle Policy retrieves a plan with a similar description and
condition met by current environment. If a match is found, the agent
executes the corresponding actions. Otherwise, the LLM is invoked to
decompose the plan. The LLM plan decomposition ensures that the actions
generated are of high quality, believable, and self-consistent.
Meanwhile, the Lifestyle Policy aims to reuse existing plans to avoid
unnecessary costs.


In the lifelong simulation of LLM agents, there are a large number of
redundant, repetitive, and trivial decisions, such as in Generative
agents [*REF*], where each agent repeatedly makes
decisions like brushing teeth, washing face, etc. Replacing the process
of invoking LLM reasoning with corresponding policies can save lots of
cost, which will be shown in our experiments.


Inter-agent interactions


In agent interactions, dialogue is the primary communication method.
These agents must integrate substantial auxiliary information into the
dialogue prompt to demonstrate advanced cognitive capabilities,
distinguishing them from basic chatbots driven by LLMs. For instance,
agents need to incorporate relevant information about the dialogue
participants and topic-related events retrieved from memory into the
dialogue prompt to enrich the conversation. However, this data is often
less comprehensive and more fragmented than the knowledge bases of
question-answering systems [*REF*]. Merely appending memory
snippets to prompts could reduce response quality and lead to higher
processing costs from increased prompt length [*REF*].
Furthermore, agent interactions differ depending on their relationships
and mutual perceptions in social simulations. Thus, agents need to
gather enough information to form contextually appropriate responses
without suffering performance drops or incurring extra costs from
excessively long prompts.


Drawing inspiration from existing text compression technologies
[*REF*], we introduce the Social Memory module, aimed at
reducing the cost of inter-agent interactions and enabling the agents to
produce more appropriate dialogues. This module is comprised of three
key components: Relationship, Feeling, and Summary Events.
Together, these components model the characteristics involved in agent
interactions. The relationship and feeling components are responsible
for recording the social connections and personal impressions between
agents. These records shape how the agent interacts with others,
influencing their behavior and approach. they can be a word or a phrase,
like &quot;Relationship: acquaintances, Feeling: friendly&quot;. Notably, the
two components are updated after each interaction to reflect any
potential changes in relationships and emotions. The summary events
component, on the other hand, focuses on summarizing all events related
to the interlocutors and the topics of discussion. These events are
retrieved from agent&apos;s memory, enabling agents to quickly access
necessary information from condensed text, thereby enhancing interaction
efficiency.


Figure (b) presents the procedure of a dialogue
interaction. Given the previous round of conversation, the agent
computes the dialogue&apos;s embedding features and retrieves relevant events
from memory. After filtering out duplicates, these events are
consolidated into summary events. As more interactions occur, additional
relevant events will be integrated into the summary events module, which
aims to compress information densely within a constrained text area,
improving the efficiency and effectiveness of information delivery. The
dialogue prompt is then formed by merging information from the last
round of conversation and Social Memory, and the LLM is invoked
generates the next round of dialogue. Upon the conclusion of the
conversation, the relationship and feeling are updated based on the
content of the dialogue.


Social Memory reduces costs of conversation by compressing auxiliary
information, offering precise descriptions of social information to
assist agents in responding consistently and appropriately. It also
supplies high-information events to aid agents in generating accurate
replies relevant to the conversation. Furthermore, as illustrated in
Figure [1], Social Memory updates after each
interaction, providing an explicit depiction of interlocutor
relationships. This mechanism can be employed to investigate the
dynamics of relationships between agents, thereby analyzing the
evolution of community relations and the emergence of social behaviors.


FIGURE


Experimental Setup


This section focuses on the experimental environment and evaluation
protocol.


Environment


We evaluate our approach under the following environments:
- Generative Agents. *REF* creates a simulated town,
populating it with multiple agents powered by LLMs that interact in
natural language, generating believable individual and emergent
social behaviors. Agents are required to create daily schedules,
which are refined into hourly plans, and then translated into
minute-level activities. All interactions are executed through
natural language descriptions, accompanied by corresponding emojis
for representation.


The Generative Agents framework comprises five modules: Perceive,
Plan, Retrieve, Reflect, and Act. The Perceive module is
responsible for sensing surrounding game objects. The Plan module
generates plans and activities. Retrieve is tasked with retrieving
relevant events from memory. Reflect deeply considers past events,
and Act determines the location and objects of interaction, as well
as generating corresponding emojis.


We implement AGA on Generative Agents framework and conduct
experiments in the provided 3-person and 25-person environments.


- VirtualHome. *REF* provides a 3D simulated
domestic setting where agents engage with their surroundings via
programmatic atomic actions. The environment encompasses 115
specific types of items, such as plate, desk, and microwave.
Each item possesses attributes that determine the permitted
interactions, including GRABBABLE, LIEABLE, etc., as well as
current states like OPEN and CLOSED. Compared to Generative
Agents, VirtualHome offers more concrete and realistic interaction.
An interaction involves specific actions, items and the item states.


We implement a LLM agent framework like Generative Agents on
VirtualHome to validate our method.


Language Models Adopted


For a fair comparison, all agents are empowered with GPT-3.5-Turbo
[*REF*], same as the Generative Agents work
[*REF*]. In the evaluation phase, we use GPT-4
[*REF*] as an evaluator to assess the agent&apos;s believable
behavior.


Evaluation Protocol


We focus on token consumption and performance. We carry out a series of
experiments to calculate the mean and standard deviation of token
consumption. To comprehensively evaluate the agent&apos;s behavior, we first
reuse the evaluation methods of Generative Agents excluding manual
assessments. These methods encompass interviewing agents with manually
designed questions and conducting case study-specific activities.


Moreover, we propose two novel methods to evaluate the emergent social
behaviors of generative agents:
- Relationship Evolution. The relationships among agents are
updated by Social Memory after interactions. The evolution of these
relationships signifies the formation of community ties, which can
be utilized to examine the social behaviors.
- LLM Evaluator. *REF* have validated LLM as an
evaluator, demonstrating comparable performance to expert
evaluators. Consequently, we employ GPT-4 to evaluate the activities
and dialogues generated by LLM agents. A 5-point Likert scale
questionnaire is deployed to differentiate whether these activities
and dialogues are from human or AI-generated.


For VirtualHome, agents are designed to experience a full day at home,
generating household tasks and decomposing them into specific actions.
We evaluate the success rate of plan execution.


Evaluations


We conduct extensive experiments to validate our method, assessing both
the token consumption and the performance. Our experiments demonstrate
that the AGA framework can reduce the cost while maintaining equivalent
performance.


Results on Generative Agents


Token Consumption


Figure [2] illustrates the ablation study of token
consumption based on multiple two game day simulations of 3-person town.
Compared to the baseline, the cost of using only the Lifestyle Policy is
40.2% of the original, while using only the Social Relationship Memory
is 58.6%, and the full AGA framework is 31.1%. It should be noted that
the advantage of the Lifestyle Policy lies in the reuse of existing
LLM-generated strategies. Hence, both Lifestyle Policy and AGA conduct
experimental comparisons after running multiple iterations, and then
storing a certain number of policies.


We also present the results of different configuration simulations in
Table [1], where the cost for a 25-person town is
42.7% of the original. In the 25-person town, the probability of
interactions between agents increases with the rise in population
density. After triggering a dialogue, an agent adjusts its plan,
invoking the reactions module to handle unexpected events. The
occurrence of more events leads to more frequent output of reflections
for deeper thinking. In order to ensure the flexibility of the LLM, we
did not modify the reactions and reflections modules of the Generative
Agents, which incurs additional costs for the 25-person version.


Case Study


We employ the same method used for Generative Agents to evaluate
believable agents in this part. We conduct interviews with agents to
evaluate their ability to remember past events, strategize for future
tasks based on those memories, react appropriately to unexpected
situations, and reflect on their previous actions to improve their
future performance. We use the same questions as Generative Agents to
inquire about five aspects: self-knowledge, memory, plans,
reactions and reflections. Our findings indicate that our agents
perform on par with the baseline, providing specific and accurate
answers to the questions. Sample responses are provided in Appendix
[7].


In addition, we can validate the emergence of social behavior through
specific events. In the 25-person town, Isabella Rodriguez, the owner of
the coffee shop, plans to host a Valentine&apos;s Day party and invite as
many people as possible the following day, while Sam is running for
mayor. After several experiments, we observe AGA got a similar result as
baseline that about *MATH* persons attend the party and *MATH* 
persons knew Sam&apos;s mayoral candidacy.


Relationship Evolution


In Social Memory, all initial relationships between agents are set to
&quot;Unknown&quot; and subsequently updated following each interaction. We
assess the emergence of social behavior by monitoring the relationship
evolution. For example, in the 3-person town, Klaus Mueller and Maria
Lopez discuss their research at a cafe, establishing a colleague
relationship. Isabella Rodriguez, the cafe owner, engages in
conversation while serving Klaus and Maria, transitioning to an
acquaintance. A specific demonstration of social behavior is that
agents modify their relationships with other agents through social
interactions, thereby strengthening their ties with the community. We
also conduct experiments on the 25-person environment. For visualization
purposes, we quantify relationships on a scale of *MATH* (the higher
value, the closer relationship. e.g., 0-strangers, 3-acquaintance,
5-co-worker, and 10-married). The result is shown in Figure
[3]. The data map reflects the actual
relationships between agents. For example, in the virtual town, John Lin
(JL in Figure [3]) is profiled as living with his wife, Mei
Lin (ML), and son, Eddy Lin (EL). Therefore, their relationship
scores are all a perfect 10. Additionally, the data map shows changing
relationships between agents, such as the relationship score between Tom
Moreno (TM) and Yuriko Yamamoto (YY), who do not know each other,
shifting to a 5. After conducting multiple experiments and calculating
average scores, the data suggests an evolution towards an
acquaintance-based community.


LLM Evaluator


The activities and dialogues conducted by the agents are recorded and
evaluated using GPT-4, employing a 5-point Likert scale to discern
whether the responses originated from an AI or a human. In this scale, a
higher score indicates a closer resemblance to human-like responses. The
complete questionnaire is provided in Appendix
[8.1]. The results presented in Table
[2] suggest that our method achieves scores
comparable to the baseline. Due to the advantages of LLM in natural
language dialogues, both methods approach human-level performance.
However, the performance of agents in activity is considered potentially
indicative of either AI or human behavior. The primary reason for
suspecting AI is the overly detailed, minute-level activities generated
by Generative Agents, which seems more akin to AI generation. A more
comprehensive summary of the criteria used by GPT-4 to discern between
AI and human entities can be found in Appendix
[8.2]. We hope these suggestions from the LLM
evaluator will guide us to create more believable agents in the future
work.


FIGURE 


TABLE 


Results on VirtualHome


In the VirtualHome, we have designed an agent with the aim of
experiencing a fulfilling day at home. The agent is required to generate
a sequence of activities to achieve the goal. For each activity, the
agent examines the items involved and the associated actions, creates a
corresponding action sequence, and ultimately translates this sequence
into actionable instructions, such as &quot;\&lt;char0\&gt; \[walk\] \&lt;curtains\&gt;
(32)&quot;. The complete implementation is provided in the appendix
[9] and an example activity generated by AGA is provided in Figure.


In contrast to *REF* and *REF* tested on
manually designed household tasks, we aim for the agent to generate
believable activities and decompose them into appropriate action
sequences, leading to the absence of a clear method to determine task
completion. Therefore, we request LLM agent to assess task completion
based on the action sequences performed and the relevant objects, and
decide whether to proceed to the next activity.


Table [3] presents the token consumption and task
success rates for two methods on VirtualHome. The token data is derived
from the total tokens required by the LLM agent to complete all
activities in a day. *MATH* refers to task success rate evaluated by
LLM, while *MATH* means evaluated by human. Both evaluation methods
indicate that AGA does not result in significant performance changes.
However, AGA costs only 3.4% of the baseline. Because VirtualHome
involves only single-agent interaction, the contribution predominantly
stems from the Lifestyle Policy. In some precise tasks, LLM agents tend
to predict task completion earlier, resulting in a higher success rate
than human evaluations. Furthermore, our analysis revealed that the main
cause of task failure is the inability to execute the generated plan
with the items present in the current room or the available actions.


FIGURE


Further Analysis


We delve into the mechanisms of emergent social behavior of generative
agents in sandbox environment. Through repeated experiments on
Generative Agents, we record the cumulative count of different types of
activities generated by the agents, as illustrated in Figure
[4]. The blue line represents the cumulative
number of new activities generated by three agents over successive runs.


An intriguing observation is that after numerous trials, the agents
cease to generate new activities. We discover a high consistency between
the behaviors generated by the agents and their inner profiles, aligning
with the perspectives of *REF*, who posit that dialogue
agents are role-playing characters described in the pre-defined dialogue
prompt. For instance, in a pre-defined 3-person town environment of
Generative Agents, Klaus Rodriguez and Maria Lopez are both students at
Oak Hill College. Designed with the lifestyle of having lunch at Hobbs
Cafe, they meet there to discuss their studies while encountering the
cafe owner, Isabella Rodriguez. Isabella, pre-set to host a Valentine&apos;s
Day party, invites as many people as possible, resulting in Klaus and
Maria being invited to the party the next day.


The randomness produced by LLM agents, based on the probability sampling
of the next token, primarily influences the variability in text
descriptions rather than the diversity of behaviors. This implies that
the emergence of social believable behavior is limited, which can be
completely overridden by policies.


From another perspective, traditional AI in the gaming sector is fully
predictable due to its limited behavioral patterns, thus failing to
provide an experience akin to human interaction. If the behaviors of LLM
agents are also limited, they stand to lose their advantage of
flexibility. Therefore, We propose a method that introduces variability
into the responses of LLM agents. Since agents generate replies based on
given prompts, diversifying the prompts is essential for eliciting
varied responses in identical scenarios. In each interaction, we
influence the agent&apos;s decision-making by randomly sampling events from
the agent&apos;s memory to serve as auxiliary information. The specific
details of this implementation are presented in the appendix
[10]. After all, human cognition is not
entirely logical. In the field of cognitive science, the concept of
mind wandering has long been introduced, referring to the phenomenon
where humans experience task-unrelated or stimulus-unrelated thoughts
[*REF*; *REF*]. Similar implementations have been
observed in LLM agents. For instance, *REF* demonstrates an
LLM agent composed of both rational and emotional systems, with behavior
that can be influenced by various factors. Figure
[4] illustrates the result of the mind
wandering method [*REF*], which entails incorporating randomly
sampled memory events into prompts. Compared to the original
implementation, the LLM agents exhibit a broader range of activities.


FIGURE


Conclusions and Future Work


This paper presents Affordable Generative Agents, a cost-efficient
framework for crafting believable interactions between agents and their
environment, as well as among agents themselves. A plethora of
experiments conducted across diverse environments substantiate our claim
that our approach can achieve performance on par with the baseline in a
cost-effective manner. Furthermore, our in-depth analysis reveals the
emergence of a finite set of believable behaviors in agents, indicating
the possibility of replacing them with cost-controlled policies.
Alongside this, we propose optimization strategies to encourage a
broader spectrum of behaviors in agents. Our work highlights the
promising potential of believable LLM agents for diverse future
applications.


While we expect AGA to serve as a starting framework for the generation
of affordable and believable agent interactions to the community, our
work has several limitations. First, AGA must be executed repeatedly to
store certain policies, with its primary benefit being cost savings
through batching rather than optimizations for single inferences.
Integrating AGA with existing cost-effective methods is a future
direction to move on. Second, optimizing the creation and storage of
Lifestyle Policy presents a significant opportunity. Additionally,
latency is a general issue for LLM agents to be considered in the
future.