Towards Objectively Benchmarking Social Intelligence for Language Agents at Action Level


Introduction


Large language models (LLMs) have become increasingly powerful owing to
the escalation in model magnitude and technical advancements. Recent
arts such as the GPT series [*REF*; *REF*; *REF*] showcase
human-level performance in diverse tasks within the natural language
domain. Capitalizing on the instruction-following and emerging
commonsense ability of LLMs, the derived language agents also
demonstrate human-level ability in various
scenarios [*REF*; *REF*; *REF*; *REF*; *REF*].


The endeavor to craft comprehensive language agents to simulate humans
has prompted substantial attention toward the evaluation of social
intelligence. In recent times, numerous simulators have been developed
for the social simulation of language
agents [*REF*; *REF*; *REF*; *REF*].
However, the challenge of quantitatively evaluating the emergent social
intelligence in these simulations persists. While previous works have
introduced numerous benchmarks and evaluation
metrics [*REF*; *REF*], we contend these
evaluations face two fundamental issues. Firstly, these evaluations
predominantly focus on the language level, assessing social intelligence
through posed questions. However, it remains conceivable that an agent
may claim they will perform a certain action without actually committing
to it, rendering language-level evaluations inadequate for reflecting
actual behaviors. Secondly, many evaluations hinge on subjective
metrics, diminishing the reliability of the evaluation results. We argue
that designing a social intelligence benchmark that can be evaluated
objectively at the action level, rather than subjectively at the
language level, is necessary.


In this paper, we introduce the Social Tasks in Sandbox Simulation
(abbreviated as STSS), an action-level benchmark that assesses
task-oriented social intelligence with objective metrics based on the
Smallville environment [*REF*]. We craft 30
templates of social tasks across 5 categories with corresponding
mechanisms to automatically evaluate agents within the simulation. The
overarching architecture of the STSS benchmark is illustrated in figure, 
where the agent is initiated with a social
task and is measured through goal achievement in the simulation. Since
the simulation may be economically expensive, we additionally construct
a complementary language-level benchmark by capturing chat scenarios
within the simulator, as a preliminary benchmark that aligns with
existing language-level benchmarks such as SOTOPIA [*REF*].
We conduct a comparative analysis between our proposed STSS benchmark
and prior works in *REF*.


Notably, our benchmark evaluates language agents instead of merely
language models, considering that the architecture of agents, i.e.,
how to use the language models is also important to social intelligence.
To verify our hypothesis, we introduce an additional Target-Driven
Planning (abbreviated as TDP) module that aims at social tasks for the
generative agents [*REF*]. While our framework is
versatile for evaluating language models within a fixed baseline agent
architecture, it concurrently serves as a testbed of language agent
architectures.


We conduct evaluations with various popular LLMs on both levels of our
benchmark. The results reveal that the social tasks remain challenging
even for cutting-edge models like GPT-4 [*REF*], underscoring
the potential space for improvement in language models. Moreover, a
well-designed TDP module demonstrates a substantial enhancement in the
performance of language agents at both levels, suggesting that our
benchmark can also serve as a testbed of agent architectures employing
fixed LLM.


Our contributions can be summarized as follows:
- We introduce STSS, a two-level benchmark for evaluating the social
intelligence of language agents, encompassing an action-level
evaluation in sandbox simulations and a preliminary language-level
evaluation in interactive conversations.
- We design a target-driven planning module for language agents to
investigate the influence and importance of agent architecture in
executing social tasks.
- We conduct extensive experiments with several state-of-the-art
models to gauge the capabilities of existing language agents. The
result also suggests the effectiveness of our benchmark in
evaluating agent architecture.


TABLE


Related Work


Social Intelligence Evaluation


Social Intelligence (SI) is widely recognized as the ability to
understand others and to act wisely in social situations
[*REF*]. In the pursuit of benchmarking social intelligence,
previous arts such as SOCIAL IQA [*REF*] and Social-IQ
[*REF*] evaluate the model through question answering (QA)
within given contexts. Recent works either proposed specialized
benchmarks [*REF*] for SI or evaluated SI on
LLMs [*REF*; *REF*; *REF*], both
revealing imperfections of current LLMs.


Moving beyond static and non-interactive QA, recent research advocates
evaluating LLMs in interactive benchmarks
[*REF*; *REF*]. Originating from social
requirements, target-oriented dialogue has been investigated both in
terms of model and benchmark [*REF*; *REF*; *REF*; *REF*].
Moreover, numerous studies also address the quality of conversations,
such as the personalization of the chatbot
[*REF*; *REF*; *REF*]. Taking a step
further, SOTOPIA [*REF*] introduces explicit social
goals/persona on the agents.


However, existing benchmarks either operate solely at the language level
or rely on subjective evaluation metrics. In contrast, our STSS
benchmark introduces an objective metric that assesses both language and
grounded actions, thereby enhancing the reliability of the evaluation.


Language agents


There has been a growing interest in constructing language agents by
integrating them with external environments and anchoring the language
to actions [*REF*; *REF*]. Recent studies
demonstrate that language agents can undertake complex tasks, including
game playing [*REF*], acting as research assistants
[*REF*; *REF*], robotic planning
[*REF*; *REF*], and even simulating humans
[*REF*]. A mass of benchmarks and environments have been built
for evaluating the language agents. ALFWorld [*REF*]
established the bridge between language and embodied robotics tasks.
AgentBench [*REF*] provides a comprehensive benchmark
including 8 tasks. Motivated by the human-like social behaviors of the
language agents, there also have been social simulators allowing
multi-agent interaction in a shared environment
[*REF*; *REF*; *REF*; *REF*].


It has been substantiated that the utilization of LLMs can significantly
influence the performance of language agents.
Chain-of-Thought(CoT) [*REF*], along with various other
works [*REF*; *REF*; *REF*; *REF*]
demonstrates the significance of prompt designation to the performance
of LLMs. Advanced agent architecture such as ReAct [*REF*] and
Relfexion [*REF*] further improve the performance in both
pre-hoc and post-hoc ways  [*REF*]. BOLAA [*REF*]
systematically investigates agent architectures and concludes that a
well-designed architecture with dedicated modules for specific tasks can
significantly enhance agent performance. For human simulation,
Generative Agents [*REF*] designs a complete
pipeline and preliminarily verifies the idea of social agents.


While simulations have provided a decent testbed of social behavior, few
works attempt to objectively evaluate the social intelligence of
language agents quantitatively. Our STSS benchmark fills the gap between
social intelligence evaluation and human behavior simulation, with the
potential to generalize to more realistic scenarios such as embodied
tasks and even real-world applications.


TABLE


Social Tasks in Sandbox Simulation


To objectively assess the social intelligence of language agents at the
action level, we suggest building benchmarks within interactive
simulators. The simulator can concretize the textual output into
tangible actions, facilitating a more objective and quantitative
evaluation through analysis of the state and trajectory of the
simulation environment.


We extend the interactive sandbox environment and the generative agent
architecture introduced by *REF* to accommodate the
social intelligence evaluation. Specifically, we devise a set of social
task templates and corresponding metrics, which can be quantified by
analyzing the actions of the agents. Take the example of holding a
Valentine&apos;s Day party, the metric could be the number of participants
at the correct time and location. These templates are then instantiated
to specific tasks, each associated with corresponding initial states of
the environment, being available for assessing language agents within
the simulation.


Task Designation *REF* 


We aim to evaluate several fundamental capabilities in social
interaction, including planning, expression, and
negotiation. Confronting with a social task, the agent naturally
needs to formulate a comprehensive plan and execute necessary actions
such as guest invitations and delivering key messages during
conversations, requiring the ability of planning across various
domains. When conversing with others, the capabilities of expression
and negotiation exhibit their significance. The agent not only needs
to express their thoughts and convey information sufficiently and
clearly but also needs to negotiate with others when encountering
matters to be determined. Any omission or miscommunication could hamper
the success of the task, even the other agent might still make verbal
consent.


Driven by the need for objective action-level evaluation, the design of
task templates focuses on the triad of who, when, and where.
Broadly, we design task templates that are categorized into five
distinct types facing different scenarios with diverse constraints:
1. Public activity with a fixed time and a fixed location, such as
a Valentine&apos;s Day party or a public lecture. The agent is required
to invite as many other agents as possible to the activity,
explicitly expressing the designated time and location in each
invitation. The objective metric is the number of participants who
attend at the correct location during the specified time.
2. Appointment with unconstrained time and location, e.g., a
student is trying to schedule a discussion with the professor. The
agent needs to locate the target person and arrange the appointment
to accomplish the designated purpose. The challenge lies in
negotiating with the target person to figure out a convenient time
and location for both agents through conversation. The criterion is
whether they execute the required action at the same place and time.
3. Inviting companions for an activity with unconstrained time and
a fixed location, such as gathering friends for a shopping outing.
The agent is tasked with inviting others to the activity and
negotiating the timing. The metric assesses how many participants
joined the activity at the correct location, with the initiator
being a mandatory attendee.
4. Online activity with a fixed time and no location requirement,
encompassing scenarios like an online workshop or playing online
games together. In contrast to public activities, online activities
do not necessitate participants to appear in a specific physical
location.
5. Asking for help, incorporating a series of activities at fixed
locations that have no time constraint. In this category of tasks,
the agent is assumed to be occupied or unable to do something,
seeking assistance from others. For example, Wolfgang, facing an
upcoming exam, needs a friend to borrow books from the library and
deliver the books to his dormitory. The criterion evaluates whether
other people execute the specific behavior correctly, e.g., first
borrowing books at the library and then delivering the books
at the dormitory of the student.


We incorporate a total of 30 social activity templates in the benchmark
across the five categories, where more details of the categories are
elaborated in *REF*. The task templates designate the
task type, activity name and description, and other task-related
information. They can then be instantiated to specific tasks by
determining the variables such as the background characters. To
simplify, we evaluate only one social task in each complete simulation,
where the task is assigned to a single agent at the beginning and the
other background characters remain agnostic to the task. To manage the
computational cost, we only sample 10-12 background characters in each
simulation.


Metrics


The performance is assessed through the trajectories in the simulation.
Each type of task is associated with a set of rules to analyze the agent
trajectories including their actions and positions, and generate a
quantitative score to measure the success of the social task. For
collective activities including public activity, online activity,
and inviting companions, scores are determined based on the ratio
between the actual and expected number of participants. For
appointment, the agent receives a full score if the goal was perfectly
achieved and a half score if the location is mismatched. For asking for
help, the score is the ratio of correctly performed sub-tasks. More
details can be found in the appendix. For all the tasks, a keyword
filter is applied to ensure that agents execute relevant actions and
filter out irrelevant agents who accidentally pass by.


We clarify that our benchmark presently concentrates on objective and
event-oriented evaluation, i.e., we only take the success of the task
into account, excluding the measurement of other aspects such as caring
about subjective feelings, personalization, or higher-level motivations.
We leave the holistic evaluation of both subjective feelings and
high-level social goals as future work.


FIGURE


Language-Level Benchmark


While comprehensive simulation-based evaluation is effective and
objective, it may be expensive in terms of both time and economic sense.
Besides, implementing a language agent to simulate human-like behavior
in environments such as generative agents [*REF*]
may impose a relatively high minimum requirement on the capabilities of
the language model. This hinders the universal availability of the
benchmark. To mitigate this constraint, we suggest building an
accompanying benchmark that focuses on situational dialogue, which is a
critical capability for social intelligence.


A conversation task is defined as a tuple (task, performer, target
character) and is curated by freezing the state from simulations. We
collect 325 conversation scenarios from numerous simulations of 30 tasks
instantiated from the templates. To ensure diversity, each task includes
at most two conversation instances for the same pair of characters
(although scenarios may already have different background information).
For appointment tasks, only conversations between the task performer and
the target person are incorporated into the benchmark. In contrast to
previous conversation benchmarks [*REF*; *REF*],
our conversation tasks are sliced samples from complete simulations and
the characters are associated with self-consistent memories in the
simulation worlds. These memories provide more diversity and naturalness
than purely synthetic tasks.


Metrics


To better align the conversation tasks with the simulations, we adopt an
event-oriented assessment for the generated conversations. For each type
of task, a set of goal achievement conditions is designed to measure the
conversation, e.g., correctly conveying or reaching a consensus on the
time or location of the event. The measurement of such conditions
resembles reading comprehension problems and can be solved by language
models. Compared to grading the conversation in continuous space, the
goal conditions present binary classification problems, which can be
easily aligned to human evaluation.


We introduce two metrics in the situational conversation benchmark:
- Success rate (SR). A task is considered successful if and only if
the agent achieves all the goal conditions (for example, conveying
all the key information such as time and location). The score is
either 1 or 0 for each task.
- Goal condition SR (GCSR). We borrow the idea of goal-condition
success from embodied vision-and-language tasks
[*REF*], where the score is the ratio of achieved
conditions. For example, in the public activity tasks, if the
agent (1) successfully makes the invitation; (2) appropriately
informs the location; and (3) forgets to inform the time, then the
goal-condition score will be *MATH*.
In consideration of the practical application, the produced conversation
is expected to be not only informative but also fluent, clear, and
concise. To encourage more effective conversations, we additionally
adopt summary-level evaluation, where the conversion is first summarised
by the other person (act by an improved version of the generative
agent), and then the same metrics are applied to evaluate whether the
summary includes the key information. The summary-level evaluation
additionally inspects the quality of utterance. For example, even if
containing key information, chaotic or blurry expressions may hinder the
listeners from correctly summarizing the information, let alone
performing the actions.


We also note that the number of conversations can differ sharply across
tasks, e.g., there will be only one valid conversation in appointment
tasks, but a task where a social butterfly is holding a party may
include up to 10 conversations. To enrich the dimension of evaluation,
we employ micro average and macro average for the scores. Let
*MATH* be the set of tasks, *MATH* denote the
conversation set of task *MATH*, and *MATH* be the score of a conversation
*MATH*, the micro average score is defined as: *MATH*. 
And the macro score averages across tasks, being defined as:
*MATH*. 


TABLE 


Task Driven Planning


Our benchmark focuses on evaluating language agents rather than merely
language models. Consistent with prior research findings
[*REF*], we assert that designing specialist agent architecture
is important to the application of language models. To enhance language
models in social tasks, we propose an additional module namely
target-driven planning (TDP) for the generative agent. We implement a
simple baseline, as presented in *REF*, where the language model is
first asked to provide a general plan, and then offer specific thoughts
for daily planning and conversation. These thoughts are then injected
into the corresponding module of the generative agent. The TDP module is
also applicable to the accompanying conversation benchmark, although
only the conversation module is active.


Our implementation does not incorporate advanced prompt techniques such
as CoT [*REF*] and ReAct [*REF*]. We clarify that our
implementation primarily serves as a baseline to inspect the
significance of language agent architectures in performing social tasks.
There remains ample room for improvement in the designation of agent
architectures.


Experiments


In this section, we present the experiments conducted on our benchmarks
to assess the social intelligence of language agents through performing
social tasks. Our experimentation centers on addressing two key
questions: (1) To what extent can existing language models and agents
execute social tasks within our STSS benchmark? (2) What is the
significance of the agent architecture in the context of social
intelligence?


Experiment Setup


We incorporate 4 language models in the evaluation: GPT-35 [^1]
[*REF*], GPT-4 [^2] [*REF*], Llama-2-13b-chat
[*REF*], and Baichuan-2-13b-chat [*REF*].


For the agents, we utilize prompt templates and parameters of generative
agents [*REF*] with minor adjustments for robustness
in both levels of the benchmark. In the simulation, GPT-35 is employed
as the language model for background characters, as this practice is
known to provide human-level intelligence [*REF*].
The evaluated model is used to generate target-driven plans,
conversations, and high-level planning for the performer agent, while
the low-level simulation is delegated to GPT-35. For language-level
evaluation, the task performer is operated by the evaluated language
agent, while the others are handled by vanilla generative agents with
GPT-35. We also use GPT-35 as the evaluator of conversation and summary.


Due to the minimum requirement of the instruction-following ability in
the simulation, we exclusively GPT-35 and GPT-4 in the complete
simulation. Since Llama2 and Baichuan2 exhibit instability when
following the complex instructions from the generative agent, we
simplify the prompt template of conversation generation in their
evaluation. Please refer to the appendix for more details about the
experimental setup to reproduce the empirical results.


In addition, to study the significance of the agent architecture, we
conduct experiments on all models using the vanilla generative agent
(GA) and the one with target-driven planning (GA + TDP). For GA, we
adhere to the practice of *REF*, injecting the
social goal description into the currently field of the agent persona.


Overall performance


We present the language-level evaluation results in TABLE and the action-level evaluation in TABLE.


In general, at the language level, GPT-4 performs the best, followed by
GPT-35, Baichuan2, and Llama2. When evaluating the conversation, GPT-4
with TDP achieves a success rate of 81.2% and a goal-condition success
rate of 90.2%, demonstrating its robust ability in target-oriented
conversations. Surprisingly, Baichuan2 outperforms GPT-35 in the vanilla
setting, although GPT-35 performs better with the TDP. We hypothesize
that Baichuan2 possesses a better innate inferential capability for
planning, whereas GPT-35 excels in generating higher-quality English
utterances. Even though GPT-35 surprisingly performs comparably to GPT-4
with TDP in the conversation-level evaluation, it falls short when
evaluating the summary, implying GPT-4 still produces higher-quality
content. The consistent discrepancy between conversation and summary
scores indicates a ubiquitous need for improvement in conversation
quality.


While GPT-4 showcases commendable performance in the sampled situational
dialogues, executing social tasks in a sandbox simulation remains
challenging. Even the most proficient agent (GPT-4 with TDP) only
achieves an average score of 0.550. When comparing task types, we find
that making appointments and inviting companions emerge as the most
challenging tasks. In these tasks, agents must inform others and
negotiate regarding time or locations, necessitating the seamless
integration of all abilities mentioned in
Section [3.1].


Comparing Language-Level and Action-Level Evaluations


We observe a general consistency between the action-level and the
language-level evaluations, as reflected in the positive correlation
between the scores achieved by each agent. However, all agents
experience a decrease in performance from language-level to
action-level. This supports our hypothesis that the sandbox simulation
is more complex and challenging. By inspecting the number of initiated
conversations, we identify threefold challenges: (1) The agent may not
always meet enough other agents to chat with in the simulation, which
depends on its itinerary and fundamentally its ability to plan; (2) The
agent may struggle to effectively communicate information or determine
an appropriate time and location during a conversation; (3) Even when
information is accurately conveyed and understood, other agents may not
always correctly ground it into actions.


Sandboxes can simulate the nature of the real world, exposing new
drawbacks of language agents that can not be reflected by language-level
evaluation alone. For instance, GPT-4 with TDP unexpectedly failed in an
inviting companions task where Wolfgang needs to invite friends to
exercise with at Johnson Park. In the simulation, Wolfgang achieved a
verbal agreement with several agents to meet at Johnson Park tomorrow at
6:00 am. However, despite the appointment, most agents failed to wake up
until 7:00 am the next day, resulting in the appointment being missed.


TABLE


Effectiveness of TDP


For all language models, the GA + TDP architecture consistently
outperforms the vanilla GA. The TDP module yields notable improvements
in both levels of evaluation, and can even compensate for the shortage
of model capabilities. For instance, GPT-35 and Baichuan2 agents with
the TDP module can outperform the vanilla GA using GPT-4. Despite GPT-35
being empirically weaker than GPT-4, their conversation scores become
almost comparable after equipping the TDP module. This suggests that
when a language model exhibits capabilities above a certain threshold,
the agent architecture may be the more noteworthy part for further
improvement.


The case of Isabella&apos;s Valentine&apos;s Day party at Hobbs Cafe, previously
presented in *REF*, provides insight into the
performance boost facilitated by TDP. A comparison between invitations
from Isabella with/without the TDP module is demonstrated in
Table [1]. The conversation reminder generated by the
TDP prompts the agent to clearly state the date, time, and location of
the party, which is vital for the success of the task. The invitation
generated with the TDP also encourages the invitee to bring more
friends, further facilitating the success of the social task. Please
refer to the appendix for more details.


Conclusion


In this paper, we introduce the Social Tasks in Sandbox Simulation
(STSS) benchmark, which is designed to assess language agents by
engaging them in social tasks within a sandbox simulation. The
accompanying language-level benchmark also serves as a preliminary
evaluation for weaker models. Additionally, we propose a target-driven
planning module for generative agents to investigate the significance of
designing a specialized agent architecture in social intelligence. A
comprehensive evaluation involving four prominent language models
validates the efficacy of our benchmark for assessing both language
models and agent architectures.


Limitations 


Though simulators offer numerous advantages, economic considerations
pose a potential limitation. Based on the 2023 pricing of the OpenAI
API, evaluating a single language agent on the 325 scenarios at the
language level costs approximately $7.5, whereas a comprehensive
sandbox evaluation of 30 tasks costs around $300. Due to the cost
constraints, we do not conduct repetitive simulations, which could cause
random fluctuations in the scores.


We also acknowledge that a gap exists between the sandbox simulation and
the real world. Before migrating agents to realistic applications, it
might be necessary to study the alignment between simulated performance
and human evaluation.