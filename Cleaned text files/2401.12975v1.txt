HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments


Introduction


Embodied agents operate in a dynamic world that exhibits constant changes. This world experiences various changes at every moment, including the rising and setting of the sun, the flow of rivers, weather variations, and human activities. To successfully navigate and function in such an ever-changing environment, robots are required to perceive changes in their surroundings, reason the underlying mechanisms of these changes, and subsequently make decisions in response to them.


FIGURE


To simulate a dynamic world, it is necessary to create environments that can spontaneously undergo changes. Currently, various simulation platforms have emerged in the field of embodied AI, including iGibson [*REF*], Habitat [*REF*], SAPIEN [*REF*], VirtualHome [*REF*], AI2THOR [*REF*], ThreeDWorld (TDW) [*REF*], etc. Existing tasks on these simulation platforms involve agent exploration and agent-driven interactions, but they lack support for environment-driven changes, which are rather influential and unpredictable. The iGibson 2.0 [*REF*] platform partially supports spontaneous environmental changes to a limited extent, but these changes are limited to the propagation of a few variables between individual objects.


In this paper, we propose the HAZARD challenge, an innovative exploration of embodied decision-making in dynamic environments, by designing and implementing new capabilities for physical simulation and visual effects on top of the ThreeDWorld. HAZARD manifests itself in the form of unexpected disasters, such as fires, floods, and wild winds, and requires agents to rescue valuable items from these continuously evolving and perilous circumstances.


The HAZARD challenge places agents within indoor or outdoor environments, compelling them to decipher disaster dynamics and construct an optimal rescue strategy. As illustrated in Figure [1], the scenarios vary in severity and complexity. An indoor fire scenario might involve the rapid spread of flames, threatening flammable target objects. In an indoor flood scenario, an overwhelming volume of water inundates the house, jeopardizing non-waterproof targets. In an outdoor wind scenario, strong winds scatter lightweight objects across roads, making retrieval a challenging task for agents. To successfully rescue target objects from these disasters, agents must effectively transfer them to safe zones such as backpacks or shopping carts.


To facilitate this endeavor, we introduce a comprehensive benchmark comprising these disaster scenarios, complete with quantitative evaluation metrics. We also provide an API to employ large language models (LLMs) for action selection. This API integrates visual observations and historical memories into textual descriptions, thereby providing a semantic understanding of the dynamic environment. To optimize the use of LLMs, we compress a large volume of low-level actions by A\ algorithm, significantly reducing the frequency of LLM queries.


We evaluate both LLM-based agents and several other decision-making pipelines on our benchmark, including a rule-based pipeline that operates based on a simple set of rules, a search-based pipeline that utilizes the Monte Carlo tree search (MCTS) algorithm for action selection, and a reinforcement learning-based pipeline. Through our experiments, we find while the LLM pipeline is capable of understanding and considering certain basic factors, such as object distance, it may encounter challenges in comprehending and effectively handling more complex factors, such as the dynamic nature of environmental changes.


The main contributions of our work are: 1) designing and implementing a new feature that enables the simulation of complex fire, flood, and wind effects for both indoor and outdoor virtual environments in TDW; 2) developing a comprehensive benchmark, HAZARD, for evaluating embodied decision-making in dynamically changing environments, as well as incorporating the LLM API into our benchmark; and 3) conducting an in-depth analysis of the challenges posed by perception and reasoning for existing methods, especially LLM-based agents in tackling the proposed benchmark.


Related Work


Simulators for Embodied AI The recent advance of embodied AI has largely been driven by the development of simulation platforms. While earlier platforms primarily focused on supporting agent exploration [*REF*; *REF* -lab; *REF*; *REF*; *REF*], recent platforms [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*] have advanced by enabling physical agent-driven interactions. In this paper, we specifically focus on the impact of environment-driven changes on embodied agents, which remains a relatively unexplored area of research. Earlier works either supported spontaneous changes in the environment within limited ranges [*REF*], different environmental impacts on agent actions [*REF*], or just focused on identifying such changes, which occurred only during each reset [*REF* -the-difference].


Embodied AI with Large Language Models Recently, large language models (LLMs) [*REF*; *REF*; *REF*; *REF*] have made remarkable strides in the field of AI, and their potential in embodied AI tasks has also been widely investigated and explored [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
LLMs are capable of providing contextual information [*REF*], building inner monologues [*REF*], providing model initializing weights [*REF*], and error correction [*REF*] to enhance the planning of embodied agents. More directly, LLMs can generate policy code [*REF*] or produce plans for embodied agents [*REF*; *REF*]. Different from previous works, we explore the planning and decision-making ability of LLMs in dynamically changing environments. We also implement APIs for LLM-based agents in HAZARD, providing support for future research in this area.


Search and Rescue Search and rescue (SAR) is a widely explored area for robotics. Most of the existing projects and programs in robot SAR focus on using unmanned aerial vehicles [*REF*; *REF*; *REF* -fire; *REF*; *REF*; *REF*], unmanned ground vehicles [*REF*; *REF*; *REF*], and unmanned underwater vehicles [*REF*; *REF*; *REF*] for searching one or more target in mostly static scenes. Some previous works also provide a dynamic simulation of disasters but are restricted to 2D abstract environments [*REF* -rescue-simulation] or limited scenes [*REF*]. Different from these works, we provide simulation for dynamically changing environments in embodied scenes and focus on decision-making in these environments.


The HAZARD Challenge


Overview


The HAZARD challenge sets itself apart through its distinctive dynamic scenarios, requiring high-quality physical and visual simulation of significant environmental changes such as fire, flood, and wind. To realize our objectives, we have developed an environment change simulation framework, premised on the robust foundation of the ThreeDWorld platform. This framework offers a reliable physical simulation system, with a versatile renderer capable of producing visual effects tailored to each type of environmental change. In this section, we delve into the implementation details of both how we simulate these dynamic environment changes and our new dataset.


FIGURE


Scenes


Fire


In the fire scenario, we simulate an indoor scene of a room on fire.
This dynamic environment presents various changes to the agent, including the spreading visual effects of the fire, changes in temperature, and the transformation of objects as they burn or become burnt. To capture these effects accurately, we implement a temperature system and integrate it with the visual effects using the ThreeDWorld simulator.


Temperature The temperature system simulates the heat transfer process among all objects in the environment. According to the second law of thermodynamics, in this system, heat is transferred from objects with higher temperatures to those with lower temperatures. However, it is infeasible to simulate real heat transfer to a fine-grained level due to computational limits. Instead, we adopt an expedient approach described below.


For each object *MATH* in each time frame, we update the temperature *MATH* where *MATH* is the decay rate.
*MATH* simulates the heat transfer from the environment and is defined as the weighted average of room temperature *MATH* and surrounding objects *MATH*. The weight for *MATH* is a pre-defined constant *MATH* and the weight for *MATH* is *MATH* where *MATH* is the distance threshold and *MATH* is the distance between objects *MATH* and *MATH*. In summary, we have pre-defined constants *MATH*, decay rate *MATH* and distance threshold *MATH* to simulate real-life heat transfer between objects.


Not only the objects can catch on fire. In our setting, we divide the floor into 2-D grids where each grid can burn separately. Since the grid size is set small, it&apos;s infeasible to assign a temperature to each grid and update them every frame. Instead, we use a spreading model where a fire having burnt for time *MATH* has probability *MATH* to spread to a nearby grid. *MATH* is a linear function so that after sufficient time the fire must spread.


Object status and visual effect To simulate the visual effects generated by a spreading fire, we define three distinct object statuses similar to [*REF*]: normal, burning, and burnt. Objects in the normal status exhibit no additional visual effects. An object becomes burning once its temperature reaches the ignition point. As illustrated in Figure [2], burning objects are adorned with a fire visual effect on the top, with the scale of this visual effect gradually amplifying as the object combusts over subsequent frames.
After a designated burning duration, an object becomes burnt and has a black hue to represent the burnt state.


Flood


The flood scenario is designed to simulate the spread and rise of water within an indoor room. This scene introduces the following changes to the agent: (a). The flood gradually submerges the objects within the scene, making them challenging to recognize and causing damage to non-waterproof objects, and (b). objects with low density have the tendency to float on the flood surface and may be swept away by the force of the flood.


Visual effect As Figure [2] shows, the flood surface is designed to be translucent, allowing the agent to perceive both the flood surface itself and the objects located beneath it. To simulate the spread of the flood, we rotate the surface into a sloped position and make it gradually rise and move forward. This combination of visual effects accurately depicts the dynamic nature of a spreading flood.


Physical simulation For each object in the flood, we incorporate two forces to simulate the physical effects of the flood: buoyancy and drag force. The buoyancy force, denoted as *MATH*, acts in the opposite direction to gravity and its magnitude is determined by *MATH*, where *MATH* denotes the product of the volume of the submerged portion of the object, *MATH* denotes the gravitational acceleration, and *MATH* denotes the density of the flood. On the other hand, the drag force, denoted as *MATH*, is calculated using the drag equation *MATH*, where *MATH* represents the relative velocity of the object with respect to the fluid, *MATH* denotes the drag coefficient, and *MATH* represents the vertical area of the object. The direction of *MATH* is opposite to the relative speed of the object in relation to the fluid.


Wind


In contrast to the fire and flood scenarios, the wind scenario simulates an outdoor scene where objects are affected by intense and turbulent winds. As a result, the primary dynamic feature of this scenario is the movement of objects induced by powerful wind forces. In real life, determining wind forces is a complex topic in aerodynamics, so we take an ideal model. Specifically, we assume the wind has a fixed velocity everywhere and objects have a face vertical to this velocity. With some physics derivation, The force is *MATH* where *MATH* is the air density, *MATH* is the relative wind velocity and *MATH* is the vertical area facing the wind. We modify it by setting *MATH* in which *MATH* and *MATH*.
*MATH* is a random vector of fixed length so that *MATH* simulates a random turbulence minor compared to *MATH*. We also apply a random torque to each object to account for forces applied not on the center of mass. The magnitude of forces and torques is hand-adjusted to create a dynamically changing scene.


To reduce the difficulty of the HAZARD challenge, environmental impacts like temperature, flood forces, and wind forces do not affect agents in the default setting. Additionally, we explore a more challenging scenario where agents are influenced by these environmental effects in Appendix [8].


Procedural Scene Generation 


Based on the ProcGenKitchen pipeline from ThreeDWorld simulator, we develop a procedural generation pipeline tailored for the HAZARD challenge. This pipeline enables the generation of diverse scenes with dynamic changes. Initially, a basic scene is generated using ProcGenKitchen. Subsequently, we introduce additional elements to this basic scene in a random manner, including (a). target objects and additional objects on both floor and surfaces of the existing objects, such as tables and chairs, (b). agents&apos; initial positions and directions, and (c). initial positions of the fire sources. This procedural generation pipeline ensures the creation of various dynamically changing scenes for the HAZARD challenge.


Benchmark details


FIGURE


Problem definition In the HAZARD challenge, the objective for an embodied agent is to rescue a predetermined set of objects, denoted as target objects, and bring them to a given safe location such as a bag held by the agent or a shopping cart. As Figure [3] shows, at each step, an agent receives an RGB-D observation, semantic segmentations of the observation, and a blurred environment-specific observation (temperature in the fire scenario and water level in the flood scenario). We also provide a perceptional version of HAZARD which excludes semantic segmentation from inputs. Given these observations, agents must make appropriate choices from the available agent action space at each step to accomplish the mission.


Target objects The objects that agents are required to rescue are denoted as &quot;target objects.\&quot; These objects are randomly placed on the floor or other surfaces within the environment. In each fire or flood scene, we randomly select 4 categories of objects from a universal target category pool, which consists of 22 object categories for fire or flood scenarios, and 11 object categories for wind scenarios. All objects falling within these selected categories are considered target objects. The target category pool encompasses objects that exhibit diversity across four attributes: object value, waterproof capability, ignition point, and susceptibility to the wind.


Agent action space In our study, we utilize ThreeDWorld Replicant, a humanoid agent with basic actions such as Move Forward By, Turn By, Reach For, and Reset Arm. To accomplish the proposed task, we introduced compressed actions derived from these foundational actions.
Specifically, the Explore action combines several Turn By actions, enabling agents to swiftly perceive their environment. A robust Pick Up action, formed by Reach For and Reset Arm actions, allows the agent to grasp target objects effectively. Additionally, a Drop action helps the agent put down held objects. Therefore, once the agent reaches the object, it can utilize the Pick Up action to hold the object and subsequently use the Drop action to position it in a safe location.
HAZARD also supports direct utilization of low-level actions, including &apos;move_by&apos;, &apos;turn_by&apos;, and &apos;turn_to&apos;.


Support for LLM-based pipelines The proposed benchmark also supports using LLMs as decision-makers. However, a significant challenge arises when querying the LLM for actions at each step, as it can lead to frequent queries and subsequently lower inference speed. To solve this problem, we implement agent navigation to objects using the A\ algorithm. This navigation algorithm compresses a large number of Move Forward By and Turn By actions into a single Walk To action. As a result, apart from the Pick Up and Drop actions, the LLM is only queried to select which objects to Walk To. Based on this design, we implement efficient APIs for LLM-based pipelines in HAZARD.


Evaluation metrics For HAZARD challenge, we use rescued value rate (Value), averaged rescue step (Step), and averaged damaged rate (Damage) as evaluation metrics. The rescued value rate is calculated as the ratio of the total value of the rescued objects to the total initial value of all target objects. Objects that are damaged due to environmental changes will lose half of their value. Specifically, in the fire scenario, objects lose their value once they start burning. In the flood scenario, a non-waterproof object loses its value if it becomes submerged by the flood. The averaged rescue step, as a measurement of efficiency, is defined as the averaged step number to rescue an object for an agent. To improve the efficiency of the evaluation process, we set a time limit of 1,500 frames for tasks fire and flood while a longer 3,000 frames is set in the wind scenario. The averaged damaged rate (Damage) is calculated as the proportion of damaged objects among the objects that the agent rescued. Note that objects can only be damaged in a fire or flood scenario.


Building LLM-Based Pipeline for Embodied Agents 


FIGURE


Taking inspiration from recent research [*REF*; *REF*], we employ an LLM as the decision maker to determine the target destination for embodied agents.
As illustrated in Figure [4], to enable the perception of LLM, we convert the information from the environment into text descriptions. Then LLM is required to select a proper action, which is subsequently converted into multiple low-level actions for execution. We provide the LLM decision maker in this step with the following information:


- Task description: The first part of the prompt consists of a manually designed description of the current task. The description briefly introduces the challenges the disaster agent needs to face, the overall goal of the agent, and the format of the following prompt parts.
- Target information: After the task description, we provide target information for LLMs, including the names, values, and properties of target objects.
- Current state: To provide LLM with comprehensive information about the current state, we convert visual observations, thermal or water-level signals, and semantic masks into a 2D semantic map. For the perceptional version of HAZARD, input semantic mask is replaced with a segmentation proposal provided by a perception model. Then the LLM is provided with a textual description of the semantic map, including object distance, temperature, water level, and value.
- Observation memory: To help LLMs infer the dynamics of the environment and predict future changes, we have designed an observation memory to store the historical states. During inference, LLMs utilize the observation memory by incorporating the description of each historical state into the prompt.
- Available actions: The prompt concludes with a list of the currently available actions. Therefore, LLMs make decisions by solving a multi-choice problem.
- Other information: The prompt also includes other necessary information, such as the agent&apos;s history of actions taken and its current status.


The detailed format of the prompt can be referred to as shown in Figure [7] in the Appendix.


Experiments


Experimental Setup


Traning and testing setup To create the dataset for HAZARD, we choose 4 distinct indoor rooms for the fire and flood tasks, and 4 outdoor regions for the wind task. Within each room or region, we generate 25 diverse scenes using our procedural generation pipeline (described in Section [3.2.4]). One indoor room and one outdoor region are selected as the test set. As a result, we obtain a total of 100 unique scenes for each task, with a train-set split ratio of 3:1.


Details of language model backbone We evaluate the LLM-based agent in Section [4] with 3 different LLM backbones, including Llama-13b-chat model [*REF*], OpenAI GPT-3.5-turbo (August 3 Version), and OpenAI GPT-4. We use max tokens of 512, temperature of 0.7, top p of 1.0 as hyper-parameters during inference.


Perception Module To get semantic segmentations in the perceptional version of HAZARD, we use OpenMMLab detection framework [*REF*] to implement our perceptional model. We collect *MATH* images with ground truth segmentation in each training instance and use the collected data to fine-tune a Mask-RCNN [*REF*] model provided by OpenMMLab.


Baselines


We implement several baseline agents for evaluations as follows.


Random agent An agent randomly selects low-level actions to execute.
To emphasize the hardness of our challenge, we provide more informative actions including walking to the nearest target object (container), picking up (dropping) the nearest object and exploring.


RL model We also trained reinforcement learning models using Proximal Policy Optimization (PPO) [*REF*]. The actions are the same as described in the random agent. We design the function that rewards picking up and dropping correctly while penalizing actions that fail or have no effect. To make the reward function smoother, we also add a factor of the distance from the agent to the nearest object.


Rule-based agent An agent randomly chooses a target object to rescue. After selecting the target object, the agent automatically walks to the object, picks it up, and drops it into a safe place.


MCTS agent Monte Carlo Tree Search (MCTS) [*REF*] is a commonly used algorithm in decision-making problems, which has an effective balance of exploration and exploitation. Since it is hard to get the ground truth frame costs of each action, we design several kinds of heuristic costs for MCTS, such as navigation heuristics, grasp heuristics, drop heuristics, and exploration heuristics. After that, we use MCTS to find an action plan with minimal total cost.


Greedy agent A simple greedy agent that persists in rescuing the nearest target object with the lowest cost. This agent chooses actions randomly when there are no target objects in observation or memory.


Experimental Results


TABLE 1


Quantitative results According to the quantitative results in Table [1], the proposed HAZARD benchmark presents a significant challenge, as all the Random, Rule, and Greedy methods exhibit poor performance across all three scenarios. The MCTS method inherently reasons the environment changes through simulation and therefore performs the best among baseline methods. Surprisingly, although not finetuned on the training data, the LLM pipeline demonstrates superior performance compared to most baseline methods on three scenarios, showing its strong zero-shot decision-making capabilities. Furtherly, the results indicate a clear difference in decision making ability among different LLMs, as the GPT-4 model outperforms both GPT-3.5-turbo model and LLaMa-13b chat model by a large margin. Notably, in the wind scenario, all methods struggle with a remarkably low Value score, highlighting the difficulty of reasoning the movements of objects.


Perceptional results According to Table [1], all methods show reduced performance in the with perception scenario, highlighting the challenges of perception in dynamic environments. The perception model struggles to detect objects submerged in water or obscured by flames. Interestingly, in wind scenarios, this perceptual difficulty can be beneficial, as agents stop pursuing objects blown far away. Notably, despite these challenges, LLM-based agents still demonstrate competitive and robust decision-making ability.


FIGURE


Qualitative results As illustrated in Figure [5], the LLM pipeline shows the ability to take into account basic attributes during decision-making processes, enabling it to make rational choices in some cases. For instance, in the fire scenario, the LLM pipeline demonstrates comprehensive consideration of object temperature, distance, and value in the reasoning path. Accordingly, the LLM finally selects a valuable target with a low temperature, which is an optimal choice in this situation.


Failure cases In Figure [6], we provide two failure cases of the LLM pipeline. In the first case on the left, the LLM struggles with effectively considering dynamics during decision-making. The LLM pipeline chooses to walk towards the closest target, the backpack object. However, the object is swiftly carried away by the wind, leading to the failure of the LLM in catching up with the backpack object. In the second case on the right, the LLM pipeline suffers from inconsistency between its reasoning and prediction. Despite the thought process indicating that the optimal choice is to search for other target objects, which leads to hairbrush with id 66, it ultimately fails to select the desired target as its final decision.


FIGURE


Conclusion


We introduce HAZARD, a novel challenge with dynamically changing environments. To support environment changes, we develop a simulation system on top of the ThreeDWorld platform. This system includes a physical simulator and a visual effect generator, enabling simulations of fire, flood, and wind scenarios. Leveraging this framework, we design an object rescue task for embodied agents and generate a dataset for this task. Subsequently, we evaluate and analyze the performance of large language model (LLM) agents and existing baseline methods using the generated dataset. However, the HAZARD challenge focuses only on object rescue. In the future, we will introduce more actions to the simulator to allow agents to mitigate environmental changes (e.g., using an extinguisher to put out fires).