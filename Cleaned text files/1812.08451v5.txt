Optimizing Quantum Error Correction Codes with Reinforcement Learning


Introduction


Quantum computers hold the promise to provide advantages over their classical counterparts for certain classes of problems [*REF*; *REF*; *REF*].
Yet, such advantages may be fragile and can quickly disappear in the presence of noise, losses, and decoherence. Provided the noise is below a certain threshold, these difficulties can in principle be overcome by means of fault-tolerant quantum computation [*REF*; *REF*]. There, the approach to protect quantum information from detrimental effects is to encode each logical qubit into a number of data qubits. This is done in such a way that physical-level errors can be detected, and corrected, without affecting the logical level, provided they are sufficiently infrequent [*REF*]. Quantum error correction (QEC) codes thus allow for devices usually referred to as quantum memories [*REF*] that can potentially store quantum information for arbitrarily long times if sufficiently many physical qubits are available. However, physical qubits will be a scarce resource in near-term quantum devices. It is hence desirable to make use of QEC codes that are resource-efficient given a targeted logical error rate.
Yet, while some types of errors can be straightforwardly identified and corrected, determining the most suitable QEC strategy for arbitrary noise is a complicated optimization problem. Nevertheless, solutions to this complex problem may offer significant advantages, not only in terms of resource efficiency but also error thresholds [*REF*; *REF*].


Here, we consider a scenario where certain QEC codes can be implemented on a quantum memory that is subject to arbitrary noise. Given the capacity to estimate the logical error rate, our objective is to provide an automated scheme that determines the most economical code that achieves a rate below a desired threshold. A key contributing factor to the complexity of this task is the diversity of the encountered environmental noise and the corresponding error models. That is, noise may not be independent and identically distributed, may be highly correlated or even utterly unknown in specific realistic settings [*REF*; *REF*]. Besides possible correlated errors, the error model might change over time, or some qubits in the architecture might be more prone to errors than others.
Moreover, even for a given noise model, the optimal choice of QEC code still depends on many parameters such as the minimum distance, the targeted block error rate, or the computational cost of the decoder [*REF*]. Determining these parameters requires considerable computational resources. At the same time, nascent quantum computing devices are extremely sensitive to noise while having only very few qubits available to correct errors.


For the problem of finding optimized QEC strategies for near-term quantum devices, adaptive machine learning [*REF*] approaches may succeed where brute force searches fail. In fact, machine learning has already been applied to a wide range of decoding problems in QEC [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Efficient decoding is of central interest in any fault-tolerant scheme.
However, only a limited improvement can be obtained by optimizing the decoding procedure alone since decoders are fundamentally limited by the underlying code structure. Moreover, a number of efficient decoders already exist for topological codes [*REF*; *REF*; *REF*]. This paper deals with an entirely different question: Instead of considering modifications of an algorithm run on a classical computer to support a quantum computation, we ask how the structure of the quantum mechanical system itself can be changed, both a priori and during a computation.
More specifically, we present a reinforcement learning (RL) [*REF*] framework for adapting and optimizing QEC codes which we apply to a family of surface codes in a series of simulations.
Through an adaptive code selection procedure, our approach enables tailoring QEC codes to realistic noise models for state-of-the-art quantum devices and beyond. The proposed scheme can be employed both for off-line simulations with specified noise models and for on-line optimization for arbitrary, unknown noise, provided that potential hardware restrictions are taken into account. In particular, we demonstrate how the presented learning algorithm trained on a simulated environment is able to transfer its experience to different, physical setups. This transfer learning skill is both remarkable and extremely useful: Maintaining the option to switch from one scenario to another, we can train a learning algorithm on fast simulations with modeled environments before optimizing the QEC code under real conditions. Transfer learning thus offers the ability to bootstrap the optimization of the actual quantum device via simulations.
These simulations are comparably cheap since resources are much more limited under laboratory conditions. We show that our scheme is sufficiently efficient for setups expected to be available in the near-future [*REF*; *REF*; *REF*].
In addition, these methods are parallelizable and hence expected to perform well also for larger system sizes. Our results thus suggest that RL is an effective tool for adaptively optimizing QEC codes.


Framework &amp; Overview


We consider a situation where a learning algorithm referred to as an &apos;agent&apos; interacts with an &apos;environment&apos; by modifying a given QEC code based on feedback from the environment. As illustrated in Fig. [1], this environment consists of a topological quantum memory [*REF*] (subject to noise) and its classical control system guiding the QEC. In each round of interaction, the agent receives perceptual input (&apos;percepts&apos;) from the environment, that is, some information about the current code structure is provided.
The agent is tasked with modifying the code to achieve a logical error rate below a desired threshold. To this end, the agent is able to perform certain actions in the form of fault-tolerant local deformations [*REF*; *REF*] of the code.
The agent is rewarded if the logical qubits have been successfully protected, i.e., if the logical error rate drops below the specified target. The problem of adapting QEC codes thus naturally fits within the structure of RL. Here it is important to note that the agent optimizing the quantum memory is oblivious to the details of the environment. In particular, the agent cannot discern whether the environment is an actual experiment or a simulation.


FIGURE 


The quantum memory we consider is based on the surface code [*REF*; *REF*], one of the most promising candidates for practical QEC [*REF*]. In particular, this versatile code is easily adaptable to compensate for a wide range of different types of errors [*REF*]. Accordingly, we consider independent and identically distributed (i.i.d.) as well as non-i.i.d.
errors on data qubits. We assume that a logical qubit is initially encoded in an 18-qubit surface code that can be extended by the agent by adding up to 50 additional data qubits via fault-tolerant local deformations [*REF*]. Our simulations assume arbitrary qubit connectivity which allows the most flexibility to illustrate our approach, but the method can be adapted to different topologies that one may encounter in a specific hardware, as we discuss in Sec. [3.3]. At the heart of the classical control system is the SQUAB algorithm [*REF*; *REF*] simulating an optimal decoder in linear-time. SQUAB returns an estimation of the logical error rate in the (simulated) QEC procedure. As a specific learning model we employ Projective Simulation (PS) [*REF*], which has been shown to perform well in standard RL problems [*REF*; *REF*; *REF*], in advanced robotics applications [*REF*], and recently PS has been used to design new quantum experiments [*REF*].


TABLE 


Within this framework, we demonstrate that agents can learn to adapt the QEC code based on limited resources to achieve logical error rates below a desired value for a variety of different error models. Moreover, we show that agents trained in such a way perform well also for modified requirements (e.g., lower thresholds) or changing noise models, and are thus able to transfer their experience to different circumstances [*REF*; *REF*]. In particular, we find that beneficial strategies learned by agents in a simulated environment with a simplified error model that allows for fast simulation of QEC [*REF*] are also successful for more realistic noise models. This showcases that agents trained on simulations off-line could be employed to &apos;jump start&apos; the optimization of physical quantum memories on-line in real-time. One reason for this adaptivity is that the agent obtains no information about the specifics of the noise, i.e., the latter appears to the agent as a black box environment. This further implies that our approach is hardware-agnostic, i.e., it can be applied to the same problem on different physical platforms, including trapped ions [*REF*], superconducting qubits [*REF*], or topological qubits [*REF*]. Depending on the particular platform, on-line optimization may be limited by specific hardware constraints (e.g., nearest-neighbour interactions), restricting, e.g., the possible code deformations (see Sec. [3.3]). Nevertheless, off-line optimization still remains a viable option even for architectures with severe restrictions.
Finally, note that the presented adaptive QEC framework in Fig. [1] goes beyond the particular code, decoder, noise model, and RL method used here, and hence offers potential for extensions in all of these regards.


The remainder of this paper is structured as follows. In Sec. [3], we briefly review the surface code in the stabilizer formalism [*REF*] with emphasis on its inherent adaptability.
Sec. [4] introduces the concept of RL and the specific algorithm used. In Sec. [5] various different scenarios within our framework are investigated. In Sec. [6](sec:on-off-line){reference-type=&quot;ref&quot; reference=&quot;sec:on-off-line&quot;}, we discuss the efficiency of our simulations and analyze the prospect of using results obtained in a simulated environment for more realistic, experimental settings. We conclude with a summary and outlook in Sec. [7].


Adaptable Quantum Memory 


Surface Code Quantum Memory


The purpose of a quantum memory is to encode logical information in such a way that small enough perturbations do not change the stored information. This is achieved through QEC, which can, in parts, be understood within the stabilizer formalism [*REF*]. Although a detailed background in QEC is not necessary to understand the basic principle of our approach, which can be viewed as the problem of restructuring a lattice (see Sec. [3.2]), let us briefly illustrate the stabilizer formalism for the example of a surface code quantum memory [*REF*].


FIGURE 


The surface code can be defined on any lattice embedded on a 2D manifold where data qubits are located on lattice edges (see Fig. [2]). These codes are called surface codes because each code in the class can be associated with a tessellation of a 2D manifold or surface [*REF*]. For brevity, we consider lattices embedded on tori which define a subclass of surface codes, called toric codes [*REF*]. However, the methods introduced in this paper can be easily extended to other surfaces or lattices with boundaries [*REF*], and we hence just refer to surface codes in the following. Now, let us define products of Pauli operators for each vertex *MATH* and plaquette *MATH* as follows, *MATH*. Here, *MATH* is the set of edges adjacent to a vertex *MATH* or plaquette *MATH* and *MATH*, *MATH* are Pauli-operators acting on the *MATH* qubit (indices enumerate data qubits located on the edges of the lattice). We define the surface code&apos;s stabilizer group *MATH* as the group generated by all *MATH* and *MATH* under multiplication. This group is Abelian, since all generating operators commute and have common eigenvectors. The *MATH* eigenspace defines the codespace *MATH* of the surface code. That is, a vector *MATH* iff *MATH*  *MATH*.
Elements of *MATH* are called stabilizers. Measuring the observables associated to these stabilizers allows checking whether the system state is still within the codespace. For a surface code defined on a torus, the code space is isomorphic to *MATH*. It encodes *MATH* logical qubits into *MATH*  data qubits where *MATH* is the number of edges.


To each logical qubit one further associates a set of logical Pauli operators which define the centralizer of *MATH*, i.e., operators that commute with *MATH*. Trivial logical operators are stabilizers since they act as the identity on the logical subspace. In the surface code, nontrivial logical operators are strings of tensor products of Pauli operators along topologically nontrivial cycles, i.e., noncontractible paths, of the torus. Each logical qubit is defined by a logical *MATH* -operator along a noncontractible path of the lattice and a logical *MATH* running along a topologically distinct noncontractible path of the dual lattice (see Fig. [2](fig:sc){reference-type=&quot;ref&quot; reference=&quot;fig:sc&quot;}). Since such paths necessarily cross an odd number of times, they anticommute.


Logical operators change the encoded information. Thus, it is desirable to detect and correct errors before they accumulate to realize a logical operation. Errors which are neither stabilizers nor logical operators can be detected by stabilizer measurements since they anticommute with some stabilizers. In order to perform stabilizer measurements, one first associates so-called syndrome qubits with vertices and faces. Then, the syndromes, i.e., the eigenvalues of stabilizers, are obtained by entangling syndrome qubits with adjacent data qubits and subsequent measurements [*REF*]. In a fixed architecture the required entangling gates between syndrome and data qubits may be represented by the edges of the lattice (for *MATH* -stabilizers) and its dual (for *MATH* -stabilizers).


The length of the shortest path of any noncontractible loop on the torus is the distance *MATH* of the surface code. That is, the distance is the minimal number of physical errors that need to occur in order to realize a logical operator. Any number of errors below *MATH* can be detected but not necessarily corrected and less than *MATH* errors can always be corrected in the stabilizer formalism [*REF*].


Adaptable Surface Code 


So far, we have assumed the standard notion of a surface code defined on a square lattice on a torus (see Fig. [2](fig:sc){reference-type=&quot;ref&quot; reference=&quot;fig:sc&quot;}). However, surface codes can be defined on arbitrary lattices. Since the surface code stabilizers in Eqs. ) and ) are identified with vertices and plaquettes of the lattice, adapting the lattice can change the underlying code and its performance dramatically [*REF*]. This feature makes surface codes particularly useful for biased noise models, e.g., noise models where *MATH* - and *MATH* -errors occur with different probability. Even the loss tolerance is affected by the underlying lattice.


In our framework, we will be exploring the space of lattices to make use of the adaptability of the surface code. To search this space in a structured fashion, we introduce a set of elementary lattice modifications.


Specifically, we consider modifications that can be performed with a set of basic moves which were first illustrated in Ref. [*REF*].
The two basic moves are exemplified in Fig. [3]: (i) As illustrated in Fig. [3](a), addressing the primal lattice, two non-neighbouring vertices can be connected across a plaquette which is effectively split by the additional edge. That is, the number of *MATH* -stabilizers is increased while an additional data qubit is added.
(ii) Conversely, the same operation can be performed on the dual lattice producing an additional vertex (i.e., *MATH* -stabilizer) as illustrated in Fig. [3](b). Note that the reverse operation, i.e., deleting an edge by merging two vertices or two faces, respectively, is also possible. While the inclusion of qubit deletion greatly increases the number of possible moves (actions) in each step, the set of reachable codes is the same as if one had started from a single data qubit using only the operations in Fig. [3]. Here, we have chosen not to explicitly include such deletions to simplify the simulations. All of these moves can be implemented fault-tolerantly and map a surface code to a surface code while changing the underlying stabilizer group [*REF*]. This can be considered as simple version of code deformation [*REF*] which is employed here to explore the space of surface codes. Indeed, code deformation can in principle be employed in a much more general way to map topological stabilizer codes to other topological stabilizer codes. However, for our needs the two basic moves explained above already allow for sufficiently diverse modifications while limiting the number of available actions.
Nevertheless, note that the number of available moves increases with the lattice size.


FIGURE 


Hardware constraints 


Before we further go into detail on our numerical study focussing on setups with the full flexibility required to implement the moves in Fig. [3], let us briefly consider the implications of the variations in hardware of currently available setups. For instance, limitations on how freely qubits that are physically present but not yet part of a specific code structure can be connected to the latter vary between different platforms such as ion trap architectures [*REF*; *REF*; *REF*] or superconducting qubit devices [*REF*]. While our method can be used directly when all-to-all connectivity between qubits is given, e.g., in many current ion traps [*REF*; *REF*; *REF*], modifications can be made to the set of allowed code deformations when this is not the case, provided that some minimal requirements are met.
That is, for any fixed architecture to be able to compensate for biased and potentially correlated noise it is required that (i) the connectivity of the lattice can be changed, (ii) data qubits can be removed and (iii) syndrome qubits can be removed. While changing the connectivity of the lattice enables one to adapt to biased error channels in accordance with Ref. [*REF*], removing qubits allows the adaptability to correlated noise sources arising, e.g., from manufacturing defects. In very restricted scenarios, for example, when only nearest neighbour interactions are possible, these requirements are not met. In such cases, one may still perform code optimization off-line using an estimated noise model in order to produce a blueprint for manufacturing or initialization. However, already a 2-nearest neighbour architecture (featuring connections between syndrome qubits and next nearest data qubits) provides the required features, as can be seen from Fig.


At this point, it should also be mentioned that the flexibility provided by higher connectivity in terms of code adaptability is not the only relevant factor: Higher connectivity might be subject to a trade-off with lower fidelities due to higher technical demands. At the same time, allowing for flexible code deformations and adaptive on-line error correction is certainly not the only motivation for non-nearest neighbour connections. In particular, nearest neighbour architectures may suffer from limitations on fault-tolerant implementations of logical gates [*REF*; *REF*]. Indeed, the first successful experiments with a non-nearest neighbour architecture of superconducting qubits have been successfully performed [*REF*], suggesting that (variations of) our methods are applicable across a range of platforms currently in use or under development. Despite the hardware constraints, we still restrict our actions to fault-tolerant, local deformations since this provides a meaningful way of exploring the family of surface codes in the context of RL for in-situ optimization.


FIGURE 


Reinforcement Learning and Projective Simulation 


In generic RL settings [*REF*], one considers an agent that is interacting with an environment (see Fig. [1]). At each time step the agent obtains information from the environment in terms of perceptual input -- percepts -- and responds through certain actions. Specific behavior, i.e., sequences and combinations of percepts and actions, trigger reinforcement signals -- rewards. Typically, the agent adapts its behavior to maximize the received reward per time step over the course of many rounds of interactions. The RL method as we use it makes no assumptions about the environment and applies even if the environment is a black box. In our case, the environment is a surface code memory subject to noise and its classical control system, including the means to estimate logical error rates. Here, knowledge of the environment is also restricted: The agent is never directly provided with information about the noise model. This is because, in practice, acquiring knowledge about the noise requires involved process tomography. Since the agent does not aim to characterize the noise process, but rather to alleviate its effects, a complicated noise estimation procedure is not necessary.


The specific learning agent that is considered in this paper is based on the Projective Simulation (PS) model for RL. PS is a physics-motivated framework for artificial intelligence developed in Ref. [*REF*]. The core component of a PS agent is its clip network which is comprised of units of episodic memory called clips (see Fig. [4]). There are two sets of clips constituting the basic network, percept and action clips. In an interactive RL scenario, an agent perceives the environment through the activation of a percept clip *MATH* and responds with an action. The latter, in turn, is triggered by an action clip *MATH*. Percept clips can be regarded as representations of the possible states of the environment, as perceived by the agent. Similarly, action clips can be seen as internal representations of operations an agent can perform on the environment. A two-layered clip network as in Fig. [4] can be represented by a directed, bipartite graph where the two disjoint sets comprise the percepts *MATH* and actions *MATH*, respectively. In this network each percept (clip) *MATH*, *MATH* (where *MATH* is the number of percepts at time step *MATH*) is connected to an action (clip) *MATH*, *MATH* (with *MATH* being the number of actions available for a percept *MATH*) via a directed edge *MATH* which represents the possibility of taking action *MATH* given the situation *MATH* with probability *MATH*. The agent&apos;s policy governing its behavior in this RL setting is defined by the transition probabilities in the episodic memory. Learning is manifested through the adaption of the clip network via the creation of new clips and the update of transition probabilities. Each time a new percept is encountered, it is included into the set *MATH*. A time-dependent weight, called *MATH* -value, *MATH* is associated with each edge *MATH*.
The transition probability from percept *MATH* to action *MATH* is given by the so-called softmax function [*REF*] of the weight *MATH* where *MATH* is the softmax parameter.


When a percept is triggered for the first time all *MATH* -values are set to  *MATH* such that the transition probabilities are initially uniform.
That is, the agent&apos;s behavior is fully random in the beginning. Since random behavior is rarely optimal, changes in the transition probabilities are required. The environment reinforces such changes by issuing nonnegative rewards *MATH* in response to an action of the agent. Then, the agent must ask the question, given a percept *MATH*  which is the action *MATH* that will maximize the received reward.
Therefore, the transition probabilities are updated in accordance to the environment&apos;s feedback such that the chance of the agent to receive a reward in the future is increased. In other words, the environment&apos;s feedback *MATH* controls the update of the *MATH* -matrix with entries *MATH*. However, there are many other contributions to the update independent of the environment&apos;s immediate feedback. Particularly noteworthy are contributions that reinforce exploratory over exploitative behavior. A detailed description of how the feedback is processed in the agent&apos;s memory is given in Appendix [8].


Once an agent has learned to optimize the reward per interaction step, the clip network in Fig. [4] is a representation of the agent&apos;s policy: Nonuniform transition probabilities *MATH* mark a systematic decision-making aimed at maximizing the reward per time step while accounting for exploratory behavior. Hence, the network can also be interpreted as an agent&apos;s memory encoding knowledge of the reward landscape of the environment.


FIGURE 


Optimizing Quantum Memories - a reinforcement learning problem 


In this section, we show that RL can be employed within the framework of Fig. [1] to successfully optimize QEC codes w.r.t. their resource efficiency. In our framework for adaptive quantum error correction, we consider a PS agent that interacts with a specific, simulated environment -- the surface code quantum memory subject to noise and its classical control managing the QEC procedure. We use the term environment in the sense it is used in RL rather than referring to the source of noise as is common in, say, open-system quantum physics.


To be more precise, we start each trial by initializing a distance-three surface code quantum memory with 18 qubits (edges) defined on a square lattice *MATH* embedded on a torus (see Fig. [2]). Note that the choice of initial code is ad hoc and could be replaced by any other small-scale surface code designed to suit a given architecture. The code is subject to an unknown Pauli noise channel *MATH* which may change in time and may differ for different data qubits. The classical control simulates the QEC procedure under this noise model and estimates the logical error rate *MATH*. Having a basic set of moves at its disposal (see Fig. [3]), the agent is tasked with growing the lattice until the logical error rate is below a target *MATH*  or at most *MATH* additional qubits have been added. This choice of an upper limit should be viewed in light of recent progress in the development of quantum devices with more than *MATH*  qubits [*REF*; *REF*]. Note the difficulty of the simulation task. A single trial requires to simulate the perfomance of up to *MATH* QEC codes. The basic set of moves is rather generic and could be restricted further to suite the requirements of a given hardware. For instance, actions could be restricted to the boundary of a system. Once the desired logical error rate is reached, the agent receives a reward *MATH*, the trial ends and the quantum memory is reset to *MATH*. If the desired logical error rate is not reached before *MATH* qubits have been introduced, the code is reset without reward. The details of the algorithm, specifically the environment, are presented in Appendix [9] and the agent&apos;s clip network, i.e., episodic memory, is visualized in Fig. [4]. Note that this scenario -- although restricted to surface codes -- is already extremely complex and versatile. In Appendix [10], we analyze the difficulty of this problem and show that there is no hope of solving it by random search.


FIGRUE 


In the following, we verify that the task outlined above is indeed an RL problem where learning is beneficial. To this end, we start by considering some instructive cases where the solutions are known before moving on to a more complicated scenario with unknown optimal strategy.
The variables that are being changed in between scenarios are the error channel *MATH* or the rewarded logical error rate *MATH*. However, if not stated otherwise, we set *MATH*. For illustrative reasons, the error models *MATH* that appear in the following are represented by single-qubit Pauli error channels. In fact, the simulated quantum memory will be subject to a so-called erasure channel [*REF*; *REF*] which models the behavior of the actual Pauli channel. This particular choice of error model is motivated by the availability of an optimal decoder for erasure noise on a surface code [*REF*]. In Sec. [6.1], this simplified channel is analyzed in more detail and we verify that it is suitable to model actual error channels.


QEC codes for i.i.d. error channels 


As a first simple task, we consider two straightforward scenarios with known good solutions as they will nonetheless be crucial in the evaluation of the behavior of the RL agent. First, let us consider an error channel that can only cause *MATH* -errors with probability *MATH* on each individual, data qubit independently. Formally, we can write this as a quantum channel acting on each single-qubit state *MATH* as, *MATH*. Assuming the initial logical error rate is above the target value *MATH*, the RL agent is then tasked with modifying the 18-qubit surface code by increasing the number of data qubits according to the rules described in Fig. [3]. At the beginning of each trial, an agent has a reservoir of 50 additional qubits at its disposal to reduce the logical error rate below *MATH*. Fig. shows that the agent indeed learns a strategy, i.e., a policy, which adds -- on average -- *MATH* data qubits to the initial code in order to reduce the logical error rate as desired.
Fig. can be understood as follows. In the very beginning, the agent has no knowledge about the environment and performs random modifications of the surface code. For the specific error channel in Eq., a random strategy requires on average 20 additional data qubits to reach the desired logical error rate *MATH*. What follows are many trials of exploring the space of surface codes. During each trial, the agent&apos;s episodic memory is reshaped and modified as described in Sec. [4] and Appendix [8]. This learning process effectively increases the probability for the agent to select sequences of actions which lead to a reward quickly. As can be seen from Fig the length of a rewarded sequence of actions is gradually reduced with each trial. Ideally, the agent&apos;s behavior converges to a policy that requires a minimum number of additional qubits. In Fig. we observe that agents converge towards a policy which requires, on average, *MATH* additional qubits. Let us now confirm that the strategy found by the RL agents agrees with the best known strategies for this problem. The error channel in Eq. can only cause logical *MATH* -errors. That is, we are looking for the surface code with the maximum number of *MATH* -stabilizers and minimum number of *MATH* -stabilizers [*REF*]. Starting from the surface code in Fig. [2] we must therefore ask the question how to increase the number of *MATH* -stabilizers given the available actions in Fig. [3]. Since *MATH* -stabilizers are identified with vertices in the graph, repeatedly applying an action as displayed in Fig. will increase the number of *MATH* -stabilizers. We hence expect a sequence of these actions to provide good strategies in this case, resulting in a surface code on a lattice with low connectivity. Indeed, we can confirm this by looking at surface codes constructed by agents that have been successfully applied to this task. In Fig. [5], a particularly interesting example solution is shown: It can be seen that the distance of the code w.r.t. *MATH* -errors along one cycle has been increased from *MATH* to *MATH* by inserting *MATH* new qubits at very specific points in the lattice. In other words, any logical *MATH* -operator crossing the lattice from left to right in Fig. [5] is a product of at least *MATH* single-qubit *MATH* -operators. Just looking at the initial lattice in Fig. [2] it is not obvious that this can be done with only 4 actions. This is already a nontrivial result for this, at first glance, simple problem.


FIGURE 


Now, let us consider the well-understood scenario of a i.i.d. error channel. That is, each qubit in the quantum computer is subject to a quantum channel of the form *MATH* where we choose *MATH* such that either *MATH* - or *MATH* -errors can happen with probability *MATH* everywhere. Otherwise the task remains the same as before. We can see from Fig. that the agent again learns to improve the logical error rate while optimizing over the number of required qubits.


The optimal surface code to protect against depolarizing noise is defined on a square (self-dual) lattice [*REF*]. That is, the best known strategy to grow the surface code using the actions available (see Fig. [3]) is to retain the same connectivity both on the primal and dual lattice such that there are the same number of *MATH* - and *MATH* -stabilizers. Indeed, looking at some examples from agents trained on this task, we can confirm that the agents also learn this strategy: The most successful agents end up creating surface codes where the number of *MATH* - and *MATH* -stabilizers differ by at most  *MATH*. On average, the ratio between number of *MATH* - and *MATH* -stabilizers is *MATH* with standard deviation of *MATH*. The corresponding primal and dual lattices tend to have similar average connectivities, too. The average ratio between primal and dual connectivity is  *MATH* with standard deviation of  *MATH*.


QEC codes for correlated error channels 


Next, let us tackle a complicated, practical scenario where the optimal strategy is unknown: We consider spatial dependencies on top of an i.i.d. error channel. This particular situation is motivated by the common problem of manufacturing defects. Correlated errors arising e.g., from manufacturing defects can be present in any fault-tolerant architecture because active error correction requires multi-qubit operations such as stabilizer measurements. Most architectures using topological codes have a spatial layout which can be associated with the lattice of the surface code [*REF*; *REF*] such that correlated errors will most likely be local. Consider for instance an ion trap quantum computer using an arrangement of multi-zone Paul trap arrays [*REF*]. Dephasing is the prevalent noise in ion traps [*REF*], so we would already have to consider an asymmetric error channel. Moreover, due to e.g., a manufacturing defect, two Paul traps in this arrangement could fail and produce *MATH* -errors on internal qubits.


To be precise, consider the error channel in Eq., with *MATH* and *MATH*. This is similar to the simplest case in Sec. [5.1] where *MATH*. In our construction, a correlated error as motivated above is modeled by an increased *MATH* -error rate on edges in the neighbourhood *MATH* of a vertex or plaquette *MATH*. Here, we choose two neighbouring plaquettes *MATH* and modify the error channel as follows, *MATH* where *MATH* labels the qubits. We further assume that all qubits have base error rates of *MATH* and *MATH*. In addition, the base probability *MATH* of an *MATH* -error on qubit *MATH* is increased by *MATH* if *MATH* and *MATH* if *MATH*. This serves two purposes. First, we can evaluate the behavior of the agent with regard to the two plaquettes *MATH*. How is the lattice structure changed in and around these plaquettes? Second, we can understand how the agent handles a deterministic error on the edge neighbouring both *MATH*. Will this edge be treated differently? In fact, it is far from clear what the optimal strategy is given the available actions displayed in Fig. [3]. Nevertheless, we observe that the PS agent learns to protect the surface code against this error channel while optimizing the used resources, see Fig. [6].


FIGURE 


Now, let us evaluate successful strategies employed by the best agents in this scenario. In Fig. [7] the relevant part of the final dual lattice as constructed by a successful agent is depicted. We can see that the agent increases the number of *MATH* -stabilizers in the immediate vicinity of the flawed plaquettes, decreasing the connectivity of affected plaquettes. Interestingly, the agent also finds a way of moving the flawed plaquettes apart given the available actions, thereby removing any deterministic error appearing on the edge between these plaquettes.
At the same time, due to the prevalent *MATH* -errors, connectivity throughout the lattice is balanced between vertices and plaquettes: the average connectivity of the dual lattice is *MATH* and *MATH* for the primal lattice. Similarly, the ratio between *MATH* - and *MATH* -stabilizer is *MATH*.


QEC codes with changing requirements 


So far, we have kept the rewarded logical error rate fixed. However, it is in general also desirable to be able to adapt to stricter target thresholds if required. We therefore consider a scenario where the error channel is initially fixed to that of Eq. with *MATH*. Then, after having learned the optimal strategy from before, the agent is tasked with further decreasing the logical error rate to a quarter of the initial value. As one can observe from Fig. [8], the agent can indeed draw on its knowledge from the first stage of the task to find a solution for the second stage.


In the particular scenario of varying target thresholds, the performance of the agent can potentially be improved by issuing a reward which is proportional to the inverse logical error rate. This provides more feedback about the reward landscape in form of a gradient which can be followed. Such a modified reward landscape can also be exploited by simpler optimization algorithms such as Simulated Annealing [*REF*]. In addition, there exist other approaches for optimizing QEC procedures for fixed error models [*REF*; *REF*] and for variational optimization based on directly available experimental data (QVECTOR [*REF*]), which have been shown to improve noise robustness for small numbers of qubits.


FIGURE 


However, our main focus here lies on exploring the observed ability to capitalize on previously obtained knowledge encoded in a memory. As we will see in Sec. [6], this ability can be viewed as part of a more general transfer learning [*REF*; *REF*] skill that represents a main strength of RL for QEC. While common optimization methods usually do not feature a memory, it should be noted that RL can be combined with optimization techniques to improve initial exploration, thereby ameliorating the training data used for learning [*REF*; *REF*].


Simulation vs. Experiment 


In the previous sections, the main focus has been to determine whether RL agents in our framework are able to adapt and optimize a surface code quantum memory to various types of noise with different requirements. We have found that this is indeed the case, showcasing the prospect of RL in on-line optimization of quantum memories. Although we have evaluated our procedures only via simulations, our results suggest that such approaches can be successful also in practice in future laboratory experiments. This is because our framework for optimizing QEC codes in Fig. [1] is independent of whether the environment is a real or simulated quantum device. In either case, the interface between environment and agent remains unchanged. For instance, we estimate the logical error rate of the quantum memory using a Monte Carlo simulation. In a real device, this can be done by preparing a quantum state, actively error correcting for a fixed number of cycles, and then measuring the logical operators by measuring all data qubits in a fixed basis. The logical error rate should then be interpreted as the probability of a logical error per QEC cycle. Repeating this measurement provides an estimation of the lifetime of the quantum memory. Moreover, the code deformations which constitute the actions available to an agent are designed with a physical device in mind [*REF*; *REF*].


FIGURE 


Ultimately, one is of course interested in performing optimization on actual quantum devices. However, as long as these devices are sufficiently small-scale to allow for simulations that are (much) faster than the timescale of operating the actual device, it is advantageous to perform the bulk of optimizations off-line in simulations before transferring the results to the actual device for further optimizations.
In other words, to fully capitalize on the important features of transfer-learning and pre-training in quantum-applied machine learning, it is crucial to ensure that the simulations are as efficient as possible, and that the chosen RL model is able to transfer experience from simulations to real environments. In the following, we therefore discuss the efficiency and practicality of our simulations and show that the RL agents we consider are capable of transfer learning.


Simulation Efficiency of QEC 


To provide sufficiently efficient off-line simulation, the individual components of our QEC simulation have been carefully selected. For instance, note that stabilizer QEC can be simulated efficiently classically [*REF*; *REF*; *REF*; *REF*].
However, estimating the logical error rate, which requires a large number of samples from QEC simulations, remains computationally expensive. In our simulations, we hence make use of a simplified error model to allow for faster estimations of logical error rates. The simplified error model is based on the quantum erasure channel [*REF*] since there exists a linear-time maximum likelihood decoder for surface codes over this channel which is optimal both in performance and speed [*REF*]. The use of this software, SQUAB [*REF*; *REF*], within our RL framework is hence providing the means for sufficiently fast exploration of the space of surface codes. In essence, the erasure error channel is very similar to a Pauli error channel where the location of an error is known exactly. More specifically, we introduce an asymmetric erasure channel where we choose two erasure probabilities *MATH* that can have spatial and temporal dependencies. Since *MATH* and *MATH*  stabilizers are separated in the surface code, error correction of *MATH*  and *MATH* errors can also be treated independently. In the simulation of *MATH* errors over the erasure channel, we erase each qubit *MATH* in the surface code with probability *MATH*, and replace it by a mixed state of the form *MATH*. The set of erased qubits is known. The simulation proceeds analogously for *MATH* errors.


Since our simulations cover setups with up to *MATH* qubits, we have indeed good reason to believe that our simulations are sufficiently efficient to optimize QEC codes for near-term quantum devices without the need of experimental data. However, one may argue that our software only provides a simulation for a simplified noise model, the erasure channel. In order to prove that the results are relevant in practice, we compare logical error rates of a set of small surface codes subject to erasure errors to the rates obtained from simulating standard Pauli noise on the same codes. To this end, we assume that each qubit is affected independently by a *MATH* -type Pauli error and we perform error correction with both SQUAB, and the Union-Find decoder introduced in Ref. [*REF*]. We report the average logical error rate for each code after 10,000 trials with an erasure rate *MATH*  (SQUAB) and *MATH* -error rate *MATH* (Union Find). In Fig.  we observe qualitatively the same behavior: codes that are considered to perform well according to the estimation using SQUAB are also considered to perform well using the Union-Find decoder. The difference between the two error channels lies predominantly in the magnitude of the logical error rates. It is therefore better to select codes using SQUAB, allowing for a faster and thus more precise exploration of the space of topological codes.


Transfer learning in QEC 


The usefulness of off-line simulations for QEC code optimization emerges from the application of the results to on-line settings. In this transition, deviations of the error model used in the simulation from the actual noise, or dynamical changes of the latter can lead to non-optimal performance if no further action is taken. Here, machine learning comes into play. That is, a central agenda of machine learning is to develop learning models that are capable of successfully applying previously obtained knowledge in different environments. This possibility, called transfer learning [*REF*], is an active area of research in general AI [*REF*]. Here we should note that the ability to transfer knowledge is indeed much desired but not manifestly present in all machine learning models. At the same time, there is the risk of confusion with the more generally encountered ability for generalization [*REF*; *REF*].
Let us quickly illustrate the difference: On the one hand, generalization is the ability of a learning agent to be effective across a range of inputs and knowing what to do in similar situations. Transfer learning, on the other hand, is the ability to attain knowledge in one scenario, and then being able to use this knowledge in a different (new but related) learning setting. One of the objectives of transfer learning is to jump start the learning process in a new, but similar scenario, and is fundamentally linked to the AI problem of learning how to learn [*REF*; *REF*].


Transfer learning can provide a significant improvement in performance as compared to optimization approaches without learning mechanisms, as well as considerable reductions of learning times in comparison to untrained RL agents. Put simply, agents capable of transfer learning do not have to start from scratch every time the environment or task changes slightly [*REF*; *REF*; *REF*; *REF*].
As we will discuss in this section, within the RL setting we consider here, agents trained on the original simulations may transfer their experience from simulations to practical applications. The usefulness of this approach will of course depend on how similar the simulated error model is to the real error channel.


FIGURE 


In order to demonstrate the potential of transfer learning in the context of optimizing and adapting quantum memories, we consider a scenario where the agent is first trained on one error model *MATH*, and then encounters a different error model *MATH*. This change in noise could occur not only because the agents are switched from a simulated training scenario to an actual experiment but also, e.g., due to environmental changes, equipment malfunctions or malicious attacks. Generally, under the assumption that the noise can only vary slightly, we can expect learning to be beneficial since strategies that helped in one scenario should still be favorable in protecting against similar error channels. Then, an RL algorithm can further optimize over a given policy and re-adjust to the new environment. This scenario capitalizes on the strength of RL since it requires a long-term memory that can be exploited between tasks. If exploration of the search space is encouraged over exploitation of rewards, the agent&apos;s memory contains global information about the environment. In other words, given some sub-optimal QEC code, such an agent knows a strategy to adapt this code efficiently such that the desired logical error rate is reached. In fact, the feedback which is given only if a certain logical error rate has been achieved, is specifically designed to encourage exploration of the search space.


The specific choice of error models for this scenario is partially motivated by the results in Fig. There we observe that the main difference between the simulated erasure and more realistic Pauli error channels lies in the magnitude of the estimated logical error rate. We therefore now consider a quantum memory which is first subjected to the simple noise channel in Eq. with *MATH* and the agent is tasked again to reduce the logical error rate below *MATH*.
Then, after having learned a beneficial strategy for finding a good QEC code, the same agent is confronted with an increased error rate of *MATH*. Fig. shows that, in this second stage of the task, the agent benefits from having learned to protect the quantum memory against the first error channel. In particular, we see from Fig. that agents not initially trained on the first noise channel behave randomly and do not find reasonably good strategies to deal with the high noise level. This is because at a physical error rate of *MATH* the initial code requires many modifications before the desired logical error rate can be reached. In fact, the required number of basic modifications is so large that a random search is just not sufficient to find a reasonably good code in the allotted time of *MATH* trials. Although the observed behaviour of the learning agents showcases the benefit of a memory, the error models are nevertheless too similar to reveal the potential for transfer learning.


Let us therefore consider another scenario with more drastic changes in the noise model. In particular, we attempt to model a setting more closely resembling the transfer of knowledge from simulation to experiment: That is, we start by training *MATH* agents on an error channel *MATH* that captures partial knowledge of the real error channel disturbing a quantum system. Since dephasing is the prevalent noise in many quantum computation architectures [*REF*; *REF*; *REF*], we choose the error model from Eq. with *MATH* for training. Here, we deliberately neglect generally more realistic Pauli errors to showcase the transfer learning ability, but we allow sufficiently many iterations so the agents can optimize their behavior w.r.t. to the unrealistic noise model. Then, in the second stage of this scenario, we select (at random) one of the best agents and confront it with a more realistic error model *MATH* which features both *MATH* -, and *MATH* -errors as well as spatial correlations. The error model, similar to that in Eq., is *MATH* where *MATH* labels the qubit the channel acts on. All qubits have base error rates of *MATH* and *MATH*. In addition, the base probability *MATH* of an *MATH* -error on qubit *MATH* is increased by *MATH* if *MATH* where *MATH* labels one specific plaquette of the lattice to model, e.g., a manufacturing defect as in Sec. [5.2].


To further challenge the learning algorithm in this scenario, we only allow a limited number of trials in the second stage (less than 10% of the first stage) since any actual experiment would be much more expensive than simulations. Despite the significant difference between the error models *MATH* and *MATH*, we observe in Fig. that the agents are able to significantly capitalize on the knowledge obtained from the initial training simulations (blue curve). In contrast, the same agent without pre-training (plotted in green in Fig. barely learns at all during the allotted number of trials.


This advantage is indeed remarkable. However, it would be of no practical use if the benefits from transfer learning were not robust to variations in the experimental noise channel. That is, we expect the advantage of transfer learning to extend to other noise channels, too.
Therefore, let us exchange the error channel *MATH* by another, new model *MATH* where we replace spatial correlations by a doubled base *MATH* -error rate, i.e. *MATH* with *MATH* and *MATH*. Now, choosing the same pre-trained agent as before and transferring it to the new setting, we find that, despite the drastic difference between *MATH* and *MATH*, the advantage gained from transfer learning is still substantial (which can be concluded from the comparison between the purple and pale rose curve in Fig. This remarkable and robust advantage showcases the benefits of transfer learning for QEC in resource-limited, near-term quantum devices.


Discussion 


Reinforcement learning (RL) has recently seen a great deal of success, from human-level performance in Atari games [*REF*] to beating the world champion in the game of Go [*REF*]. In 2017, RL was included in the list of 10 breakthrough technologies of the year in the MIT technology review [*REF*]. As machine learning is claiming its place in the physicist&apos;s toolbox [*REF*], RL is starting to appear in quantum physics research [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].


In this work, we have presented an RL framework for adapting and optimizing quantum error correction (QEC) codes. This framework is based on an RL agent that interacts with a quantum memory (and its classical control), providing the latter with instructions for modifications of the code to lower the logical error rate below a desired threshold. For the simulations discussed here, the quantum memory is realized as a surface code to which the agent may add qubits by way of fault-tolerant code deformations [*REF*; *REF*]. The agent receives a reward once the specified logical error rate is reached. The internal structure of the agent has been modeled within the Projective Simulation [*REF*] approach to RL. The classical control system estimates the logical error rate in the simulated QEC procedure using an optimal linear-time decoder [*REF*; *REF*].


Our results demonstrate that the agents learn to protect surface code quantum memories from various types of noise, including simple i.i.d.
errors, but also more complicated correlated and non-isotropically distributed errors. In particular, this RL approach provides interesting solutions for nontrivial QEC code optimization problems. A particularly noteworthy feature is the ability of the agents to transfer their experience from one noise model or task to another even if they are seemingly very different. This suggests that such a QEC strategy based on RL can be used to switch from off-line optimization to on-line adaptive error correction. That is, provided a reasonable guess for the type of expected errors, one may start by training an RL agent on simulations. Then, the trained agent can be used to bootstrap optimization in the actual hardware.


The scope of our simulations has been designed specifically with such applications to current state-of-the-art quantum computing platforms [*REF*; *REF*; *REF*] in mind. Starting with 18 initial qubits, the agents we consider are able to extend this number by up to 50 additional qubits, thus also accounting for expected near-term technology developments. A potential bottleneck for extensions to much larger qubit numbers lies in the scaling behavior of the learning complexity. There, one expects that the increase in learning time scales unfavorably with the increase in the size of the percept space (and action space, both of which depend on the number of qubits). We envisage that this problem can be circumvented through parallel processing by assigning individual agents to different surface code patches of a fixed size. All agents can then operate in parallel, allowing one to exploit a number of already available RL techniques for parallel learning that explore the use of shared experience between agents [*REF*; *REF*].
Due to the local structure of the problem of optimizing topological QEC codes, parallel RL may potentially yield close to optimal policies even for significantly larger lattice sizes. Moreover, note that the search of our RL agent to find QEC codes is largely specified by the history of performed actions. Thus, one might be able to reduce the complexity of RL approaches by employing long short-term memories (LSTMs) [*REF*], in order to enable deliberation with respect to a few past actions. Since actions are also localized, this might yield enough information to evaluate an optimal policy that is partially blind to the lattice size.


At the same time, the practical success of machine learning techniques also depends on suitably narrowing down the optimization problem. For example, in Ref. [*REF*], neural networks were used to determine sequences of quantum gates and measurements to protect a logical qubit. This allows the algorithm to search the whole space of quantum circuits. However, this comes at a cost. The space of all possible QEC strategies that the algorithm explores is so vast that scaling inevitably becomes an issue. While this work demonstrates a successful strategy on up to *MATH* data qubits subject to uncorrelated bit-flip errors, significant advances would be needed to generalize this approach to larger, potentially varying numbers of data qubits and more realistic noise. In contrast to the approach of [*REF*], the method developed here can adapt and optimize QEC codes in terms of their resource efficiency, i.e., the number of data qubits needed to achieve a desired maximal logical error rate, and operates without detailed information about or precise simulation of the underlying quantum states.


Moreover, let us note that the presented general framework for RL-based optimization of error correction codes goes beyond the specific simulations we have performed here. That is, the approach is directly applicable also if one wishes to consider RL paradigms other than PS, QEC codes other than surface codes, or noise models other than those considered. A particular contributor to this flexibility is that both the errors and the error decoding appear as part of a black-box environment to the agent, who only ever perceives whether the logical error rate is above or below target threshold. For instance, one is left with the option of choosing different decoders, and even of incorporating machine learning into the decoder itself [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Having this freedom in choosing the decoder is particularly relevant if different types of QEC codes are considered, e.g., if one allows fault-tolerant code switching through code deformation [*REF*] or lattice surgery [*REF*] as part of the optimization.
In summary, while the simulations have been carried out within a specific setting, this framework lays the groundwork for applying sophisticated RL algorithms to more general (topological) QEC codes and more realistic noise models [*REF*; *REF*; *REF*; *REF*].