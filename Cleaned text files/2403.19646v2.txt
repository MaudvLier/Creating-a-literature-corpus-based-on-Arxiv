Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis 


Introduction


changes in the Earth&apos;s surface, as a dynamic indicator of the Earth&apos;s
system, profoundly affect the planet&apos;s evolution and the survival of
humankind. Observing and analyzing these changes is pivotal for
advancing sustainable human development [*REF*]. Remote sensing
(RS) satellite imagery, offering a unique &quot;God&apos;s perspective,\&quot; emerges
as an effective tool for monitoring Earth&apos;s dynamic changes. Remote
sensing image change interpretation (RSICI) has emerged as a significant
research focus [*REF*]. RSICI aims to identify and analyze
changes from images captured at different times in the same geographical
area. It can provide decision-making support for environmental
protection [*REF*], urban planning [*REF*],
resource management [*REF*], etc.


Current RSICI technology primarily encompasses change detection and
change captioning. Change detection accurately localizes the spatial
location of surface changes at the pixel level
[*REF*; *REF*; *REF*; *REF*; *REF*].
In contrast, change captioning aims at articulating the attributes and
meanings of changes using natural language, emphasizing the
semantic-level understanding of changes
[*REF*; *REF*; *REF*; *REF*; *REF*].
Despite significant advancements in both areas, their respective
limitations prevent us from obtaining comprehensive change
interpretation information by utilizing a single technology.
Specifically, while change detection can accurately localize the changed
area, it lacks a deeper understanding of changes&apos; underlying meanings,
such as the characteristics of ground objects or the spatial
relationships between them. Conversely, change captioning can provide
rich semantic-level interpretation information but may fall short in
providing precise pixel-level change localization. Therefore, there is a
pressing need to explore a multi-level change interpretation (MCI)
approach providing both pixel-level and semantic-level change
information, facilitating precise change localization while delving into
the essence and implications of the changes.


Moreover, practical applications often require comprehensive analysis
and further processing of both pixel-level and semantic-level change
interpretation results to meet specific needs. For instance, users may
require statistical analysis of changed object numbers, consuming
significant time and effort from researchers and demanding users&apos;
technical proficiency.


To address these challenges, we propose an interactive Change-Agent
based on a novel MCI model and a large language model (LLM).
Fig. illustrates the advancements achieved by our Change-Agent compared to
previous technologies. Different from the previous single technology,
with the help of the MCI model, our Change-Agent can simultaneously
achieve precise pixel-level change detection and semantic-level change
captioning, thereby providing users with comprehensive change
interpretation information. Notably, our Change-Agent boasts interactive
capabilities, enabling users to communicate their queries or
requirements regarding surface changes. Leveraging the MCI model and the
LLM, the Change-Agent can adeptly interpret changes according to user
needs, intelligently analyze and process the change interpretation
information, and ultimately deliver tailored outcomes aligning with user
expectations. The interactive Change-Agent bridges the gap between users
and remote sensing expertise.


FIGURE


As illustrated in Fig. (b), the MCI model is a pivotal cornerstone toward
the realization of the Change-Agent. It enables the Change-Agent to
perceive visual changes comprehensively, serving as the change
perception foundation of the Change-Agent. However, there has been no
prior work investigating the multi-task learning of change detection and
change captioning. To address this gap, we built the LEVIR-MCI dataset.
Each pair of bi-temporal remote sensing images in this dataset contains
both pixel-level change detection masks and semantic-level change
descriptions. Building upon this dataset, we propose a novel MCI model
with two branches of change detection and change captioning. Within the
two branches, we propose BI-temporal Iterative Interaction (BI3) layers
utilizing Local Perception Enhancement (LPE) and the Global Difference
Fusion Attention (GDFA) modules to enhance the model&apos;s discriminative
feature representation capabilities. The trained MCI model equips our
Change-Agent with eyes, enabling the agent to realize comprehensive
change interpretations of surface changes.


Recent advancements in LLMs such as ChatGPT [*REF*] and
Llama2 [*REF*] have been remarkable. Trained on vast corpora, LLMs
have acquired extensive knowledge and demonstrated powerful capabilities
in instruction comprehension, planning, reasoning, question answering,
and text generation [*REF*; *REF*]. These notable
progress have provided robust support for our LLM-based Change-Agent. In
this work, the LLM serves as the brain of the Change-Agent, assuming a
pivotal role in internal scheduling, planning, and user-intent
understanding. Integration with LLMs imbues our agent with enhanced
flexibility and intelligence, enabling a nuanced understanding of user
intentions and provision of customized change interpretation and
intelligent analysis services such as changed object counting,
estimation of change causes, prediction of future changes, etc.


Our Change-Agent facilitates accurate and comprehensive change
interpretation of RS images. It reduces the workload and time costs for
researchers while enhancing the efficiency and convenience of
interaction between users and remote sensing data. Extensive experiments
demonstrate the effectiveness of the proposed MCI model and highlight
the promising potential of our Change-Agent. Besides, the integration of
the MCI model and LLM presents novel perspectives for analyzing surface
changes.


Our contributions can be summarized as follows:
-  We built a multi-level change interpretation dataset. The dataset
contains bi-temporal images as well as diverse change detection
masks and descriptive sentences. It provides a crucial data
foundation for exploring multi-task learning for change detection
and change captioning.
-  We propose a dual-branch MCI model, which can provide pixel-level
and semantic-level change interpretation. Besides, we propose BI3
layers with LPE and GDFA to enhance the change interpretation
capability of the model. Experiments validate the effectiveness of
our method.
-  Leveraging the MCI model and an LLM, we construct a Change-Agent
that achieves an interactive and comprehensive interpretation and
analysis of surface changes. It has intelligent dialogue and
customized service capabilities, opening up new opportunities for
intelligent remote sensing applications.


Related work


A comprehensive change interpretation model should provide both
pixel-level and semantic-level change information. However, previous
work has primarily focused on either change detection or change
captioning alone. Change detection aims to generate pixel-level change
maps revealing the locations of changes in bi-temporal images, while
change captioning aims to describe the changes via language. In this
section, we will briefly review these approaches.


Remote Sensing Change Detection


Early change detection approaches were traditionally categorized into
algebra-based, transformation-based, and classification-based methods.
These methods relied on techniques such as change vector analysis (CVA)
[*REF*; *REF*], principal component analysis (PCA)
[*REF*; *REF*], multivariate alteration detection (MAD)
[*REF*; *REF*], and post-classification comparisons
[*REF*; *REF*]. Traditional change
detection methods have laid the foundation for modern techniques in
remote sensing change detection [*REF*].


With the emergence of deep learning, the field of remote sensing change
detection has witnessed significant advancements
[*REF*; *REF*; *REF*; *REF*; *REF*].
Compared to traditional methods, deep learning significantly improves
the performance of change detection with its superior multi-temporal
feature learning capability. These methods utilize various architectures
including convolutional neural networks (CNNs)
[*REF*; *REF*; *REF*], recurrent neural
networks (RNNs) [*REF*; *REF*; *REF*; *REF* -Unet; *REF*],
auto-encoders [*REF*; *REF*; *REF*], and Transformers
[*REF*; *REF*; *REF*; *REF* -Former; *REF*; *REF*].
For instance, Chen et al. [*REF*] introduced a bi-temporal image
Transformer (BIT) [*REF*], where images are represented as semantic
tokens, and the Transformer encoder and decoder refine visual tokens to
identify changed areas efficiently and effectively. Li et al.
[*REF*] proposed an end-to-end TransUNetCD model based on
Transformer and UNet structure. The model utilizes a Transformer encoder
to extract global contextual features. A CNN-based decoder upsamples the
multi-scale features to generate the change map. Bandara et al.
proposed a pure transformer model named ChangeFormer [*REF*],
whose backbone is a siamese SegFormer [*REF*]. Zhang et
al. [*REF*] propose a deep siamese network with
multi-scale multi-attention. They enhance global representation using a
contextual attention module and fuse multi-scale features from the
siamese feature extractor for detecting objects of varying sizes and
irregularities. Some surveys, such as
[*REF*; *REF*; *REF*] provide a more
comprehensive review of change detection in deep learning.


Remote Sensing Change Captioning


Remote sensing change captioning (RSCC) is a recently emerged multimodal
task involving remote sensing image processing and natural language
generation. Given the intricacy of this task, prevailing methodologies
predominantly leverage deep learning techniques. Early methods in this
domain largely drew inspiration from the encoder-decoder framework
commonly utilized in image captioning tasks. Within this framework, the
encoder is responsible for extracting features from bi-temporal images
and performing the difference-aware fusion of these features. The
decoder typically employs recurrent neural networks or Transformers to
convert visual features into natural language. Compared to tasks like
image captioning, which extract semantic representations from single
images, change description faces the significant challenge of extracting
robust semantic change representations.


Chouaf and Hoxha et al. [*REF*; *REF*] pioneered remote sensing
change captioning by introducing a model that employs a pre-trained CNNs
[*REF*] as the encoder and tried to utilize an RNN and Support Vector
Machines (SVMs) as the decoder to generate textual descriptions. To
improve the change-awareness of the model, they proposed an image-based
fusion strategy and a feature-based fusion strategy. To facilitate
research on change captioning, Liu et al. [*REF*] introduced a
large LEVIR-CC dataset and benchmarked some methods from other fields.
Besides, they proposed a Transformer-based method named RSICCformer. The
approach incorporates Siamese cross-encoding modules and multistage
bitemporal fusion modules, emphasizing change regions through
differencing features and capturing multiple changes of interest.


In a further advancement, some attention-based improvements have emerged
[*REF*; *REF*; *REF*]. For instance,
PSNet [*REF*] enhances the perception of changed objects of varying
sizes using a progressive scale-aware network with difference perception
layers and scale-aware reinforcement modules. Chang et al.
[*REF*] proposed an attention network containing a hierarchical
self-attention module followed by a residual unit for identifying
change-related attributes and constructing the semantic change
embedding. Different from the encoder-decoder framework, Liu et al.
[*REF*] proposed a new paradigm to decouple the RSCC task
by designing an image-level classifier and a feature-level encoder.
Besides, a multi-prompt learning strategy is proposed to effectively
exploit the potential of the LLM for captioning.


FIGURE


LEVIR-MCI Dataset


To cultivate a robust MCI model, foundational to the Change-Agent, we
propose an MCI dataset named LEVIR-MCI (LEVIR Multi-level Change
Interpretation) [^1], featuring both pixel-level change information in
the form of change detection masks and semantic-level insights
encapsulated in descriptive sentences. The LEVIR-MCI dataset contains
10077 bi-temporal images. Each image has a spatial size of 256 × 256
pixels with a high resolution of 0.5 m/pixel and has a corresponding
annotated mask and five annotated captions. The dataset will be publicly
available at: website.


The proposed LEVIR-MCI dataset is an extension of our previously
established change captioning dataset, LEVIR-CC [*REF*]. In this
dataset, we further provided each pair of bi-temporal images with
additional change detection masks highlighting changed roads and changed
buildings. Unlike its predecessor, the LEVIR-MCI dataset provides
diverse annotations from different interpretation perspectives for each
pair of images, further enhancing its utility for comprehensive change
interpretation.


Fig. presents some examples from our proposed
LEVIR-MCI. Each pair of bi-temporal images within the dataset is
annotated by a change detection mask delineating pixel-level
alterations, along with one of five descriptive sentences elucidating
the semantic-level changes. Such complementary annotations provide a
multifaceted view of change interpretation, thereby bolstering the
comprehensiveness of change interpretation models. To facilitate a
nuanced understanding of the dataset, and considering our previous
research has analysed the change captions, we will elucidate additional
insights into the annotated change detection masks.


FIGURE


TABLE


Number of Changed Objects


In Table, we meticulously analyse changed object
masks in the LEVIR-MCI dataset. The dataset comprises over 40,000
annotated instances of changed roads and buildings. While the number of
changed roads is fewer than buildings, generally exhibit longer spans
and cover larger areas, as evidenced in Fig. With such a vast collection of annotated
masks, our LEVIR-MCI dataset can also serve as a fertile data ground for
developing innovative multi-category change detection methodologies.


Scale and Deformation of Changed Objects 


An insightful examination of object scale and deformation is conducted
through an analysis of road and building areas and the corresponding
rectangular bounding boxes, as illustrated in Fig. The &apos;area&apos; on the horizontal axis
represents the area of a single object, and the &apos;bbox_area&apos; on the
vertical axis represents the area of the corresponding bounding box. The
dispersion of points offers insights into the diversity of object scale
and deformation. Notably, points of roads exhibit a relatively broader
spectrum with dispersed diversity, attributable to their narrow and
curved nature. In contrast, points of buildings display a more
concentrated distribution, reflecting their predominantly rectangular
form. This analysis elucidates the intricate interplay between object
scale and shape, shedding light on the nuanced characteristics of
changed objects.


FIGURE


METHODOLOGY


In Fig., we present the overview of the proposed
Change-Agent. We utilize the MCI model as eyes and an LLM as the brain
to build our Change-Agent. The MCI model enables the Change-Agent to
perceive visual changes comprehensively, serving as the change
perception foundation of the Change-Agent. The constructed LEVIR-MCI
dataset supports the multi-task training of the MCI model, which
contains a change detection branch and a change captioning branch. As
another pivotal role of the agent, the LLM can use its inherent rich
knowledge to implement agent scheduling and provide insightful analyses
and decision-making support. In the subsequent section, we will
introduce the composition of the MCI model and the LLM-based scheduling
of the agent.


Multi-level Change Interpretation Model


The MCI model serves as a central component of the Change-Agent, tasked
with extracting and interpreting change information from bi-temporal
remote sensing images. The proposed LEVIR-MCI dataset lays the
groundwork for training the MCI model. The MCI model adopts a
dual-branch architecture with a shared bottom, focusing on two pivotal
tasks: change detection and change captioning. Specifically, a Siamese
backbone network extracts multi-scale visual features from bi-temporal
images, enabling learning in two branches. The lower-level features
offer detailed information, while higher-level features are rich in
semantics. Our change detection branch utilizes multi-scale features to
refine change mask predictions, while the change captioning branch
leverages the highest-level visual features to generate descriptive
sentences. Through multi-task learning, we train a robust MCI model
capable of simultaneously generating change detection masks and change
captions.


Bi-temporal Iterative Interaction Layer


In the change detection and change captioning branches, we propose a
novel BI-temporal Iterative Interaction (BI3) layer to effectively
enhance and fuse bi-temporal features. The structure of the BI3 layer is
illustrated in Fig. [1]. The BI3 layer utilizes the Local Perception
Enhancement (LPE) module and the Global Difference Fusion Attention
(GDFA) module to extract the discriminative features of interest.


FIGURE


Motivated by the analysis presented in Section
[3.2] regarding the various scales and
deformations of roads and buildings, we propose the LPE module with a
residual structure. The LPE module employs convolution kernels of
different sizes to extract multiple feature maps across different
scales. This design enriches the diversity of feature information and
improves the model&apos;s local feature perception capabilities. Assuming the
bi-temporal features are denoted as *MATH*.


The GDFA module leverages differencing features to generate spatial
attention weights and perform the interaction and fusion between
features. This facilitates the model to focus on changes of interest and
ignore irrelevant disturbances. Taking the left GDFA module in Figure 5
as an example, assuming *MATH* and
*MATH* are the bi-temporal features
obtained from the LPE module, the GDFA module can be formulated as
follows: *MATH*.


Through the combination of LPE module and GDFA module, the BI3 layer
improves the model&apos;s feature representation and change discrimination
capabilities. Following the LPE and GDFA modules, Layer Normalization
(LN) [*REF*] is applied to the bi-temporal features. Subsequently, a
Multi-Layer Perceptron (MLP) with residuals further refines the
normalized features to obtain enhanced bi-temporal features.


FIGURE 


FIGURE


Change Detection and Captioning Branch


The structure of the change detection branch is depicted in Fig.
[2].Leveraging multi-scale bi-temporal features extracted from the backbone
network, this branch facilitates refined mask prediction. Specifically,
multiple Bi-temporal Iterative Interaction (BI3) layers with residual
connections iteratively enhance and refine bi-temporal high-level
features, effectively capturing semantic changes. Since the low-level
features contain more detailed information, they are crucial for refined
change boundary detection. We further incorporate multiple
Convolution-Based Bi-temporal Fusion (CBF) modules for bi-temporal
feature fusion across four scales. Subsequently, through deconvolution,
features are progressively integrated from bottom to top, bolstering the
model&apos;s discrimination capability for changes and enhancing change
detection accuracy. For the input *MATH*, the
CBF module can be represented as follows: *MATH*.


The change captioning branch focuses on semantic-level change
interpretation, translating visual changes into textual descriptions, as
depicted in Fig. [3]. This branch leverages multiple BI3 layers to
interactively process high-level semantic features extracted from the
backbone network, thereby obtaining bi-temporal visual features
revealing changes of interest. Subsequently, a convolution-based domain
bridging module further processes the bi-temporal features to facilitate
the transition from the visual domain to the textual domain. Finally,
these processed features are fed into a Transformer decoder to generate
descriptive sentences elucidating the changes. The processing flow of
the domain bridging module can be expressed as follows: *MATH*. 


FIGUER


LLM: Brain of Change-Agent


Recently, LLMs have garnered significant attention for their remarkable
achievements. By harnessing extensive web knowledge, LLMs have
demonstrated substantial promise in attaining human-level intelligence,
exhibiting noteworthy capabilities in tasks encompassing instruction
comprehension, planning, reasoning, and proficient natural language
interactions with humans [*REF*; *REF*; *REF*; *REF*].
Building upon these advancements, there has been a growing research
field that employs LLMs as central controllers for constructing
autonomous agents [*REF*; *REF*; *REF*; *REF*].


Inspired by these developments, we harness the capabilities of LLMs as
the cognitive core responsible for orchestrating the scheduling of our
Change-Agent. As depicted in Fig. [4], the LLM meticulously plans task execution
based on user instructions. Despite excelling at text-related tasks,
LLMs lack inherent visual perception capabilities. To bridge this gap
and enable change interpretation and analysis akin to human capability,
we equip our agent with a suite of Python tools. These tools include a
visual feature extraction backbone, change detection branch, change
captioning branch, and relevant Python libraries. Leveraging these
tools, the LLM autonomously crafts Python programs, subsequently
executed by a Python interpreter to accomplish tasks beyond its
intrinsic capabilities. Subsequently, the LLM processes the results and
furnishes feedback to the user. In addition to furnishing change masks
and descriptions, our agent can implement object counting, image
processing, change cause estimation, prediction of future changes, and
more. In essence, our Change-Agent adeptly provides users with pertinent
information, harnessing the rich knowledge of the LLM in tandem with the
provided tools.


To facilitate the accurate generation of formatted Python code and tool
invocation by the LLM, a meticulously crafted text prompt is essential.
Inspired by the prompt utilized in [*REF*], we devised a text
prompt instructing the model on proper tool utilization. This prompt is
input to the LLM in the role of a system instruction, as shown in Fig. [5].


FIGURE


EXPERIMENTS


Objective Function for Multi-task Training


During the training phase of our MIC model, we perform multi-task
learning on both change detection and change captioning. Utilizing the
meticulously constructed LEVIR-MCI dataset, we iteratively update the
model parameters in a supervised learning framework.


For the change detection, we utilize the cross-entropy loss function to
quantify the dissimilarity between predicted change masks and ground
truth annotations. Specifically, the loss function can be mathematically
defined as follows: *MATH*.


Similarly, for change captioning, we utilize the cross-entropy function
to compute the loss between predicted sentences and their corresponding
ground truth sentences. The formulation of the captioning loss function
can be expressed as follows: *MATH*.


To balance the losses of these two tasks during model training, we adopt
a normalization approach, scaling the losses of both tasks to the same
order of magnitude. This ensures that each task contributes equally to
the overall loss function and facilitates effective simultaneous
optimization of the change detection and change captioning.
Mathematically, the total loss can be expressed as follows: *MATH*.


Evaluation Metrics


To evaluate the performance of our MCI model in detecting multi-category
changes in remote sensing images, we use the Mean Intersection over
Union (MIoU). The MIoU metric assesses the spatial overlap between
predicted change masks and ground-truth masks, providing a reliable
measure of pixel-level change detection accuracy. Mathematically, the
calculation of MIoU is defined as: *MATH*.


To evaluate the performance of our MCI model in describing changes
between bi-temporal images, we adopted several evaluation metrics
commonly used in previous change captioning studies
[*REF*; *REF*; *REF*; *REF*]. These metrics are
as follows:
-  BLEU-n [*REF*]: The BLEU-n metric measures the n-gram similarity
between generated sentences and ground-truth sentences. We adopt
n=1, 2, 3, and 4 to compute the BLEU-n score.
-  METEOR [*REF*]: The METEOR computes the harmonic mean of
precision and recall of single words. Besides, it incorporates a
penalty factor to consider the fluency of generated sentences.
-  ROUGE *MATH* [*REF*]: The ROUGE *MATH* metric measures the similarity
of the longest common subsequence between generated sentences and
ground-truth sentences. It is more concerned with the recall rate
and is suitable for evaluating long sentences.
-  CIDEr-D [*REF*]: The CIDEr metric treats each sentence as a
document and represents it in the form of Term Frequency Inverse
Document Frequency (TF-IDF) vectors. After computing TF-IDF for each
n-gram, CIDEr-D is obtained by performing the cosine similarity.
By employing this comprehensive suite of evaluation metrics, we aim to
provide a thorough assessment of the change detection and captioning
capabilities of our MCI model. Higher scores across these metrics
indicate more accurate change masks or superior sentence quality.


Experimental Details


We implemented all models using the PyTorch deep learning framework and
trained them on the NVIDIA GTX 4090 GPU. During training, we minimized
the loss function defined by Equation and employed the Adam optimizer [*REF*]
with an initial learning rate of 0.0001 to optimize the model
parameters. We set the maximum number of epochs to 200 and utilized word
embeddings with a dimensionality of 512. The backbone for feature
extraction is the Siamese weight-shared Segformer-B1
[*REF*]. Initially, we trained the backbone until the sum of
BLEU-4 and MIoU indicators did not increase for 50 consecutive epochs.
Subsequently, we froze the backbone network and continued training the
two branches separately.


TABLE


Multi-level Change Interpretation Performance


Currently, there is a dearth of methods that simultaneously address
change detection and change captioning. To assess the efficacy of our
model in handling both tasks, we conducted comprehensive performance
evaluations by comparing against established methods in each respective
field on the proposed dataset.


For the change detection evaluation, we have benchmarked several
well-known methods, including FC-EF [*REF*], FC-Conc [*REF*],
FC-Di [*REF*], and BIT [*REF*]. Meanwhile, for the change
captioning assessment, we compare our model with several existing
state-of-the-art (SOTA) methods, including Capt-Rep-Diff [*REF*],
Capt-Att [*REF*], Capt-Dual-Att [*REF*], DUDA [*REF*],
MCCFormer-S [*REF*], MCCFormer-D [*REF*], RSICCFormer
[*REF*] and Chg2Cap [*REF*].


The comprehensive performance comparisons, as shown in Table, underscore the superiority
of our proposed methodology across both change detection and change
captioning tasks. Our method not only achieves simultaneous proficiency
in both tasks but also surpasses previous methods. Specifically, our
model exhibits superior performance in change detection with a +1.01%
improvement on MIoU. Moreover, in terms of change captioning, our model
exhibits a noteworthy advancement, showcasing a +1.56% improvement on
BLEU-4 and a substantial +3.68% improvement on CIDEr-D.


Through the above performance analysis, we gain insights into the
model&apos;s ability to generate pixel-level and semantic-level
interpretation information of changes observed in remote sensing images.
This highlights the efficacy of our proposed MCI model in providing a
comprehensive and in-depth analysis of surface changes.


TABLE


Ablation Studies


BI3 Layer


As a critical component of our MCI model, the BI3 layer incorporates LPE
and GDFA modules. The LPE module enhances feature representation through
local perception enhancement,improving the model&apos;s sensitivity to
multi-scale changes. The GDFA module effectively integrates the
differential information, allowing the model to capture the change area
more accurately. In Table, we conducted ablation experiments on the
effectiveness of the LPE and GDFA modules. The baseline utilizes
Transformer encoding layers to process bi-temporal features. The
experimental results show that integrating the LPE and GDFA modules
leads to improved performance in both change detection and change
captioning tasks.


FIGURE 


TABLE


Balancing Change Detection and Captioning


In our exploration of multi-task learning for change detection and
change captioning, achieving an effective balance between these tasks
posed a significant challenge. Initially, when we simply combined the
individual losses *MATH* and *MATH* to form
the total loss and minimized it to optimize the multi-task model
parameters, we observed large discrepancies in the magnitudes of these
two losses, as illustrated in Fig. [6]. This imbalance during training could potentially
impact overall change interpretation performance. To address this issue,
we adopt a balancing strategy detailed in Equation to normalize the two losses and achieve
the training balance between the two tasks. Table shows the impact of these two
strategies on model performance. It&apos;s evident that our balancing
strategy plays a crucial role in harmonizing the training process,
leading to more balanced performance across both change detection and
captioning tasks.


In addition, we conducted a comparative analysis of single-task and
multi-task learning, as shown in Table. While multi-task learning yields poorer
change detection performance compared to single change detection
training, it contributes to improving change captioning capabilities,
particularly on critical BLEU-4 and CIDEr-D metrics. This observation
could be attributed to the shared knowledge and feature representation
learning across tasks. Pixel-level change detection can be advantageous
for accurately localizing changes when generating change descriptions.
However, it&apos;s worth noting that ground-truth change descriptions
sometimes overlook minor changes, which could be detrimental to
pixel-level change detection. The trade-off between change detection and
change captioning underscores the complex interplay between pixel-level
and semantic-level interpretation of surface changes.


In conclusion, achieving a balance between change detection and
captioning, and fostering collaborative improvement between them,
remains a subject worthy of further exploration. We look forward to
discovering more effective methods to address this challenge in future
research.


FIGURE


Qualitative Change Interpretation Results


To illustrate the effectiveness of our proposed model in simultaneously
generating change masks and change descriptions, Fig. presents comparative results between our
proposed model and the baseline model. The experimental results
demonstrate the superior capabilities of our model in joint
interpretation of change detection and change captioning.


In terms of change detection, our model exhibits remarkable proficiency
in identifying subtle changes in small buildings, as depicted in the
first row of Fig. Moreover, our model produces more refined road
masks in rows 5 and 7 compared to the baseline. Unlike the baseline,
which erroneously detects some buildings in the sixth and eighth rows,
our model performs admirably.


Regarding change captioning, our model accurately recognizes and
describes the changes in the bi-temporal images in the fourth and eighth
rows of Fig. Conversely, the baseline erroneously concludes
that no changes have occurred, resulting in a critical error in the
description. Furthermore, it is evident from the first four rows of Fig.
that our model not only accurately identifies
the types of changing objects but also describes their spatial positions
and the relationships between the changing objects and surrounding
features.


FIGURE


Interaction Between User and Change-Agent


Utilizing the MCI model trained on the LEVIR-MIC dataset, we have
constructed an LLM-based Change-Agent. The MCI model provides our agent
with powerful change interpretation capability. The LLM serves as the
brain behind our agent, facilitating seamless interaction and
comprehensive interpretation of changes.


In Fig., we provide two examples of conversations
between users and our Change-Agent, illustrating its adaptability and
versatility. In the first conversation, our agent showcases its
adeptness in understanding user instructions and subsequently carries
out tasks such as change mask prediction, change captioning, changed
object counting, and mask post-processing. This automated approach
significantly enhances user efficiency and offers a more intuitive
experience in change interpretation. In the second conversation, our
agent leverages the MCI model to describe the changes in the images, and
leverages the rich knowledge inherent in the LLM to provide insightful
answers to user queries regarding change causality and future change
predictions, aiding in user&apos;s decision-making processes.


Fig. presents a specific example to illustrate
the executable code generated by the LLM within our Change-Agent. When
the user requests to perform change detection, display building areas in
green, display road areas in blue, and count changed buildings, the LLM
efficiently generates executable Python code, demonstrating the agent&apos;s
versatility and responsiveness to user needs.


Through these interactive capabilities, our Change-Agent serves as a
versatile tool for facilitating nuanced and insightful analysis of
changes in remote sensing images, fostering seamless communication and
collaboration between users and remote sensing expertise.


Discussion and Future


Our research introduces the LEVIR-MIC dataset, which lays a solid data
foundation for facilitating multi-task learning and exploring the
interconnection between change detection and change captioning. Besides,
our Change-Agent opens up new avenues for change interpretation and
brought a new thinking to the remote sensing field. However, there are
still some issues worthy of further exploration:
-  Multi-Task Learning. Challenges such as task balance and inter-task
relationships remain in multi-task learning. Future work can focus
on designing better model structures and training strategies to
improve overall interpretation performance on change detection and
change captioning.
-  Scheduling of Change-Agent. The hand-craft system prompts inputted
into the LLM play a crucial role in facilitating accurate agent
scheduling. Optimizing system prompts could enable to better grasp
user intentions and plan more accurate and rational task execution
pathways.
-  Tool Expansion. Beyond change interpretation, providing the agent
with additional models can enhance its capabilities for handling
complex remote sensing image processing tasks, offering users more
comprehensive interpretation services. Additionally, the agent&apos;s
updateability ensures it remains up-to-date with new tools and
models.
-  Multi-Agent Systems. Introducing multiple agents with diverse
capabilities and fostering collaboration among them could lead to
more flexible and collaborative image processing and analysis,
contributing to the development of more efficient and intelligent
remote sensing systems.


Conclusion


In this paper, we have addressed the critical need for a comprehensive
and intelligent interpretation of Earth&apos;s surface changes through
developing a novel Change-Agent. Our Change-Agent can follow user
instructions to achieve comprehensive change interpretation and
insightful analysis according to user instructions. The Change-Agent,
fueled by an MCI model and an LLM, exhibits robust visual change
perception capabilities and adept scheduling. The MCI model, comprising
two branches of change detection and captioning, offers pixel-level
change masks and semantic-level textual change descriptions. In these
two branches, we propose BI3 layers with LPE and GDFA to enhance the
model&apos;s discriminative feature representation capabilities.
Additionally, we build a dataset comprising diverse change detection
masks and descriptions to support model training. Experiments validate
the effectiveness of the proposed MCI model and underscore the promising
potential of our Change-Agent in facilitating comprehensive and
intelligent interpretation of surface changes.