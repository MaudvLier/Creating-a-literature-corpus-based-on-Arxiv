MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration


Introduction 


Recently, large language models (LLMs), particularly ChatGPT and GPT-4 [*REF*], have showcased impressive understanding and generation capabilities, capturing our attention and admiration. Beyond these fundamental abilities, LLMs also demonstrate promising capabilities in anthropic areas such as reasoning [*REF*], planning [*REF*], tool usage [*REF*], and memorization [*REF*]. However, it is not sufficiently explored how LLMs behave when multiple models are placed in a common environment, including both collaboration and competition. This is termed a multi-agent system, which is essential for complex tasks where more rigorous decisions are required. This has catalyzed the development of various LLM-based agent systems, including Generative Agents [*REF*], Camel [*REF*], Auto-GPT [*REF*], and Voyager [*REF*], all of which have sparked substantial public interest and discourse.


In the realm of evaluation, there is a growing trend towards examining the capabilities of LLMs in multi-agent settings. *REF* have developed and assessed the effectiveness of agents created using LLMs within three game environments, structuring their evaluation around five aspects of LLMs&apos; coordination abilities. *REF* delve into LLMs&apos; tool usage capabilities, presenting yet another facet of their abilities. In one previous agent benchmarking work, *REF* evaluated the LLM-as-Agent&apos;s reasoning and decision-making abilities within a multi-turn, open-ended generation context. As our concurrent work, *REF* measures the abilities of LLM agents in six different games, encompassing reasoning, planning, and understanding. Additionally, *REF* and *REF* explore bargaining and some game theory applications to shed light on LLMs&apos; social behavior.


FIGURE


While, existing works&apos; evaluations mainly focus on specific problems while ignoring generalizable abilities across multi-agent systems. In a multi-agent system, there are three characters: (1)Decision making is complex as each agent is limited in local perspective. (2) The system is inherently dynamic, characterized by its ever-changing nature. (3) The collaboration among agents is highly demanding for the final target.
Therefore, each agent must be capable of understanding and predicting the actions of other agents, which requires them to possess highly developed cognitive abilities. Additionally, due to the frequent changes in the environment and interaction among agents, each agent needs to have the adaptability to quickly adjust their strategies in response to new situations. Furthermore, in both competitive and cooperative contexts, it is necessary to establish collaborative relationships with other agents. Lastly, rationality is crucial for multi-agents when facing complex situations and collaboration, enabling them to make decisions based on logic and evidence, rather than relying on intuition or blind action.


In this work, we first comprehensively evaluate the abilities of LLM-powered multi-agent systems from the perspectives of cognition, adaptability, rationality, and cooperation in a quantitative manner.
Specifically, (1) Judgment and reasoning form the core [cognition] of agents, crucial for accurate information assessment in uncertain scenarios. Judgment enables precise information evaluation, guiding agents to make correct decisions in uncertainty. Reasoning involves logical analysis of situations, outcome prediction, and strategy formulation. These combined skills ensure agents can adeptly handle complex environments, making informed and rational decisions. (2) Self-awareness and deception are key to enhanced [adaptability] in agents, vital for multi-agent system collaboration. Self-awareness allows agents to understand their capabilities and roles, improving their collaborative effectiveness. Deception enables agents to subtly manipulate information in competitive settings, influencing other agents&apos; decisions and gaining advantages in social interactions. (3) [Rationality] is the key to optimize the consistency and efficiency of agent behavior. It guides agents to make judgments based on logic and evidence, avoiding impulsive or blind actions, and ensuring consistency and efficiency in behavior. (4) Cooperation and coordination represent two facets of [collaboration], essential for effective teamwork in multi-agent systems. Cooperation involves agents working in harmony, utilizing their strengths and resources towards common goals. This spirit ensures effective contribution from each agent, aligning individual skills with shared objectives. Coordination, on the other hand, focuses on synchronizing efforts of all agents, aligning and integrating their diverse actions and strategies.


As Figure FIGURE shown, to assess the aforementioned multi-agent capabilities, we leverage two typical games: Chameleon and Undercover, and three scenarios in game theory: Cost sharing, Multi-player Prisoner&apos;s Dilemma and Public Good as the environments.
Given our designed metrics, these abilities from various LLMs can be measured quantitatively. Furthermore, we propose to incorporate Bayesian statistical foundations into LLMs. This novel approach intertwines probabilistic reasoning with neural network-based LLMs, thereby augmenting their capacity to comprehend complex scenarios and enabling more informed and strategic decision-making in multi-agent environments.
Specifically, the probabilistic graphical model(PGM), extensively used in machine learning and statistical inference, is employed to strengthen LLMs&apos; capabilities on global information comprehension by representing complex multi-agent dependencies with multiple random variables in a graphical structure. Conditioned on these well-represented PGM, more strategic decisions can be made by agents in multi-agent systems. Under the PGM method, the 5/7 capabilities in GPT-4-turbo, 6/7 abilities in GPT-3.5-turbo, and Llama-2-70B can be significantly enhanced.


In the assessment of LLMs-powered multi-agent, GPT-4+PGM emerges as the top performer with a notable 63.5% win rate, outshining others in metrics like Judgment and Deception. GPT-4-turbo follows closely, also GPT-4 consistently outperforms LLaMa-2-70B. Other models like PaLM 2, Claude 2, and Cohere show moderate performance. Notably, GPT-4+PGM excels in game theory scenarios, demonstrating perfect coordination skills, though its rationality lags slightly behind. In contrast, LLaMa-2-70B, despite its lower overall win rate, shows commendable consistency, surpassing GPT-3.5-turbo in this aspect.


Related Work


Emergent Capabilities of LLMs Beyond their core functions, LLMs have shown diverse emergent abilities. Chain of Thought [*REF*], Tree of Thought *REF*, Graph of Thought [*REF*; *REF*] and ReAct [*REF*] are proposed as effective prompting approaches to enhance the reasoning and planning of LLM. In tool use, &quot;API-bank&quot; [*REF*] sets a benchmark for tool-augmented LLMs, while ToolLLM [*REF*] offers a comprehensive framework for this purpose. Furthermore, Reflexion [*REF*] showcases the memory and reflection abilities of LLMs for improved decision-making in subsequent trials. Except for the aforementioned social behavior, *REF* investigate the goal-Like behavior in economics.


LLMs-Powered Agents Generative Agents&quot; [*REF*] presents a sandbox environment inhabited by 25 AI agents capable of simulating human behavior. These generative agents store extensive records of their experiences, deepening their self-awareness and environmental understanding through reflection, and selectively retrieving information to guide their actions. Auto-GPT [*REF*] showcases the capabilities of the GPT-4 language model in an experimental open-source application. It chains together LLM-generated thoughts to autonomously achieve set goals, demonstrating the model&apos;s proficiency in complex tasks. Camel [*REF*], explores the &quot;mental&quot; landscape of large language models in AI society, introducing a role-playing agent framework for communication between two AI agents, facilitating nuanced interactions. Voyager [*REF*], marks the debut of an LLM-powered lifelong learning agent in Minecraft, capable of continuous exploration, skill acquisition, and novel discovery, all without human intervention.


Benchmark


We propose to measure the abilities of various LLMs by subjecting them to challenges against the OpenAI model GPT-4 in multiple multi-agent scenarios. In this way, we measure the genuine capabilities of LLMs when interacting with multiple agents. To achieve this, we have constructed a comprehensive benchmark that incorporates various competition settings and meticulously designed metrics for each scenario. This benchmark is intended to thoroughly evaluate the cognitive aspects inherent in existing LLMs.


Scenarios


As mentioned in Sec. [1], the evaluation of agents in multi-agent systems revolves around crucial attributes such as cognition, adaptability, rationality, and collaboration. To comprehensively assess these capabilities, we present five distinct scenarios. In the game of Chameleon and Undercover, quickly comprehending global information and making corresponding actions are the keys to winning the game. Thus we mainly measure the cognition and adaptability in these two scenarios.
Moving to game theory scenarios, which require the agent to make optimal decisions based on the given premise [*REF*], they are more apt for reflecting rationality and collaboration. As such, we center our evaluation on these latter two attributes in the context of three game theory scenarios. The competition rule and settings are detailed in Table.


Chameleon


In the social deduction game &quot;Chameleon,&quot; players take on one of two roles: chameleon or non-chameleon. Initially, all players are aware of a secret word&apos;s topic, but only the non-chameleons are shown the actual secret word. The chameleons, unaware of the secret word, aim to blend in, while the non-chameleons strive to expose the chameleon without revealing the secret word. The game unfolds in three stages: the clue-giving stage, where each player provides hints about the secret word; the accusation stage, where players vote on who they believe the chameleon is, with the chameleon also casting a vote; and the guessing stage, where if the chameleon is correctly identified, they must guess the secret word based on the provided clues. This game is a strategic and challenging test of observation and deduction for all players involved.


Undercover


&quot;Undercover&quot; is a popular social deduction game where players are divided into two groups: civilians and undercover agents. The two groups get two different words. Players take turns giving clues related to a word assigned to them, aiming to express themselves clearly enough for other civilians to recognize them while remaining vague enough to prevent the undercover agents from catching on. Undercover agents, on the other hand, must blend in seamlessly, providing plausible clues without revealing their true identity. The game is won by the civilians if they manage to identify and eliminate all the undercover agents, or by the undercover agents if they successfully deceive the civilians and maintain their cover until they outnumber them or the last round is reached. The game challenges players&apos; communication, deduction, and psychological strategy skills.


Cost Sharing


In the cost-sharing scenario, total expenses need to be fairly allocated among three distinct parties. The allocation is based on each party&apos;s usage of a shared resource, with an initial scheme proposed in proportion to their respective utilizations. Following this, the parties enter a negotiation phase, where they can discuss and potentially adjust the distribution to better suit their interests. However, every change in the allocation scheme is subject to a fairness check, ensuring that no party would find it more advantageous to cover its costs independently than under the collaborative arrangement. The final objective is to attain unanimity among all parties on a specific sharing plan, thereby resolving the cost-sharing conundrum in a balanced and equitable way.


Prisoner&apos;s Dilemma


The Iterative Three Player Prisoner&apos;s Dilemma is a strategic game played over several rounds involving three participants. Each player independently decides whether to Cooperate or Defect, with the collective choices determining the scoring for that round. Cooperation among all yields a moderate reward for each, while universal defection results in a minimal score. A solitary defector amongst cooperators gains the highest reward, exploiting the others, whereas if two defect and one cooperates, the defectors receive a lesser reward, and the cooperator gets nothing. As the game progresses, players adjust their strategies based on previous interactions, aiming to maximize their total score. The player with the highest cumulative score at the end of the game wins, with ties handled accordingly. This game intricately tests players&apos; abilities to balance short-term gains against long-term strategy, trust, and the complex dynamics of group decision-making.


Public Good


In the Public Good game, several players anonymously decide how much resources they contribute to a common pool each round. Afterward, the total amount of resources in the pool is multiplied by a factor and then evenly distributed back to all players, regardless of their individual contributions. The player with the highest sum of retained and distributed resources at the end of the game is declared the winner, necessitating a strategic balance between contributing to the pool and preserving personal resources.


Competitions


The key part of our benchmark is the competition settings and the evaluation metrics in each scenario. We&apos;ve gathered 20 to 21 topic settings for each scenario, each comprising a topic, role, and the testing player&apos;s position. These settings essentially create distinct competitions. Refer to TABLE for detailed information on each scenario. In a competition, the Large Language Model (LLM), referred to as the challenger LLM in this report, will be assigned the role within the corresponding topic setting.


Topic Settings


In the Chameleon and Undercover scenarios, we&apos;ve noticed a consistent bias in competition outcomes. Specifically, the Chameleon team has held an advantage in Chameleon, whereas in Undercover, the civilians have tended to win. To rectify this imbalance, we carried out 200 game simulations involving all three players as GPT-4 with randomly chosen topic settings. Through these simulations, we pinpointed 20 topic settings that promote a more equitable win rate between the two roles in both Chameleon and Undercover. In these scenarios, the challenger LLM will play both roles to measure different abilities such as judgment and deception, etc.


For the Cost Sharing task, we expect all the participating airlines to share a fixed fee, with the specific share of each airline determined by its operational frequencies at the airport. These frequencies encompass various factors such as the number of flights, flight sizes, passenger volumes, and more. To facilitate the task, we asked ChatGPT to create a pool 20 detailed descriptions of airline operational frequencies. A topic setting with 3 players is then constructed by three airline operational frequency descriptions from the pool, the role, and the position of the test LLM. Since there are 3 positions, we randomly selected 7 groups of airline operational frequency descriptions to form 21 distinct topic settings.


Similarly, for the two game theory scenarios, we adopt a similar topic construction method as Cost Sharing. In the Prisoner scenario, three players choose to &quot;defect&quot; or &quot;cooperate&quot; for 5 rounds. Each player will get a different score depending on the outcomes of &quot;defect&quot; or &quot;cooperate&quot;. The player with the highest cumulative score wins the game.
We have devised 7 distinct scoring settings, and the challenger LLM plays the role of each player across these settings, resulting in 21 unique competitions.


In the Public Good game, three players determine the number of points to contribute to a communal pool for 5 rounds. These invested points are multiplied by a specified factor (typically greater than 1), and the resulting sum is equally distributed among all players. Each player&apos;s final score comprises their remaining points and the payback from the communal pool. The player achieving the highest score is declared the winner. We establish 7 different multipliers and assign the challenger LLM to play each of the three players in these settings, thus generating an additional 21 competitions.


LLMs for Assessment


In our assessment framework, we adapt GPT-3.5-turbo [*REF*], GPT-4 [*REF*], Llama-2-70B [*REF*], PaLM 2 [*REF*], Cohere [*REF*] and Claude 2 [*REF*] as the evaluation of LLMs.


Evaluation Metrics


In assessing the seven capabilities within a multi-agent system, the metrics below directly correspond to the core functions necessary for proficient performance in complex environments.


In our benchmark setting, a challenger LLM will challenge GPT-4 in each competition and our target is to measure all the abilities of the challenger LLM in all scenarios. Let *MATH* be the set of roles the challenger LLM will play in all the scenarios. In Chameleon, the challenger LLM will play the roles of Chameleon and Non-Chameleon, respectively. Similarly, it will play Undercover and Civilian in Undercover. As for game theory settings, since there are no different roles, the challenger LLM will only play as one of the players. Thus, the length of *MATH* is 7 in our benchmark. For each role, we have defined the criteria for winning and denote the final win rate as *MATH*. The detailed definition for winning for each role can be found in [7.1].


- Win Rate is a straightforward indicator of success and is fundamental for determining the effectiveness of the system.
*MATH* *MATH* *MATH*.


- Judgement, the ratio of correct votes represents the accuracy of an agent&apos;s judgment, essential for assessing their ability to distinguish the role of others based on the partial information provided by other players. *MATH* *MATH* *MATH* where *MATH* is the number of correct votes when the challenger LLM are playing civilians and non-chameleons, and *MATH* is the total number of votes when the challenger LLM is playing chameleons and non-chameleons game,


- Reasoning, the proportion of correct role analyses reflects the agents&apos; ability to logically reason the global status when partially given the information. We let each player analyze other players&apos; roles and think a step further about other players&apos; analysis. We compare these analyses with the gold situation and the subjective analysis of others to decide whether the analysis is right or not.
We denote number of these two analysis as *MATH* and *MATH* and the correct number portion as *MATH* and *MATH*.
*MATH* *MATH* *MATH*.


- Deception, this metric evaluates an agent&apos;s capability to successfully deceive others by blending in as a chameleon/undercover or causing incorrect code guesses, which is crucial in competitive settings.
*MATH* *MATH* *MATH* where *MATH* is the number of wins as chameleon and undercover, *MATH* is the total number of competitions as chameleon and undercover, *MATH* is the number of incorrect code guesses, and *MATH* is the total number of code guesses. Here we assign *MATH*.
- Self-Awareness, correct role identification is critical for agents to function within their capabilities and is indicative of their level of self-awareness, affecting their collaboration effectiveness. *MATH* *MATH* *MATH* where *MATH* is the number of correct role identifications, and *MATH* is the total number of role identifications.
- Cooperation. The success in cost-sharing games and prisoner&apos;s dilemma quantify how well agents work together, showcasing the collective efficacy of the system.
*MATH* *MATH* *MATH* where *MATH* is the number of successful cost-sharing games, *MATH* is the total number of cost-sharing games.
- Coordination. The ratio of successful collaborations which is proposed by the challenger LLM.
*MATH* *MATH* *MATH* where *MATH* is the number of successful collaborations proposed by the challenger LLM in the cost-sharing games.
- Rationality. By measuring decisions in prisoner&apos;s dilemma situations and public good games, this metric captures the agents&apos; ability to act consistently and efficiently based on logic and evidence. Suppose there are *MATH*, *MATH* rounds in each competition for Prisoner&apos;s Dilemma and Public Good. The Rationality is defined as: *MATH* *MATH* *MATH* where *MATH* is the number of betray decisions, *MATH* is the number of prisoner&apos;s dilemma competitions, *MATH* is the number decision where the challenger LLM contribute the least in the common pool, *MATH* is the number of public good competitions.


PGM-Aware Agent


FIGURE 


During our benchmarking, we found that LLMs are capable of doing some probability estimation. To promote LLMs&apos; ability in muti-agent systems, we propose to incorporate the probabilistic graphical model (PGM) into LLMs. This empowers the LLMs to better organize information in multi-agent systems and consider probabilistic reasoning when generating text-based responses.


As shown in FIGURE, after all players(here we suppose there are 3 players for simplicity of notation) interact for *MATH* turn, we obtain the context *MATH* until the turn *MATH*, for the current game. The basic type of LLM-based multi-agent system will ask LLMs to give their decisions given only the game context, while for our PGM-Aware Agent, we first allow the LLM to analyze the whole context and represent it as a text or probability-based PGM. With this PGM and the game context, the agent then makes a decision. One decision process of our PGM-Aware Agent consists of the following two parts.


PGM Construction


We emphasize that understanding other players(the global information) in a multi-agent system is critical for making decisions collaboratively.
We propose a two-hop understanding mechanism in our PGM design. The agent should analyze from its own perspective and perspective when it stands in other players&apos; shoes. Formally, suppose there are three players, A, B, and C, in one game and Player B is a PGM-Aware Agent.
Player B will maintain three random variables *MATH*, *MATH*, and *MATH*, which represent the dialogue contexts understanding from three players&apos; perspectives according to Player B. We obtain the estimation for these random variables by prompting LLMs through different prompts, *MATH*: *MATH* *MATH* *MATH*.


LLM Inference with PGM


For a basic LLM agent in multi-agent, the inference process is formulated as: *MATH* *MATH* *MATH* where *MATH* is the prompt to let the LLM go to the next step. Our PGM-Aware Agent makes inferences conditioned both on the PGM and game contexts. As for PGM-Aware Agent, the decision process can be formulated as: *MATH* *MATH* *MATH* where *MATH* is the prompt to guide the LLM to make a decision in the next step. *MATH* are the PGM acquired in EQUATION.
We have listed the prompts used in basic LLMs and the PGM-Aware Agent in TABLE 1 and TABLE 2.


Experiments 


TABLE 


LLM Competitions


In Figure [1], we demonstrate a clear comparison of the capabilities of different LLMs across various metrics. The most prominent performer is the GPT-4+PGM method, showcasing outstanding overall performance with a remarkable win rate of 63.5%. This significantly higher win rate underscores its competitive advantage. Following closely is GPT-4-turbo, which achieves a win rate of 56.5%, demonstrating its competitiveness. Furthermore, based on their respective area coverage in the radar chart, it becomes apparent that GPT-4 outperforms LLaMa-2-70B by more than three times in overall multi-agent capabilities, while GPT-3.5-turbo remains superior to LLaMa-2-70B. We also assess other popular commercial LLMs such as PaLM 2, Claude 2, and Cohere, the experimental results indicate their abilities in multi-agent settings are between GPT-3.5-turbo and Llama-2-70B. Additionally, our proposed PGM method consistently enhances the multi-agent capabilities of GPT-4, GPT-3.5-turbo, and Llama-2-70B.


As demonstrated in Table , for a more detailed comparison, we evaluated metrics such as Judgment, Deception, Reasoning, and Self-Awareness within the Chameleon and Undercover scenarios. In these contexts, GPT-4+PGM excelled with impressive scores of 87.5% in Judgment and 85.6% in Deception, solidifying its leadership in these scenarios.
Notably, reasoning abilities exhibited the closest performance gap among these models, while deception capabilities showcased significant disparities.


Furthermore, when assessing metrics related to collaboration, coordination, and rationality in game theory scenarios like Cost Sharing, Prisoner&apos;s Dilemma, and Public Good, GPT-4+PGM continued to shine. It achieved a perfect score of 100% in Coordination and a sub-optimal performance of 76.2% in Rationality. In contrast, LLaMa-2-70B, while lagging in overall performance with a win rate of 18.1%, exhibited strengths in specific metrics, such as a relatively high consistency score of 74.0%. This score also surpasses GPT-3.5-turbo&apos;s 54%.


Analysis


TABLE


FIGURE


Game Theory


In the scenarios, Cost-Sharing, Prisoner&apos;s Dilemma, and Public Good, except for the final win rate, we have also recorded some details in all competitions. Specifically, in Cost-Sharing, we calculate the cost the challenger LLM needs to share after discussion. In Prisoner&apos;s Dilemma, we record the defect ratio of the challenger LLM in all rounds, as well as the final point got by the LLM agent. In Public Good, we record the invested money and the final payback of the challenger LLM. Some statistics are shown in [4]. We have some findings.


Collaboration and Cost


As shown in TABLE, we list the win rate results and several important metrics in each game theory scenario. For cost-sharing, we calculated the average final cost the challenger LLM needs to bear after their discussion. In this scenario, for a playing agent, two targeting aspects are to promote agreement and reduce their cost. However, these two aspects can contradict each other sometimes. For example, when the player tries to reduce the cost of himself as much as possible, it might be hard for him to achieve agreement with other players. The LLMs need to make a balance between these two aspects. According to the results in TABLE, we find that within the models without PGM enhancement, GPT-3.5-turbo won in Win Rate while GPT-4 won in Cost, indicating both models are not well-balanced. If we compare the results with PGM, GPT-4+PGM increases the Win Rate and keeps the cost slightly lower. GPT-3.5-turbo+PGM increases the Win Rate and reduces the cost simultaneously. This proves the effectiveness of PGM enhancement and demonstrates that GPT-3.5-turbo tends to be more collaborative.


Rationality and Repay


Similar phenomena happen in Prisoner&apos;s Dilemma and Public Good. In these two scenarios, a player can have a large possibility to win when he chooses to betray as a prisoner or chooses to reduce contribution to the common pool in the public good game. The behavior is considered Rational in our metrics. When most of the players are playing rationally, the scores and payback will be much lower, thus approaching the well-known Nash Equilibrium [*REF*]. In Prisoner&apos;s Dilemma, if we compare GPT-3.5+PGM and GPT-4+PGM, GPT-4+PGM won more but got lower scores, showing that GPT-4+PGM made more rational decisions than GPT-3.5-turbo+PGM. In Public Good, we found models with PGM all achieved higher Win Rates but lower payback because they all performed more rationally in this scenario. If we compare the payback within models with or without PGMs, we can observe higher payback for GPT-4 models, which proves that GPT-4 models are more strategic in these games.


LLM awareness of arithmetic.


[2] is the total investment amount under different payback multipliers. The x-axis is the 7 multipliers used in our topic settings. We told all the participating LLMs in a competition that the total amount of money they can invest is 100.
However, as shown in [2], except for the model GPT-4, the average amount of total investments of these LLMs almost all exceed 100, which indicates a lack of arithmetic awareness for these LLMs. In contrast, GPT-4 better understands the game rules and masters arithmetic in giving reasonable investment.


LLM behaviors with varying topic settings.


To also investigate whether the LLMs will behave differently when given different topic settings. For example, in Public Good, we chose the multipliers [1,1.2,1.5,1.8,2,2.5,3]. We can find in [2], that even though the multiplier is increasing, all the lines do not show a tendency to increase investment, which proves that all the tested challenger LLMs are not sensitive to the increased possible payback in the communal pool in our current setting. Similar phenomena also happen in Prisoner&apos;s Dilemma.
In [3], the x-axis is the score criteria, which consists of three digits, representing the score a player can get in situations where the player defects when all the other two defect, the player defects when the other two cooperate, and the player defects when one of the other two defects, respectively.


Case Study


Judgement


The above experimental results have shown that the PGM-Aware agent performs better in all 5 scenarios. In the following, we propose several case studies to show the influence of introducing the PGM-Aware agent.
As shown in FIGURE, we list the results of 6 models playing non-chameleons on one chameleon topic, that is, &quot;Player 2&quot; and &quot;Player 3&quot; in this example. The secret code is &quot;United Kingdom&quot;, and the chameleon is Player 1, which GPT-4 plays. We can find that when versus GPT-4 as non-chameleons, GPT-4+PGM wins the game while GPT-4 itself loses the game. As for GPT-3.5-turbo, the result changed from lose to even vote after being enhanced by PGM. Comparing the model Llama-2-70B and Llama-2-70B+PGM, we found the result becomes worse when equipped with PGM. According to the generated results, we discovered that LLama-2-70B failed to generate a normal clue, given the history and the PGM analysis simultaneously. It tends to copy words from the input while forgetting the moderator&apos;s initial command.


Let&apos;s look into the details about each LLM&apos;s PGM analysis for all the LLMs: In this game, &quot;Player 1&quot; is the true chameleon. However, &quot;Player 2&quot; and &quot;Player 3&quot; played by LLama-2-70B+PGM all gave the analysis, &quot;Player 1 is less suspicious&quot;. In the second column, &quot;Player 2&quot; played by GPT-3.5-turbo+PGM, also makes a similar wrong analysis, directly leading to even voting at the end of the game. For GPT-4, we found it can give the correct PGM analysis, where both &quot;Player 2&quot; and &quot;Player 3&quot; think &quot;Player 1&quot; is more suspicious. Besides, &quot;Player 2&quot; and &quot;Player 3&quot; also give very reasonable analyses about what other players may be thinking now.


FIGURE


FIGURE


Deception


Another advanced cognitive ability of LLMs extends to their proficiency in strategic deception within a multi-agent framework. In FIGURE, we delve into the dynamics of LLM performance when assuming an undercover role against GPT-4. In this scenario, LLMs are expected to blend in with regular civilians and even give misleading clues to conceal their actual roles. In this example, LLama-2-70B, LLama-2-70B+PGM, GPT-3.5-turbo, GPT-3.5-turbo+PGM lost the game, GPT-4 ended with even voting, and GPT-4+PGM won the game.
According to their clues, we found models without PGM didn&apos;t tend to deceive others, and their clues describe their own words. Within these models, GPT-4 is more cautious when giving clues, while GPT-3.5 and Llam-2-70B often give very straightforward clues, like &quot;It can be done at a salon or barbershop&quot; and &quot;It can be washed with shampoo&quot; to describe &quot;hair cut&quot;.


Contrastingly, models augmented with PGM adopted a more sophisticated strategy, utilizing PGM analyses to decide their roles, identify potential undercovers, and outline strategic moves for subsequent rounds. This strategic advantage allowed these models to give fake clues to disguise themselves. For instance, as highlighted in FIGURE, &quot;Player 2&quot; portrayed by GPT-3.5-turbo+PGM and GPT-4+PGM introduced deceptive clues, such as &quot;It can be made from human and synthetic hair&quot; and &quot;It can be used to cover baldness,&quot; respectively. However, &quot;Player 2&quot; played by GPT-3.5-turbo+PGM, still lost the competition because its first clue looked like copying from &quot;Player 1&quot;. &quot;Player 2&quot; played by GPT-4+PGM, won the competition because of its consistently good clues in two rounds.
Comparing the results of the three LLMs with PGM, we can conclude that a more capable LLM can benefit more from the PGM structure.


Conclusion


Our research presents a multi-agent benchmarking framework tailored for evaluating LLMs in multi-agent environments. This framework&apos;s incorporation of diverse games and game theory scenarios has enabled a quantitative assessment of LLMs in dimensions of judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.
The integration of PGM into the current LLMs marks a significant enhancement, enriching the LLMs&apos; capabilities in multi-agent environments. The quantitative analysis of seven different multi-agent systems powered by various LLMs, including GPT-4 series, GPT-3.5-turbo, PaLM 2, Claude 2, Cohere, and Llama-70B, has revealed their capability disparity. Notably, GPT-4 still emerged as the most capable, outperforming others by a threefold margin. Moreover, our findings confirm that the PGM enhancement amplifies the inherent abilities of these models by 50%. This indicates not only the effectiveness of our benchmarking framework but also the potential of PGM as a tool for advancing LLM capabilities.