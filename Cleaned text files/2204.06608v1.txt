Modularity benefits reinforcement learning agents with competing homeostatic drives


Introduction


Humans (and other animals) must satisfy a large set of distinct and
possibly conflicting objectives. For example, we must find food, water,
shelter, socialize, maintain our temperature, reproduce, etc..
Artificial agents that need to function autonomously in natural
environments may face similar problems (e.g., balancing the need to
accomplish a specified goal with the need to recharge, etc.). Finding a
way to balance disparate needs is thus an important challenge for
intelligent agents, and can often be a source of psychological conflict
in humans.


In standard reinforcement learning (RL), monolithic agents act in order
to maximize a single future discounted reward
[*REF*]. The standard way to generalize this to
problems with multiple objectives is scalarization. For example, in
homeostatically-regulated reinforcement learning (HRRL), an agent is
modelled as having separable homeostatic drives, and is rewarded based
on its ability to maintain all its \&quot;homeostats\&quot; at their set points.
This is done by combining deviations from all set-points into a
single reward which, when maximized, minimizes homeostatic deviations
overall [*REF*].


This approach faces several challenges typical to RL. First, to avoid
settling on a sub-optimal policy, an agent must trade-off exploitation
of knowledge about its primary objective with some form of exogenous
exploration (typically by acting randomly or according to an
exploration-specific bonus). Second, sample inefficiency follows from
the &quot;curse of dimensionality&quot;: as environmental complexity increases, an
agent must learn how exponentially more states relate to its objective.
Third, RL agents tend to over-fit their environment, performing poorly
out-of-domain (i.e. they are not robust to distribution shifts). In the
broader context of balancing multiple objectives, reward scalarization
might be undesirable if the relative importance of different objectives
is unknown or variable [*REF*]. Finally, is not clear that
the brain itself uses a common currency to navigate such trade-offs
[*REF*].


What is the alternative? Given that objectives may conflict with each
other due to environmental constraints, and that agents only have one
body with which to act, conflict must be resolved at some point between
affordance and action. The monolithic solution resolves conflict at the
level of reward (i.e. close to affordance). We suggest resolution could
occur later; a set of modules with separate reward functions could
submit action values to a decision process that selects a final action
[*REF*; *REF*]. This approach has the potential to
address the three aforementioned challenges. Exploration might emerge
naturally as a property of the system rather than having to be imposed
or regulated as a separate factor, as specialist modules are &quot;dragged
along\&quot; by other modules when those have the &quot;upper hand\&quot; on action.
Modules might also have smaller sub-sets of relevant features to learn
about, improving sample efficiency, and be less sensitive to
distribution shifts in irrelevant features, improving robustness.


Here, we report simulations that provide evidence for benefits of such a
modular approach with respect to exploration, sample efficiency, and
robustness using deep RL in the context of homeostatic objectives. We
construct a simple but flexible environment of homeostatic tasks and
construct a deep RL implementation of the HRRL reward function. We then
use this framework to quantify differences between monolithic and
modular deep Q-agents, finding that modular agents seem to explore well
on their own, achieve homeostasis faster, and better maintain it after
distribution shift. Together, these results highlight the potential
learning benefits of modular RL, while at the same time offering a
framework through which psychological conflict and resolution might be
better understood.


Methods


Environment


To study conflicting needs, we constructed a toy grid-world environment
containing multiple different resources. Specifically, each location
*MATH* in the environment contained a vector of resources of length *MATH* 
(i.e., there were *MATH* overlaid resource maps). The spatial distribution
of each individual resource was specified by a normalized 2D Gaussian
with mean *MATH* and co-variance matrix *MATH* (see also
Figure [1]).


The agent received as perceptual input a 3x3 egocentric slice of the *MATH* 
resource maps (i.e. it could see all the resource levels at each
position in its local vicinity). In addition to the resource landscape,
the agent also perceived a vector *MATH* 
consisting of *MATH* internal variables with each representing the agent&apos;s
homeostatic need with respect to the resource. We refer to these
variables as \&quot;internal stats\&quot; or just \&quot;stats\&quot; (such as osmostat,
glucostat, etc.) which we assume are independent (*MATH* is only affected
by acquisition of resource *MATH*) and have some desired set-point *MATH* 
(see Figure [2]). Set-points were fixed at *MATH* and did
not change over the course of training.


The agent could move in each of four cardinal directions and, with each
step, the individual stats, *MATH*, increased by the amount of resource
*MATH* at the agent&apos;s next location. Additionally, each internal stat
decayed at a constant rate to represent the natural depletion of
internal resources over time (note: resources in the environment
themselves did not deplete). Thus, if the agent discovered a location
with a high level of resource for a single depleted stat, staying at
that location would optimize that stat toward its set-point, however
others would progressively deplete. Agents were initialized in the
center of the grid, with internal stats below their set-points, and were
trained for *MATH* steps for a single episode (i.e. agents had to
learn in real time as internal stats depleted). For all experiments, the
internal stats started at the same level and shared the same set-points.
Environmental parameters are summarized in Table [1].


FIGURE


Models


Monolithic agent


We created a monolithic agent based on the deep Q network (DQN)
[*REF*]. The agent&apos;s perceptual input was a concatenation of
all local resource levels along with all internal stat levels at each
time step (we used neural networks as function approximators since stats
were continuous variables). It&apos;s output was 4 action logits subsequently
used for *MATH* -greedy action selection. We used the HRRL reward
function [*REF*] which defined reward at each
time-step *MATH* as drive reduction, where drive *MATH* was a convex
function of set-point deviations; see equation.


*MATH*.


Modular agent


We created a modular agent based on greatest-mass Q-learning (GmQ)
[*REF*], which consisted of a separate DQN for each of the 4
resources/stats. Here, each module had the same input as the monolithic
model (i.e. the full egocentric view and all 4 stat levels), but
received a separate reward *MATH* derived from only a single stat.
The reward function for the *MATH* th module was therefore defined as in
equation, where drive *MATH* depended on the *MATH* th resource only.


*MATH*.


To select a single action from the suggestions of the multiple modules,
we used a simple additive heuristic. We first summed Q-values for each
action across modules, and then performed standard *MATH* -greedy
action selection on the result. More specifically, if *MATH* was the
Q-value of action *MATH* suggested by module *MATH*, greedy actions were
selected as *MATH*.


Common features


Schematics for both models are shown in Figure [2]. All Q-networks
were multi-layered perceptrons (MLP) with rectified linear
nonlinearities trained using a standard temporal difference loss
function with experience replay and target networks [*REF*].
The Adam optimizer was used to perform one gradient update on each step
in the environment. For both models, *MATH* was annealed linearly
from its initial to final value at the beginning of training at a rate
that was experimentally manipulated as described below. Hyperparameters
are summarized in Table [1].


TABLE


Results


Optimizing monolithic DQN for homeostasis


We first characterized whether a standard DQN could reliably perform the
task of homeostasis in our environment. Figure [4] summarizes the
mean internal stat levels of 10 models averaged over all 4 stats and
over the final 1k steps of training for different desired set-points.
The model reliably achieved each set-point by the end of training,
indicated by points tracking the identity line. The slight
under-shooting (i.e. points slightly below the diagonal line) may
reflect a bias from stats being initialized far below their set-points.


We then fixed the set-points *MATH* for all stats, and optimized
the performance of the DQN baseline by performing a search over
performance-relevant hyper-parameters, such as the discount factor
*MATH*. To quantify performance, we calculated the average homeostatic
deviation per step *MATH* after the exploration annealing phase (using
*MATH* and *MATH*) as in equation. Lower *MATH* indicates better homeostatic performance.


*MATH*.


Baseline DQN performance over a range of discount factors *MATH* is
shown in Figure [5]. We selected the best performing setting of *MATH* and matched this
and other parameters (see Table [1]) between DQN and GmQ to compare them in the following
two head-to-head experiments.


FIGURE


Modularity provides an exploration benefit


To investigate the impact of modularity on the need for exploration, we
systematically varied the number of steps used to anneal *MATH* in
*MATH* -greedy exploration from its initial to final value. We varied
the *MATH* annealing time for both models from 1 (i.e. minimal
exploration of *MATH* only) to 10k (i.e. annealing from
*MATH* to *MATH* over 10k steps).


Figure [7] shows the results of varying the amount of exploration annealing for both models.
While DQN gains incremental performance benefits from increasing periods
of initial exploration, GmQ displays a striking indifference to the
exploration period; with only 1 step of annealing, it performs as well
or better than the best DQN models. In other words, DQN requires careful
tuning of an appropriate exploration annealing period, but GmQ achieves
good performance with effectively no exogenously specified exploration.


Modularity provides robustness in the face of perturbation


Finally, we tested how robust each model was to a perturbation
out-of-domain that occurred halfway through training, i.e. at time-step
15k. At that point, a single internal stat variable (i.e. *MATH*) was
clamped to a value of 20 (a value previously unseen by the network), and
did not change (thus contributing no drive reduction and therefore 0
reward). We tested how well homeostasis was maintained for remaining
stats after this perturbation.


Figure [8] shows the time-course of the four stats from the beginning of training, through a
perturbation at time-step 15k, using *MATH* *MATH* -annealing steps.
First, it can be seen that GmQ achieves stable homeostasis first,
whereas DQN over-shoots set-points initially and takes longer to
stabilize. Second, when stat 4 is clamped, DQN displays a significant
disturbance to homeostasis of remaining stats, without clear recovery,
whereas GmQ is robust in the face of the perturbation.


FIGURE


Discussion


We have shown that in a grid-world task with competing homeostatic
drives, a simple modular agent based on greatest-mass Q-learning (GmQ)
requires less hand-coded exploration, learns faster, and is more robust
to environmental perturbations compared to a traditional monolithic deep
Q-network (DQN). Our findings in the context of competing drives
complement work showing mixture of expert systems display improved
sample efficiency and generalization [*REF*]. We also
believe the exploration benefits we observed are novel. The problem of
exploration in RL is fundamental, and existing solutions make use of
noise, explicit exploratory drives/bonuses, and/or other forms of
auto-annealing [*REF*; *REF*]. We
suggest an additional class of strategies, namely, exploration as an
added benefit of having multiple independent drives, since exploitation
from the perspective of one module is exploration from the perspective
of another. We hypothesize that the ability of modules to suggest
conflicting actions may provide modular agents with an implicit source
of exploration.


Future Work


Our toy environment has highlighted some initial benefits of modularity
(exploration, sample efficiency and robustness), and we predict that
these advantages will be amplified in more complex environments, or as
the number of drives/objectives increases, due to the curse of
dimensionality (more states to explore, learn about, or perturb). We aim
to test our agents in rich 3D environments with homeostatic objectives.
Next, modular drives immediately pose the problem of coordination. While
we simply summed Q-values, more complex arbitrators (such as an
additional RL agent that dynamically re-weights individual drives) might
better exploit the benefits of modularity in the context of multiple
objectives. Finally, humans experience psychological conflict, with
various resolution mechanisms long described by psychodynamic theories
[*REF*]. Modular RL, with its implicit conflicts and
resolutions, could, for the first time, offer a formal,
computationally-explicit, and normative explanatory framework that could
undergird and/or replace elements of psychodynamic theory for
understanding the mechanisms responsible for conflict and resolution in
the human brain.