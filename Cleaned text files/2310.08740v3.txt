A Zero-Shot Language Agent for Computer Control with Structured Reflection


Introduction 


Prior works have shown promises in using large language models (LLMs) for action generation, e.g. [SayCan] [*REF*], [ReAct] [*REF*], [ToolFormer] [*REF*], and [SwiftSage] [*REF*] over a variety of live environments (e.g.
[MiniWoB++] [*REF*; *REF*], [AlfWorld] [*REF*], and [AlphaCode] [*REF*]. A shared approach is to use LLMs to follow expert traces, comprehend environment changes, plan future actions, and execute an action by composing API calls; all in the form of text. Some works have shown that iteratively attempting a task with several rounds of self-reflection can substantially improve task completion, e.g., [Reflexion] [*REF*], [Self-Refine] [*REF*]. During the process, LLMs are prompted to update a prior execution plan according to feedback from the environment. Such updates become part of the prompt for the action generator in the next round.


Recently, [MiniWoB++] has been used as a testbed for LLM&apos;s capacity at modularized computer tasks. To learn a task, a common approach is the use of extensive trace examples of the task for direct supervision (e.g., CC-Net [*REF* -v162-humphreys22a], WebGUM [*REF*]), self-supervision [*REF*], or few/many-shot prompting (e.g., [RCI] [*REF*], [Synapse] [*REF*]). They have achieved more than *MATH* task completion rate on dozens of computer tasks, seemingly to have solved the computer control problem.


However, the requirement of expert traces for learning to perform a task limits the agent&apos;s ability on new tasks. Without using carefully selected traces as guidance, can an agent autonomously learn and improve its control on a computer? To address this question, we propose a zero-shot agent. We build our agent on top of PaLM2 [*REF*], a recent LLM, and our agent employs a unified instruction prompt set across different tasks, without extensive tailoring for individual tasks.


In addition, recent works, e.g., [RCI] [*REF*], [AdaPlanner] [*REF*], and [Synapse] [*REF*], utilize a screen representation that may include much more information than what is presented to a user on the screen. For instance, Fig. [6] shows an example of elements that are not shown on the screen yet present in the HTML that is fed to the LLM. Using such additional information arbitrarily reduces the difficulty for the agent to perform the task. Yet in general use cases, such information might not be readily available, and relying on such information can potentially hamper the applicability of the agent. We manually examined *MATH* relatively challenging tasks on [MiniWoB++] that are supposed to span multiple screens, and found *MATH* of them contained such information -- multi-screen information in a single observation -- in their HTML.


FIGURE


Our contributions are as follows: Firstly, we employ a compact screen representation that assumes much less information than what is used by previous works, thus resulting in a more general and realistic test environment. Secondly, we propose a simple yet efficient action planner that can accurately plan out executable actions on a state in one pass.
With recent capacity of LLM, we show that such a &quot;naive&quot; strategy can solve almost all the easy tasks on the [MiniWoB++] benchmark. For more challenging tasks, we take inspiration from Reflexion [*REF*] and propose a structured thought management strategy to facilitate reflection, allowing the agent to effectively learn and improve from exploration failures. With a few rounds of attempts, our agent achieves comparable performance with prior few/many-shot state-of-the-art. To the best of our knowledge, our agent is the first zero-shot design for computer control tasks.


Background 


LLMs have become an emerging tool for planning and executing necessary steps to fulfill a top-level goal. These models have exhibit high capacity to follow in-context traces to solve complex tasks.


Planning &amp; Reflection


[ReAct] [*REF*] used intermediate thoughts to guide long-chain of action planning in a live environment. Beyond one trial of planning, [Reflexion] [*REF*] and [Self-Refine] [*REF*] recently found the ability of LLM to self-criticize and self-improve can iteratively learn and solve a given task via multiple trials. Nevertheless, recent planning works require extensive and customized trace prompts for the LLM agent to learn accurate planning. [SwiftSage] [*REF*] reduces the extensive planning calls to LLM by training a small and fast planning module to facilite long-chain of actions. In a similar motivation, our zero-shot agent is based on an efficient staged planner.
We list a detailed comparison in Tab. .


Language/Vision Agent for Computer Control


[MiniWoB++] [*REF*] has several dozens of fundamental computer tasks hosted as live environments. Recent advances on this benchmark have benefited from extensive human annotation to facilitate behavior cloning and reinforcement learning, such as [CC-Net] [*REF* -v162-humphreys22a] and [Pix2Act] [*REF*]. Beside these models that rely on multimodal or vision input, another track is using LLMs as an off-the-shelf agent, and use prompted inference for action generation, such as [RCI] [*REF*], [AdaPlanner] [*REF*], and [Synapse] [*REF*]. We highlight our technical differences in Tab.


Environment Interface


The role of a language agent is to comprehend screen representations (Sec. [3.1] &amp; [3.2]), execute actions on the environment (Sec. [3.3]), and react to environment feedback (Sec. [3.4]).


Treatment of Screens 


The definition of a screen observation varies by modality in recent works. The screen observation for a vision-based model [e.g.
*REF* -v162-humphreys22a; *REF*] can be constrained by various viewport configurations. For a language model, specifically those taking in HTML code, a screen observation is often the page source of a screen, e.g., ones instantiated from a [MiniWoB++] task template with a given random seed. When HTML elements are not constrained by viewport configuration, the need for scrolling action is gone. However, as discussed in Sec. [1], we do not immediately use the expanded HTML if the expansion requires a UI action: we only expand the HTML representation when the agent actually takes the action. The design relaxes the assumption about the environment, and forces the agent to learn to behave rationally when it is given limited information.


Compact Screen Representation 


Raw HTML code tends to be verbose, which poses a practical challenge for LLMs that often have an inherent limit on the input or context length.
*REF* designed a technique for structured prompting example selection to extract more informative trace examples, as a way to reduce the input content to LLMs. MindAct [*REF*] ranked and consolidated element of interests to represent web page snippet.
Alternatively, we take inspiration from  *REF* to heuristically simplify the HTML code of each screen, retaining key attributes for each leaf element, i.e., id, class, text, placeholder, value, and position on a 3x3 screen grid. Such simplification has shown to give compelling results on UI understanding tasks. An example is shown in Fig. [7].


FIGURE


Action Space 


For each action, our agent model outputs commands in a format that is specific to the action type. Specifically, we use three types of actions as shown in Tab. . Recent LLMs such as PaLM-2 [*REF*] are good at following such an output format. More prompt details are given in Appx. [10]. To deterministically ground an action command on [MiniWoB++] environment, we follow the approach in prior language-only works [e.g. *REF*] to access HTML elements by XPATH pattern. When grounding click actions on the actual environment, we use the compact element id (Sec. [3.2]) which is aligned to the actual HTML element in the raw HTML. For the type action, we decompose it into a click action followed by a series of keyboard presses for the text.


Environment Feedback 


[MiniWoB++] differs from [TextWorld]-like environment [*REF*] in that state change from taking an action is not naturally phrased out. Instead, an agent will need to observe the entire screen change implicitly, making it less convenient for the agent to adopt Chain-of-Thought [*REF* -cot] reasoning. We broadly categorize trial ending conditions into: 1) correct, 2) cycle, 3) no change, 4) incomplete, 5) exception, and 6) failed. Condition 2) and 3) compare the HTML code of the current screen to those of prior screens. Condition 5) happens when a grounding action can not be successfully executed. During multiple trials, each ending condition is associated with a prompt for the reflection inference.


FIGURE


Planning Strategies


In this section, we summarize the planning strategies used in recent LLM-based planning works to motivate our staged planning. With a given goal, an agent model is to issue actions based on prior interaction with the environment. For brevity, let us denote the interaction as a sequence of state and action pairs *MATH*.


Iterative Planning


In iterative planning [e.g. *REF*; *REF*], the agent model loops over generating an &quot;atomic&quot; action *MATH* , grounding it on the environment for execution, and then observing for the next state *MATH*. That is, *MATH* *MATH* *MATH* where *MATH* denotes the planning model. Such planning is a common choice for environments that require observation by exploration. With responsive environments [e.g.
*REF*; *REF*], such an agent can benefit from a long history of interaction that can be easily connected to Chain-of-Thought reasoning [*REF* -cot].


Plan-Then-Adapt 


Recently, *REF* observed that an initial, and yet rough, plan could help iterative planning. Formally, *MATH* *MATH* *MATH* where *MATH* adapts those initial steps to be executable actions (*MATH* &apos;s) on the environment. In practice, both *MATH* and *MATH* use the same LLM.


Conceptually, this is similar to zero-shot planner [*REF*] and ReAct [*REF*] that intermediate thought can help plan long chain of actions. The downside though is that the agent needs to follow carefully crafted few-shot trace examples to make a good initial plan.
[AdaPlanner] [*REF*] addresses this issue with an adaptive plan refiner that monitors the state-action compatibility and issues refined actions when there is a mismatch. This line of planning often needs to deal with hallucinations in the initial plan since, after all, the agent model only observes *MATH* but needs to plan for unobserved states.


Staged Plan-And-Follow 


Prior works essentially add on extra planning components to the agent.
Instead, we adopt a simpler planning strategy. For computer environment, agents often sees a state where multiple actions can be executed on, without the need to observe nuanced state changes, e.g., multiple selection on a list. In such cases, iterative planning on a single screen can be less efficient, and often, unnecessary. On the other hand, plan-then-adapt generates actions beyond executable ones that could confuse the LLM agent during the adaptation step. Furthermore, both approaches require the agent to iteratively generate the next action, requiring an LLM to have a large context window.


To address these issues, we take a step in the middle by maximally planning actions that are visible on the current state all at once.
After the planning, the agent only needs strictly follow the generated plan, and such process repeats over multiple screens. Formally, *MATH* *MATH* *MATH* where each stage is essentially generating *MATH* executable actions for state *MATH*. Note that, we also omit former states in Eq.  to make inference more efficient. In practice, we found that a simple statement, in natural language, that summarizes what functionality is achieved by the action, is a good representation for the state-action pair *MATH*.


Implementation details.


We once again rely on the underlying LLM to follow execution instructions as in Eq. . Prompt details are in Appx. [11]-[14]. In rare occasions, agent model predicts fewer steps (e.g., forgetting to submit) or more steps (e.g., hallucinating non-executable actions) than needed. For the former, we loop over the planning in Eq. until no further plan is generated. For the latter, we halt the execution of the current plan and resort to self-reflection for correction.


Structured Self-Reflection 


In practice, a high-level human instruction or goal can be ambiguous, and an environment can be partially hidden. Therefore, agents are prone to making mistakes; this is even true for a human user when executing a task [*REF*]. Once a negative signal is given, such as no change or failed (Sec. [3.4]), we ask the agent to reflect on its past action trajectory, suggest an improved version, and then retry the task. An example of successful reflection from our agent is shown in Fig.


In recent works [*REF*; *REF*], reflection is conducted at the end of each trial by accumulating a text entry. The entry is essentially a natural language statement about what should have been done instead. At trial *MATH* , *MATH**MATH* *MATH* where *MATH* consists of a list of *MATH* pairs, each denotes to update the wrong action *MATH* to *MATH*. In the followup trial *MATH* , accumulated entries in *MATH* are prefixed to the prompt for the agent planner *MATH*.


The amount of entries maintained in the reflection memory is limited by multiple factors. For one, it increases the input length of LLM.
Moreover, it requires the LLM agent to handle the thought structures through multiple trials. In practice, *REF* limited the memory size *MATH*.


Structured Thought Management 


In a zero-shot setting, reflection puts a heavy burden on LLM&apos;s capacity to follow a complex instruction set (in addition to those in Sec. [3.3] &amp; [4.3]). After all, in this case, LLM has no expert traces to follow, thus need to learn from trials. With increasing the number of trials, reflection memory essentially forms a combinatorial problem for the agent to solve. For a time step *MATH* , there can be multiple failed actions in historical trials, thus should be avoided. For a trial *MATH* , if the agent identified a critical mistake at time *MATH* , reflections on later time steps can be considered outdated.


In our preliminary study, we found it is important to devise a way to help an LLM agent maintain this memory. Otherwise, when a reflection entry *MATH* is given, even a state-of-the-art LLM can still (1) repeat the same mistake *MATH* , (2) fail to follow the reflected plan to even reach *MATH* , and (3) bounce between wrong actions it collected in prior trials.


To make reflection run more reliably and efficiently, we propose a structured self-reflection in Algo. . When a suggested action *MATH* is given by the reflection agent, we enforce our agent to plan exactly *MATH* at step *MATH*. Moreover, to avoid looping over two failed actions at the same step, we use a disabled action set *MATH* to memorize them and jointly disable these actions in the environment.
Finally, we clear reflection entries for future steps if an early entry is updated. With this management, our agent is no longer bounded by LLM&apos;s input limit, and has a memory size *MATH*.


ALGORITHM


Note that in line 6, we use the staged planner in Eq. [4.3] which does not depend on the iteratively updated *MATH* , thus is different from recent works (Eq.).


Interplay with Staged Planning.


Suppose the staged planner predicted *MATH* but execution had failed, and the reflection step identified *MATH* as the earliest mistake, thus suggested *MATH*. In the next trial, we will repeat the executions from *MATH* to *MATH* , and intercept the agent planner at step *MATH* to enforce the execution of *MATH*.
For steps after *MATH* , we bring our planner to the next stage. In the worst case where an agent fails at every step, our staged planning essentially falls back to the plan-then-adapt (Sec. [4.2]), except having no initial plan.


Constraining Action Space 


For an updated action *MATH* at reflection trial *MATH* , we enforce it to be taken at the associated time step if and only if *MATH* is not an failed attempt before trial *MATH*. It can be tricky to prompt LLM to follow such simple combinatorial constraint in text, especially as a mixture of positive and negative signal surrounded by other instructions. Therefore, we found it is crucial to explicitly disable those previously failed actions in the corresponding screen representation. This, however, does not mean removing the corresponding element from the HTML pseudo code. We instead only remove the id attribute, and still allow the element information to be presented to LLM. We only do so for click-type actions.


For non-click actions, the disable set *MATH* cannot be easily enforced on the environment and the LLM agent. We can indeed prompt the underlying LLM saying certain special keys are invalid or certain texts not to input. However, we did not observe a positive impact from doing so in our preliminary experiment. Thus, we fallback to only deterministically generate the *MATH* at time step *MATH*. We locate the time step by prompting the reflection agent to output in format: &quot;For action index= *MATH* , you should *MATH* &quot;. This differs from prior work [*REF*] which uses reflection memory as sticky entries in LLM prompts across all time steps.


Experiments


We start with categorizing tasks by their planning complexities to have an isolated testbed. Then we experiment with our staged planning in Sec. [6.3]-[6.4]. Finally, we examine if our zero-shot agent can learn from mistakes in Sec. [6.5]. Our prompts are in Appx. [10]-[14]. Complete results are in Appx. [15].


Setup


We focus on 43 [MiniWoB++] tasks that are suitable for evaluating language-based models. This differs from prior work since we excluded those 1) require visual signal to solve (e.g., count-shape and grid-coordinate); and 2) expose insufficient language API to operate (e.g., enter-date and enter-time); The motivation for this filtering is simple: even if some filtered tasks can be solved by an LLM agent, it does not generalize. Furthermore, we do not include terminal as the synthetic console supports a very limited set of commands while the LLM, in our preliminary experiment, tends to use more sophisticated ones.


We separate these *MATH* tasks into three categories: 1) *MATH* -screen- *MATH* -step, 2) *MATH* -screen- *MATH* -step, and 3) *MATH* -screen- *MATH* -step.
If the task involves state update (e.g. expanding dropdown list or openning hidden tab), the task is n-screen. If the task can be solved by just one action, it is 1-step; otherwise n-step. The task distribution is reported in Tab. .


For each task, we evaluate with *MATH* different random seeds, starting from seed= *MATH* , similar to Pix2Act [*REF*]. Performances are reported as the correct completion rate over multiple runs. For validation and prompt design, we use seed *MATH*. For the LLM agent, we use the FLAN-PaLM2 L [*REF*] with temperature *MATH* across all evaluations for better reproducibility.


Model Comparison


For each task category, we compare with prior best models that rely on language as input signal, including supervised models, i.e., [WebN-T5] [*REF*] and [CC-Net] [*REF* -v162-humphreys22a], and agents based on prompted inference, i.e., [RCI] [*REF*] with GPT-3.5 and [AdaPlanner] [*REF*]. For few-shot models, we focus on comparing with agents that have reflection capacity. Non-reflective agents, such as [Synapse] [*REF*], have techniques that are orthogonal to our work, and thus can potentially be combined with ours.
Furthermore, we notice each work often used slightly different set of tasks. For a fair comparison, we will also report performances on the shared set of tasks.


Single Step Tasks 


We compare our zero-shot agent on the easiest category (1-screen-1-step) tasks against recent state-of-the-art. As shown in Fig. [8], our agent achieves *MATH* accuracy on correctly finishing 9 tasks, even without reflection. One exception is the ambiguous click-widget which, without in-context trace prompt, can be easily failed. For instance, the task could ask agent to click on text widget, however, input text and text area are not deemed correct. With 3 rounds of reflection trials, our agent achieved *MATH* completion rate on it. Overall, we have *MATH* with *MATH* trial, and *MATH* with *MATH* trials. In comparison, with few-shot trace examples, RCI [*REF*] achieved *MATH* with 1 round of reflection (at the plan level).


FIGURE


Iterative Planning v.s. Staged Planning 


We compare these two approaches using *MATH* -screen- *MATH* -step tasks. We hope these experiments can answer that, with a given state, whether one should query the agent for actions one at a time or once for all. We compare the prior state-of-the-art works with our staged planning in Tab. , showing that one can simply plan out all executable actions on a screen and &quot;blindly&quot; execute them. Doing so can substantially reduce LLM queries and still achieve high completion rate.


We report detailed completion rate on all *MATH* 1-screen-n-step tasks in Fig. [9]. Our agent achieved *MATH* completion in *MATH* trial, and *MATH* in *MATH* trials.


FIGURE


Reflective Planning on Challenging Tasks 


Here, we move on to more challenging (n-screen-n-step) tasks to show the impact of our efficient reflection. Task-wise completion rate is reported in Fig. [10]. Firstly, we observe without examplar traces, zero-shot agent tends to fail on the first trial. This happens often in tasks that requires exploring across multiple screens, e.g., click-menu-2, click-tab-2-hard, and search-engine. After a few rounds of exploration, our agent achieved substantially better completion rate by avoiding previous negative signals recorded in the memory. Our agent continues to improve even with *MATH* , suggesting more efficient reflection than prior work e.g., RCI only capable of one round of reflection at plan level.


Again, we compare with prior best models in Tab. . The few-shot models exploited inconsistent screens (as discussed in Sec. [1]), thus our work is in an unfair disadvantage against them. Despite such disadvantage, our agent achieved performance comparable to them.
Importantly, our agent does not require in-context trace examples for few-shot, sometimes many-shot, prompting, and no customized and detailed environment feedback. Finally, we note that the gap on complex tasks between supervised model and unsupervised ones is still large.


FIGURE


Analysis &amp; Discussions


Ablation on Reflection Strategies


Here, we compare our structured reflection against the &quot;original&quot; reflection mechanism. We should note that reflection is a general scope that has different formations [e.g.
*REF*; *REF*] and was introduced on environments (e.g., [AlfWorld]) that are significantly different from [MiniWoB++]. Moreover, it was often used along with iterative planning strategy, which is not directly compatible with our staged planning.


FIGURE


Therefore, we use an adapted version for comparison: an agent that uses structurally managed timestep while structurally thought management is turned off. This setting is the comparison between Both v.s. w/o structured mem in Fig. [11] where we select *MATH* challenging tasks and run *MATH* seeds for each setting. Clearly, our structured reflection is a beneficial add-on.


Ablation on Action Constraint


A useful technique we proposed in Sec. [5.2] is to delete the id field in the HTML pseudo code to heuristically discourage LLM agent from issuing the corresponding action. This is essentially minimal change to the input. In Fig. [11], we ablate on this small change by comparing Both v.s. w/o action constraint and show that it is beneficial to apply the action constraint.


Statistical Significance over Trials


We evaluate statistical significance across various trials on the n-screen-n-step tasks. For each task, we consider all *MATH* example predictions. This gives us *MATH* samples for each comparison.
Using t-test [*REF*], the results are indeed significant (*MATH*) as shown in Tab. For task-wise significance, see Appx. [16].


Planning Call Reduction


In Tab., we highlight the efficiency boost by suing our staged planning formulation. We illustrate the result on 1-screen-n-step tasks that require relatively long action traces (*MATH* actions) on a single screen, and compare the number of planning calls for completed traces as well as failed traces.


Compact Screen &amp; Input Length Limit


Representing user interface as HTML puts a high demand on LLM&apos;s context capacity. One instance is the social-media-all task that can span more than a dozen candidates, each with multiple options. As a result, flattening the complete set of state-action pairs can easily run over the input limit for the reflection agent, since it needs to observe the entire trace. On this task, we noticed that nuanced actions does not substantially change the screen. Therefore, we always stick to the first screen when constructing prompt for the reflection agent. A more autonomous method can be state filtering in [Synapse] [*REF*].


Lining up the separation of what HTML elements to expose to LLM is important for evaluation. As we have seen that many of the MiniWoB++ tasks are already easy for today&apos;s LLM. Exposing more unseen elements risks hiding the actual challenge in the navigation tasks. For instance, exposing unseen elements basically simplifies n-screen-n-step tasks into 1-screen-n-step ones. However, our experiment shows that n-screen-n-step ones are actually much harder to deal with.


Capacity of Staged Planning 


To better understand the planning capacity and limit of our staged planning, we experiment with 1-screen-n-step tasks that have extensive number of candidates. Namely, we use click-checkboxes and social-media as probe tasks, and report in Tab. Both tasks are multi-choice, but differ in their candidate structure complexity.


For the click-checkboxes task, we separate examples by their number of actions required. The screen representation for this task is relatively simple as each checkbox corresponds to a line of text. This differs from the social-media task where each candidate has multiple actionable, sometimes ambiguous, items, thus putting a stronger requirement to LLM for disambiguation. We observe a pattern in Tab. that with flat and less ambiguous screen, LLM has high capacity to accurately plan out multiple steps in one inference call. In such case, one could just execute all planned steps without needing repetitive planning calls. But with complex screen constructions, the capacity of one-pass planning is reduced by a large margin. Prior work (i.e. RCI) constrained the number of candidates in the social-media task to *MATH*. We observe that relaxing such constraint introduces significant difficulty for planning. Therefore, multiple trials of reflection can help the agent in these complex scenarios.


Conclusions


We proposed the first zero-shot agent for computer control tasks. Our agent design generalizes the workflow for easy and complex tasks via efficient planning and structured thought management. We evaluated our agent on the [MiniWoB++] benchmark, showing that our agent, with often one pass of planning query, outperforms the best iterative planning agent as well as supervised state-of-the-art on simple tasks.
For complex tasks, we show that our agent design performs on par with the best LLM-based model via more efficient planning and reflection, without requiring manually crafted trace prompts and ad-hoc environment feedback.