The Archimedean trap: Why traditional reinforcement learning will probably not yield AGI


Introduction


Whenever we measure anything using a particular number system, the
corresponding measurements will be constrained by the structure of that
number system. If the number system has a different structure than the
things we are measuring with it, then our measurements will suffer
accordingly, just as if we were trying to force square pegs into round
holes.


For example, the natural numbers make lousy candidates for measuring
lengths in a physics laboratory. Lengths in the lab have properties such
as, for example, the fact that for any two distinct lengths, there is an
intermediate length strictly between them. The natural numbers lack this
property. Imagine the poor physicist, brought up in a world of only
natural numbers, scratching his or her head upon encountering a rod with
length strictly between two rods of length *MATH* and *MATH*.


It is tempting to think of the real numbers *MATH* --- i.e., the
unique complete ordered field --- as a generic number system with whatever
structure suits our needs. But the real numbers do have their own
specific structure. That structure is flexible enough to accomodate many
needs, but we shouldn&apos;t just take that for granted. One particular
property constraining the real numbers is the following.


LEMMA


Rather than directly prove Lemma, we will prove a generalized
result which, we will argue, is more adaptable to other structures.


LEMMA


PROOF


Lemma follows from Lemma by letting *MATH*.


The above property is automatically inherited by subsystems of the
reals, such as the rational numbers *MATH*, the natural numbers
*MATH*, the integers *MATH*, or the algebraic numbers. All
inherit the Generalized Archimedean Property in obvious ways.


Lemma allows us to adapt the notion
of Archimedeanness to other things than real numbers, even to things for
which there is no notion of arithmetic at all (Lemma would not adapt to such
things). All we need is a notion of &quot;significantly less than&quot;. For any
set of things, some of which are &quot;significantly less than&quot; others, we
can ask whether or not the property in Lemma holds. We will make this formal
in Section [2].


EXAMPLE


Note that the above example does not require us to have any notion of
multiplying fuzziness by a natural number *MATH* (as we would need to have
if we wanted to adapt Lemma). This illustrates the enhanced
adaptability of Lemma.


The structure of this paper is as follows.
- In Section [2] we formally adapt Lemma to obtain a notion of
Archimedeanness for non-numerical structures, and demonstrate that
non-Archimedean such structures cannot accurately be measured using
the real numbers.
- In Section [3] we argue that traditional
reinforcement learning probably will not lead to AGI because its
rewards are overly constrained.
- In Section [4] we discuss non-traditional
variations of reinforcement learning that avoid the problem of
overly constrained rewards.
- In Section [5] we summarize and make concluding remarks.


Generalized Archimedean Structures 


The real numbers possess the Archimedean property, but other structures
may or may not. To make this more precise, we introduce the following
formalism, adapting from Lemma.


DEFINITION


For any real number *MATH*, a prototypical example of an Archimedean
significantly-ordered structure is the real numbers with *MATH* defined
such that *MATH* if and only if *MATH*.


DEFINITION


The following proposition formalizes the dilemma we illustrated in Example.


PROPOSITION


PROOF


Proposition tells us that we cannot accurately measure
non-Archimedean structures using real numbers. Any attempt to do so
will necessarily be misleading, because ordering relationships among the
non-Archimedean structures will fail to be reflected by the real-number
measurements given to them. We will inevitably end up like the puzzled
physicist brought up in a world of only natural numbers, confronted by a
rod of length *MATH*.


REMARK


EXAMPLES


Reinforcement learning 


In reinforcement learning (RL), an agent interacts with an environment,
taking actions from a fixed set of possible actions. With every action
the agent takes, the environment responds with a new observation and
with a reward. In traditional RL, these rewards are real numbers (many
authors further constrain them to be rational numbers).


By restricting rewards to be real (or rational) numbers, we
unconsciously constrain RL to only be applicable toward tasks of an
inherently Archimedean nature. For example, *REF* point out
that in tasks related to cancer treatment [*REF*], &quot;the
death of a patient should be avoided at any cost. However, an infinitely
negative reward breaks classic reinforcement learning algorithms and
arbitrary, finite values have to be selected.&quot; This problem could be
avoided if instead of real numbers, rewards were drawn from a suitable
non-Archimedean number system containing negative infinities. Doing so
would be a departure from traditional RL.


EXAMPLE


Or, to re-use the cancer example, assume there are certain bad
procedures the robotic surgeon could take, each one significantly worse
than the previous, but all still significantly better than killing the
patient. There is no way to assign real-valued rewards to these actions,
and to killing the patient, in such a way that each bad action gets
punished significantly harsher than the previous, but still
significantly more forgivingly than the punishment for killing the
patient.


The reader might object by challenging the non-Archimedeanness of music
and of medical procedures. But we only used those to make the examples
more intuitive. If the reader insists, we can resort to mathematical
tasks.


EXAMPLE


The reader might object to Example on the grounds that judging the
proof-theoretical strength of a theory is inherently non-computable
anyway. The example could be modified so that instead of typing up
mathematical theories, the agent has to type up mathematical subtheories
in (say) the language of Peano arithmetic, accompanied by consistency
proofs in (say) ZFC. It can be shown that the proof-theoretical strength
of mathematical theories is still non-Archimedean, even when restricted
to subtheories of arithmetic whose consistency can be proven in ZFC.


The reader might object that the above theories-with-proofs example is
contrived. But an AGI with human or better intelligence should have no
problem at least comprehending and attempting such a task (regardless of
whether or not the AGI is able to perform well at it). When we prove
that the Halting Problem is unsolvable, we do so by considering
contrived programs that we could write if the Halting Problem were
solvable. The contrivedness of those programs does not invalidate the
proof of the unsolvability of the Halting Problem. Again, when we prove
that C++ templates are Turing complete [*REF*], we do so by
considering extremely bizarre C++ templates that would never arise
naturally in a software studio. This does not invalidate the proof that
C++ templates are Turing complete.


Finally, the reader might object that approximating infinite rewards
with arbitrary large finite rewards is good enough. Who cares (the
argument might go) whether pushing a button gives the agent infinite
pleasure or only a million units of pleasure? Either way (the argument
goes) the agent is going to learn to prefer that button over a button
that gives only *MATH* units of pleasure. The following example shows that
this logic breaks down in non-Markov environments.


EXAMPLE


Our critic could respond to Example by making the approximation dynamic, say,
making the *MATH* th press of the blue button grant *MATH* 
reward, but at this point, the critic is clearly just hard-coding the
correct actions into the reward function, something which is only
possible in Example because the environment is simple enough that
we can completely understand it ourselves. For the kinds of non-trivial
environments where AGI would actually be useful, such carefully
engineered reward approximations would quickly become intractible.


Reinforcement Learning is useful for many practical tasks, but at least
in its traditional flavor, it is too constrained (by its arbitrary
choice of number system for its rewards) to apply to certain
non-Archimedean tasks, which, however contrived they are, could
certainly be attempted by an AGI. Traditional reinforcement learning
will probably not lead to AGI.


Non-traditional reinforcement learning 


We have argued that traditional RL will probably not lead to AGI,
because an AGI is capable of attempting non-Archimedean tasks whose
rewards are too rich to express using real numbers. There are at least
two potential ways to change RL so as to make it applicable to such
tasks and, thus, at least potentially capable of leading to AGI. Of
course, there is no guarantee that removing the roadblock in this paper
will cause RL to lead to AGI. There might be other roadblocks besides
the inadequate reward number system.


Preference-based reinforcement learning


A lot of exciting research has been done on non-traditional variations
of RL where, instead of giving the agent numerical rewards for taking
actions, one instead informs the agent about the relative preference of
various actions or action-sequences. See [*REF*] for a
survey. This nicely side-steps the problems from this paper.


Reinforcement learning with other number systems


The most obvious way to modify RL to avoid the problems presented in
this paper is to change which number system is used. As far as this
author is aware, the choice to use real (or rational) numbers for
rewards was not made based on any fundamental criteria. The real
(or rational) numbers are currently a useful pragmatic choice because
they are easy to compute with using 21st century software and 21st
century school curricula, but that&apos;s hardly relevant in the field of
genuine AGI. One might say the real numbers were a good choice because
they are familiar, but even that is arguable: in general, students are
usually not taught what the real numbers actually are, unless they
major in pure mathematics at the university level. Anyway, the
familiarity argument is totally irrelevant in the field of AGI.


Various non-Archimedean number systems exist. Number systems can be
discrete or continuous; the nature of reinforcement learning clearly
suggests a continuous number system. We will consider three continuous
number systems: formal Laurent series; hyperreal numbers; and surreal
numbers.


Formal Laurent series


David [*REF*] described the following real-number-extending
number system (which he called the &quot;superreals&quot;, but that vocabulary
does not seem to have caught on).


DEFINITION


For brevity, we will write *MATH* for the formal Laurent series
*MATH* where *MATH* and *MATH* for
all *MATH*. Likewise, we may write, for example,
*MATH* for the formal Laurent series
*MATH* where *MATH*, *MATH*, and
*MATH* for all *MATH*, and similarly for other finite sums
of powers of *MATH*.


We can consider the real numbers *MATH* to be embedded in the
formal Laurent series by way of the embedding *MATH*.
Having done so, the intuition is that, for example, *MATH* is
what we might call a &quot;first-order infinitesimal number&quot;, smaller than
every positive real; *MATH* is what we might call a &quot;second-order
infinitesimal number&quot;, smaller than every positive first-order
infinitesimal number; and so on. Likewise, *MATH* is what we
might call a &quot;first-order infinite number&quot;, bigger than every real;
*MATH* is what we might call a &quot;second-order infinite number&quot;,
bigger than every first-order infinite number; and so on. Thus, the
formal Laurent series should suffice to address the specific problem
described by [*REF*] in which an infinite negative reward is
required when the RL agent kills the cancer patient.


There are natural ways to define arithmetic on formal Laurent series,
but we will avoid those details here. The advantage of the formal
Laurent series number system is that it is relatively concrete, compared
to the more abstract hyperreal or surreal numbers discussed below.


EXAMPLE


There is a natural way to consider formal Laurent series as a
significantly-ordered structure, generalizing the notion of
&quot;significantly greater than&quot; from Lemma.


DEFINITION


LEMMA


PROOF


Unfortunately, although the formal Laurent series contain infinities and
infinitesimals, in a sense we will make formal, they still do not
contain &quot;enough&quot; infinities and infinitesimals to accomodate the fully
general environments that an AGI should be able to navigate. To make
this formal, we introduce a weaker notion of Archimedeanness.


DEFINITION


To be semi-Archimedean is a weaker condition than to be Archimedean, but
it is still a condition, and one which there is evidently no reason to
assume should constrain reinforcement learning rewards in general. For
example, just as it is unclear whether musical beauty is Archimedean,
likewise, it is unclear whether musical beauty is even semi-Archimedean.
If musical beauty is not semi-Archimedean, then Example does not merely suggest the inadequacy of
real number rewards, but of rewards from any semi-Archimedean number
system. And if musical beauty is too informal, we can still fall back to
mathematical theories (Example), for the strength of mathematical theories
can be shown not to be semi-Archimedean.


In the following theorem, we show that the formal Laurent series are
semi-Archimedean. By the above paragraph, this suggests that even if we
extended reinforcement learning to permit formal Laurent series rewards,
the resulting framework would still probably not lead to AGI.


THEOREM


PROOF


Hyperreal numbers


The field of mathematics where the calculus is formalized with infinite
and infinitesimal quantities is called nonstandard analysis
[*REF*]. The numbers most commonly associated with this field are
the so-called hyperreal numbers.


The hyperreal numbers can be introduced axiomatically or by means of a
semi-constructive method which depends on usage of a certain black box,
a device known as a free ultrafilter. Logicians have proven that free
ultrafilters exist but that, unfortunately, it is impossible to
concretely exhibit one. This severely limits (if not completely ruins)
the practical usefulness of reinforcement learning with hyperreal
rewards.


Nevertheless, the hyperreals might be useful for proving abstract
structural properties about AGI. It can be shown that the
hyperreals are not even semi-Archimedean (much less Archimedean). Thus,
for the purpose of proving abstract theorems about RL agents with fully
generalized rewards, the hyperreals would be more appropriate than the
formal Laurent series.


Surreal numbers


All of the well-known non-Archimedean extensions of *MATH* 
(including formal Laurent series and hyperreals) are subsystems of the
so-called surreal numbers [*REF*; *REF*; *REF*]. The
surreal numbers were initially discovered during John Conway&apos;s attempts
to study two-player combinatorial games like Go and Chess, so it would
not be surprising if they turn out to be important in the eventual
development of AGI.


Unlike the hyperreals, the construction of the surreal numbers does not
depend on any non-constructive black boxes such as free ultrafilters.
They are constructed as the union of a hierarchy *MATH* of
subsystems where *MATH* ranges over the ordinal numbers. Assuming that
agents with AGI are implemented using computers with no additional power
beyond the Church-Turing Thesis, then for the purposes of AGI, it would
be appropriate to restrict our attention to some computable subset of
the surreal numbers, which would presumably be the union of some
hierarchy *MATH* where *MATH* ranges over the computable ordinal
numbers. For any particular level *MATH* in this hierarchy, we could
consider the sub-universe *MATH* of surreal-reward RL environments
with rewards restricted to *MATH*.


Assuming AGI agents are Turing computable, no individual AGI can
possibly comprehend codes for all computable ordinals, because the set
of codes of computable ordinals is badly non-computably-enumerable. This
is profound, because it seems to suggest that any particular AGI can
only comprehend RL environments in *MATH* if that AGI can comprehend
*MATH*. In other words, for any particular RL environment *MATH* with
computable surreal number rewards, there must be some minimal computable
ordinal *MATH* such that *MATH* has rewards from *MATH*; if an AGI is
not intelligent enough to comprehend *MATH*, then it seems like there
should be no way for the AGI to comprehend *MATH* either. We would
submit this state of affairs as evidence in favor of our thesis
[*REF*] that a machine&apos;s intelligence ought to be
measured in terms of the computable ordinals which the machine
comprehends.


The above paragraph points at a possible joint path toward AGI
incorporating both machine learning and symbolic logic --- toward &quot;the
integration of Symbolic and Statistical AI&quot; [*REF*] --- perhaps a
much-needed reconciliation of these two approaches.


Alternate number systems: tentative verdict


For many simple environments not too far outside of traditional RL,
formal Laurent series could probably serve as a fairly practical number
system. But formal Laurent series have limitations suggesting that RL
with formal Laurent series rewards will probably not be enough to reach
AGI, for the same reason that RL with real number rewards will probably
not be enough.


Because of their dependence on free ultrafilters, the hyperreal numbers
will probably never be of practical use as RL rewards, but it could
conceivably be possible to use them to prove abstract structural results
about AGI from a bird&apos;s-eye view.


The surreal numbers (or a computable subset thereof) seem like the most
promising candidate for RL rewards that could plausibly lead to AGI. We
would certainly hesitate to call them &quot;practical&quot;, though. To work with
any but the most trivial of surreal numbers, one would need to implement
sophisticated symbolic-logical machinery, and that&apos;s just to get one&apos;s
foot in the door. This does, however, offer a ray of hope in that doing
deep learning techniques with surreal numbers could be a way to combine
both symbolic logic and statistical methods into a joint approach.


Conclusion 


In traditional reinforcement learning, utility-maximizing agents
interact with environments, receiving real (or rational) number rewards
in response to actions, and using those rewards to update their
behavior. We have argued that the decision to limit rewards to real
numbers is inappropriate in the context of AGI, because the real numbers
have the Archimedean property, which makes it impossible to use them to
accurately portray the value of actions when a task involves inherently
non-Archimedean rewards. Thus, we argue, traditional RL probably will
not lead to AGI, because a genuine AGI should have no trouble
comprehending and at least attempting tasks that inherently involve
non-Archimedean rewards. We suggested two possible ways to modify
traditional reinforcement learning to fix this bug: switch to
preference-based reinforcement learning, or else generalize
reinforcement learning to allow rewards from a non-Archimedean number
system.