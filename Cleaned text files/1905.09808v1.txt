MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies


Introduction


Reinforcement learning is commonly applied to solve tasks from scratch.
While tabula rasa learning can achieve state-of-the-art performance on
a broad range of tasks [*REF*; *REF*; *REF*; *REF*; *REF*],
this approach can incur significant drawbacks in terms of sample
efficiency and limits the complexity of skills that an agent can
acquire. The ability to transfer and re-purpose skills learned from
prior experiences to new domains is a hallmark of intelligent agents.
Transferable skills can enable agents to solve tasks that would
otherwise be prohibitively challenging to learn from scratch, by
leveraging prior experiences to provide structured exploration and more
effective representations. However, learning versatile and reusable
skills that can be applied to a diverse set of tasks remains a
challenging problem, particularly when controlling systems with large
numbers of degrees-of-freedom.


In this work, we propose multiplicative compositional policies (MCP), a
method for learning reusable motor primitives that can be composed to
produce a continuous spectrum of skills. Once learned, the primitives
can be transferred to new tasks and combined to yield different
behaviors as necessary in the target domain. Standard hierarchical
models [*REF*; *REF*] often activate only a single
primitive at each timestep, which can limit the diversity of behaviors
that can be produced by the agent. MCP composes primitives through a
multiplicative model that enables multiple primitives to be activated at
a given timestep, thereby providing the agent a more flexible range of
skills. Our method can therefore be viewed as providing a means of
composing skills in space, while standard hierarchical models compose
skills in time by temporally sequencing the set of available skills. MCP
can also be interpreted as a variant of latent space models, where the
latent encoding specifies a particular composition of a discrete set of
primitives.


The primary contribution of our work is a method for learning and
composing transferable skills using multiplicative compositional
policies. By pre-training the primitives to imitate a corpus of
different motion clips, our method learns a set of primitives that can
be composed to produce a flexible range of behaviors. While conceptually
simple, MCP is able to solve a suite of challenging mobile manipulation
tasks with complex simulated characters, significantly outperforming
prior methods as task complexity grows. Our analysis shows that the
primitives discover specializations that are reminiscent of previous
manually-designed control structures, and produce coherent exploration
strategies that are vital for high-dimensional long-horizon tasks. In
our experiments, MCP substantially outperforms prior methods for skill
transfer, with our method being the only approach that learns a
successful policy on the most challenging task in our benchmark.


Preliminaries


We consider a multi-task RL framework for transfer learning, consisting
of a set of pre-training tasks and transfer tasks. An agent is trained
from scratch on the pre-training tasks, but it may then apply any skills
learned during pre-training to the subsequent transfer tasks. The
objective then is to leverage the pre-training tasks to acquire a set of
reusable skills that enables the agent to be more effective at the later
transfer tasks. Each task is represented by a state space
*MATH*, an action space *MATH*, a dynamics
model *MATH*, a goal space *MATH*, a goals distribution *MATH*, and a reward
function *MATH*. The goal specifies task specific
features, such as a motion clip to imitate, or the target location an
object should be placed. All tasks share a common state space, action
space, and dynamics model. However, the goal space, goal distribution,
and reward function may differ between pre-training and transfer tasks.
For each task, the agent&apos;s objective is to learn an optimal policy
*MATH* that maximizes its expected return
*MATH* over the distribution of goals from the task, where
*MATH* denotes the distribution over trajectories *MATH* induced by the policy
*MATH* for a given goal *MATH*. *MATH* represents the time horizon, and
*MATH* is the discount factor. Successful transfer cannot
be expected for unrelated tasks. Therefore, we consider the setting
where the pre-training tasks encourage the agent to learn relevant
skills for the subsequent transfer tasks, but may not necessarily cover
the full range of skills required to be effective at the transfer tasks.


Hierarchical policies are a common model for reusing and composing
previously learned skills. One approach for constructing a hierarchical
policy is by using a mixture-of-experts model [*REF*; *REF*; *REF*; *REF*; *REF*],
where the composite policy&apos;s action distribution *MATH* is
represented by a weighted sum of distributions from a set of primitives
*MATH* (i.e. low-level policies). A gating function determines
the weights *MATH* that specify the probability of activating each
primitive for a given *MATH* and *MATH*, *MATH*.
Here, *MATH* denotes the number of primitives. We will refer to this method
of composing primitives as an additive model. To sample from the
composite policy, a primitive *MATH* is first selected according to
*MATH*, then an action is sampled from the primitive&apos;s distribution.
Therefore, a limitation of the additive model is that only one primitive
can be active at a particular timestep. While complex behaviors can be
produced by sequencing the various primitives in time, the action taken
at each timestep remains restricted to the behavior prescribed by a
single primitive. Selecting from a discrete set of primitive skills can
be effective for simple systems with a small number of actuated
degrees-of-freedom, where an agent is only required to perform a small
number of subtasks at the same time. But as the complexity of the system
grows, an agent might need to perform more and more subtasks
simultaneously. For example, a person can walk, speak, and carry an
object all at the same time. Furthermore, these subtasks can be combined
in any number of ways to produce a staggering array of diverse
behaviors. This combinatorial explosion can be prohibitively challenging
to model with policies that activate only one primitive at a time.


Multiplicative Compositional Policies


In this work, we propose multiplicative compositional policies (MCP), a
method for composing primitives that addresses this combinatorial
explosion by explicitly factoring the agent&apos;s behavior -- not with
respect to time, but with respect to the action space. Our model enables
the agent to activate multiple primitives simultaneously, with each
primitive specializing in different behaviors that can be composed to
produce a continuous spectrum of skills. Our probabilistic formulation
accomplishes this by treating each primitive as a distribution over
actions, and the composite policy is obtained by a multiplicative
composition of these distributions, *MATH*.
Unlike an additive model, which activates only a single primitive per
timestep, the multiplicative model allows multiple primitives to be
activated simultaneously. The gating function specifies the weights
*MATH* that determine the influence of each primitive on the
composite action distribution, with a larger weight corresponding to a
larger influence. The weights need not be normalized, but in the
following experiments, the weights will be bounded
*MATH*. *MATH* is the partition function that ensures
the composite distribution is normalized. While the additive model
directly samples actions from the selected primitive&apos;s distribution, the
multiplicative model first combines the primitives, and then samples
actions from the resulting distribution.


Gaussian Primitives


Gaussian policies are a staple for continuous control tasks, and
modeling multiplicative primitives using Gaussian policies provides a
particularly convenient form for the composite policy. Each primitive
*MATH* will be modeled by a Gaussian with mean *MATH* and diagonal covariance
matrix *MATH*, where *MATH* denotes the variance of the *MATH* th action
parameter from primitive *MATH*, and *MATH* represents the
dimensionality of the action space. A multiplicative composition of
Gaussian primitives yields yet another Gaussian policy
*MATH*. Since the primitives model each action parameter with an independent Gaussian, the
action parameters of the composite policy *MATH* will also assume the
form of independent Gaussians with component-wise mean *MATH* and
variance *MATH*, *MATH*. Note that while *MATH* determines a
primitive&apos;s overall influence on the composite distribution, each
primitive can also independently adjust its influence per action
parameter through *MATH*. Once the parameters of the
composite distribution have been determined, *MATH* can be treated as a
regular Gaussian policy, and trained end-to-end using standard automatic
differentiation tools.


Pre-Training and Transfer


FIGURE


The primitives are learned through a set of pre-training tasks. The same
set of primitives is responsible for solving all pre-training tasks,
which results in a collection of primitives that captures the range of
behaviors needed for the set of tasks.
AlgorithmÂ illustrates the overall training process.
*MATH* denotes the objective for the pre-training tasks
for a given set of primitives *MATH* and gating function *MATH*, and
*MATH* denotes the objective for the transfer
tasks. When transferring primitives to a new task, the parameters of the
primitives are kept fixed, while a new policy is trained to specify
weights for composing the primitives. Therefore, the primitives can be
viewed as a set of nonlinear basis functions that defines a new action
space for use in subsequent tasks. During pre-training, in order to
force the primitives to specialize in distinct skills, we use an
asymmetric model, where only the gating function *MATH* observes
the goal *MATH*, and the primitives have access only to the state *MATH*,
*MATH*. This asymmetric model prevents the degeneracy of a single primitive
becoming responsible for all goals, and instead encourages the
primitives to learn distinct skills that can then be composed by the
gating function as needed for a given goal. Furthermore, since the
primitives depend only on the state, they can be conveniently
transferred to new tasks that share similar state spaces but may have
different goal spaces. When transferring the primitives to new tasks,
the parameters of the primitives *MATH* are kept fixed to prevent
catastrophic forgetting, and a new gating function *MATH* is
trained to specify the weights *MATH* for composing the primitives.


Related Work


Learning reusable representations that are transferable across multiple
tasks has a long history in machine learning
[*REF*; *REF*; *REF*]. Finetuning remains a
popular transfer learning technique when using neural network, where a
model is first trained on a source domain, and then the learned features
are reused in a target domain by finetuning via backpropagation
[*REF*; *REF*]. One of the drawbacks of this procedure is
catastrophic forgetting, as backpropagation is prone to destroying
previously learned features before the model is able to utilize them in
the target domain [*REF*; *REF*; *REF*].


Hierarchical Policies:


A popular method for combining and reusing skills is by constructing
hierarchical policies, where a collection of low-level controllers,
which we will refer to as primitives, are integrated together with the
aid of a gating function that selects a suitable primitive for a given
scenario [*REF*; *REF*; *REF*]. A common
approach for building hierarchical policies is to first train a
collection of primitives through a set of pre-training tasks, which
encourages each primitive to specialize in distinct skills
[*REF*; *REF*; *REF*; *REF*; *REF*].
Once trained, the primitives can be integrated into a hierarchical
policy and transferred to new tasks. End-to-end methods have also been
proposed for training hierarchical policies
[*REF*; *REF*; *REF*]. However, since standard
hierarchical policies only activate one primitive at a time, it is not
as amenable for composition or interpolation of multiple primitives in
order to produce new skills.


FIGURE


Latent Space Models:


Our work falls under a class of methods that we will refer to broadly as
latent space models. These methods specify controls through a latent
representation that is then mapped to the controls (i.e. actions) of the
underlying system [*REF*]. Similar to
hierarachical models, a latent representation can first be learned using
a set of pre-training tasks, before transferring to downstream tasks
[*REF*; *REF*]. But unlike a standard hierarchical
model, which activates a single primitive at a time, continuous latent
variables can be used to enable more flexible interpolation of skills in
the latent space. Various diversity-promoting pre-training techniques
have been proposed for encouraging the latent space to model
semantically distinct behaviors [*REF*; *REF*; *REF*].
Demonstrations can also be incorporated during pre-training to acquire
more complex skills [*REF*]. In this work, we present a
method for modeling latent skill representations as a composition of
multiplicative primitives. We show that the additional structure
introduced by the primitives enables our agents to tackle complex
continuous control tasks, achieving competitive performance when
compared to previous models, and significantly outperforming prior
methods as task complexity grows.


Experiments


We evaluate the effectiveness of our method on controlling complex
simulated characters, with large numbers of degrees-of-freedom (DoFs),
to perform challenging long-horizon tasks. The tasks vary from simple
locomotion tasks to difficult mobile manipulation tasks. The characters
include a simple 14 DoF ant, a 23 DoF biped, a more complex 34 DoF
humanoid, and a 55 DoF T-Rex. Illustrations of the characters are shown
in FigureÂ [1], and examples of transfer tasks are shown in
FigureÂ [2]. Our experiments aim to study MCP&apos;s performance on complex temporally
extended tasks, and examine the behaviors learned by the primitives. We
also evaluate our method comparatively to determine the value of
multiplicative primitives as compared to more standard additive mixture
models, as well as to prior methods based on options and latent space
embeddings. Behaviors learned by the policies are best seen in the (supplementary video 2).


Experimental Setup


Pre-Training Tasks:


The pre-training tasks in our experiments consist of motion imitation
tasks, where the objective is for the character to mimic a corpus of
different reference motions. Each reference motion specifies a sequence
of target states *MATH* that the
character should track at each timestep. We use a motion imitation
approach following *REF*. But instead of training separate
policies for each motion, a single policy, composed of multiple
primitives, is trained to imitate a variety of motion clips. To imitate
multiple motions, the goal *MATH* 
provides the policy with target states for the next two timesteps. A
reference motion is selected randomly at the start of each episode. To
encourage the primitives to learn to transition between different
skills, the reference motion is also switched randomly to another motion
within each episode. The corpus of motion clips is comprised of
different walking and turning motions.


FIGURE


Transfer Tasks:


We evaluate our method on a set of challenging continuous control tasks,
involving locomotion and object manipulation using the various
characters. Detailed descriptions of each task are available in the
supplementary material.


Heading: First we consider a simple heading task, where the objective
is for the character to move along a target heading direction
*MATH*. The goal *MATH* 
encodes the heading as a unit vector along the horizontal plane. The
target heading varies randomly over the course of an episode.


Carry: To evaluate our method&apos;s performance on long horizon tasks, we
consider a mobile manipulation task, where the objective is to move a
box from a source location to a target location. The task can be
decomposed into a sequence of subtasks, where the character must first
pickup the box from the source location, before carrying it to the
target location, and placing it on the table. The source and target are
placed randomly each episode. Depending on the initial configuration,
the task may require thousands of timesteps to complete. The goal
*MATH* encodes the target&apos;s position *MATH* and orientation *MATH* 
represented as a quaternion, the source&apos;s position *MATH* and
orientation *MATH*, and box&apos;s position *MATH*, orientation *MATH*,
linear velocity *MATH*, and angular velocity *MATH*.


Dribble: This task poses a challenging combination of locomotion and
object manipulation, where the objective is to move a soccer ball to a
target location. Since the policy does not have direct control over the
ball, it must rely on complex contact dynamics in order to manipulate
the movement of the ball while also maintaining balance. The location of
the ball and target are randomly initialized each episode. The goal
*MATH* encodes the target location
*MATH*, and ball&apos;s position *MATH*, orientation *MATH*, linear velocity
*MATH*, and angular velocity *MATH*.


FIGURE


TABLE 


Model Representation:


All experiments use a similar network architecture for the policy, as
illustrated in FigureÂ [3]. Each policy is composed of *MATH* primitives. The
gating function and primitives are modeled by separate networks that
output *MATH*, *MATH*, and *MATH*, which are then
composed according to EquationÂ to produce the composite policy. The state
describes the configuration of the character&apos;s body, with features
consisting of the relative positions of each link with respect to the
root, their rotations represented by quaternions, and their linear and
angular velocities. Actions from the policy specify target rotations for
PD controllers positioned at each joint. Target rotations for 3D
spherical joints are parameterized using exponential maps. The policies
operate at 30Hz and are trained using proximal policy optimization (PPO) [*REF*].


Comparisons


We compare MCP to a number of prior methods, including a baseline model
trained from scratch for each transfer task, and a model first
pre-trained to imitate a reference motion before being finetuned on the
transfer tasks. To evaluate the effects of being able to activate and
compose multiple primitives simultaneously, we compare MCP to models
that activate only one primitive at a time, including a hierarchical
model that sequences a set of pre-trained skills
[*REF*; *REF*], an option-critic model
[*REF*], and a mixture-of-experts model (MOE) analogous to
Equation. Finally, we also include comparisons to
a continuous latent space model with an architecture similar to
*REF* and *REF*. All models, except for the
scratch model, are pre-trained with motion imitation
[*REF*]. Detailed descriptions of each method can be found
in the supplementary material. FigureÂ [4] illustrates learning curves for the
various methods on the transfer tasks and
Table summarizes their performance. Each environment
is denoted by \&quot;Task: Character\&quot;. Performance is recorded as the
average normalized return across approximately 100 episodes, with 0
being the minimum possible return per episode and 1 being the maximum.
Three models initialized with different random seeds are trained for
each environment and method.


FIGURE


FIGURE


Our experiments show that MCP performs well across the suite of tasks.
For simple tasks such as heading, all models show similar performance.
But as task complexity increases, MCP exhibits significant improvements
to learning speed and asymptotic performance. Training from scratch is
effective for the simple heading task, but is unable to solve the more
challenging carry and dribble tasks. Finetuning proved to be a strong
baseline, but struggles with the more complex morphologies. With higher
dimensional action spaces, independent action noise is less likely to
produce useful behaviors. Models that activate only a single primitive
at a time, such as the hierarchical model, option-critic model, and MOE
model, tend to converge to lower asymptotic performance due to their
limited expressivity. MOE is analogous to MCP where only a single
primitive is active at a time. Despite using a similar number of
primitives as MCP, being able to activate only one primitive per
timestep limits the variety of behaviors that can be produced by MOE.
This suggests that the flexibility of MCP to compose multiple primitives
is vital for more sophisticated tasks. The latent space model shows
strong performance on most tasks. But when applied to characters with
more complex morphologies, such as the humanoid and T-Rex, MCP
consistently outperforms the latent space model, with MCP being the only
model that solves the dribbling task with the T-Rex.


We hypothesize that the performance difference between MCP and the
latent space model may be due to the process through which a latent code
*MATH* is mapped to an action for the underlying system. With the latent
space model, the pre-trained policy *MATH* acts as a decoder
that maps *MATH* to a distribution over actions. We have observed that this
decoder has a tendency to overfit to the pre-training behaviors, and can
therefore limit the variety of behaviors that can be deployed on the
transfer tasks. In the case of MCP, if *MATH* is the same across
all primitives, then we can roughly view *MATH* as specifying a convex
combination of the primitive means *MATH*. Therefore, *MATH* 
forms a convex hull in the original action space, and the transfer
policy *MATH* can select any action within this set. As
such, MCP may provide the transfer policy with a more flexible range of
skills than the latent space model. To test this hypothesis, we evaluate
the different models on transferring to out-of-distribution tasks using
a simple setup. The environment is a variant of the standard Gym Ant
environment [*REF*], where the agent&apos;s objective is to run along a
target direction *MATH*. During pre-training, the policies are
trained with directions *MATH*. During
transfer, the directions are sampled from a holdout set
*MATH*. FigureÂ [5] illustrates the learning curves on the
transfer task, along with the trajectories produced by the models when
commanded to follow different target directions from the pre-training
and transfer tasks. Indeed we see that the latent space model is prone
to overfitting to the directions from pre-training, and struggles to
adapt to the holdout directions. MCP provides the transfer policy
sufficient flexibility to adapt quickly to the transfer tasks. The
scratch and finetune models also perform well on the transfer tasks,
since they operate directly on the underlying action space.


FIGURE


FIGURE


Exploration Behaviors


To analyze the exploration behaviors produced by the primitives, we
visualize the trajectories obtained by random combinations of the
primitives, where the weights are sampled from a Gaussian and held fixed
over the course of a trajectory. Figure
[6] illustrates the trajectories of the
humanoid&apos;s root produced by various models. Similar to MCP, the
trajectories from the latent space model are also produced by sampling
*MATH* from a Gaussian. The trajectories from the hierarchical model are
generated by randomly sequencing the set of primitives. The model
trained from scratch simply applies Gaussian noise to the actions, which
leads to a fall after only few timesteps. Models that are pre-trained
with motion imitation produce more structured behaviors that travel in
different directions.


Primitive Specializations


To analyze the specializations of the primitives, we record the weight
of each primitive over the course of a walk cycle.
FigureÂ [7] illustrates the weights during
pre-training, when the humanoid is trained to imitate walking motions.
The activations of the primitives show a strong correlation to the phase
of a walk cycle, with primitive 1 becoming most active during left
stance and becoming less active during right stance, while primitive 2
exhibits precisely the opposite behavior. The primitives appear to have
developed a decomposition of a walking gait that is commonly
incorporated into the design of locomotion controllers [*REF*].
Furthermore, these specializations consistently appear across multiple
training runs. Next, we visualize the actions proposed by each
primitive. FigureÂ [7] shows a PCA embedding of the mean action
from each primitive. The actions from each primitive form distinct
clusters, which suggests that the primitives are indeed specializing in
different behaviors.


Conclusion


We presented multiplicative compositional policies (MCP), a method for
learning and composing skills using multiplicative primitives. Despite
its simplicity, our method is able to learn sophisticated behaviors that
can be transferred to solve challenging continuous control tasks with
complex simulated agents. Once trained, the primitives form a new action
space that enables more structured exploration and provides the agent
with the flexibility to combine the primitives in novel ways in order to
elicit new behaviors for a task. Our experiments show that MCP can be
effective for long horizon tasks and outperforms prior methods as task
complexity grows. While MCP provides a form of spatial abstraction, we
believe that incorporating temporal abstractions is an important
direction. During pre-training, some care is required to select an
expressive corpus of reference motions. In future work, we wish to
investigate methods for recovering sophisticated primitive skills
without this supervision.