Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL): Towards Biological Self-Autonomous Agent


Introduction


Reinforcement learning (RL) has been of particular interest in recent years in the area of Machine Learning (ML) and Artificial Intelligence (AI). Dramatic advances have been made [Mnih et al., 2013; Silver et al., 2016, 2018] particularly due to the progress in Deep Learning (DL) [Krizhevsky et al., 2012]. These advances are also due to the easy applicability of the general RL framework to many fields, such as Economics [Lussange et al., 2020], Psychology [Shteingart and Loewenstein, 2014], Control Theory [Kretchmar, 2000] and Neuroscience [Niv, 2009]. Additionally, the intermingling of Neuroscience and AI has further advanced the applicability of RL to real-world problems [Kriegeskorte and Douglas, 2018; Richards, 2019].
A natural next step to these advancements is designing self-autonomous agents that may mimic behaviour of the real-world biological and psychological agents (e.g. rodents, primates, humans). More specifically, the overarching goal is to develop agents that can express both physiological and psychological needs akin to biological organisms and are capable of learning, acting and adapting in a given environment depending upon their internal states and the external environment. In this context, homeostatic and allostatic regulation principles are relevant. Following these principles, [Man and Damasio] defined a class of robots capable of exhibiting emotions, and equipped with the ability to learn and adapt in unfamiliar environments while simultaneously observing their internal states [Man and Damasio, 2019].


Computational integration of these homeostatic and allostatic principles in the agent can be achieved using RL methodologies.
However, RL alone proves insufficient as it seeks to maximise the rewards based on action. Whereas, in the case of bio-mimetic agents, the requirement is to learn both reward maximization and homeostatic deviation minimization [Staddon, 1983; Toates, 1986]. In this context, the Homeostatic Regulation Theory (HRT) is particularly relevant [Keramati and Gutkin], [2011]. While RL and HRT may seem divergent, [Keramati and Gutkin] linked the two theories by demonstrating that the goal of reward maximization (RL) and homeostatic deviation minimization (HRT) is equivalent when the reward function is based on the internal state of the agent. This compound theory proposed by [Keramati and Gutkin] is now known as the Homeostatically Regulated Reinforcement Learning (HRRL) and lends feasibility to the development of bio-mimetic agents. In fact, HRRL has been found to effectively model primitive behaviours such as resource consumption, and evolved behaviours such as risk aversion, alcohol tolerance, cocaine addiction or anticipatory control [Keramati and Gutkin, 2014; Keramati, 2013]. Thus, RL and Homeostatic Regulation Theory together provide a more robust and practical mechanism to develop self-autonomous bio-mimetic agents.


Despite the feasibility that HRRL lends in self-autonomous agent development, it has some limitations. According to the current HRRL framework, the internal state of the agent is fixed when it is in an inactive state. Moreover, in the current HRRL framework, homeostasis is considered an episodic event rather than a continuous goal. Thus, the traditional HRRL theory does not consider the possibility of homeostatic deviation even when the agent is in an inactive state.
Naturally, such a stance is not compatible with the actual behaviour of the real-world biological agents. In fact, biological agents continuously monitor their internal state and are aware of homeostatic deviation that may result due to the internal processes required for survival and sustenance. Thus, a threat to homeostatic balance is actively present and biological agents continuously pursue homeostatic regulation which involves physiological or behavioural change [Ramsay and Woods, 2014].


A second limitation of the current HRRL framework is the discrete mapping of action and time [Keramati and Gutkin, 2011], i.e. the actions taken by the agent are assumed to be carried out at discrete and regular time steps. Whereas in the real-world actions are generally carried out in a continuous and smooth manner. Moreover, the current HRRL framework is based on a discount factor which does not model the notion of temporality between actions. In our present work, we aim to address the above explained shortcomings of the current HRRL framework. To this end, we advance the HRRL framework to the continuous-time and continuous-space (CTCS) paradigm, thereby formulating the CTCS-HRRL model. The main contributions of our work are:


- Dynamic Self-Regulating Agents: Homeostatic behaviour is embodied within the agent irrespective of its state (inactive or active). This embodiment is based on real-world observations.
Psychological and behavioural attributes are also embodied to emulate the real-world biological agents. These behaviours are sleeping, resting, walking instead of running. Thus, agent&apos;s self-regulation is guided by an active knowledge and awareness of its internal states.
- Continuous time implementation: Unlike the previous learning models that focused on discrete-time learning, we extend these models by introducing the continuous-time learning framework in the HRRL. We also demonstrate the transferability of the theoretical results in discrete model to the continuous model.
- Agent-Environment Interaction and Self-Learning Agent: Limited research currently exists on the role of agent-environment interaction in agent&apos;s decision-making. We embed the agent-environment interaction in our simulation experiments to mimic the decision making of the real-world biological agents. Due to this, the agent learns policies which are more realistic and ecologically valid.


Background and Related Work


In this section, we critically discuss the scientific works which have attempted to incorporate agent&apos;s internal state dynamic into learning and motivation. We subsequently place our work in the context of the discussion.


Negative Feedback Models: These models relate to the control theory and formulate homeostatic deviation as a negative feedback state. This homeostatic deviation indicates the drive of the organism towards a particular resource, and is assumed to guide the behaviour of the organism. Thus, in these models, the behavior of the organism is dependent on its internal state only. Greater the homeostatic deviation, greater the motivational drive to fulfil the need.
Heuristically, the negative feedback models only measure the organism&apos;s discomfort or the negative effect. However, behaviour selection or prioritization is not discussed.


Drive Reduction Theory: A natural extension of the negative feedback models is the Drive Reduction Theory proposed by Hull [Hull, 1943]. According to the theory, the organism selects actions (behaviours) to reduce its drive, or the homeostatic deviation. Although DRT has explained the adaptive behaviour, learning [Staddon, 1983] and motivational systems [Toates, 1986], it fails to explain the conceptual and mathematical basis of action selection. Furthermore, it does not explain the peculiar behaviour of resource consumption in the absence of homeostatic deviation [Wingfield, 2005]. Specifically, the behaviour taken in the anticipation of a future perceived need or perceived homeostatic deviation, such as anticipatory consumption, is not explained by DRT.
This gap is crucial to address because in the real world, such behaviour (anticipatory consumption or response) are common. E.g.
Overconsumption of food, addiction etc. This drawback motivate the formulation of a theory that can provide a more ecologically valid explanation for anticipatory or compulsive behaviours, while rooted in homeostatic regulation. Hullian drives address this drawback.


Hullian Drives: A Hullian drive is a drive that varies between 0 and 1. The 0 denotes total dissatisfaction and 1 denotes total satisfaction. Hullian drive has been used to explain the agent&apos;s behaviour and motivation in reinforcement learning based settings. For example, [Konidaris and Barto] used Hullian drive-based reward model weighted by the time-dependent coefficients to indicate the drive priority. However, external information on drive priority is counter-intuitive and incompatible with the intelligence of real-world biological agents, as they are able to discern these priorities automatically through an internal mechanism. Thus, in our work we do not externally provide the information of drive priority to the agent, instead let the agent learn that on its own and accordingly modify its policies. Secondly, in the work by [Konidaris and Barto], the agent is penalized if its actions do not follow the drive priority. We instead achieve this regulatory effect using a function of time and control that accounts for correlations between different drives, and assist the agent to take decisions accordingly. A third drawback of the work using Hullian drive [Konidaris and Barto, 2006] is the use of SARSA algorithm which is not always robust for small time-steps, a necessity for agent learning in an unknown environment. This limitation is addressed in our study.


Homeostatically Regulated RL (HRRL or HRL): In addition to the Negative Feedback Models, Drive Reduction Theory, Hullian Drive, we also discuss the HRRL. According to HRRL, agent&apos;s drive trigger homeostasis-ensuring actions. The selection of actions is guided by reinforcement learning framework. Thus, in HRRL the rewards and punishments are derived directly from the internal state deviations and the Hullian drivefunction [Keramati and Gutkin], [20]11).
The similarity between Homeostasis achievement and Reinforcement Learning was proved when [Keramati and Gutkin] showed that maximization of the sum of discounted rewards (RL) is equivalent to the minimization of the sum of discounted drives (homeostasis).
Thus, a conceptually robust reinforcement learning framework for drive reduction theory was produced which addressed the gaps in the earlier models. Although theoretical and mathematical results for HRRL have been achieved, the numerical or computer-based simulations that support these theoretical results are lacking. In this work, we advance the HRRL theory by performing numerical simulations using an artificial agent in an unknown environment.


In the next sections we discuss the methods, experimental set up, theoretical and simulation based results.


Methods


According to the general reinforcement learning framework [Sutton and Barto, 2018], an agent, in a certain state, selects actions from a set of available actions in that state.
The choice of the action changes the agent&apos;s current state and confers it a reward (either positive or negative). A series of such actions at each state-time t that maximise the discounted sum of future rewards constitute a policy. Ultimately, the task of the agent is to discover this policy for a given task in a particular environment. In our work, we have an agent in a square environment in which it has to consume the resources as per its need (internal state) and maintain homeostasis. For this task, we use certain notations that we discuss next.


FIGURE 1


Let n be an integer, ζt ∈ R^n^ the state (internal and external) of the agent at time t and *MATH* the trajectory function of the agent&apos;s state. We denote the space of possible actions at time t when the agent is in *MATH* and the space of all actions by A. The policy function determining the agent&apos;s choices is denoted by *MATH*, and the reward received by following this policy at time t by *MATH*.
We denote by Π the set of all admissible policies. Note that in the deterministic case and for a fixed policy, the initial state (ζ0, t0) and the policy function π completely determine the path function ζ and the reward function r. The value function, for an agent in ζt at time t following a policy π, is defined as follows *MATH* where *MATH* is the discount factor that accounts for time preference. The optimization problem for the agent is *MATH* where it is trying to find the policy function that maximizes the value function for each state at each time.


Internal Environment Let x R^n^int be a vector describing agent&apos;s internal state where each feature needs to be regulated, and let *MATH* denote its homeostatic set point. Assume that each feature of x is bounded. This is a constraint from the agent&apos;s embodiment, which will not be valid if a feature is too small or too large, and will either be regulated automatically by the organism or will cause the agent&apos;s death. We assume that the agent knows the differences between the individual internal variables and their respective set points: *MATH*, where the order of the difference is arbitrarily chosen and can be evaluated as RL comparison-rewards [Matignon, 2006].


External Environment We also define the external environment of the agent within its view-field e ∈ R^n^ext, and the entire world *MATH*. At any time t, the agent in ζt can perform an action a ζt,t. The agent is limited in its choice by its environment (for example by the place in which it is) and by its internal state and time (because some actions depend on energy). The action taken, in turn, will have a consequence (a control u R^n^int+next) for its internal state and its environment. We assume that from the agent&apos;s point of view, the dynamics of ζ is described by an equation of the form *MATH* where f, g are functions and S is a stochastic process. f, g and S are unknown to the agent at the beginning of its task: it does not have the information of how its body and the external world react and has no estimate of the behavior of the stochastic process S.


We can distinguish several potentially overlapping causes of a change in the agent&apos;s internal state and environment:


(a) an unconscious automatic autoregulation of the organism, modeled by the function f and its variable ζ, which can account for internal processes of the agent&apos;s body (e.g., animal physiology, robot mechanics, and multi-component interactions). We note that from the perspective of biology, self-regulation is a common physiological process [Polynikis et al., 2009; Pattaranit and van den Berg, 2008]. For e.g. the human kidney uses a mechanism called tubuloglomerular feedback to regulate the glomerular filtration rate in response to changes in sodium concentration [Versypt et al., 2015; Thomson and Blantz, 2008]. Autoregulation of cerebral blood flow has been demonstrated in the presence of Carbon Dioxide [Panerai et al., 1999]. This change in ζ takes place without the agent taking any action of its own. If the agent does not perform any action, its internal state will naturally deviate from its current state;


(b) a control that the agent exercises and that has an impact on its environment and its internal state, modeled by the function f and its variable u. For example, the action of moving, which modifies the environment while requiring an effort, and which thus has an impact on the internal state;


(c) the time that changes and modifies the external environment and the internal state, modeled by the function f and its variable t.
For example, the time of day and the current season will drive the temperature and brightness of the environment in a certain direction. Time can also model old age, by progressively modifying the function f, and thus the way the body reacts over time;


(d) a stochastic control that the agent undergoes, leading to an unexpected change mostly in the environment, modeled by the g function and the stochastic process S. Stochasticity intervenes in everything that the agent cannot control, in particular the behavior of other agents around it or the weather.


The control, by changing the environment, has a direct impact on the actions that can be taken in the future, since these will depend on the new environment. By changing the internal state, it also has a direct impact on the agent&apos;s drive d. Control brings the agent to a more or less comfortable state, depending on whether it is moving towards or away from its homeostatic set point *MATH*, or equivalently whether the drive is decreasing or increasing.


Agent&apos;s Goal The agent&apos;s goal is to minimize its drive for the two resources by finding the optimal policy that allows it to take actions aligned with the achievement of homeostasis.


The deviation function *MATH* for an admissible policy π, which represents × → the integral of the agent&apos;s discounted drive over its remaining lifetime, is given by *MATH* where ζ follows equation [3] (and thus δ depends on π) on *MATH* with the initial condition *MATH* the control function u satisfies *MATH* (we will say that the control function is associated with the policy if it meets this last condition) and the expected *MATH* value is here because of the stochasticity in [3]. Note that the integral is well-defined thanks to the discount factor and the fact that x and *MATH* are bounded. Concretely, the value of the deviation function *MATH* indicates how bad it is for the agent to follow the policy π, starting from the state ζt at time t. The problem of the agent is thus: *MATH* with the same conditions as before. At a given time t, ζt values are continuous, action at values are discrete and the associated control ut is generally small.


Hamilton-Jacobi Bellman Equation *MATH* where J^∗^ is the optimal deviation function, ua is the deterministic control resulting from the action a and d is the drive function, with the conventions that d(ζt, ua) is the drive of the new state of the agent after performing the action a in ζt (entire world = internal state + external environment), and that symbolically d(ζt) = d(δt). The intuition behind this equation is obtained with the optimality principle and by making the analogy with the known discrete Bellman equation [Sutton and Barto, 2018]. Because Q-learning is not robust in the presence of small-time steps [Tallec et al., 2019], we rely on this equation to propose our algorithm.


Experiment


1. Description of the experiment


We consider a closed 2D environment (Figure [1]). The agent is identified by its coordinates in the plane (grey patch in Figure [1]). The environment contains hidden resources necessary for the agent&apos;s survival. For the sake of this experiment, we have two stationary resources, blue and green as shown in Figure [1]. For a biological agent, these resources could act as a reservoir of proteins, a source of carbohydrates, water, or any other element crucial for their survival. The agent&apos;s internal state is determined by these resources in its system.


For the simulation purposes, we used a square environment with two resources (R1 and R2) and one single agent. The agent&apos;s starting internal state for each resource is very minimal, but not so low as to cause muscular fatigue and prevent movement. The homeostatic set points are R1 = 1 and R2 = 2. Thus, the task of the agent is to maintain homeostasis (minimise the deviation function J) over changing internal states. The possible actions for the agent are: walk, run, go to the resource, consume resource, and rest in case of excess fatigue (muscular or sleep-related). At each instant, the agent can move forward by an elementary distance for the action of walking (up, down, right, or left) or by a greater elementary distance for the action of running (only when the agent is near the resource).


In addition to certain actions that an agent can take, we endow the agent with physiological properties (internal state or body dynamics including fatigue and the state of immobilized sleep). The model of the internal state (the body) of the agent includes two types of fatigue, &quot;muscle&quot; fatigue, which depends on how far the agent has moved without resting (e.g., continuous movement), and &quot;sleep&quot; fatigue, if it has not recovered for too long. Splitting fatigue into such two separate terms, allowed us to reflect multiple behavioral and physiological aspects that cause natural agents (animals) to rest.
However, homeostatic state is dependent only on the concentration of resources, and not muscle or sleep fatigue. At any time and in any place, the agent can choose the action of sleeping for a minimal renewable duration. This action will immobilize it for a certain duration.


When the agent has reached a certain threshold of muscular fatigue, it cannot take action of running, and at other threshold, it cannot walk.
These thresholds are pre-decided and incorporated in the code. Such threshold based conditions ensure that the agent is immobile to recover from the muscular fatigue. Similarly beyond a certain threshold of sleep-related fatigue, the only action that becomes possible is sleeping. Thus, our agent mimics the natural biological agent. In this environment, the agent begin with zero knowledge and its goal is to minimise the deviation function (J). The agent explores the environment and eventually learns to base its action exploiting previous actions. Agent has access to ζ only and based on its actions accrues rewards, and updates the deviation function.


We run the simulation for 6000, 8000, 10000, and 14000 iterations to study the agent&apos;s learning behaviour. Since the biological process of homeostasis is continuous and never ending, the program/simulation never really ends. But a saturation stage can be noticed which establishes that the agent has thoroughly learned about resource positions and directly reaches to those reservoirs in times of internal drive or homeostatic deviation. Next we present the learning algorithm for the agent.


Learning algorithm for the agent


Here we present the Algorithm [1] that allows the agent to learn by interacting with its environment. The algorithm is based on the principle of policy improvement, wherein at each step a value function is evaluated, and the policy is updated directly using this value function. The classical reinforcement learning heuristics to improve the quality of learning are deliberately not implemented here, as the goal is not to propose the most efficient algorithm possible.
In contrast, our goal is to present a proof of concept that sufficiently demonstrates the possibility for an agent to learn from zero knowledge by following a natural and plausible approach to action selection and gradually learning from the accumulated experience.


ALGORITHM 1


Update the transition function and the deviation function by performing a gradient descent step on *MATH* We discretize time by ∆t time steps. The discretization in time is necessary to build the algorithm, but the proposal of a continuous theoretical framework is justified by a better modeling, an economy of notations, or the possibility to make adaptive time steps. In the initial state, the agent does not have access to the functioning of its internal state (the body), represented by the function f. Over time, the agent learns to approximate this function through its experiences. We thus have a model-based algorithm, since the transitions between internal states are modeled. On the other hand, the drive function d is known initially, modeling the agent&apos;s interoception.


The agent&apos;s action is either taken randomly with probability ϵ to facilitate exploration, or based on the HJB equation and estimates of J and f. Note that, for a certain policy π, the deviation function J is defined by equation [4].
However, this equation requires the calculation of and integral over the lifetime of the agent following this policy, which is impossible for the agent since it does not have access to the information of its future. Therefore, the agent maintains an estimate of J instead, and updates it according to Algorithm [1].


The estimated transition and deviation functions are neural networks.
The estimated deviation function J is updated at each step by minimizing an associated error [Doya, 2000]. The gradient of the neural networks with respect to the inputs is also computed by backpropagation.


Theoretical results


1. An equivalent formulation of the optimization problem


We define the reward at time t for an agent whose internal state follows the trajectory function *MATH*. 


Intuitively, the reward, which can be positive or negative, is proportional to the variation of the drive of the agent, and thus to what the agent has gained (or lost) in comfort with respect to the stasis point between time t and t + dt. This variation of the drive is implied by the control and thus by the action that the agent has taken at time t.


Lemma 1 The pursuit of homeostatic stability is equivalent to the maximization of the reward. Formally, we have *MATH*.


Proof On doing an integration by parts (valid even in the case where the function ζ is continuous everywhere and piece-wise, which is the case when f is continuous and u is piece-wise continuous) we have: *MATH* We can then conclude the proof.


We have reformulated the problem in an equivalent way using the classical variables of reinforcement learning, which are the reward and the value function. This property has already been proved in the discrete case [Keramati and Gutkin, 2011]. It establishes a link between the maximization of the integral of the discounted rewards and the minimization of the integral of the discounted drive.


2. Properties of the reward and the drive function


In this section, we take the derivative of the reward function with respect to several quantities (realized in discrete time in Keramati), and study the sign to show the underlying properties of this function, reflecting behaviors in the agent.


We define the drive as *MATH* We consider a situation in which an agent starts at time t0 = 0 with a state δ0 = [δ0,1, δ0,2,...]^T^, where δ0,1 and δ0,2 represent the levels of the agent&apos;s first two needs. From t0 onwards, the agent continuously consumes the same resource which gives it a control u = [m, 0,..., 0] constant in time, with m the quantity of resource consumed per unit of time. Let us consider a time t sufficiently close to t0 so that the regulating effect of the body is negligible compared to the quantity of resource ingested. We have δt = δ0 + tu and the drive and the reward at time t are *MATH*.


Effects of deviation from the homeostatic set point for the feature receives an outcome: Taking the derivative of the reward with respect to|δ0,1|, we find that *MATH*.


The first case means that if an agent has exceeded its homeostatic set point for a need, and it continues to consume a resource affecting this need, it will receive a punishment (negative reward) that is proportional to the homeostatic setpoint deviation. The second case means that for an agent deprived of a resource, a fixed amount consumed of that resource will have a greater motivational outcome if the agent&apos;s initial need for the resource was high rather than low, as observed in [Hodos, 1961].


Cross need interactions, effects of deviation from the homeostatic set point for a feature that does not receive an outcome: Taking the derivative of the reward with respect to |δ0,2|, we find that *MATH*. 


In the first situation, *MATH* and the agent is still below its homeostatic set point for the first need at time t. The agent will gain a positive reward by consuming the resource affecting its first need, but the negative derivative means that this reward will be reduced if δ0,2 increases. The interpretation of the second situation is similar, but the reward is now negative, since the agent has exceeded its homeostatic set point for the first need. Such inhibitory effects occur in nature, as shown experimentally by [Dickinson and Balleine, 2002]. For example, food deprivation tending to suppress water-related responses.


Effects of resource dose: Taking the derivative of drive with respect to tm, which is the amount of resource consumed at time t, we find that: *MATH*.


This means that if an agent has not reached its homeostatic setpoint for a need at time t, i.e. δ0,1 + tm 0, then its training would have been smaller.
On the other hand, if the agent has reached closer to its homeostatic setpoint δ0,1 + tm 0, then its amount consumed is closer, as shown for rats in [Skjoldager et al., 1993]. &lt;Need clarity&gt; Next we present the results of the experiment and discuss them.


Results and Discussion


Figure [2] shows the results of resource concentration for the agent w.r.t time (i.e. iterations). In the initial stages, the agent begins with very limited amount of both R1 and R2, and as it explores the environment, its energy decreases as is indicated by the zero and later negative concentration for the resources (shock state). Corresponding to this, the muscular fatigue and sleep fatigue also increases for the agent (Figure [3]).


FIGURE 2


Gradually, with environmental exploration the agent is reaches the resource reservoirs and registers the changes in its internal state due to this exploratory action. As a result of this, the agent slowly learns the actions that lead to the resources given its internal state and position. In the Figure [2a], which graphs the change in the internal concentration of the resources for 6000 iterations, it is observed that the agent begins to marginally achieve the homeostasis for R1. By 8000 iterations (Figure [2b]), the agent has also started to approach its homeostatic set point for Resource R2 (red line). This behaviour reinforces the agent&apos;s intelligence related to homeostatic deviation from R2. The agent strives for R2 but in the process, the concentration of R1 is also maintained closer to its homeostatic set point. Thus, it appears that the agent intends to learn a policy that leads to global homeostasis.


FIGURE 3 To verify whether the observed graphs in Figures [2a] and [2b] are results or purely exploratory or exploitative actions, we tested the algorithm for 10000 (Figure [2c]) and 14000 (Figure [2d]) iterations. These graphs confirm the learning process for the agent, and we observe that no matter how long the iterations last, the resource concentrations lie close to their respective homeostatic set points. By, 14000 iteration the agent begins to achieve a plateau in its behaviour, which reflects that the agent has learned to directly leap (walk in a directed manner) to the resource reservoirs in times of homeostatic deviation. Note that the final concentrations for Resources R1 and R2 are much higher than the initial starting point, confirming that the agent has learnt to take actions that lead to homeostasis.


Overall, it is observed that 70% actions taken are exploitative and 30% exploratory over the life-course of the agent for each iteration case. The change in muscular and sleep fatigue for the agent as its learns to achieve homeostasis in an unknown environment is shown in Figure [3]. As mentioned before, when the agent starts its exploration, the muscular fatigue rises very fast. Gradually, as the agent learns to identify resource position and consumes the resource, both muscle and sleep fatigue reduce to minimum values. In fact, both kinds of fatigue achieve relative stability after 2000 iteration.


FIGURE 7


We also show the graph of Deviation Function loss w.r.t iterations (Figure [4]). These plots show that in the beginning of the task, the agent&apos;s homeostatic deviation increases because it is exploring an environment without too much resources in its system.
This is also seen in Figure [2]. After 3000th iteration, the resource concentration does not fall below 1 for each of the resources, indicating that learning has started to become concrete in the agent. As the iterations increase, agent learning gets solidified and the agent can immediately go to the resource points to satiate itself, without having to explore the environment too much.
After 2000 iterations, the Loss of Deviation function begins to decline, gradually approaching zero as iterations increase.


Figure [5] shows the trajectory of the agent movement in the environment as determined by the actions taken in each iteration.
Exploration is seen as the trajectory away from the resource points, until by Figure [5e], the agent has learnt the resource positions and is able to consume them by directly walking to the resources. The line joining the two resources gradually becomes darker (Exploitation) as opposed to trajectories beyond these positions (Exploration). The dark pathways in the graphs testify the policy learnt by the agent.


Finally, as per the neural network based algorithm designed by us, the agent reaches an optimal point and its learning is solidified in this environment such that when the agent senses resource depletion, due to normal bodily processes, it is able to replenish itself quickly having learned the resource positions. Thus, the agent can dynamically maintain homeostasis in real world in a continuous-time manner.


It is vital to note that as the biological process of homeostasis never ends, in the simulations also, our program is never ending and continues indefinitely as seen in Figure [5f]. But, we closely observe the agent&apos;s behaviour for multiple iterations far apart: 6000, 8000, 10000 and 14000. This allows us to observe and study the long-term behaviour of the agent and discover any anomalies in the agent&apos;s behaviour pertaining to homeostatic regulation. The end result of this experiment is, a solidified neural trace/path (Figure [5f]) that is reflective of an established learning (policy) in the given environment. The complete simulations are shared on Github [Bhargava, 2023].


It is vital to discuss our results in the context of other studies.
[Yoshida et al.] proposed a neural homeostat in which the agent stabilises its internal physiology through interaction with the environment [Yoshida et al., 2021]. Authors considered two kinds of homeostasis: primary homeostasis relating to direct internal body control and behavioural homeostasis which entails change in agent behaviour (drinking water, eating food) depending on its interaction with the environment. In our work we do not distinguish homeostasis as internal or external because the external behaviour and internal deviation are closely linked to each other. The internal deviations lead to external actions, which in turn affect the internal state or physiology of the agent. We consider agent-environment interaction as a separate phenomenon through which the agent learns about the environment boundaries and gradually about the resource points.


[Yoshida et al.] also consider three types of information received by the agent: exteroception, proprioception, and interoception and compute total reward as the sum of homeostatic reward and proprioceptive cost. Essentially, the reward is defined by coupling the agent&apos;s internal state dynamics and the environment. The authors find that simple food-capturing reward does not result in homeostatic behaviour. This is in contrast to our work wherein reward is modeled as sum of both immediate and long-term reward, and the agent is able to achieve homeostatic behavior and learning. Furthermore, the agent we considered was guided only be interoception and through the exploration of the environment could learn about information from the environment and resource points. Thus, no exteroception was used in our work. In a different work, [Yoshida] formulates homeostasis achievement as a survival problem, and presents maximization of the multi-step survival probability as the solution.
In the most recent work, [Yoshida et al.] attempted to increase learning speed of the homeostatic RL agent, by introducing interoceptive soft-behaviour switching in the algorithm.


FIGURE 5


Another difference in our work from contemporary research is the focus on monolithic agent for a classical two-resource problem. The concept of modular agent is proposed by [Dulberg et al.] according to which each sub-agent achieves the homeostasis through divide and conquer approach. Unlike the global optimization problem in the monolithic agent, this work emphasizes the use of decentralized control which is rooted in adaptation rather than centralized control.
Authors demonstrate that such a modular approach is able to solve the challenge of multi-dimensional homeostatic regulation and each sub-network corresponding to the sub-agent learn distinct policies based on separate reward components. [Arikawa et al.] confirm the applicability of HRRL for foraging strategies. Authors considered three foraging rules that the agent can use depending upon their environment: Closest Distance (CD), No Interaction (NI) and Equal Distance (ED). For each of these they suggested different rules for internal state update. However, it is unclear how the agent will decide which foraging strategy to use. Moreover, in their experiment, the episode terminated if the internal states deviated from the homeostatic set point. In contrast, in our experiment the agent continued to explore the environment and forage to find and consume resources required to maintain homeostasis.


Our work is partially similar to that of Walter [Lettvin, 1954] in which robotic agents have to recharge themselves by searching for the batteries at the recharge stations scattered in an environment. However, in our case, we use only one agent and the aim is to mimic an autonomous biological agent. In our work we demonstrate resource foraging behaviour while also accounting for muscular and sleep fatigue.
Explaining embodied behaviours aligned with the physiology of the biological agent may be more complex and complicated, as these may not be explicitly associated with organism survival. For e.g. pro-social behaviours, desire for recognition, gambling etc. Nevertheless, it is possible that these behaviors are somehow translated, on a small scale, into a set of characteristics that could be represented in terms of x motor function variables. Indeed, [Juechems and Summerfield] argue that even non-physiological motivations can be modeled using the HRRL framework. Seeking long-term goal is an example of this, wherein intermediate goals lead to the final goal. Thus, there appears to be a structural and conceptual similarity between the learning mechanism for complex goals and primitive goals for homeostasis.


Finally, our model is essentially limited by knowledge of the human body and the structure of more abstract needs, which means that defining specific training functions for complex goals is a challenge.
HRRL is also challenged by the number of internal variables that can become very large, making it difficult for the algorithm to converge and thus for the agent to learn. A research goal could be to model robots possessing automatically learned human characteristics, which can evolve and interact together in an environment. A guiding example is discussed by [Dulberg et al.]. Despite its limitations, the simplicity of our model and its ability to have an arbitrary choice of scale may have an impact on this goal. We have made simplifications on the generality of the simulation, notably on the lack of stochasticity by not putting other agents in the environment, but we could in the future make simulations with several agents including prey and predators. In a multi-agent simulation, we could also create a colony of agents in which each member implements de facto empathy for its cohorts, as suggested by [Man and Damasio, 2019].


Conclusion


We extended the HRRL framework in continuous time space by developing an agent capable of dynamic and long-term homeostatic regulation.
Basic properties of food foraging in an unknown environment, and physiological attributes like muscle and sleep fatigue were embodied in the agent. Through computer simulations we demonstrated that the agent was able learn to select action that lead to homeostasis in an unknown environment which contained the resources necessary for its survival. The characteristics of muscle and sleep fatigue were also discussed as the agent learnt to achieve homeostasis. Aligned with the unending biological process of homeostasis our simulation results showed that the agent continued its learnt policy to maintain homeostasis for the two resources. Finally, we call this framework in continuous-time and space, as the Continuous-Time Continuous-Space HRRL: CTCS-HRRL.