Introduction

Purposeful behavior typically occurs when an agent exhibits specific
preferences over different states of the environment. Mathematically,
these preferences can be formalized by the concept of a utility function
that assigns a numerical value to each possible state such that states
with higher utility correspond to states that are more desirable
[*REF*]. Behavior can then be understood as the attempt to
increase one&apos;s utility. Accordingly, utility functions can be measured
experimentally by observing an agent choosing between different options,
as this way its preferences are revealed. Mathematical models of
rational agency that are based on the notion of utility have been widely
applied in behavioral economics, biology and artificial intelligence
research [*REF*]. Typically, such rational agent models
assume a distinct reward signal (or cost) that an agent is explicitly trying to optimize.

However, as an observer we might even attribute purposefulness to a
system that does not have an explicit reward signal, because the
dynamics of the system itself reveal a preference structure, namely the
preference over all possible paths through history. Since in most
systems not all of the histories are equally likely, we might say that
some histories are more probable than others because they are more
desirable from the point of view of the system. Similarly, if we regard
all possible interactions between a system and its environment, the
behavior of the system can be conceived as a drive to generate desirable
histories. This imposes a conceptual link between the probability of a
history happening and the desirability of that history. In terms of
agent design, the intuitive rationale is that agents should act in a way
such that more desired histories are more probable. The same holds of
course for the environment. Consequently, a competition arises between
the agent and the environment, where both participants try to drive the
dynamics of their interactions to their respective desired histories. In
the following we want to show that this competition can be
quantitatively assessed based on the entropy dynamics that govern the
interactions between agent and environment.

Preliminaries

We introduce the following notation. A set is denoted by a calligraphic
letter like *MATH* and consists of elements or symbols.
Strings are finite concatenations of symbols and sequences are
infinite concatenations. The empty string is denoted by *MATH*.
*MATH* denotes the set of strings of length *MATH* based on
*MATH*, and FORMULA is the set of finite strings.
Furthermore, FORMULA is defined as the set of one-way infinite sequences
based on *MATH*. For substrings, the following shorthand notation
is used: a string that runs from index *MATH* to *MATH* is written as
FORMULA *MATH* FORMULA is a string starting from the first index. By
convention, *MATH* if *MATH*. All proofs can be found in the appendix.

Rewards

In order to derive utility functions for stochastic processes over
finite alphabets, we construct a utility function from an auxiliary
function that measures the desirability of events, i.e. such that we can
assign desirability values to every finite interval in a realization of
the process. We call this auxiliary function the reward function. We
impose three desiderata on a reward function. First, we want rewards to
be mappings from events to reals numbers that indicate the degree of
desirability of the events. Second, the reward of a joint event should
be obtained by summing up the reward of the sub-events. For example, the
&quot;reward of drinking coffee and eating a croissant&quot; should equal &quot;the
reward of drinking coffee&quot; plus the &quot;reward of having a croissant given
the reward of drinking coffee&quot;. This is the additivity requirement
of the reward function. The last requirement that we impose for the
reward function should capture the intuition suggested in the
introduction, namely that more desirable events should also be more
probable events given the expectations of the system. This is the
consistency requirement.

We start out from a probability space
*MATH*, where *MATH* is the sample
space, *MATH* is a dense *MATH* -algebra and *MATH* is
a probability measure over *MATH*. In this section, we use
lowercase letters like *MATH* to denote the elements of the
*MATH* -algebra *MATH*. Given a set *MATH*, its
complement is denote by complement *MATH* and its
powerset by *MATH*. The three desiderata can then be
summarized as follows: DEFINITION 1.

The following theorem shows that these three desiderata enforce a strict
mapping rewards and probabilities. The only function that can express
such a relationship is the logarithm.

THEOREM 1

Notice that the constant *MATH* in the expression
*MATH* merely determines the
units in which we choose to measure rewards. Thus, the reward function
*MATH* for a probability space
*MATH* is essentially unique. As a
convention, we will assume natural logarithms and set the constant to
*MATH*, i.e. *MATH*.

This result establishes a connection to information theory. It is
immediately clear that the reward of an event is nothing more than its
negative information content: the quantity *MATH* is
the Shannon information content of *MATH* measured in
nats [*REF*]. This means that we can interpret rewards as
&quot;negative surprise values&quot;, and that &quot;surprise values&quot; constitute losses.

PROPOSITION 1

The proof of this proposition is trivial and left to the reader. The
first part sets the bounds for the values of rewards, and the two latter
explain how to construct the rewards of events from known rewards using
complement and countable union of disjoint events.

At a first glance, the fact that rewards take on complicated
non-positive values might seem unnatural, as in many applications one
would like to use numerical values drawn from arbitrary real intervals.
Fortunately, given numerical values representing the desirabilities of
events, there is always an affine transformation that converts them into
rewards.

THEOREM 2

Note that Theorem 2 implies that the probability
*MATH* of any event *MATH* in the *MATH* -algebra
*MATH* generated by *MATH* is given by
*MATH*. Note that for singletons *MATH*, *MATH* is the Gibbs
measure with negative energy *MATH* and temperature
*MATH*. It is due to this analogy that we call the
quantity *MATH* the temperature parameter of the transformation.

Utilities in Stochastic Processes


In this section, we consider a stochastic process *MATH* over
sequences *MATH* in *MATH*. We specify the
process by assigning conditional probabilities
*MATH* to all finite strings
*MATH*. Note that the distribution FORMULA for all
*MATH* is normalized by construction. By the
Kolmogorov extension theorem, it is guaranteed that there exists a unique probability space *MATH*. 
We therefore omit the reference to *MATH* and talk about the process *MATH*.

The reward function *MATH* derived in the previous section
correctly expresses preference relations amongst different outcomes.
However, in the context of random sequences, it has the downside that
the reward of most sequences diverges. A sequence *MATH* 
can be interpreted as a progressive refinement of a point event in
*MATH*, namely, the sequence of events
FORMULA. One can exploit the interpretation of the index as time to
define a quantity that does not diverge. We define thus the utility as
the reward rate of a sequence.

DEFINITION 2

A utility function that is constructed according to
Definition 2 has the following properties.

PROPOSITION 2

Part (i) provides trivial bounds on the utilities that directly carry
over from the bounds on rewards. Part (ii) shows how the utility of a
sequence determines its probability. Part (iii) implies that the
expected utility of an interaction sequence is just its negative entropy
rate.

Utility in Coupled I/O systems

Let *MATH* and *MATH* be two finite sets, the first being
the set of observations and the second being the set of actions.
Using *MATH* and *MATH*, a set of interaction sequences is
constructed. Define the set of interactions as *MATH*. A pair
*MATH* is called an interaction. We underline symbols
to glue them together as in *MATH*.

An I/O system *MATH* is a probability distribution over
interaction sequences *MATH*. *MATH* is uniquely
determined by the conditional probabilities
*MATH*. However, the semantics of the probability distribution *MATH* are only fully
defined once it is coupled to another system. Note that an I/O system is
formally equivalent to a stochastic process; hence one can construct a
reward function *MATH* for *MATH*.

Let *MATH*, *MATH* be two I/O systems. An interaction
system FORMULA that
describes the probabilities that actually govern the I/O stream once the
two systems are coupled. *MATH* is specified by the equations *MATH*.
Here, *MATH* is a stochastic process over *MATH* 
that models the true probability distribution over interaction sequences
that arises by coupling two systems through their I/O streams. More
specifically, for the system *MATH*,
*MATH* is the probability of producing
action *MATH* given history *MATH* and
*MATH* is the predicted probability of
the observation *MATH* given history
*MATH*. Hence, for *MATH*, the sequence
*MATH* is its input stream and the sequence *MATH* 
is its output stream. In contrast, the roles of actions and observations
are reversed in the case of the system *MATH*. This model of
interaction is very general in that it can accommodate many specific
regimes of interaction. By convention, we call the system *MATH* 
the agent and the system *MATH* the environment.

In the following we are interested in understanding the actual utilities
that can be achieved by an agent *MATH* once coupled to a
particular environment *MATH*. Accordingly, we will compute
expectations over functions of interaction sequences with respect to
*MATH*, since the generative distribution *MATH* describes
the actual interaction statistics of the two coupled I/O systems.

THEORM 3

Accordingly, the interaction system&apos;s expected reward is given by the
negative sum of the entropies produced by the agent&apos;s action generation
probabilities and the environment&apos;s observation generation
probabilities. The agent&apos;s (actual) expected reward is given by the
negative cross-entropy between the generative distribution *MATH* 
and the agent&apos;s distribution *MATH*. The discrepancy between the
agent&apos;s and the interaction system&apos;s expected reward is given by the
relative entropy between the two probability distributions. Since the
relative entropy is positive, one has FORMULA. This
term implies that the better the environment is &quot;modeled&quot; by the agent,
the better its performance will be. In other words: the agent has to
recognize the structure of the environment to be able to exploit it. The
designer can directly increase the agent&apos;s expected performance by
controlling the first and the last term. The middle term is determined
by the environment and only indirectly controllable. Importantly, the
terms are in general coupled and not independent: changing one might
affect another. For example, the first term suggests that less
stochastic policies improve performance, which is oftentimes the case.
However, in the case of a game with mixed Nash equilibria the overall
reward can increase for a stochastic policy, which means that the first
term is compensated for by the third term. Given the expected rewards,
we can easily calculate the expected utilities in terms of entropy
rates.

COROLLARY 1

This result is easily obtained by dividing the quantities in
Theorem 3 by *MATH* and then applying the chain rule for
entropies to break the rewards over full sequences into instantaneous
rewards. Note that *MATH*,
*MATH* are the contributions to the utility due the
generation of interactions, and *MATH*,
*MATH* are the contributions to the utility due to
the prediction of interactions.

Examples

FIGURE

One of the most interesting aspects of the information-theoretic
formulation of utility is that it can be applied both to control
problems (where an agent acts in a non-adaptive environment) and to game
theoretic problems (where two possibly adaptive agents interact). In the
following we apply the proposed utility measures to two simple toy
examples from these two areas. In the first example, an adaptive agent
interacts with a biased coin (the non-adaptive agent) and tries to
predict the next outcome of the coin toss, which is either &apos;Head&apos; (H) or
&apos;Tail&apos; (T). In the second example two adaptive agents interact playing
the matching pennies game. One player has to match her action with the
other player (HH or TT), while the other player has to unmatch (TH or
HT). All agents have the same sets of possible observations and actions
which are the binary sets *MATH* and *MATH*.

Example 1.

The non-adaptive agent is a biased coin. Accordingly, the coin&apos;s action
probability is given by its bias and was set to
*MATH*. The coin does not have any biased
expectations about its observations, so we set
*MATH*. The adaptive agent is given by the
Laplace agent whose expectations over observed coin tosses follows the
predictive distribution *MATH*,
where *MATH* is the number of coin tosses observed so far and *MATH* is the
number of observed Heads. Based on this estimator the Laplace agent
chooses its action deterministically according to *MATH*,
where *MATH* is the Heaviside step function. From these
distributions the full probability over interaction sequences can be
computed. Figure 1A shows the entropy dynamics for a typical single run.
The Laplace agent learns the distribution of the coin tosses, i.e. the
KL decreases to zero. The negative cross-entropy stabilizes at the value
of the observation entropy that cannot be further reduced. The entropy
dynamics of the coin do not show any modulation.

Example 2.

The two agents are modeled based on smooth fictitious play
[*REF*]. Both players keep count of the empirical
frequencies of Head and Tail respectively. Therefore, each player *MATH* 
stores the quantities *MATH* and
*MATH* where *MATH* is the number of moves observed so
far, *MATH* is the number of Heads observed by Player 1 and *MATH* is the
number of Heads observed by Player 2. The probability distributions *MATH* and
*MATH* over inputs is given by these
empirical frequencies through *MATH*.
The action probabilities are computed according to a sigmoid
best-response function *MATH*, and *MATH* 
respectively in case of Player 2 that has to unmatch. This game has a
well-known equilibrium solution that is a mixed strategy Nash
equilibrium where both players act randomly. Both action and observation
entropies converge to the value *MATH*. Interestingly, the
information-theoretic utility as computed by the cross-entropy takes the
action entropy into account. Compare Figure 1B.

Conclusion

Based on three simple desiderata we propose that rewards can be measured
in terms of information content and that, consequently, the entropy
satisfies properties characteristic of a utility function. Previous
theoretical studies have reported structural similarities between
entropy and utility functions, see e.g. [*REF*], and recently,
relative entropy has even been proposed as a measure of utility in
control systems [*REF*; *REF*; *REF*]. The
contribution of this paper is to derive axiomatically a precise relation
between rewards and information value and to apply it to coupled I/O
systems.

The utility functions that we have derived can be conceptualized as
path utilities, because they assign a utility value to an entire
history. This is very similar to the path integral formulation in
quantum mechanics where the utility of a path is determined by the
classic action integral and the probability of a path is also obtain by
taking the exponential of this &apos;utility&apos; [*REF*]. In particular,
we obtain the (cumulative time-averaged) cross entropy as a utility
function when an agent is coupled to an environment. This utility
function not only takes into account the KL-divergence as a measure of
learning, but also the action entropy. This is interesting, because in
most control problems controllers are designed to be deterministic (e.g.
optimal control theory) in response to a known and stationary
environment. If, however, the environment is not stationary and in fact
adaptive as well, then it is a well-known result from game theory that
optimal strategies might be randomized. The utility function that we are
proposing might indeed allow quantifying a trade-off between reducing
the KL and reducing the action entropy. In the future it will therefore
be interesting to investigate this utility function in more complex
interaction systems.