AGILE: A Novel Framework of LLM Agents


Introduction


Large Language Models (LLMs) have exhibited remarkable capabilities such
as instruction following, reasoning, and zero-shot
learning [*REF*; *REF*; *REF*; *REF*],
which have greatly catalyzed the development of autonomous agents based
on LLMs [*REF*; *REF*; *REF*],
also known as LLM agents. Recent works propose several essential
components or workflows to enhance the abilities of LLM agents, such as
planning [*REF*; *REF*; *REF*],
reflection [*REF*; *REF*], tool-use [*REF*; *REF*; *REF*] and
life-long learning [*REF*]. However, it remains unclear how
to integrate all components into a unified framework and optimize them end-to-end.


FIGURE


In this paper, we introduce a novel framework for LLM agents to unify
various components and streamline their learning and operation
processes. As shown in Figure [1](agent_arch){reference-type=&quot;ref&quot;
reference=&quot;agent_arch&quot;}(a), the architecture of the agent system, named
AGILE, comprises four modules: LLM, memory, tools, and executor.
Furthermore, the agent can interact with both users and experts. The
LLM, functioning as the predictor of all actions, generates instructions
and processes responses. The executor, working as the controller of all
actions, interprets the LLM instructions to activate the corresponding
modules and collects their responses for the LLM. For example, the
executor can fetch a text from the memory and append it to the context
of LLM, or extract an excerpt from the context and append it to the
memory. The executor can also follow instructions of the LLM to utilize
a search tool. In addition to skills such as reasoning, planning, and
reflection, we propose a new ability called seeking advice, which
means that the agent proactively consults human experts when it
encounters a problem unsolvable. The agent can reflect on the expert
feedback and memorize it for future use. Furthermore, we propose a
training method based on reinforcement learning (RL), which
simultaneously trains the policy of invoking different modules and the
reasoning, planning, reflection, and seeking advice abilities of the LLM
agent in an end-to-end fashion.


While the proposed agent framework is general, in this paper, we
evaluate it in complex question answering (QA). It is a task an LLM
agent has the potential of outperforming existing solutions such as the
use of an LLM alone. However, existing QA
benchmarks [*REF*; *REF*; *REF*; *REF*]
are designed for specific subsets of capabilities (e.g., reflection,
memory retrieve, etc.) which cannot simultaneously investigate the
ability to combine all modules and capabilities of the agent.


To address this, we have developed a new benchmark called ProductQA.
ProductQA comprises 88,229 question-answer pairs in customer service
divided into 26 QA tasks, each corresponding to a distinct Amazon
product category. This benchmark is based on real Amazon user queries
and includes fact-based questions, reasoning questions, and product
recommendation queries. It comprehensively evaluates agents&apos; abilities
to handle historical information and accumulated knowledge, leverage
tools, interact with humans, perform self-evaluation, and conduct
reflection. Additionally, the training and testing tasks are made
disjoint to assess the agent&apos;s ability to adapt to new product
categories.


We evaluate our agent framework on two tasks, ProductQA and
MedMCQA [*REF*]. For ProductQA, we use a two-stage training
method based on Vicuna-13b [*REF*]. In the first stage, imitation
learning is employed to create &apos;agile-vic13b-sft&apos;. In the second stage,
the policy gradient algorithm of PPO [*REF*] produces
&apos;agile-vic13b-ppo&apos;. Experimental results show that &apos;agile-vic13b-ppo&apos;
improves the relative total performance score by 9.2% over GPT-4 and by
90.8% over GPT-3.5. Ablation studies confirm that all modules in
Figure [1] are
indispensable. Specifically, removing tools or memory usage negatively
impacts the agent&apos;s performance, leading to a 25.9% or 17.4% increase in
seeking advice, respectively, or a 9.3% or 4.0% relative decrease in the
total score, respectively. Disabling the seeking advice function results
in a 10.7% decrease in accuracy. Finally, &apos;agile-vic13b-ppo&apos; achieves a
2.3% relative increase in total score compared to &apos;agile-vic13b-sft&apos;,
demonstrating the necessity of PPO training. On MedMCQA, we train an
&apos;agile-mek7b-ppo&apos; agent, initialized from Meerkat-7b [*REF*],
following the same two-stage procedure. Our agent improves the base
LLM&apos;s accuracy from 53.4% to 85.2% by seeking advice on 31.6% instances.
This accuracy surpasses the SOTA accuracy of 79.1% by
GPT4-MedPrompt [*REF*]. When all agents are able to seek
advice, our agent also outperforms the GPT-4 agent in terms of the total
score.


The main contributions of this paper are summarized as follows: 
- We propose a novel framework of LLM agents, formulated in the
context of reinforcement learning. It facilitates end-to-end
learning of agents. Notably, this framework enables the agent to
seek advice from human experts, providing two advantages: 1) It
ensures high-level accuracy when dealing with complex and
challenging questions, and 2) it fosters learning from humans,
thereby enhancing its abilities to adapt to new tasks.
- We develop a benchmark, ProductQA, to comprehensively evaluate the
agent&apos;s capabilities in complex question answering.
- We perform experiments on ProductQA and MedMCQA to verify our
framework and show that AGILE agents based on 13B and 7B LLMs
trained with PPO can surpass GPT-4 agents.


Methods


RL formulation of agent


Our agent framework comprises four elements: LLM, memory, tools and
executor, see Figure [1](agent_arch){reference-type=&quot;ref&quot;
reference=&quot;agent_arch&quot;}(a). The LLM possesses a context, defined as
the sequence of tokens it utilizes to generate the next token. In RL
terminology, the agent conducts a token-level Markov decision process
(MDP). The action space *MATH* corresponds to the LLM&apos;s
vocabulary, with each token representing an action. Hence, the LLM
serves as the policy model. The agent&apos;s state consists of the
(context, memory) pair. Upon predicting a new action *MATH* (i.e., a new
token), the LLM transfers control to the executor. The executor applies
predefined logic to transition from the current state *MATH* to the next
state *MATH*, implementing the state transition function
*MATH* in RL, and then returns
control to the LLM to predict the next action. Concurrently, the
environment issues a reward *MATH*.


Let us examine the state transition more closely. For each action, the
executor&apos;s first operation is to append the token to the context,
preparing the LLM for generating the next token. Then, the executor
checks a registered list of functions. Each function is designed to
execute a set of operations, including memory I/O, tool usage, and
interaction with the environment. If the action (i.e., the token)
matches a function name, the executor will execute the associated
function implementation, further mutating the agent state. For instance,
if the token is &apos;[GetQuestion]&apos;, the executor will prompt the user for a
new question and append it to the context; if the token is
&apos;[UpdateMemory]&apos;, the executor will write a specific segment of the
context into the memory; if the token is &apos;[ClearContext]&apos;, the executor
will reset the context to &apos;[BOS]&apos;. In summary, the LLM interacts with
the memory and tools by predicting function names, relying on the
executor to execute these functions. See
Table for a full list of functions defined
for a QA agent and see Figure [1](b) for a running example.


Policy learning


We frame the policy learning problem as a task of training a language
model. Consider an agent trajectory *MATH*, we
derive a training sequence denoted as *MATH*, where *MATH* 
represents the tokens that the executor appends to the context at step
*MATH*. If *MATH* is a function name token, then *MATH* is the concatenation
of *MATH* and extra tokens appended by the function execution; otherwise,
*MATH*. In this sequence, *MATH* (the first token of each
*MATH*) are referred to as action tokens. The LLM context at step *MATH*,
denoted by *MATH*, is a subsequence of the prefix *MATH*;
*MATH* may be shorter than *MATH* because the executor can
delete context tokens.


In Imitation Learning (IL), we generate trajectories by observing human
experts or more proficient agents, then we derive the training sequences
to fine-tune the LLM. It is important to point out that (1) the loss is
calculated on the action tokens only, and (2) *MATH* should serve as the
attention mask for tokens in *MATH*, as it reflects the true context
perceived by the LLM at the time of action prediction. In reinforcement
learning (RL), we treat the LLM as the policy model, from which training
sequences can be sampled and individual action tokens are assigned
rewards. Consequently, the LLM can be optimized using policy gradient
methods, such as PPO [*REF*]. Analogous to the IL setup,
we apply policy gradient updates exclusively to the action tokens and
employ *MATH* as the attention mask.


In some situations, an agent may produce very long trajectories,
potentially yielding training sequences that span millions of tokens and
are impractical for training. We can leverage the structure of the
trajectory to partition it into smaller segments. For instance, if the
agent resets its LLM context at the beginning of every QA session, then
we can partition by the session boundary. Nevertheless, these sessions
are not entirely independent; actions taken in earlier sessions can
influence memory, creating lasting effects on subsequent sessions. To
tackle this challenge of long-range dependencies, we propose a training
algorithm detailed in Appendix [7].


Interaction with human experts


Our agent framework enables the agent to proactively seek advice from
human experts. For example, the agent can invoke a &apos;[SeekAdvice]&apos;
function to request expert advice. This approach helps in two ways.
Firstly, the agent can request the correct answer when its confidence is
low, ensuring sufficient accuracy for the application. Secondly, the
agent can use &apos;[Reflection]&apos; to distill general knowledge from the
expert advice before storing it in memory. This accumulation of
knowledge allows the agent to adapt to new tasks that it has not
encountered during training.


Seeking advice involves complex decision-making. The agent must estimate
its own confidence in the current session, predict the potential value
of the advice for future sessions, and consider the cost of human
resources. The optimal trade-off is difficult to annotate manually but
aligns well with our RL framework. Specifically, the present risk,
future value, and cost of action can all be represented as RL rewards,
allowing this skill to be trained as part of the policy model on an
end-to-end basis.


The ProductQA dataset 


We believe that product question answering in a real online shopping
environment offers a comprehensive challenge for evaluating LLM agents.
First, it demands expert knowledge about millions of products, including
their technical specifications, usage in particular scenarios, and
compatibility with other products. Second, answering some questions
requires the use of tools, such as a product search tool. Third, the
continuous emergence of new products necessitates the adaptability of
the agent. This has motivated the creation of the ProductQA dataset.


The ProductQA dataset consists of 26 QA tasks, each representing a
distinct group of products within a specific category. Each group
encompasses 17-20 products. We collected 20 groups for training and 6
for testing, allowing for assessing the agent&apos;s adaptability to new
tasks. We collected an average of 3,393 question-answer pairs for each
product group. The questions within the same group are correlated, as
knowledge from one answer may aid in addressing other questions. The
dataset statistics are presented in
Table .


The dataset is annotated by 20 professional annotators, each with at
least a college degree, employed by a commercial data annotation
company. We pay the company at market rates for professional annotation.
See annotation guidelines in Appendix. In addition, we release the code for
the data pre-processing before human annotation.


Product collection


We gather products from the Amazon Review Data [*REF*],
which includes product metadata as well as reviews. We initially filter
the Amazon Review Data to retain only popular products with at least 100
reviews, then cluster them by category tags. From these clusters, we
select 26 based on the size of the cluster, each defined as a product
group. Subsequently, we sample products from each product group. See
Appendix for more details about product group and
product selection.


After the products are collected, annotators compile an information
table for each product group. An example of such a table is presented in
Table. To enhance the efficiency of the
annotation process, we employ GPT-4 to extract as many product features
as possible from the reviews. These features, together with the product
metadata, are provided to the annotators for table creation.


QA collection


We identify three predominant types of questions in online shopping
contexts: 1) Fact-QA: questions concerning specific product details;
2) Search-QA: searches for product recommendations tailored to user
preferences; 3) Reasoning-QA: questions whose answers require
domain-specific reasoning, such as the implications of a product
feature. Accordingly, we annotate question-answer pairs for these types.
Each question is annotated with both a detailed paragraph-long answer
and a concise short answer. The long answer should resemble a response
from human customer service, while the short answer consists of a few
words. We train the model to predict both answer types. The accuracy of
the long answers is evaluated using GPT-4 (see
Appendix [12] for the prompt); the short answers are
assessed by exact match and are used for defining rewards for RL
training.


Fact-QA


Fact-QAs are constructed from product reviews. For each product, we
provide GPT-4 with a batch of 30 reviews, prompting it to generate 20
questions and their corresponding answers before moving on to the next
batch. We encourage GPT-4 to create diverse questions. The results are
then given to annotators to refine and finalize the question-answer
pairs.


Search-QA


Starting with an information table for a given product group, we
generate random SQL expressions using a set of predefined rules. These
expressions are then translated into natural language questions by
GPT-4. The answers are obtained by executing the SQL queries.
Subsequently, human annotators thoroughly revise the QA pairs.


Reasoning-QA


As the first step, we collect professional knowledge for each product
group. To enhance efficiency, we utilize GPT-4 to generate candidate
knowledge entries based on the technical specifications from the
information table. These entries are then curated and refined by human
annotators. Here is an example of a knowledge entry: Motherboards with
the ATX form factor are ideally suited for high-performance computing
tasks and gaming, due to their ample expansion slots for graphics cards
and other peripherals that boost computing capabilities. Finally,
annotators develop question-answer pairs from these knowledge entries.


Experiments 


Experimental setting 


Dataset


We evaluate our agent on two complex QA tasks: ProductQA and MedMCQA.
MedMCQA [*REF*] is a dataset for multiple-choice QA. It
consists of questions from medical school entrance examinations, with
182,822 / 4,183 / 6,150 instances in the train/dev/test splits. We
report results on the dev set without checkpoint selection.


Agent definition


Our agent can invoke functions defined in
Table. In a typical workflow, the agent
prompts the user for a new question at the session start, then retrieves
memory to get relevant information. The memory can be initialized as
empty (ProdcutQA) or with domain knowledge (QA pairs from MedMCQA
training dataset). The agent can optionally use tools (e.g. product
search in ProductQA) to get more information, then decide whether to
predict an answer directly or seek human advice. If the agent seeks
advice, it obtains a human answer (ground-truth answer in our setting).
The agent then uses a reflection round to extract general knowledge from
the human answer, writing both the human answer and the reflected
knowledge to its memory. Finally, the agent submits an answer to the
user. In our setting, submitting a correct answer incurs a *MATH* reward,
while submitting a wrong answer incurs a *MATH* reward. Seeking human
advice has a fixed *MATH* reward, where *MATH* represents seeking advice
cost. Assuming the human advice always contains a correct answer, then
the possible total rewards are *MATH*.


Training


The training consists of two stages. First, we construct trajectories
from the training data and employ imitation learning to train the agent.
Then we apply Algorithm for further optimization by
reinforcement learning. See Appendix [8] for implementation
details. The agent&apos;s LLM is initialized from Vicuna-13b-1.5 for
ProductQA and Meerkat-7b for MedMCQA. We fine-tune the model for 2
epochs with a learning rate of 1e-5 and a batch size of 64. We implement
PPO for 1 epoch with a learning rate of 1e-6 and a batch size of 64. The
training runs on NVIDIA-H800.


Evaluation and baselines


We report three metrics for the agent: (a) Advice rate: the rate of
seeking human advice; (b) Accuracy: the rate of predicting the correct
answer; (c) Total score: the average reward across all sessions, taking
the advice rate and the accuracy both into account.


We compare our agent against two types of baselines: 1) Prompting
GPT-3.5 (gpt-3.5-turbo-0301) and GPT-4 (gpt-4-0613) [*REF*] to
directly answer the question, without working in an agent manner, noted
as &apos;gpt3.5-prompt&apos; and &apos;gpt4-prompt&apos;. 2) Prompting GPT-3.5 and GPT-4
within the AGILE framework, noted as &apos;agile-gpt3.5-prompt&apos; and
&apos;agile-gpt4-prompt&apos;. We carefully designed prompts for all baselines and
they are shown in Appendix [12].


Results on ProductQA 


As Table shows, our AGILE agent outperforms all
baselines on ProductQA. Notably, the average total score of
&apos;agile-vic13b-ppo&apos; across six test groups shows a relative improvement
of 9.2% in short answers and 5.0% in long answers to &apos;agile-gpt4-prompt&apos;
where the seeking advice cost is added into the prompt. Concretely,
&apos;agile-vic13b-ppo&apos; uses a comparable number of seeking advice to achieve
7.4% higher accuracy in short answers than &apos;agile-gpt4-prompt&apos;, and as
Figure shows, this accuracy improvement is
consistent across the whole trajectory. Our &apos;agile-vic7b-ppo&apos; agent also
outperforms &apos;agile-gpt4-prompt&apos; in average total scores. Note that the
GPT-4 agent knows the seeking advice cost from its prompt (see
Figure [4]).


We investigate the impact of varying the seeking advice cost. As shown
in Figure [2], when the cost decreases, both the
advice rate and the accuracy increase, indicating greater utilization of
human assistance. Specifically, with a high cost of 0.5, the advice rate
is close to 0, and at a low cost of 0.1, the accuracy is close to 1.
This result demonstrates that by adjusting the cost and through RL
training, we can effectively manage the trade-off between accuracy and
human cost. For instance, the agent can achieve 94.1% accuracy on the
Motherboards task with a seeking advice cost of *MATH* (refer to Table
[1]). This capability is especially important
in realistic scenarios that demand high accuracy levels. In most
experiments, we set the cost at a medium level with *MATH*.


FIGURE


Ablation study


We present ablation studies in
Table to assess the contributions of individual agent
components and the effects of RL training. The table indicates that
disabling the option to seek advice (w/o Advice) leads to a 10.7% drop
in accuracy and a 5.0% relative reduction in total score. Forcing the
agent to seek advice at the initial part of the trajectory (Non-adapt
Advice) causes a 4.2% decrease in accuracy, underscoring the value of
adaptive decision-making. Removing reflection and memory capabilities
(w/o Memory and w/o Reflection) both increase the frequency of
advice-seeking, as the agent struggles to accumulate or leverage
valuable knowledge, consequently decreasing the total score.
Furthermore, disabling tool use (w/o Tool-Use) causes a substantial
25.9% increase in the advice-seeking rate because the agent&apos;s
capabilities are diminished, making it more reliant on external advice.
Lastly, RL training improves the relative total score by 2.3%, lowers
the advice-seeking rate, and boosts accuracy, demonstrating that RL
training effectively optimizes the policy.


In Appendix [10], we present detailed examples of
&apos;agile-vic13b-ppo&apos; illustrating how memory, tools, seeking advice, and
reflection enhance the agent workflow.


FIGURE


Trend of advice rate


Figure demonstrates a consistent decrease in
the advice rate of &apos;agile-vic13b-ppo&apos; as more sessions are added to the
trajectory. This decline can be attributed to the agent progressively
accumulating knowledge and becoming more independent. Additionally, the
figure illustrates that disabling RL training or reflection leads to a
significant increase in the advice rate, underscoring the importance of
RL training and reflection in reducing human costs.


Results on MedMCQA


Our &apos;agile-mek7b-ppo&apos; agent, based on the smaller Meerkat-7b
[*REF*] model, reaches an accuracy of 85.2% with an advice rate
of 31.6%. As Table shows, this represents a 31.8% accuracy
increase over the base model &apos;Meerkat-7b-prompt&apos; and a 6.1% increase
over the state-of-the-art &apos;gpt4-Medprompt&apos; [*REF*]. Table also shows that the ability to seek
advice alone contributes a 23.2% accuracy gain, meaning that each
instance of seeking advice corrects an average of 0.73 prediction
errors. This indicates that PPO training effectively helps the agent
identify its mistakes. For a fair comparison, we also evaluate
&apos;agile-gpt3.5-prompt&apos; and &apos;agile-gpt4-prompt&apos;, which incorporate GPT-3.5
and GPT-4 within our AGILE framework. These agents also leverage
advice-seeking to enhance accuracy, but without RL training, their total
scores are lower than &apos;agile-mek7b-ppo&apos;. Finally, through ablation
studies, we confirmed the essential roles of memory, reflection, seeking
advice, and RL training in achieving high performance. Removing these
components leads to a significant drop in total scores, detailed in
Table.


Related work


LLM agents


Large Language Models (LLMs) have demonstrated substantial capabilities
in following instructions, reasoning, and planning. Numerous research
works, as shown in Table, utilizing prompt engineering,
have constructed remarkable LLM agents capable of autonomously resolving
complex tasks across various
environments [*REF*; *REF*; *REF*; *REF*; *REF*].
Furthermore, extensive works identify key components in the design of
LLM agents, including planning [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*],
tool-use [*REF*; *REF*; *REF*], and
reflection [*REF*; *REF*]. In this work, we
enable the agent to utilize memory, tools and proactively learn from the
environment. We then formulate the entire process within an RL framework
so that all agent skills can be jointly optimized end-to-end.


Human-agent interaction


Although LLMs face practical challenges, such as
hallucination [*REF*] and a lack of long-tail
knowledge [*REF*], consulting human experts can help
mitigate these issues. Several studies have incorporated human experts
into agent workflows. For instance, [*REF*] establishes a static
pipeline in which an agent passively accepts advice from a superior LLM.
[*REF*; *REF*] train models to proactively ask
questions via behavior cloning. However, these methods ignore the fact
that the decision to seek advice must be based on the LLM&apos;s own
knowledge and capabilities [*REF*; *REF*; *REF*].
[*REF*] use a calibrated version of an LLM&apos;s token
probabilities as a confidence measure, yet token probabilities tend to
be overconfident [*REF*], and existing calibration methods don&apos;t
generalize well to our agent setting when the LLM makes multiple
decisions in sequence.


LLM agent benchmarks


Several benchmarks have been designed to assess the capabilities of
agents. For instance, the Webshop [*REF*] and
Mind2Web [*REF*] datasets evaluate agents&apos; tool usage and
planning abilities within a web environment.
HotPotQA [*REF*] and TriviaQA [*REF*] focus on
agents&apos; reasoning and tool usage for question answering.
ALFWorld [*REF*] examines planning and navigation
skills, while ScienceWorld [*REF*] provides an
interactive text-based environment to evaluate agents&apos; scientific
aptitude. As illustrated in
Table, despite these existing
benchmarks, none comprehensively addresses all the core challenges of
real-world agent applications, such as handling long-tail knowledge,
human-agent interaction, long-term memory usage, tool usage,
self-evaluation, and reflection. This motivated us to develop ProductQA.


Conclusion and future work


In this work, we introduce a novel framework of LLM agents, called
AGILE. First, the whole system of AGILE is trained end-to-end by
reinforcement learning. Second, AGILE has the ability of seeking advice
from external human experts. In addition, we develop a challenging
dataset of complex QA, ProductQA, for comprehensive evaluation of an
agent&apos;s capabilities. Extensive experiments demonstrate that within our
framework, an agent based on a smaller model after RL training can
outperform GPT-4.


AGILE is a general agent framework and we can certainly consider
multiple extensions of it. An agent can be equipped with more tools,
such as multimodal perception, manipulations in physical environments,
logical reasoning, among others. We posit that AGILE&apos;s activities can be
categorized into two distinct types: utilizing its LLM alone, and
integrating the LLM with other tools. These two approaches conceptually
align with the human cognitive processes known as System 1 and System 2.
Furthermore, AGILE&apos;s memory serves as a repository for the accumulation
of experiences and knowledge, which is crucial for self-improvement.
Consequently, AGILE offers an architecture for an very powerful agent
that has the potential to attain human-level intelligence.


AGILE also includes interactions between the agent and external human
experts. The framework can be extended to allow interactions with humans
or machine agents in various roles such as students or teachers, and in
different formats such as debates or coordination. Furthermore, AGILE
can be employed in multi-agent systems.