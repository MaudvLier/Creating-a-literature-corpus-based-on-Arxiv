LLMdriven Imitation of Subrational Behavior: Illusion or Reality?


Introduction 


In the recent years, Large Language Models (LLMs) have been among the
most impressive achievements in Artificial intelligence (AI) and
Machine Learning (ML), raising questions about whether we are close
to an Artificial General Intelligence (AGI) system *REF*. Foundation models like ChatGPT *REF* have shown remarkable
performance across a wide range of novel and complex tasks, including
coding, vision, healthcare, law, education, and psychology *REF*. These models perform
close to human experts, without needing additional retraining or
finetuning, apparently mimicking human abilities and reasoning
*REF*. In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving
complex problems: LLMs can simulate chain of thoughts, generating a
reasoning path to decompose complex problems into multiple easier
steps to solve *REF*. We could conclude that LLMs are able to
replicate highlevel cognitive and reasoning capabilities of humans
*REF*, especially with bigger foundation models like GPT4 *REF*. If we assume this is true, a natural question
arises: &quot;Can we leverage LLMs to simulate aspects of human behavior
that deviate from perfect rationality so as to eventually improve
our understanding of diverse human conduct?&quot;


Modeling human behavior has seen significant interest in various
domains, ranging from robotics for humanrobot collaboration
*REF*, to finance and economics for modeling human investors and consumers *REF*. In studies of
humanrobot interactions, it is commonly embraced that humans must not
be modeled as optimal agents *REF*, and an irrational human when correctly modeled,
can communicate more information about their objectives than a
perfectly rational human can *REF*. For instance, humans exhibit bounded
rationality *REF*, often making satisfactory but not optimal decisions due to their limited
knowledge and processing power. They are also known to be myopic in
their decision making as they care more about shortterm rewards than
longterm rewards. While the discount factor in reinforcement learning
(RL) may capture myopicity through exponential discounting, there is sample evidence to show that other temporal discounting models such as
hyperbolic discounting *REF* and quasihyperbolic discounting *REF* are more realistic in capturing
timeinconsistent human behavior *REF*. The latter models are of particular interest in
modeling household consumption and saving behavior in economic studies *REF*.
Empirical evidence from humans also demonstrates that the rate of
discounting reduces with an increase in the amount of reward *REF*.


When combined with the variability of the rate parameter across
humans, this makes the calibration of parameters challenging for
different humans, reward amounts, and scenarios. For example, works on
estimating parameters for temporal discounting models from human
studies exhibit disagreement among each other due to additional
experimentspecific considerations *REF*. The nonexponential discounting models, along
with the dependence of discount factor on the reward pose challenges
to classical RL algorithms relying on the validity of the Bellman
equation. Other models have been proposed to encompass different human
biases, including prospect theory, illusion of control, and
optimism, which often employ nonlinear regression techniques to
calibrate their parameters *REF*. However, such approaches demand substantial
effort in collecting samples of human behavior, and it is impractical
to obtain a set of parameters capable of representing individuals with
different levels of bias and rationality. Also, recruiting human
subjects for behavioral experiments comes with concerns surrounding
ethics, privacy and general subject disinterest in the absence of monetary incentives *REF*.


Recent work studies the possibility of using LLMs to simulate and
analyze human behaviors *REF*. G. Aher *REF* evaluate how LLMs effectively simulate different
aspects of human behavior, reproducing findings from some classic
economic and psychology experiments. J. J. Horton *REF* investigates the
computational capabilities of GPT3 to model economic agents
according to classical behavioral economics literature. Other work
employs interactive LLM agents to simulate more sophisticated societal
contexts, where they are eventually able to reproduce realistic social
interactions and emergent behaviors *REF*. Such remarkable performance
in agentbased simulation could be related to the ability of LLMs to
emulate different individuals, as studied in *REF*. Their work shows how LLMs
effectively emulate different sociodemographic human groups under
appropriate conditioning procedures. Similarly, P. Schramowski et
al. *REF* discover that LLMs, because of how they are trained and designed, eventually contain humanlike
biases reflecting our behavior and morality. In fact, LLMs naturally
encode a wide range of human behavior seen in the training data
*REF*, potentially representing implicit computational models of humans. The
ability to display such human qualities is what makes LLMs a
disruptive tool, possibly leading to a new paradigm shift:
scientists could be able to easily conduct new experiments that
previously would have been time consuming or just impossible, by using LLMs for simulation *REF*.


In this work, we investigate the ability of LLMs to synthesize a
broad range of subrational human behavior in temporal decision making
tasks. Our framework learns behavioral policies through Imitation
Learning (IL) *REF* using synthetic humanlike demonstrations
generated by LLMs. This addresses the limitations of classical RL
techniques in modeling subrational behavior *REF* by overcoming the need for
human demonstration data, which is expensive to gather for different
human subgroups. Our approach assumes that LLMs represent implicit
computational models of humans. Therefore, we can generate synthetic
demonstrations to model subrational human behaviors, including
irrational and myopic behaviors. Our primary goal is to initiate a
novel research area that uses LLMs to synthesize subrational behavior,
which has received limited preliminary attention *REF* albeit 
with enormous potential. Specifically, we summarize our contributions as follows:
We propose a novel research direction of using LLMs to calibrate subrational decision making models;
We propose a framework to leverage the ability of LLMs to 
generate synthetic demonstrations in order to learn subrational agent policies through IL;
We evaluate the ability of our framework to produce subrational
behavior as observed in established human studies;
Finally, we discuss the potential benefits, opportunities and 
pitfalls of utilizing LLMs for human modeling, outlining future research directions in this domain.


Literature Overview


Deep learning models have recently found extensive application in
simulating complex systems and analyzing agent behaviors *REF*. They have been employed in
various domains, including complex financial markets *REF*, as
well as urban driving scenarios *REF*. The advent of LLMs has created more
opportunities for modelling complex systems, especially focused on
human behaviors. Recent research efforts like *REF* have utilized LLMs to create
sophisticated agentbased simulators. These simulators contain LLM based agents 
that interact with others through natural language prompts carefully designed by incorporating world.


FORMULA


J.J. Horton *REF* demonstrated the ability of LLMs to mimic human preferences arising
from different qualitative personas. For example, given the
following scenario &quot;A hardware store has been selling snow shovels
for $15. The morning after a large snowstorm, the store raises the
price to $20.&quot;, the &quot;libertarian&quot; LLM found the increase
acceptable while for the &quot;socialist&quot; LLM the increase is unfair.
Also, recent work *REF* validates the ability of LLMs to model humans from a variety of subgroups.


This prompts us to consider whether LLMs can effectively capture
subrational human behaviors without the need for extra effort in
reward design *REF*, eventually supporting or replacing traditional agentbased models *REF*.


Instead of modelling an entire system using LLM agents, which is
expensive and harder to control, we use LLMs to generate a few
synthetic demonstrations for subrational behavior. Such demonstrations can be validated and used.


FORMULA


*REF*.


Learning Subrational Behavior using LLMs


We here discuss our framework to learn subrational policies using
LLM demonstrations. We first generate synthetic human demonstration
data prompting the LLMs, and then use IL to train agent policies based
on such demonstrations. We use Markov Decision Processes (MDPs) 
as a natural underlying decision model for an agent in temporal decision making
tasks. An MDP is a tuple M = (S, A, P, R) comprising: the state
space S; the action space A; a transition function P: S × A
→ ∆(S), where ∆(S) denotes probability distributions over state
space S; and a reward function R: S × A → R. A rational agent
follows policy π: S → ∆(A) that maximizes the expected reward:


FORMULA

While such a proxy reward can align the training of policies with
userdefined objectives, it cannot generate human behavior beyond what
is reflected in the reward function. For example, *REF* argues that subrational
human behavior can arise from incorrect beliefs of environment
dynamics rather than the reward function. Also, such a proxy reward
does not handle nonexponential discounting models, such as hyperbolic
or quasihyperbolic discounting. By directly updating the policy
network to mirror humanlike behavior generated through LLMs, our
methodology addresses these limitations, bypassing the dependence on
exponential MDPs. On the other hand, we note that our work requires
more careful generation of synthetic demonstrations covering the
state space of interest. A lacking of which coverage can be handled
using approaches combining RL and IL *REF*.


This new paradigm of generating synthetic human demonstrations using
LLMs could benefit the broader research community. LLMs can compensate
for the scarcity of expert demonstrations, which are extremely
useful to learn behaviors from *REF* and calibrate human subrationality models
proposed in psychological and economic studies *REF*.
Where γ ∈ *REF* represents the discount factor that is used to prioritize immediate rewards over those got later in time.


Three challenges of using direct RL to model human behavior are: 1)
the design of a reward function R that accurately captures desired
human behavior *REF*; 2) the large amount of data and training required before RL policies reach
reasonable performance *REF*; and 3) the inability to capture
timeinconsistent human preferences when using exponential
discounting of rewards e.g., preference reversals *REF*. We propose to mitigate
these challenges by using a small set of synthetic human (or expert)
demonstrations that are generated using LLMs. Such demonstration data
can overcome the need to design an intricate reward function as we can use IL.


Generation of LLM Demonstrations


The synthetic demonstration data comprises state action pairs over
numerous episodes D = ^n^, and is generated by
prompting the LLM using either chain of i=1 thoughts to create 
a reasoning path *REF*, or summarizing the state and possible actions
using prompt engineering *REF*. We report details of data generation 
in the experimental section where we use GPT4 *REF* due to its
expressivity. Demonstration data can massively accelerate the RL
training process when initialized with a policy learned using 
supervised learning on the data. Importantly, human agents may have
irrational behavior in specific scenarios (i.e., states) which may be
difficult to identify and model. LLMs have shown the ability to
reproduce such biased behaviors *REF* providing credible and novel
advantages to using such demonstrations to model subrational human
agents. Lastly, these demonstrations can be used to finetune
exponential RL policies to capture timeinconsistent human behavior,
which would have otherwise been intractable to model using standard RL.


Imitation Learning with LLM Demonstrations


Given demonstration data D, we can use IL to learn a policy πθ for the agent, formally defined as: ALGORITHM.


Algorithm *REF* reports the proposed procedure, where we
initialize the agent policy network πΘ, and in particular the value function Q(s, a). 
Then for each state s ∈ S we extract the distribution of agent actions in 
the synthetic LLM demonstrations, using a nonparametric Kernel Density Estimation. We
then update the policy πΘ by minimizing the difference between its
estimated action distribution and the one from synthetic
demonstrations. While more sophisticathed algorithms can be employed
(e.g., *REF*) we adopt a simple algorithm as our goal is to
study the application of LLMs in generating synthetic human
demonstrations, rather than to propose a novel IL algorithm. Note that
pure IL like above requires demonstrations encompassing the entire
state space. If not, one could start with learning the policies using
an RL training phase followed by IL, similarly to *REF*.


Experiments


In this section, we test the ability of our framework to simulate
human behaviors in four economic games where humans demonstrate
subrational behavior, and show that we can replicate findings from
existing literature. We use GPT4 *REF* as the current most expressive model, and a
3layer neural network for the Q value function. We consider the following games:
- The Ultimatum Game *REF*, a game theory experiment where a player decides
 how to split an amount of money, and another player can accept or reject the split.
- The Stanford marshmallow experiment *REF* where a player is offered a choice 
between one small but immediate reward, or a bigger reward if they waited for a period of time.
- The Double or nothing gamble where a player is offered a choice to double their money or lose everything, in order to
illustrate human decisions under risk as described by prospect theory *REF*.
- The Procrastination experiment *REF*, where a student player chooses which day to
write a report and miss a good movie given an academic deadline.


Ultimatum Game


Scenario.


The ultimatum game considers two players splitting a
given amount of money T. The first player (the proposer) decides to
split the money between himself and the responder. The responder (who
knows the total sum) decides to either accept or reject the split. If
the responder rejects the proposal, both players receive nothing; if
the responder accepts, they both get the agreed amount of money. For
example, the proposer decides to split T = *MATH* 2 to the
responder and taking $8 for himself; if the responder accepts they
get *MATH* 2, respectively. Otherwise they both get nothing.


Social Preferences and Limited Rationality. 


If we assume that agents have stable, welldefined, and rational preferences, we realize
that the theory for the simple ultimatum game is rather obvious: a
rational responder would accept any nonzero proposal. In fact, even
if the proposal is unfair, getting something is better than getting
nothing. This represents a Nash equilibrium for the ultimatum game. In
reality though, human behavior is much different. The work of Nobel
laureate Richard Thaler *REF* focuses on some &quot;anomalies&quot; in the ultimatum game, defined as
demonstration of limited rationality and social preferences by human
participants. They show that a human responder will likely reject
unfair proposals, as they prefer to &quot;punish&quot; the proposer, even
though they will receive nothing. According to their own social
preference, people will prefer &quot;fair&quot; (i.e., 50:50) splits *REF*, or in general they often
reject offers less than 30% of the initial sum T *REF*.


Experimental Setting. 


We consider an initial amount of T = $10 for the game. Therefore, the game has 10 possible actions
for the proposer x ∈, where x is the amount to
offer and (10 − x) is the amount they keep for themselves.
Accordingly, each offer is a possible state for the responder who has
10 states and 2 actions a ∈{accept, reject}. In this experimental setup, we focus on modelling 
the human responder, which we consider the more interesting player
in the game. We model the proposer using a MultiArmed Bandit *REF*; and use an RL policy
network for the responder. The two agents are trained together using
as reward the amount of money they individually get. We alternately
use IL with LLM demonstrations for a human responder or one that cares about a fair split to obtain the responder policy.


FIGURE


FIGURE


Synthetic demonstrations collected using LLMs. 


We instruct GPT4 as &quot;Impersonate Jerry who cares a lot about fairness.&quot; or
&quot;Impersonate a human called Jerry&quot;. We call the two experiments
&quot;Fair&quot; and &quot;Human&quot;, respectively. We then prompt the model as:
John receives $T, and he proposes to offer Jerry x and keep Tx for
himself. If Jerry accepts they both get the agreed amount, but if Jerry rejects they both receive nothing. Continue the sentence: &apos;Jerry decides to&apos;,
where T = 10 and x ∈. We then convert each prompt and answer into a stateaction (s, a) pair. We generate 10 synthetic observations for each state x.


Results.


Figure *REF* shows rewards and distribution of money kept by the proposer. Figure *REF*.(a) shows classic
RL training for both agents, where the proposer learns to split the
money in an unfair manner, and the responder accepts any splits. In
fact, this is the expected outcome from a fully rational responder. In
Figure *REF*.(b) we use LLM demonstrations to train a
&quot;human&quot; responder model, which now rejects very low (i.e., less than
20%) splits, and thus the two agents converge to a *MATH* 2 split.
In Figure *REF*.(c) we use LLM demonstrations to train a
&quot;fair&quot; responder model, which now rejects unfair (i.e., unbalanced)
splits, and thus the two agents converge around a 50:50 split.
Finally, Figure *REF*.(d) reports the distributions of
money the proposer keeps for the three variants.


Comparison to human subject studies. 


We compare our results with those in economic literature in *REF*. These works show
that a human responder is less likely to accept offers below *MATH* 10. This is inline with our results confirming that
synthetic LLM demonstrations could be effectively used to model human
interactions, as is also highlighted in recent work *REF*. Importantly, LLMs enable
the simulation of a broader range of scenarios without any additional costs (e.g., *MATH*).


Stanford Marshmallow Experiment.


Scenario. 


The Stanford Marshmallow Experiment is a classic psychological study that explores delayed gratification in children.
In this experiment, a child is placed in a room with a tempting
marshmallow or a similar treat. The researcher presents the child with
a choice: they can either eat the marshmallow immediately or wait for
a predetermined amount of time (usually 15 minutes) without eating
it. If the child can resist the temptation and wait until the time
elapses, they are rewarded with an additional marshmallow. This
experiment is used to investigate the ability of children to delay
their desires and make choices based on longterm benefits rather than immediate gratification.


SelfControl and the Stanford Marshmallow Experiment.


If we assume that children have strong selfcontrol and can easily delay
gratification, the theory behind the Stanford Marshmallow Experiment
might seem straightforward: a child with impeccable selfdiscipline
would wait patiently for the delayed reward, in this case, an extra
marshmallow. In this ideal scenario, the child would understand that
waiting for the second marshmallow is a more desirable outcome than
consuming the first one immediately, representing a form of
selfcontrol equilibrium in this experiment. In reality, however,
the behavior of children in the marshmallow experiment is quite
different. The work of Walter Mischel *REF* uncovered what could be
considered &quot;anomalies&quot; in human selfcontrol and decisionmaking.
They demonstrated that many children struggle with selfcontrol and
are unable to resist the immediate temptation of eating the first
marshmallow, even though waiting would lead to a greater reward. In
particular, such work revealed that children&apos;s selfcontrol abilities
vary, and their choices in the experiment are influenced by factors
like age, environment, and individual differences. While incorporating
these factors into RL is nontrivial, obtaining human demonstrations
for very specific settings can be expensive or impossible, and
synthetic demonstrations from LLMs could overcome these challenges.


Experimental Setting. 


We focus on modelling a child aligned with the psychological game in this setup. 
We consider a single agent with 2 actions a ∈ for the candy in the current state. 
We consider one state where the agent can choose to accept the candy now or wait, and
two subsequent states derived by each choice. 
With initial LLM demonstrations, we are able to reproduce behavior of a 2year old child 
who takes one candy immediately, or a more rational 5year old child that waits for more candies.


FIGURE


FIGURE


Synthetic demonstrations collected using LLMs.


We instruct GPT4 as &quot;You are Janice a Y years old child.&quot;. We prompt the model as:
Janice is offered to get one candy now, or to wait for 2 more hours
to eventually get two candies. Continue the following sentence &apos;Janice decides to&apos;, where Y ∈.
Interestingly, we noticed that with &apos;15 minutes&apos; waiting like the original experiment, 
the model is more inclined to wait (Appendix *REF*), possibly indicating
different human preferences w.r.t. the original paper dated more than 50 years ago. We generate 10 synthetic observations for each age Y.


Results. 


In Figure *REF*.(a) we show a classic RL approach trained until convergence, where the reward is the amount of
candies the agent gets. The agent is completely rational, and it
learns to always wait to get two candies. Most important, it is very
hard to model different background factors for the agent, like age.
For example, can we derive an RL policy for a child of 2 years vs a
child of 5 years? A common approach could be myopic RL modelling shown
in Figure *REF*.(b) where we modify the discount factor γ
to let the agent being more myopic (i.e,. looking only for immediate
rewards). In the setting with γ = 0.3 the agent chooses to take
the candy immediately, with a lower reward. Even if we can use
myopicity modeling, it is not clear how to relate the value γ with
the age of the child. Instead, an LLM could be easily prompted to get
precise synthetic human observations. Figure *REF*.(c)
shows that if we directly train an IL policy using


Comparison to human subject studies. We compare our results with
the study of waiting behavior of nursery school children by W. Mischel
*REF* comprising 16 boys and 16 girls, with age ranging from 3 to 8 years. In this
experiment, they found that most children do not wait, when both
rewards are visible to them; while around 75% of children wait if both
rewards having been removed from their sight. Our results show a
possible reconstruction of this experiment: a 2year old child does
not wait, while a 5year old child is more rational and waits; with a
mix of both actions for a 3year old child (details in the appendix).
These findings are aligned with the aforementioned study which
suggests that the capacity to wait for longterm goals is develop
markedly at about ages 34. Nonetheless, we also note that the
original experiments are from 50 years ago, making current human
biases and needs very different (e.g., children may have easier access to treats now).


Double or Nothing Gamble


Scenario. 


We create a simple economic game named &quot;double or
nothing&quot; to illustrate human bias in decisionmaking under risk.
Assume that two people are gambling, and person A has lost an initial
bet of $5 against person B. Before the initial bet is paid, person A
(or B) can choose to play a second bet of $5 with a higher
probability (&gt; 50%) of losing (or winning). Since the second bet is
an unfair game that favors the winner (person B), a rational agent
will always deny the second bet as person A or accept the second bet
as person B. However, humans in real life are willing to take the risk
of the second bet as person A (hoping to offset their loss from the
previous bet), or deny the second bet as person B (afraid of losing their current reward).


Prospect theory. 


The expected utility theory is a fundamental
model of human decision making under uncertainty, which says that an 
agent chooses between risky prospects L based on expected utility: FORMULA, where pi is
the i=1 probability of an outcome with payoff xi. RL aligns with this hypothesis as the value of a state is determined by the expected
utility of next states weighted by their transition probabilities.
But, a substantial body of evidence has shown that human decisions
systematically violate this theory. To model such deviation, Kahneman
and Tversky introduced prospect theory *REF* which has been widely
used for explaining experimental findings on human risk taking over
several decades *REF*. The model consists of two key elements: (1) a biased value function
v(xi) that is concave for gains, convex for losses, and steeper
for losses than gains, and (2) a nonlinear probability weighting function
w(pi) which overweights small probability and underweights large
probability. Humans with prospect bias are riskaverse over gains and riskseeking over losses.


TABLE


TABLE


Experimental Setting. 


We create the &quot;double or nothing&quot; gamble environment with unfairness factor of the second bet ϵ ∈. The winner of the first
bet has probability to win the second bet against the loser. We train both players separately. 
Given ϵ as the state, the agent takes an action a ∈ to decide whether or not to add the second bet.
A rational RL agent learns to maximize the expected reward from the two bets U = L (a) Winner Rewards 
(b) Loser Rewards (c) Prob. to accept strations yield similar (lower) rewards which are subrational e.g.,
when ϵ = 0.1, both are not willing to take the risk as winner, but prefer to take the additional bet as loser.


FIGURE


Comparison to human subject studies. 


We compare behavior generated by the IL policy using LLM demonstrations to the RL policy using the
prospect theory model specified by parameters estimated from real humans in the literature *REF*.


FORMULA


Synthetic demonstrations collected using LLMs. 


We prompt GPT4 as follows:
1) You won an initial $5 bet against Tom. Before you collect your reward, you can choose to add a second bet of 
$5 with probability to win. If you win the second bet
you will double your gain from the initial bet, but if you lose you will gain nothing. Continue the following sentence &apos;you decide to&apos;;
2) You lost an initial $5 bet against Tom. Before you pay your
debt, you can choose to add a second bet of $5 with probability to win. 
If you win the second bet you will recover your loss from the initial bet, but if you lose you will double your loss. 
Continue the following sentence &apos;you decide to&apos;.


We collect 10 demonstrations for each ϵ ∈ and show the probability
of accepting the second bet as the winner/loser in Table *REF*.


Results. 


Table *REF* shows the probability of adding
the second bet in LLM demonstrations. In general, we observe that
the loser (winner) of the first bet is more (less) willing to take the
risk of playing an additional bet, unless the unfairness of the second bet is large enough (ϵ ≥ 0.3). 
We compare an IL policy trained over LLM demonstrations with a rational RL policy and the prospectbiased RL policy in Figure
*REF*. Since the second bet always favors the winner, the
rational RL agent learns to maximize their reward by accepting it as
winner and rejecting as loser. On the other hand, agents trained using prospect theory and 
LLM demon in gains and riskseeking behavior in losses. These results highlight
the simplicity of our framework in incorporating subrational and
biased behavior into agent policies without the need to design intricate or adhoc reward functions.


Academic Procrastination with Deadlines


Scenario. 


A student needs to spend one day to write a course
report due within the next H days. They receive a credit for the
submitted report on the last day. The theatre is playing movies in
increasing order of quality over the next H days. Which day does the
student choose to write the report and miss the movie? Let ct
represent the cost of missing a movie on day t ∈ so that 0 &lt;
c1 &lt; c2 · · · &lt; cH. And, R &gt; 0 is the reward received
upon submitting the report on day H. This setup is based on the example on salient costs in *REF*.


Procrastination and QuasiHyperbolic Discounting. 


Studies have shown that a large fraction of students procrastinate on their
academic deliverables until the deadline gets very close *REF*. This is an example for the lack of selfcontrol
observed in humans: they choose to watch a movie today with the
expectation that their future self would write the report tomorrow.
But when tomorrow arrives, the same decision repeats until the
deadline gets very close. Sophisticated humans who are aware about
their lack of selfcontrol expect their future selves to
procrastinate and complete the report earlier than naive humans who
are optimistic about their future selves *REF*. Such behavior is modeled by quasihyperbolic
discounting *REF* where the value function in a state st is
given by &apos;shortterm impatience&apos; representing the preference over immediate
rewards today vs tomorrow *REF*. With the assumption of exponential discounting
(as in standard RL), one arrives at timeconsistent preferences.
Timeinconsistent preferences are those where the optimal policy from
today&apos;s perspective conflicts with the optimal policy from tomorrow&apos;s
perspective *REF*. While it is unclear how to modify standard RL algorithms (that rely on the
Bellman equation) to arrive at this timeinconsistent human behavior
modeled by quasihyperbolic discounting, we show next that such
behavior can be generated through IL using synthetic human demonstrations from LLMs.


FORMULA


FIGURE


Experimental Setting. Let the deadline be H = 4 with the theatre
playing a mediocre movie on day 1, a good movie on day 2, a great
movie on day 3 and an excellent movie on day 4. We collect LLM
demonstrations for three types of students as categorized by their
Grade Point Average (GPA). As a baseline for comparison, we use a
policy learned using RL with costs c1 = 1, c2 = 2, c3 = 4,
c4 = 7 and final reward R = 14 with exponential discount factor γ = 1.


Synthetic demonstrations collected using LLMs. We instruct GPT4
as &quot;You impersonate a student with GPA x, who loves watching movies.
GPA measures the students commitment towards their academics.&quot;. We
then prompt the model as: &quot;The theatre has a line up of increasingly
better movies as days pass. The student has a deadline to submit a
course report within the next four days. The student needs to pick one
day to write the report, which means they will miss the movie on that day. 
Continue the sentence &apos;The student writes the report on day &apos;&quot;, where x ∈ {1, 3, 4.5}.


We collect 10 demonstrations for each GPA x as shown in Figure *REF*.(a), where we see that the probability of
writing the report on the first day increases as the GPA increases.


Results. 


Figure *REF*.(b) shows the training rewards
for an RL policy using the above cost and reward setting. A histogram
of the day of writing the report when using the trained RL policy is
plotted in Figure *REF*.(d). Observe that exponential
discounting with γ = 1 says it&apos;s optimal to write the report on day 1 
since it picks t that maximizes R−ct. Thus, the RL policy always corresponds 
to writing the report on day 1. On the other hand, the LLM demonstrations can be used to train a policy
by IL, whose rewards are plotted in Figure *REF*.(c) with
the day of writing the report in Figure *REF*.(d). Note that when LLM demonstrations are used, 
they give rise to one policy per student type/GPA. Observe from Figure *REF*.(c)
that the rewards at the end of training are lower for lower GPA
values, and improve as GPA increases. Since the cost of writing the
report increases with the day of writing it, this says that the
learned IL policies give earlier days of writing the report as GPA
increases. The same can be observed in Figure *REF*.(d)
where while the RL policy without any LLM demonstrations corresponds
to writing the report on the first day, the IL policies with LLM
demonstrations correspond to writing it on later days for lower GPAs.
Thus, while the RL policy gives time consistent actions, incorporating
LLM demonstrations helps us to capture procrastination, while
accounting for differences in student attributes such as GPA.


FIGURE


Comparison to human subject studies. 


We compare our results to a study of procrastinating behavior among 379 undergraduate students in
an introductory psychology course *REF*. The study examines the extent of prevalence of
procrastinating behavior, and the relationship between academic
procrastination and academic performance. The authors found a positive
correlation between selfreported procrastination and delay in
taking selfpaced quizzes, and a negative correlation between
procrastination and grade point average. Our findings firstly
capture procrastination with the help of LLM demonstrations, and
secondly are in line with this study since we observe higher delays in
writing the report for students with lower GPAs. However, we note that
this relationship between GPA and academic delay requires deeper
investigation as remarked in *REF* since the observations are based on correlation and not causation.


Discussion and Broader Context


In this work, we investigate the utility of foundation models to
synthesize subrational human decision making behavior, that deviates
from perfectly rationality *REF*. Our work is motivated by the need to capture
subrational and biased human behavior across disciplines ranging from
economics of households *REF* to financial markets *REF*, alongside the difficulty in
obtaining historical data for the same to design calibrated
subrational human models *REF*. We propose a framework to generate synthetic
human demonstration data for a variety of human subgroups using
LLMs, that is subsequently used with IL to derive behavioral policies
for each human. Such demonstration data has potential to be used to
calibrate subrational human models while being costeffective and
efficient to generate as opposed to real human studies that cover
small subsets of the human population. This data is also useful when
trying to capture human behavior that is hard to describe through
the use of a reward function *REF*, and in cases where exponential discounting does
not hold *REF*. Calibrated human models can improve realism of agentbased models of
real economic systems, that can be subsequently used for policy applications *REF*. We ground our framework
by comparing our experimental findings in four economic games with
real human studies in the literature, where we are able to reproduce similar findings.


As we advocate for further research in this field utilizing foundation
models to capture subrational behavior in a wider class of problems
beyond the games that we modeled in this work, we now discuss
challenges and potential limitations to the use of LLMs for the same.
As for most applications using LLMs, we share some of the existing
limitations such as the requirement for very careful prompt
engineering (e.g. we end our prompts saying &apos;continue the sentence&apos;
so as to limit responses by relevance), which may even introduce 
unintentional user biases into the system. Prompt engineering is
particularly important when dealing with complex, temporal scenarios
that are common in economics when translating the agent states into
natural language. This is also accompanied by their limited ability in
handling numbers that can reduce their efficiency in such scenarios
*REF*. Also, ever since they rose to popularity, LLMs have seen constant updates accompanied by
guardrails that may prevent the reproducibility of their responses.


Based on the assumption that LLMs are computational *REF* models of humans, memorization of training data by LLMs is
not a limitation, as it enables us to produce humanlike behavior
with more fidelity. That said, LLM responses could be sensitive to
protected attributes of agents like age, gender, ethnicity making it
challenging to differentiate the effects of biases in training data vs hallucinations. 
It is also unclear if one can trust the demonstrations generated by LLMs in unseen
scenarios (e.g. different ages than those seen in the marshmallow
experiment). We perform ablation studies by modifying the prompts for
the Stanford marshmallow experiment to test for reasoning (or
memorization) in the appendix. Further, evaluation of LLM responses
for their humanness or quality can be subjective, requiring human
expert knowledge. In our work, we are able to reproduce similar
behavior as was observed in real human studies, some of which were
conducted over 50 years ago *REF*. It is especially intriguing
to think of the evolution of LLMs in the coming years as they are
subsequently trained and finetuned on more and more LLM generated data!


To conclude, our work shows an initial exploration of the use of LLMs
for modeling subrational behavior demonstrating great potential with
possible broader applications. We believe that LLMs have the
capability to emerge as indispensable and costeffective tools for
modeling subrational behaviors, and help decision makers in different
scenarios. This as long as their use is accompanied by a careful
consideration of the challenges, limitations and potential biases.