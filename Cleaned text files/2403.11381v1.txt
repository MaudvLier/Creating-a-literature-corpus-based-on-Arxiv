Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot


Introduction 


The increased presence and relevance of AI agents within everyday spheres such as self-driving vehicles and customer service necessitates these entities being equipped with the appropriate capabilities to facilitate cooperation with humans and its AI counterparts. While noteworthy strides have been made in advancing individual intelligence components within AI agents, expanding the research focus to enhancing their social intelligence -- the ability to effectively collaborate within group settings to solve prevalent problems -- is now timely. This pivot aligns with the rapid progression of AI research presenting fresh prospects for fostering cooperation, drawing on insights from social choice theory and the development of social systems [*REF*].


Research into AI agents presents an avenue for generating intelligent technologies that embody more human-like features and are compatible with humans, a far cry from solipsistic approaches that overlook agent interactions. A promising illustration of this approach is the Melting Pot, an AI research tool designed to foster collaborative efforts within multi-agent artificial intelligence via canonical test scenarios. These environments emphasize non-trivial, learnable, and measurable cooperation by pairing a physical environment (a &quot;substrate&quot;) with a reference set of co-players (a &quot;background population&quot;) [*REF*].
The environments fostered interdependence between the individuals involved.


Research on AI agents has also recently been permeated by the leaps and bounds of Large Language Models (LLMs). The increasing success of LLMs encourages further exploration into LLM-augmented Autonomous Agents (LAAs). LAAs is an avenue of research that is still emerging, with limited explorations currently available [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Something common in these works is that a clear need is established. To achieve success, LAAs have to rely on an architecture that can recall relevant events, reflect on such memories to generalize and draw a higher level of inferences, and utilize those reasonings to develop timely and long-term plans [*REF*].


These architectures offer specialized modules for specific tasks, utilizing meticulously crafted prompts and flows to perform complicated tasks and navigate intricate environments. Human behavior replication has been observed in some of these architectures, notably by Park et al.
[*REF*], who managed to create convincingly realistic human behavior in simulated environments. Similar frameworks utilized by Voyager [*REF*] enabled an agent to navigate the Minecraft world and independently develop tools and skills. MetaGPT [*REF*] introduced a framework allowing for the generation of fully functional programs, simulating a business-like environment with distinct roles and predefined agent interactions.


Despite significant advancements in the field, the potential for cooperative abilities in (LAAs) has been somewhat neglected in current research. These capabilities could, however, be paramount in empowering these agents to perform innovative tasks and succeed in complex environments. This study represents an initial exploration into the inherent cooperative capabilities of LAAs. We employ an evaluation framework that includes a communication interface of scenarios from the Melting Pot project [*REF*] (in which artificial agents co-exist in environments where social dilemmas can arise), the recent architecture proposed by Park et al. [*REF*], as well as reference Large Language Models (LLMs) such as GPT4 and GPT3.5. Our results hint towards the capability for cooperative behavior, based on simple natural language definitions and cooperation metrics tailored to the chosen Melting Pot scenario. While the agents showed a propensity to cooperate, their actions did not demonstrate a clear understanding of effective collaboration within the given environment. Consequently, our analysis underscores the necessity for more robust architectures that can foster better collaboration in LAAs.


In summary, our contributions are as follows:


- Adapting the Melting Pot scenarios to textual representations that can be easily operationalized by LLMs.
- Implementing a reusable architecture for the development of LAAs employing the modules proposed in [*REF*]. This architecture includes short and long-term memories and cognitive modules of perception, planning, reflection, and action.
- Implementing &quot;personalities&quot; specified in natural language, making it clear to the agents whether they should be cooperative or not.
These descriptions are intended to discern, based on their pre-training knowledge, what they perceive as cooperation in an unfamiliar context.
- Evaluating LLM-mediated agents in the &quot;Commons Harvest&quot; game of Melting Pot using our architecture in different scenarios where we specify or not, through natural language, the personality of the agents.
- Discussing the results in terms of cooperativity metrics associated with the &quot; Commons Harvest&quot; game, the limitations of the used architecture, and the proposal of an improved architecture that fosters better cooperation among LAAs.


Related Work


Agent architectures have evolved to address the limitations of traditional LLMs, equipping them with diverse tools for autonomous operation or minimal human oversight.


A notable challenge with LLMs is their susceptibility to hallucinations and gaps in knowledge regarding recent events or specific subjects. Such constraints diminish their practicality, as they remain confined to the information acquired during training without the capability to assimilate new data. To address this issue, Shick et al. [*REF*] introduced an early solution named Toolformer. This model was trained to discern when and how to invoke APIs (tools) to enhance the LLM&apos;s performance across various tasks. The dataset was self-supervised, with API calls incorporated only when they positively impacted the model&apos;s performance. This methodology empowered the model to determine the relevance and optimal execution of API calls.


However, for executing more intricate tasks, merely invoking tools may be insufficient. Toolformer lacks the capability to reason about the rationale behind API calls and does not receive comprehensive environmental feedback to guide its subsequent actions toward achieving a goal. Recognizing this gap, the prompt-based paradigm ReAct [*REF*], developed by Yao et al., integrates reasoning with action. By providing contextual prompt examples, ReAct guides the LLM on when to engage in reasoning and when to act, resulting in enhanced performance compared to approaches that employ reasoning or action in isolation. Furthermore, to enable the agent to learn from its errors, Shinn et al. [*REF*] expanded the ReAct framework by incorporating a self-reflection module.
This addition offers verbal feedback on past unsuccessful attempts, facilitating performance enhancement in subsequent trials.


Conversely, drawing inspiration from the emulation of authentic human behavior, Park et al. [*REF*] devised an intricate agent framework. This architecture boasts a cognitive sequence structured around modules primarily anchored by diverse prompts. Leveraging distinct prompts optimizes LLM performance, enabling specialized techniques for specific tasks. Notably, memory holds a pivotal position within this framework, preserving the agent&apos;s experiences and insights.
The ability to retrieve these memories diversely amplifies their utility across multiple objectives. Moreover, Wang et al. developed the Voyager architecture [*REF*], enabling autonomous gameplay in Minecraft. This advanced framework empowers the agent to autonomously curate a discovery agenda. Remarkably, the architecture can also create its own APIs, write corresponding code, verify API functionality, and store it in a vector database for future utilization.


More recently, efforts have been directed towards enhancing agent performance through multi-agent frameworks that utilize different instances of LLMs to independently perform roles or tasks. Du et al.
[*REF*] demonstrated this through a framework designed to engage different LLM instances in a debate, aiming to improve the factuality and accuracy of the responses. Subsequently, Hong et al.
further capitalized on the potential of multiple agents. They allocated specific roles to each agent, accompanied by a sequence of predefined tasks with clear input and output expectations. These tasks establish a structured interaction pathway between agents, enabling them to achieve user-defined objectives. Demonstrating its efficacy in software-related tasks, this framework, inspired by the operational dynamics of conventional software companies, attained state-of-the-art performance in the HumanEval and MBPP benchmarks.


Similarly, Liu et al. [*REF*] introduced the BOLAA framework. This system orchestrates multi-agent activity by defining specialized agents overseen by a central controller. The controller&apos;s role is pivotal: it selects the most suitable agent for a given task and facilitates communication with it. Additionally, Zhang et al.
[*REF*] delved into multi-agent architectures, exploring the influence of social traits and collaborative strategies on different datasets.


Methodology 


Experimental setup 


Environment: This paper utilizes a scenario sourced from Melting Pot [*REF*], a research tool developed by DeepMind for the purpose of experimentation and evaluation within the realm of multi-agent artificial intelligence. The scenarios within Melting Pot are specifically crafted to establish social situations in which the ability of the agents to solve conflict is challenged and characterized by significant interdependence among the involved agents.


FIGURE 


In the course of our experiments, we selected the &quot; Commons Harvest&quot; scenario. In this scenario, agents with unsustainable practices can lead to situations where resources are depleted. This is known as the tragedy of the commons. This scenario is structured around a grid world featuring apples, each conferring a reward of 1 to agents. The regrowth of apples is subject to a per-step probability determined by the apples&apos; distribution in an L2 norm with a radius of 2. Notably, apples may become depleted if there are no other apples in close proximity. Fig.
[1] provides a visual representation of this custom-designed scenario, illustrating the presence of 3 LLM agents and 2 bots.


The LLM agents possess the capacity to execute high-level actions in each round. These actions include `immobilize player (player_name) at (x, y)`, `go to position (x, y)`, `stay put`, and `explore (x, y)`. They enable the agents to zap other players, navigate to predefined positions on the map, stay in the same position, and explore the world respectively. On the contrary, bots, characterized as agents trained through reinforcement learning, perform one movement for every two movements made by any of the LLM agents. The policies governing the bots lead them to engage in unsustainable harvesting practices and instigate attacks against other agents in close proximity.


In general, maximizing the welfare population for this scenario would require the LLM agents to restrain themselves from eating the last apple in each of the apple trees, and to attack the bots or agents that take the apples in an unsustainable way to avoid the depletion of the apples.


Simulation: In a simulation, each episode of the game involves the participation of a predetermined quantity of LLM agents and bots. The LLM agents take a high-level action on their turn and proceed to execute it until all three LLM agents have completed their respective high-level actions. Meanwhile, the bots are in constant motion, executing a move for every two moves made by any of the agents (note that a high-level action typically comprises more than one movement). The simulation concludes either upon reaching a maximum predetermined number of rounds (100 typically) or prematurely if all the apples in the environment are consumed.


Adapting the environment to LLM agents


The Melting Pot scenarios consist of several two-dimensional layers accommodating various objects, each with its own custom logic. While initially, a matrix with distinct symbols seemed the most intuitive way to communicate the game state to the LLMs, it proved challenging for LLMs like GPT3.5 or GPT4 to interpret and reason about the spatial information provided by the position of objects in the matrix. To address this issue, we opted to develop an observation generator tailored to this particular environment. In this generator, every relevant object receives a natural language description, supplemented by coordinates expressed as a vector *MATH*, denoting row and column respectively. Moreover, some relevant state changes are captured while an agent waits for its turn, and these changes are also captured and communicated to the agents. The complete list of descriptions generated for the objects and events of this environment is shown in Appendix [9].


LLM agent architecture 


The design of the LLM agents predominantly drew upon the Generative Agents architecture [*REF*]. This choice was motivated by its comprehensive nature, positioning it as one of the most versatile architectures for agents that could be readily tailored to various tasks. While the Voyager architecture [*REF*] also presented a viable option, its efficacy was somewhat limited due to its inherent inflexibility. Voyager constructs agent actions dynamically during gameplay, involving the generation and validation of code to execute actions in the environment. In the context of our specific case, it was deemed preferable to externalize actions from the architecture to enhance simplicity.


Fig. [2] illustrates the flow diagram outlining the process through which an agent initiates an action. Each action undertaken by LLM agents entails a comprehensive cognitive sequence designed to enhance the agent&apos;s reasoning capabilities. This sequence involves the assimilation of feedback from past experiences and the translation of its objectives into a viable plan, enabling the execution of actions within the environment. This architectural framework is in a perpetual state of environmental sensing, generating observations that empower the agent to respond effectively to changes in the world.


FIGURE


Memory structures 


This agent architecture employs three distinct memory structures designed for specific functions:


Long-Term Memory: This repository stores observations of the environment and the various thoughts generated by the agent in its cognitive modules. Leveraging the ChromaDB vector database, memories are stored, and the Ada OpenAI model generates contextual embeddings, enabling the agent to retrieve memories relevant to a given query.


Short-Term Memory: To facilitate rapid retrieval of specific memories or information, a Python dictionary is utilized. This dictionary stores information that must always be readily available to the agent, such as its name, as well as data that undergoes constant updates, such as current observations of the world.


Spatial Memory: Given the agent&apos;s navigation requirements in a grid world environment, spatial information becomes pivotal. This includes the agent&apos;s position and orientation. To support effective navigation from one point to another, utility functions are implemented to aid the agent in spatial awareness and movement.


Cognitive modules 


Perception module: The initial stage in the cognitive sequence is the Perception Module. This module is tasked with assimilating raw observations from the environment. These observations serve as a comprehensive snapshot of the current state of the world, offering insights into the items within the agent&apos;s observable window.


To optimize processing efficiency, the observations undergo an initial sorting based on their proximity to the agent. Subsequently, only the closest observations are channeled to the succeeding cognitive modules.
The parameter governing the number of observations passed is denoted as `attention_bandwidth`, initially configured at a value of 10.


Following this, the module undertakes the responsibility of constructing a memory, destined for long-term storage. An illustrative memory example is outlined below:


FIGURE


Ultimately, the Perceive module determines whether an agent should initiate a response based on the current observations. During this stage, the agent assesses its existing plan and queued actions to ascertain their suitability. It evaluates whether it is appropriate to proceed with the current course of action or if the observed conditions warrant the development of a new plan and the generation of corresponding actions for execution. The complete prompt is shown in Appendix [11].


Planning module: This module comes into play once observations have been sorted and filtered. The Planning modules leverage the amalgamation of current observations, the existing plan, the contextual understanding of the world, reflections from the past, and rationale to meticulously craft a newly devised plan. This plan intricately outlines the high-level behavior expected from the agent and delineates the goals the agent will diligently pursue. For the complete prompt refer to Appendix [12].


Reflection module: The Reflect module is designed to facilitate profound contemplation on observations and fellow agent thoughts at a higher cognitive level. Activation of this module is contingent upon reaching a predetermined threshold of accumulated observations. In our experimental setup, reflections were initiated every 30 perceived observations, roughly translating to three rounds in the game. The Reflect model comprises two key stages:


1. Question Formulation: In the first stage, the module utilizes the 30 retained observations to formulate the three most salient questions regarding these observations.
2. Insight Generation: The second stage involves utilizing these questions to retrieve pertinent memories from long-term memory.
Subsequently, the questions and retrieved memories are employed to generate three insights, which are then stored as reflections in the long-term memory.


The retrieval of relevant memories employs a weighted average encompassing cosine similarity, recency score, and poignancy scores. The recency score is computed as *MATH*, where *MATH* denotes the number of hours since the last memory was recorded. Meanwhile, the poignancy score reflects the intensity assigned to the memory at its point of creation.
Throughout the experiments, a uniform poignancy score of 10 was assigned to all memory types. For the complete prompt and more details on this module, refer to Appendix [13].


Action Module: This module plays the role of generating an action for the agent to undertake. As detailed in Appendix [14], the selection of the action is determined by the Language Model (LLM), which considers the agent&apos;s comprehension of the world, its current goals and plans, reflections, ongoing observations, and the available valid actions within the environment.
The creation of new action sequences occurs under two conditions: when the current sequence is empty or when the agent is responding to observations. For this prompt, we manually crafted a reasoning structure, similar to those described in Self-Discover [*REF*] to help the LLM consider different alternatives, and evaluate them before making the final decision.


Evaluation scenarios 


To assess the outcomes, we utilized the per capita average reward of the focal population as our primary metric. The focal population comprises LLM agents, and the chosen metric aligns with the Melting Pot framework&apos;s approach [*REF*], which evaluates population welfare.
We compare this metric across two sets of scenarios.


The first set of scenarios has the intention of measuring how the personality given to the agents affects the agents&apos; welfare. For this purpose, we prepared five scenarios:


::: inparaenum as a baseline we do not give the agents any personality specifications (Without personality),


we tell the agents to be cooperative (All coop.),


we tell the agents to be cooperative and provide a short description of how to be cooperative in the chosen scenario (All coop. with def.),


we tell the agents to be selfish (All selfish),


we tell the agents to be selfish and provide a definition with the expected behavior of someone selfish for the given scenario (All selfish with def.).
:::


The second set of scenarios is more challenging as the competition increases by reducing the number of trees and apples, modifying the agents&apos; initial social environment understanding, or by adding other entities to the environment (bots), these changes demand a deeper understanding from the agents and swift reactions to master the scenarios. More concretely, the


::: inparaenum first three scenarios consist of an environment where there are three agents and only one apple tree. Each scenario differs in the personality given to the agents:


All coop,


All selfish, and


without personality. The last scenario of the second set


has the same base configuration, but there are two agents and two bots, where the bots are reinforcement learning agents that were trained to harvest in an unsustainable way and to attack other agents. These bots make part of scenario 0 of the commons harvest open scenario described in Meltingpot 2.0 [*REF*].
:::


We also add a scenario aimed at demonstrating how the information an agent has about the rest of the agents can influence their behavior. In this scenario, the environment begins with the same number of trees; however, from the start of the simulation, each agent was informed that among them, one was acting entirely selfishly, representing a risk due to their unsustainable consumption.


For all the experiments, the agents receive information about the environmental rules. They are aware that the per-step growth probability of apples is influenced by nearby apples and that green patches can be depleted if all apples within them are consumed. However, the agents lack information about what is the optimal policy for each scenario, and are unfamiliar with bots and other situations in the game. The complete world context that is given to the agents is shown in Appendix [10].


Ten simulations for each scenario were done in which the LLM agents were powered by the GPT3.5 from the OpenAI API for the majority of modules, and GPT4 powered the act module. On another hand, the Ada model was used to create contextual embeddings of the memories. A detail of the simulations cost is available in the Appendix [15].


Results 


Impact of personality in population welfare


The average per capita reward obtained for the first set of scenarios is shown in Fig. [3]. The best-performing simulations were the ones where no particular personality description was given to the agents, followed by the scenarios where the agents were instructed to be selfish. Surprisingly, the scenarios where the agents were told to be cooperative had the worst performance. On further analysis, we found that these results are explained mainly by the number of times the agents decided to attack other agents (see Fig. [4]).


FIGURE


To gain a better understanding of the agents&apos; behavior, we recorded the number of times the agents decided to attack other agents, and the number of times these attacks were effective. These actions play an important part in the game because they are the only mechanism the game provides to directly interact with other agents. Therefore, helping the agents counteract other agents&apos; behavior, such as attacking other agents that are taking apples indiscriminately to avoid the depletion of the apple trees or decreasing the competition when there are too many agents near the same tree. More concretely, when an agent attacks and the ray beam hits its target (another agent), the agent that was hit is taken out of the game for the next five steps and then it is revived in a random position of the spawning area of the map.


In Fig. [4] we show the results of these attack indicators for the first set of experiments. The results depict some important differences across the scenarios, mainly reflecting the reluctance of the cooperative agents to attack, and a strange difference between the number of attacks of the selfish agents with definition and the selfish agents without definition.


LLMs appear to equate cooperation with refraining from attacking, even when attacking may be the only viable strategy to address uncooperative agents. This behavior was the main cause for cooperative instructed agents to achieve the worst average per capita reward. On the other hand, the selfishly instructed agents behave similarly to the agents lacking assigned personalities, suggesting that LLMs partially disregard the personality given and tend to cooperate by harvesting apples sustainably. The notable disparity in attack frequencies between selfish agents with and without definition is intriguing because agents with the selfish definition decided to explore more frequently rather than attack, the reason for that remains a mistery.


FIGURE 


Another important behavior to keep track of is the decisions the agents made when they were near the last apple of a tree. Whether they choose to take it or ignore it is a crucial event and highly impactful in the final per capita reward, as there are only 6 apple trees in the game, and taking the last apple of a tree would mean that the tree would be depleted and will not produce more apples. For this reason, we created an indicator that counts how many times the agents closed the distance between the last apple of a tree and its position, divided by how many times the nearest apple to the agent was the last apple of a tree.
However, this indicator does not consider that sometimes the last apple, despite being the closest to the agent, is not visible to the agent because it is outside the observation window of the agent. This limitation could have had an impact on the observed results.


In Fig. [5] we can see that the proportion of times the agents moved towards the last apple is pretty similar across all the scenarios, indicating that the personality descriptions did not cause a major effect on the awareness of the agents regarding the welfare detriment caused by the depletion of apple trees. These results highlight a limited understanding among the agents regarding the consequences of their actions.


FIGURE


Performance of the agents in more challenging scenarios


The second set of experiments consists of scenarios where the competence increases or the resources become scarcer. The purpose of these scenarios is to measure how the agents respond to the new game conditions.


One single tree scenarios. The first three scenarios in this set represent an environment involving a more intensive competition for resources. The three agents, who usually have a limited field of vision, are in constant observation of a single tree in the environment, which is situated in a confined space. The difference between each scenario lies in the type of personality assigned to each agent, with the personalities in this case being All cooperative, All selfish, and Without Personality. For practical purposes, no specific definition was given to any personality. The purpose of the scenario is to demonstrate the collective sustainability capacity that different types of agents can have where resources are highly limited.


In Fig. [6], the results for the &quot;Per capita reward&quot; contrasted with the &quot;Average amount of available apples&quot; for the described group of scenarios are observed. Upon close examination, it is noted that the slope of the reward curve for cooperative agents is less than that of the &quot;Selfish&quot; and &quot;Without personality&quot; agents, this behavior contributes to this set of agents having resource availability for a slightly longer period, as shown in the figure.
However, given the dynamics of the probability of apple reappearance, this behavior was not significant enough to allow cooperative agents to have a considerably superior reward per capita. Therefore, it is concluded that any set of agents was able to demonstrate sufficiently good sustainable behavior due to their lack of understanding of the world and their lack of communication and coordination capabilities with other agents.


FIGURE


Agents versus Bots. The fifth scenario of the second set of experiments exposes two agents to the presence of two reinforcement learning bots. The policy of the bots makes them take the apples without regard for the replenishment rate or the risk of depleting the trees, they focus solely on maximizing their rewards by taking the apples, but they also attack other agents, mainly where there are no other apples in proximity.


In Fig. [7] we get the results of the average reward per capita for the agents versus bots scenario. The initial notable observation is that the bots can consistently achieve higher rewards than the agents. This phenomenon is mainly explained by the policy of the bots that prioritize taking all the visible apples over other actions, while the agents explore the map or go to other positions of the map with higher frequency than the bots. However, it is important to note how the per capita reward for the bots stops increasing earlier than that for the agents, indicating greater difficulty for the bots to increase their rewards when trees are scarce in comparison to the agents. Moreover, we found that in half of the simulations, at least one of the agents achieved a better reward than that of a bot, leading us to conclude that sometimes the agents are capable of performing better than the greedy policy of the agents. By closer examination, we observed that in those simulations, the agents were able to find apple trees more easily than the bots and that they also tended to attack when another agent or bot was taking apples from the same tree as them.


FIGURE


In Fig. [8], a significant disparity between the number of attacks perpetrated by bots and agents is observed. Despite bots&apos; attacks occurring almost five times as frequently as those executed by agents, the latter proved to be twice as effective in their attacks. Upon manual review of the simulations, we identified that bots increased their frequency of attacks when they were unable to perceive apples within their observation window, even when the attacks were not directed towards any specific target. This finding led us to appreciate how the actions taken by the agents are comparatively more coherent than those of the bots. Furthermore, the behavior of the agents exhibited closer resemblance to human behavior, not only in terms of attacks but also in their movement patterns, in contrast to the seemingly random and redundant actions of the bots.


FIGURE


Moreover, Fig. [9] shows that the agents depleted trees with higher frequency than the agents. Thus the agents demonstrated the capacity to sometimes restrain themselves from just taking apples by trying to maximize their long-term rewards, whereas bots always prioritized their short-term reward.


FIGURE


Impact of knowledge of other agent&apos;s behavior


This experiment considers the hypothetical scenario in which all agents are previously informed that one specific agent is entirely selfish and the implications that its uncooperative behavior can have. Likewise, this agent is informed to act selfishly (providing the previously described definition of selfishness). The objective of this scenario is to highlight the behavior that agents can exhibit when possessing valuable information about their social environment.


Fig. [10] shows that, on average, agents without personality targeted Pedro, the selfish agent, exclusively in 86% of the attacks. This illustrates how the two agents without a defined personality utilized the information forcibly implanted in them to benefit the overall sustainability of the environment, as they repeatedly immobilized the agent who posed a risk due to his excessive consumption and selfish actions. This demonstrates the necessity for agents to acquire this type of information, whether independently through their observations, reflections, and understanding of the world, or through communication with another agent who has previously synthesized this information from their experiences.


FIGURE


Discussion


Importance of Cooperative Capabilities 


In the presented scenarios, experiments detailed in Section [6] revealed that the used agent architecture yielded suboptimal results when confronted with unfamiliar situations or when the LLM knowledge couldn&apos;t decisively guide optimal decision-making. Furthermore, while agents demonstrated a willingness to cooperate, their actions did not reflect a clear understanding of how to effectively collaborate within the given environment.


To address the proposed scenarios in a better way, agents needed to recognize certain principles. For instance, they should refrain from harvesting the last apple in a green patch to prevent depletion and should engage in cooperation with other agents while avoiding collaboration with the bots or uncooperative agents. Observing that the bots consistently harvested apples unsustainably, agents should have deduced that attacking the bots was necessary to protect the green patches from depletion. This ability to prioritize long-term and collective welfare over short-term rewards, as well as recognizing the divergent behavior and preferences of other entities (bots), aligns with what Dadfoe et al. [*REF*] refer to as cooperative capabilities.


This prompts a consideration of whether current agent architectures genuinely enable cooperative behavior, and if the absence of such capabilities hinders their ability to navigate more intricate tasks and environments. Dadfoe et al. [*REF*] succinctly categorize cooperative capabilities into four essential components:


1. Understanding: Agents must comprehend the world, anticipate the consequences of their actions, and demonstrate an understanding of the beliefs and preferences of others.
2. Communication: Vital for achieving understanding and coordination, communication should be intentional, serving as a tool to gather information and coordinate efforts. Agents should be equipped to assess the intentions of others and establish their own criteria for discerning relevant information. Moreover, agents do not always have common interests, the other agent could be trying to deceive or convince in its self-interest.
3. Commitment: Cooperation is often hindered by commitment problems arising from an inability to make credible promises or threats.
Agent architectures should address these issues by providing mechanisms for agents to enforce or establish credibility in their promises and threats.
4. Institutions: Social structures, such as institutions, play a crucial role in simplifying interactions between agents. These structures define the rules of the game for all entities, potentially extending to the allocation of roles, power, and resources.


In essence, cultivating collaborative capabilities within agent architectures is crucial for tackling the complexities inherent in diverse tasks and environments. Historically, agent architectures have inadequately endowed agents with such capabilities. Instances such as Generative Agents [*REF*] and the Improving Factuality and Reasoning in Language Models through Multiagent Debate [*REF*] enable agents to engage in conversations or observe the perspectives of others. However, these approaches are hampered by the absence of independent evaluation criteria and discernment specific to the current LLMs limitations.


Cooperative Agent Architecture 


FIGURE


Based on previous findings, we proposed an architecture to enhance agents&apos; cooperative capabilities (see Fig. [11]). In this architecture several new modules are proposed:


1. Understanding module: This component is tasked with a comprehensive analysis of the agent&apos;s memories, fostering a deeper comprehension of the surrounding world. The agent&apos;s proficiency extends to predicting the behaviors of fellow agents and discerning environmental changes, enabling it to take actions with a keen awareness of their potential consequences. Notably, the agent must possess the capacity to infer both the governing principles of the world and the underlying motivations guiding others&apos; actions. This inference capability extends to scenarios where these principles may deviate from common knowledge or the pre-training model knowledge.
Zhu et al. [*REF*] demonstrate that LLMs, like GPT4, can learn such rules when explicitly prompted to identify them, utilizing question-answer pairs to later apply the learned rules in problem-solving. The proposed module operates by initially extracting the rules and behavioral patterns of the world and other agents. It achieves this by prompting the LLM with historical world observations and the current state of the world, aiming to identify rules that explain the current state based on the agent&apos;s observations. These identified rules are initially stored as world hypotheses. As the agent utilizes these hypotheses to interpret the current state, they are transformed into explicit rules once they surpass a predefined threshold. Additionally, the LLM is prompted to generate predictions about future states of the environment, empowering the agent to make informed decisions guided by anticipated future scenarios.
2. Communication module: The primary objective of this module is to equip the agent with the ability to engage in intentional communication with other agents. Two key objectives have been identified to enhance cooperative capabilities: (1) The agent is encouraged to seek new information from other agents. It must decide whether there are pertinent questions that can be posed to fellow agents, aiding in a better understanding of the world or gaining insights into the preferences of others. This information is pivotal for augmenting the agent&apos;s overall comprehension. (2) Agents are provided with the opportunity to negotiate and establish agreements deemed mutually beneficial. These agreements are stored in memory in a specialized manner to hold agents accountable for their commitments. The goal is to foster improved coordination among agents, thereby enhancing collaborative efforts.
3. Constitution Module: This module plays a crucial role in establishing a shared foundation for all agents. Its primary function is to define a set of common rules, providing agents with an initial framework to comprehend the world and formulate assumptions about the behavior of other agents. The constitution also delineates the consequences, whether penalties or rewards, that agents may face for specific behaviors or interactions. This not only lends credibility to agreements among agents but also discourages undesirable behaviors, streamlining interactions and cultivating a cooperative environment.
4. Reputation System: This system is designed to hold agents accountable for their actions. It evaluates each agent based on their adherence to agreements made with other agents. Periodically, the system prompts a language model with the existing agreements and corresponding actions, requesting a reputation score. This score is then accessible to all agents, influencing communication dynamics and aiding in understanding the behavior of others. Additionally, it facilitates making predictions about future states.


Conclusion and Future Work 


Cooperative capabilities have been somewhat overlooked in LLMs agent architectures, yet they may represent the crucial element enabling agents to accomplish pioneering tasks and thrive in intricate environments. As large language models (LLMs) advance, agent architectures stand to gain significantly by attaining enhanced responses from LLMs, particularly in tasks demanding substantial reasoning or when confronted with copious information in the prompt.


In this paper, our objective is to ascertain whether LLMs-enhanced autonomous agents can operate cooperatively. To this end, we adapt the Melting Pot scenarios to textual representations that can be easily operationalized by LLMs, and implement a reusable architecture for the development of LAAs employing the modules proposed in [*REF*]. This architecture includes short and long-term memories and cognitive modules of perception, planning, reflection, and action. The common &quot;Commons Harvest&quot; game was used to test the resulting system, and the results were evaluated from the viewpoint of cooperative metrics in different proposed scenarios.


The results indicate a gap in the current agent&apos;s cooperative capabilities vis-à-vis unfamiliar situations. Agents showed a cooperative tendency but lacked adequate understanding of how to collaborate effectively in an unknown environment. The agents needed to understand complex factors like the need to conserve resources, identify non-cooperative agents, and prioritize collective welfare over short-term gains. The research, thereby, draws attention to the need for a more inclusive architecture fostering cooperation and enhancing agent capabilities, including superior understanding, effective communication, credible commitment, and well-defined social structures or institutions.


Responding to the findings, we also proposed to improve the architecture with several modules to enhance the cooperative capabilities of the agents. These include an understanding module responsible for a comprehensive analysis of the agent&apos;s memory and surroundings, a communication module to enable intentional information exchange, a constitution module that lays out common rules of engagement, and a reputation system that holds agents accountable for making decisions for the collective good. Our future efforts will be focused on building and evaluating this cooperative architecture.