HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning


Introduction


Visual reasoning (VR) involves constructing a detailed representation of a visual scene and reasoning through it in steps, similar to human cognition, often in response to textual queries or prompts *REF*. It encompasses various tasks, including but not limited to Visual Question Answering (VQA) *REF*, Visual Commonsense Reasoning (VCR) *REF*, and Visual Grounding (VG) *REF*. In recent years, advancements in Large Language Models (LLMs) *REF*, *REF*, *REF*, *REF* and their derivatives, such as VLMs *REF*, *REF*, *REF*, *REF* have sparked hope for their effectiveness in solving visual reasoning tasks. While these models have shown promising results in certain tasks like VQA and VCR *REF*, their training as single monolithic end-to-end models necessitates large-scale datasets, imposing significant computational resource requirements.
Additionally, while these models excel within their training domain, they may require further adaptation to achieve reliable performance when applied to diverse datasets or domains *REF*, *REF*, *REF*.


FIGURE 1


In recent advancements, compositional approaches *REF*, *REF*, *REF*, *REF* have emerged as effective strategies for addressing VR challenges.
These approaches break down complex tasks into simpler sub-components, employing a divide-and-conquer methodology. They employ LLMs alongside Visual Foundation Models (VFMs) without requiring extensive training.
LLMs can function as planners, code generators, or reasoner, while VFMs act as visual perception components, facilitating structured analysis and task-specific plan generation to enhance adaptability and improve generalization across diverse scenarios. A recent SoTA compositional model is ViperGPT *REF*, which utilizes LLMs to generate code programs for visual queries and solve the task in a single feed forward process. IdealGPT *REF* proposed an enhanced framework by utilising LLMs as both questioners and reasoners, with a pre-trained Vision-Language Model (VLM) serving as the answerer, Figure *REF*. In this model, LLM decomposes main questions into sub-questions, with the reasoner determining whether further sub-question generation is required through iterations or if the final output has been reached.


However, these models come with certain limitations. Primarily, the outputs generated by LLMs may sometimes lack meaning, and when these outputs proceed to subsequent steps without verification, they can impact the outputs of other components, thus adversely affecting overall performance.
Moreover, LLMs utilized in the planner or questioner during the initial step lack information from visual content (perception module) in later states to adjust their outputs *REF*, *REF*. Additionally, the process of generating subsequent questions often begins from scratch without storing information from previous steps, potentially leading to more iterations. Furthermore, these approach heavily rely on commomsense knowledge encoded in LLMs to do planning and reasoning for VR tasks.


In this paper, we present HYDRA, a HYper agent for Dynamic compositional visual ReAsoning, an innovative framework designed to address the aforementioned challenges. HYDRA is composed of three main modules planner, controller (Reinforcement Learning-based agent (RL)) and reasoner. Notably, in the planner, upon receiving textual queries, unlike prior compositional approaches, we employed LLM to generate some instruction samples with varying depths based on a distribution, instead of relying on a single instruction sample.
Furthermore, we integrate a hyper RL agent to dynamically interact with some modules to make an high-level decision on the instruction samples generated by LLM in the planner to evaluate their validity. If the RL agent detects any invalid instruction samples, a request is sent back to the planner for alternative suggestions. Conversely, if the instruction samples are considered valid, the chosen instruction sample is forwarded to the reasoner. In the reasoner, the selected instruction sample undergoes analysis by LLM, and the resulting tailored code is sent to the code generator. The code generator employs Python API code to utilize VFMs for additional visual content processing. If the reasoner output is incomplete or fails, the output is converted to textual format in the textualizermodule and then stored in State Memory Bank. Afterwards, another request is then sent back to the planner to generate new instructions, which are again fed to the controller module to select an instruction sample. This iterative process continues incrementally until the final desired output is achieved. The design of HYDRA integrates not only the incremental storage of information from previous states (incremental reasoning), considered by the RL agent, but also the capability to utilize feedback from VFMs acquired from earlier perception processes.
This enables dynamic adjustment of actions and responses based on feedback from visual perception modules. This innovative design facilitates hyper decision-making by the hyper RL agent, thereby refining reasoning capabilities and overall effectiveness. The overall design of HYDRA compared with the previous compositional approach is shown in Figure *REF*. We evaluated our framework on several popular VR datasets and compared it with the advanced models, showing state-of-the-art performance. In sum, the key contributions of this work are as follows:


1. Integrating a cognitive reinforcement learning-based agent as a controller into a framework to foster hyper decision-making and behavior across diverse environments, enhancing system cohesion, performance, and reasoning capabilities.
2. Employing LLM as a natural language planner that enables the dynamic generation of valid instruction samples for iterative processing.
The samples are vary in both the complexity and scope of perception tasks assigned with validity probabilities.
3. Applying incremental reasoning, storing information from previous states aids both the LLM and RL agent in acquiring fine-grained visual information through VFMs and the visual-perception-to-text module, thereby refining their reasoning processes.


Related Work


Single Monolithic End-to-End Methods. Recent advancements in Large Language Models (LLMs) *REF*, *REF*, *REF*, *REF* have notably improved their ability to understand and reason visual content. Their derivatives, VLMs, like Video-LLaMA *REF* and NExT-GPT *REF* excel in comprehending detailed videos and seamlessly integrating text, images, videos, and audio for cross-modal reasoning. Otter *REF*, Flamingo *REF*, and Visual ChatGPT *REF* further enhance visual reasoning by integrating visual inputs into their language understanding processes, enabling contextually relevant responses. Initiatives like InstructBLIP *REF*, M3IT *REF*, and VisionLLM *REF* emphasize instruction tuning, multilingual datasets, and vision-centric tasks, advancing language understanding and nuanced video comprehension through a blend of language and visual cues. These developments signal a significant shift towards AI systems proficient in reasoning across textual and visual domains. However, these single monolithic end-to-end models suffer from reduced interpretability, require significant computational power and extensive training data resources. Besides, these models exhibit limited generalization capabilities due to the vast scale of the trained neural networks *REF*.
Various vision challenges often necessitate distinct models, typically involving the manual selection and assembly of specific models tailored to each particular scenario. Given the exponentially large long tail of compositional tasks, the proposed data-intensive and compute-intensive single monolithic end-to-end models may fall short in solving these types of tasks *REF*, *REF*. Consequently, compositional reasoning, generalization, fine-grained spatial reasoning abilities, and counting capabilities remain significant challenges for even the most advanced, large-scale single monolithic end-to-end models *REF*, *REF*, *REF*, *REF*.


Compositional Visual Reasoning Methods. The compositional approach


TABLE 1


Model Module Task Planner Perception Reasoner Controller IR VQA VG introduces a strategy aimed at addressing the challenges faced by end-to-end VLMs *REF*, *REF*, *REF*, *REF*, *REF*. These models tackle complex tasks by breaking them down into multiple subtasks, solving each one individually, and then utilizing the intermediate outcomes to address the overarching task. These models utilize the potent chain-of-thought (CoT) functionality of LLMs acting as planners, reasoner, etc. This capability facilitates the breakdown of intricate problems into manageable and individually solvable intermediate steps through the provision of instructions *REF*, *REF*, *REF*, *REF*. The instructions may take the form of Python execution code that embodies logical operations *REF*, *REF*. For example, Visprog *REF* and ViperGPT *REF* seek to eliminate the requirement for task-specific training in both programming logic and perception modules by employing code generation models. These strategies facilitate the assembly of VLMs into subroutines, thereby enabling the production of results. An alternative strategy, emblematic of the divide-and-conquer methodology, is exemplified by IdealGPT *REF*. This approach harnesses a captioning model for the acquisition of elementary visual data and engages a LLM to serve as a planner. The high-level inquiries are methodically deconstructed into three distinct sub-questions, which are processed concurrently. Following this, perception tools (VFMs) are employed to individually address each sub-question. The outcomes are then aggregated and analyzed by the reasoning mechanism to deduce the comprehensive final response. Moreover, the activation status and the sequential order of VFMs, as utilized by visual perception tools, constitute a form of instruction *REF*. The system implements predefined functionalities based on these instructions to systematically activate perception tools in a specified sequence. This process culminates in the aggregation of data, which is subsequently analyzed by the reasoning mechanism to formulate the ultimate conclusion.


All these compositional processing heavily depends on the capability of LLMs to perform commonsense reasoning and make decisions. However, despite their capabilities, LLMs have certain limitations. Primarily, the outputs they generate may lack meaningfulness, and if these outputs proceed to subsequent steps without verification, they can adversely affect the performance of other components. Additionally, LLMs used in planning or questioning lack access to visual content information in later stages, which hinders their ability to adjust outputs accordingly. Moreover, the process of generating subsequent questions often starts anew without retaining information from previous steps, potentially leading to more iterations. Furthermore, these methodologies heavily rely on the common-sense knowledge encoded in LLMs for planning and reasoning within virtual reality tasks. In this paper, we introduce a new framework that utilizes a cognitive reinforcement learning-based agent to address these challenges. This framework enhances decision-making, system performance, and reasoning across different scenarios. Moreover, we effectively harness LLM knowledge to generate instructional samples and facilitate incremental reasoning for acquiring detailed visual information. A comparison between recent compositional VR models and our approach is presented in Table *REF*.


FIGURE 2


Approach


The design of HYDRA are provided Figure *REF* in detail, comprising several key modules: planner (FP), controller (Fθ), reasoner(FR), textualizer (FT), a State Memory Bank and meta information (η). The framework&apos;s input comprises query-image pairs, denoted as X = {Q, I}, and the final output, Yˆ, can be textual answers or bounding boxes for the visual grounding task. The planner FP, C utilizing LLM, generates some instruction samples based on the input query Q using some information from meta information and State Memory Bank.
Then, the generated instruction samples are fed to controller FC which is composed of GPT embedding and RL agent that evaluate the validity of instruction samples.


If the RL agent detects invalid instruction samples, it forwards a request to the planner for alternative instruction samples; conversely, an instruction sample is picked as the chosen sample, d∗, and sent to the reasoner. The chosen instruction sample is fed to the LLM in the reasoning module, and the corresponding


Python code is generated in the code generator submodule.
Subsequently, this Python code is executed in the executing code submodule utilizing Python APIs and VFMs. If the output is incomplete or unsuccessful, it is converted to textual format through the textualizer module and stored in the State Memory Bank. Thereafter, another request is sent to the planner to generate new instruction samples, which are then provided to the controller module to select a valid instruction sample. This iterative process continues incrementally until the desired final output is obtained.


ALGORITHM 


As HYDRA is a framework that operates through several iterations to simplify the process, we use s0:t to depict the progression from the initial state to the current state 0: t. Additionally, in the first iteration, there is no information from the previous iteration, denoted as s0 = {}.
Note that all LLMs in the planner, reasoner, and textualizer are the same, with only their prompts being changed in different modules, and for enhanced clarity, we present them separately in the figure. The algorithm of the whole inference process is provided in Algorithm *REF*. The technical details for each module, along with further elaboration, are provided in the following.


State Memory Bank &amp; Meta Information. As HYDRA progresses through multiple iterations and considers information from previous ones, all data, including code, instruction, and the output of the reasoner from former iteration, are stored in State Memory Bank, represented by a grey cylinder in Figure *REF*. Furthermore, meta information encompasses crucial data such as a subset of skills π ∈ Π and various task descriptions γ ∈ Γ tailored for different tasks that the LLM needs as a prompt. For simplicity, these are denoted as η = γ, π in the subsequent equations.


Planner Module. Highlighted in orange in Figure *REF*, this module receives Q and other data from the State Memory Bank. It generates N instruction samples (e.g., &quot;find girls&quot;, &quot;verify if the girl is on the right side&quot;), dt of varying depth, where each instruction sample can have different actions or levels of complexity.
For instance, some instructions may involve simple tasks, while others may entail more intricate actions or multi-step processes. Along with these instruction samples, corresponding confidence probabilities Pp(dt) are provided, indicating the likelihood of each instruction being accurately executed. These outputs are generated by the LLM ChatGPT and are represented by Dt = {(dt, PP (dt))}N in the yellow box. This process is described by the equation:


EQUATION


Controller Module. This module serves as the central component of HYDRA, dynamically interacting with other modules to facilitate hyper decision-making and functioning as a cognitive controller.This module integrates embedding, leveraging GPT-3 *REF*, to extract the features highlighted in a cyan circle in the Figure *REF*. It takes Dt, η and s0:t−1 and embeds them into a vector, V.


Subsequently, it passes through an RL agent, which consists of a trainable MLP


layer followed by a softmax function with an output size of N+1.
Through this module, the instruction samples undergo evaluation and if the RL agent considers them invalid, a request is sent to the planner to regenerate new instruction samples, as indicated by the red arrow in Figure *REF*. Otherwise, the chosen instruction sample, dt, is selected and proceeds to the reasoner, depicted by the green arrow.


FORMULA 


Training phase. As mentioned earlier, the RL agent is a trainable MLP layer based on Reinforcement Learning, employing the DQN algorithm *REF*. During the training phase, the objective of the RL agent is to maximize the expected cumulative reward. The reward function is designed to favour fewer iterations and correct output while penalizing more iterations and incorrect output.
We iteratively accumulate the reward function as shown below.
FORMULA where m is the performance metrics (e.g. accuracy, intersection over union), α and R1 are the hyperparameter constants. Additional details regarding this phase are provided in the supplementary material.


Reasoner Module. Illustrated in light pink in the Figure *REF*, this module consists of an LLM as code generator and a code executor sub-module. In this setup, ChatGPT receives the selected instruction sample dt from the controller module, along with necessary information from the previous iteration, s0:t−1, and η, to generate Python code. This Python code is then transferred to the execution sub-module within perception tools, such as VFMs including GLIP *REF*, BLIP2 *REF*, MiDaS *REF*, and XVLM *REF*. Python interpreter to execute the code in the Python context loaded with the predefined Python APIs. In the execution, perceptual output = FR(Q, dt, s0:t−1, η) (4)


Textualizer Module. If the perceptual output from the reasoner module is incomplete or unsuccessful, it undergoes conversion to textual format within this module, as depicted by the green in Figure *REF*. The perceptual output from the reasoner, which may consist of bounding boxes, verifications, or captions, is transformed into a textual format using a template. This conversion ensures that the input is understandable for the LLM and ensures that all information stored in the State Memory Bank has the same format that can be used in the next iterations. Subsequently, the LLM summarizes the current state information, ft, and stores it in State Memory Bank. Further details about these templates are available in the supplementary material.


Technical Details: The iterative process continues incrementally until the desired final output is achieved, which we refer to as the incremental reasoning mechanism. It&apos;s worth noting that the HYDRA does not always require iterations; by efficiently integrating the RL agent, the final output of the task can be generated in just a single iteration. That could be due to the simplicity of the task, or the RL agent may choose to select an instruction that includes all the necessary steps for generating the final output in a single iteration.


Experiments and Results


Implementation Details: To train our framework, we utilized PyTorch *REF* with NVIDIA RTX 4090 GPUs, employing a learning rate of 1 × 10−4 and a batch size of 128. The Multi-Layer Perceptron (MLP) used for the RL agent, consists of three layers with dimensions 1536, 512, and 6. The hyper-parameters for reinforcement learning are set as R1 = 100 and α = 100. During the training process, early stopping is applied once the reward converges. For a fair comparison, we evaluated the state-of-the-art (SoTA) baselines using configurations from their official code repositories and papers *REF*. We utilized the largest available backbone for the end-to-end LVMs. We also replaced ChatGPT as the code generator in ViperGPT *REF*, given the discontinuation of GPT3 Codex by OpenAI *REF*. Supplementary materials provide additional details on implementation including instructions and prompts for the planner, code generator, and controller.


Datasets and Evaluation Metric: We evaluated our framework across three key tasks in visual reasoning. Firstly, External Knowledge-dependent Image Question Answering, for which we utilize the OK-VQA dataset *REF* and evaluate performance based on accuracy (ACC) score *REF*, *REF*.
Secondly, Compositional Image Question Answering, where the GQA *REF* dataset serves as our benchmark, again measured by ACC score *REF*, *REF*.
Lastly, Visual Grounding tasks are addressed using the RefCOCO *REF* and RefCOCO+ *REF* datasets, with evaluation based on Intersection over Union (IoU) metrics *REF*, *REF*, *REF*, *REF*, *REF*.


TABLE 


External Knowledge-dependent Image Question Answering involves using external sources of information, such as databases, to provide context and answer questions about images that cannot be inferred solely from visual content *REF*. Following previous works *REF*, we additionally employ the LLM *REF* knowledge with the module llm-query. The quantitative results from Table [2a] highlight the comparison between end-to-end models and compositional models, including HYDRA, on the OK-VQA dataset. HYDRA surpasses previous models by 48.6%, showcasing a remarkable improvement. The incorporation of advanced techniques in HYDRA, such as incremental reasoning mechanisms and leveraging LLM for generating different instructions, greatly contributes to its outstanding performance.


Visual Grounding involves predicting bounding boxes based on the input prompt. HYDRA are equipped with reasoner module which contain grounding-related VFM APIs such as find, exists, and verify-property, similar to ViperGPT. Our method, as shown in Table [2b], surpasses the state-of-the-art baselines for IoU on RefCOCO *REF* and RefCOCO+ *REF* datasets. Among the end-to-end methods, grounding-specialized approaches like GLIP *REF* and ReCLIP *REF* achieve superior performance compared to the VLM KOSMOS-2 *REF*. Considering that KOSMOS-2 can also handle other text-based tasks. When comparing methods between end-to-end and compositional approaches, we observe that both compositional visual reasoning approaches (ViperGPT *REF* and HYDRA) achieve better performance than end-to-end baselines. This indicates that the compositional approach design is more adept at solving the VG task.


TABLE 


TABLE 


Compositional Image Question Answering contains complex questions. These questions require the decomposition into simpler steps for answering. Similar to previous works *REF*, we utilize the BLIP2 *REF* API simple-query to enhance our understanding of image content. As demonstrated in Table *REF* with implementation on the GQA dataset, among the end-to-end models, the 30.8% performance of MiniGPT underscores the importance of instruct tuning. Ideal-GPT surpasses ViperGPT in performance by leveraging a planner to enhance reasoning capability. Notably, ViperGPT&apos;s performance is impeded by the generation of non-executable code snippets, while HYDRA enhances code quality through the integration of multiple sampling and a RL agent controller for code validation, leading to superior performance compared to ViperGPT. Additionally, it highlights that HYDRA achieves an impressive accuracy of 47.9%, underscoring its robustness and effectiveness in handling the GQA dataset.


Generalization Analysis: Generalization abilities play a crucial role in adapting approaches to unseen data distributions without necessitating re-training. Given that the RL agent in HYDRA is the sole component requiring training, we conducted generalization experiments on the OK-VQA dataset, as presented in Table *REF*, to assess the module&apos;s capacity to operate effectively on unseen data without explicit training. ViLT *REF* is chosen as the baseline end-to-end method, which does not require expensive computational resources. Notably, the performance of our model, HYDRA, in the cross-dataset experiments (i.e., training on GQA and testing on OK-VQA) closely matches intra-dataset performance, with accuracy of 48.17% and 48.63%, respectively. These results are on par with or better than the performance of HYDRA without the RL agent, which achieved an accuracy of approximately 43.01%. Furthermore, this cross-dataset performance surpasses that of the baseline ViLT *REF*, which achieved an accuracy of 32.13%. Additionally, ViperGPT *REF* exhibits superior performance compared to ViLT, showcasing the superiority of compositional over end-to-end methods in generalizability. Comparison with ViperGPT also reveals superior performance, as both HYDRA without RL and HYDRA trained on alternative datasets achieved accuracies of 47.01% and above. These findings underscore the generalizability of the RL agent controller within HYDRA.


TABLE 


Qualitative Analysis: Figure *REF* demonstrates intermediate processes of HYDRA for two examples, one for visual question-answering and one for visual grounding tasks. We show detailed examples with multiple steps in the first example in each figure, and the brief examples only show the last iteration in the loop. It is observed that the meaningful perception results are summarized as useful feedback for the next iteration of planning and reasoning. Figure *REF* includes more qualitative examples of the results using HYDRA on these tasks. Failure Analysis. While HYDRA has achieved SoTA performance, there is still room for further improvement in its design. In complex cases, as illustrated in Figure *REF*, HYDRA may fail due to potential mistakes made by the LLMs within the reasoner and textualizer module. In future iterations, we plan to enhance the complexity of the RL agent, enabling it to exert greater control over the output of LLMs, whether functioning as code generators or summarizers.


Ablation Study


In this section, we provide an ablation study on suggested key components of HYDRA demonstrating their contributions to the final results.


Component Analysis. As previously mentioned, there are three main contributions in HYDRA: the RL agent, Sampling (involving instruction sampling numbers), and Incremental Reasoning (IR). Through this experiment, the efficacy of each component is evaluated and presented in Table *REF*. As depicted in Table *REF*, the first column displays the model, while the following three columns represent each key component: RL agent, Incremental Reasoning (IR), and Sampling respectively. The last column, denoted as ACC, represents the accuracy achieved by each model on the GQA dataset.


FIGURE 


ViperGPT+S. In this experiment, unlike the ViperGPT model, we asked the LLM to generate more than one instruction sample.
As the table shows, the performance increased by 2.5% in terms of accuracy, reaching 39.84%.


ViperGPT+RL. The RL agent has been integrated to ViperGPT, providing it with the ability to make decision on keeping or re-generating the instruction from LLM. This integration aims to enhance the model&apos;s decision-making capabilities by allowing it to learn optimal policies through trial and error. With this addition, the ViperGPT achieved 5.77% improvement on its results.


HYDRA-IR. In this experiment, we removed Incremental Reasoning, which means the model no longer processes information incrementally or adaptively over multiple steps. This removal likely impacts the model&apos;s ability to reason and solve complex tasks that require multi-step reasoning or context-dependent decision-making.
Consequently, the accuracy decreased slightly to 45.98%.


HYDRA-RL. Similar to the previous experiment, the RL agent has been eliminated from our framework. This removal removes the model&apos;s ability to learn from rewards and adjust its behavior accordingly, potentially limiting its capability to perform tasks that require adaptive decision-making or exploration of the environment. Despite this, the model still achieved an accuracy of 46.93%.


FIGURE 


Conclusion


In this paper, we introduced HYDRA, a multi-step dynamic compositional visual reasoning framework designed to improve reasoning steadily and reliably. HYDRA combines three key parts: a planner, a RL agent acting as a cognitive controller, and a reasoner. The planner and reasoner modules use an LLM to create instruction samples and executable code from chosen instructions, while the RL agent interacts with these modules to make decisions based on past feedback, adjusting its actions as needed. This flexible setup allows HYDRA to learn from previous experiences during the reasoning process, resulting in more dependable outcomes and overall better performance. In future, our goal is to enhance our framework by fostering greater interaction between the LLM in the reasoner and the texturizer module to mitigate potential errors.