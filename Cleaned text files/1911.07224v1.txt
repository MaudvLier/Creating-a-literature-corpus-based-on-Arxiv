Learning from Trajectories via Subgoal Discovery


Introduction 


Reinforcement Learning (RL) aims to take sequential actions so as to
maximize, by interacting with an environment, a certain pre-specified
reward function, designed for the purpose of solving a task. RL using
Deep Neural Networks (DNNs) has shown tremendous success in several
tasks such as playing games [*REF*; *REF*],
solving complex robotics tasks [*REF*; *REF*],
etc. However, with sparse rewards, these algorithms often require a huge
number of interactions with the environment, which is costly in
real-world applications such as self-driving cars [*REF*],
and manipulations using real robots [*REF*]. Manually designed
dense reward functions could mitigate such issues, however, in general,
it is difficult to design detailed reward functions for complex
real-world tasks.


Imitation Learning (IL) using trajectories generated by an expert can
potentially be used to learn the policies faster [*REF*].
But, the performance of IL algorithms [*REF*] are not only
dependent on the performance of the expert providing the trajectories,
but also on the state-space distribution represented by the
trajectories, especially in case of high dimensional states. In order to
avoid such dependencies on the expert, some methods proposed in the
literature [*REF*; *REF*] take the path of combining
RL and IL. However, these methods assume access to the expert value
function, which may become impractical in real-world scenarios.


In this paper, we follow a strategy which starts with IL and then
switches to RL. In the IL step, our framework performs supervised
pre-training which aims at learning a policy which best describes the
expert trajectories. However, due to limited availability of expert
trajectories, the policy trained with IL will have errors, which can
then be alleviated using RL. Similar approaches are taken in
[*REF*] and [*REF*], where the authors show that
supervised pre-training does help to speed-up learning. However, note
that the reward function in RL is still sparse, making it difficult to
learn. With this in mind, we pose the following question: can we make
more efficient use of the expert trajectories, instead of just supervised pre-training?


Given a set of trajectories, humans can quickly identify waypoints,
which need to be completed in order to achieve the goal. We tend to
break down the entire complex task into sub-goals and try to achieve
them in the best order possible. Prior knowledge of humans helps to
achieve tasks much faster [*REF*; *REF*] than using only the
trajectories for learning. The human psychology of divide-and-conquer
has been crucial in several applications and it serves as a motivation
behind our algorithm which learns to partition the state-space into
sub-goals using expert trajectories. The learned sub-goals provide a
discrete reward signal, unlike value based continuous reward
[*REF*; *REF*], which can be erroneous, especially
with a limited number of trajectories in long time horizon tasks. As the
expert trajectories set may not contain all the states where the agent
may visit during exploration in the RL step, we augment the sub-goal
predictor via one-class classification to deal with such
under-represented states. We perform experiments on three goal-oriented
tasks on MuJoCo [*REF*] with sparse terminal-only reward,
which state-of-the-art RL, IL or their combinations are not able to solve.


Related Works 


Our work is closely related to learning from demonstrations or expert
trajectories as well as discovering sub-goals in complex tasks. We first
discuss works on imitation learning using expert trajectories or
reward-to-go. We also discuss the methods which aim to discover
sub-goals, in an online manner during the RL stage from its past
experience.


Imitation Learning. Imitation Learning [*REF*; *REF*; *REF*; *REF*; *REF*]
uses a set of expert trajectories or demonstrations to guide the policy
learning process. A naive approach to use such trajectories is to train
a policy in a supervised learning manner. However, such a policy would
probably produce errors which grow quadratically with increasing steps.
This can be alleviated using Behavioral Cloning (BC) algorithms
[*REF*; *REF*; *REF*], which queries expert action at states visited by the agent, after the
initial supervised learning phase. However, such query actions may be
costly or difficult to obtain in many applications. Trajectories are
also used by [*REF*], to guide the policy search, with the
main goal of optimizing the return of the policy rather than mimicking
the expert. Recently, some works [*REF*; *REF*; *REF*] aim to combine
IL with RL by assuming access to experts reward-to-go at the states
visited by the RL agent. [*REF*] take a moderately different
approach where they switch from IL to RL and show that randomizing the
switch point can help to learn faster. The authors in
[*REF*] use demonstration trajectories to perform
skill segmentation in an Inverse Reinforcement Learning (IRL) framework.
The authors in [*REF*] also perform expert trajectory
segmentation, but do not show results on learning the task, which is our
main goal. SWIRL [*REF*] make certain assumptions on the
expert trajectories to learn the reward function and their method is
dependent on the discriminability of the state features, which we on the
other hand learn end-to-end.


Learning with Options. Discovering and learning options have been
studied in the literature [*REF*; *REF*; *REF*] which can
be used to speed-up the policy learning process.
[*REF*] developed a framework for planning based on
options in a hierarchical manner, such that low level options can be
used to build higher level options. [*REF*] propose
to learn a set of options, or skills, by augmenting the state space with
a latent categorical skill vector. A separate network is then trained to
learn a policy over options. The Option-Critic architecture
[*REF*] developed a gradient based framework to learn the
options along with learning the policy. This framework is extended in
[*REF*] to handle a hierarchy of options. [*REF*]
proposed a framework where the goals are generated using Generative
Adversarial Networks (GAN) in a curriculum learning manner with
increasingly difficult goals. Researchers have shown that an important
way of identifying sub-goals in several tasks is identifying bottle-neck
regions in tasks. Diverse Density [*REF*], Relative
Novelty [*REF*], Graph Partitioning [*REF*], clustering [*REF*] can be used
to identify such sub-goals. However, unlike our method, these algorithms
do not use a set of expert trajectories, and thus would still be
difficult to identify useful sub-goals for complex tasks.


Methodology 


We first provide a formal definition of the problem we are addressing in
this paper, followed by a brief overall methodology, and then present a
detailed description of our framework.


Problem Definition. Consider a standard RL setting where an agent
interacts with an environment which can be modeled by a Markov Decision
Process (MDP) *MATH*, where *MATH* is the set of states, *MATH* is the set of
actions, *MATH* is a scalar reward function, *MATH* is the
discount factor and *MATH* is the initial state distribution.
Our goal is to learn a policy *MATH*, with
*MATH*, which optimizes the expected discounted reward
*MATH*, where *MATH* and *MATH*, *MATH* and *MATH*.


With sparse rewards, optimizing the expected discounted reward using RL
may be difficult. In such cases, it may be beneficial to use a set of
state-action trajectories *MATH* 
generated by an expert to guide the learning process. *MATH* is the
number of trajectories in the dataset and *MATH* is the length of the
*MATH* trajectory. We propose a methodology to efficiently use
*MATH* by discovering sub-goals from these trajectories and use
them to develop an extrinsic reward function.


FIGURE


Overall Methodology. Several complex, goal-oriented, real-world
tasks can often be broken down into sub-goals with some natural
ordering. Providing positive rewards after completing these sub-goals
can help to learn much faster compared to sparse, terminal-only rewards.
In this paper, we advocate that such sub-goals can be learned directly
from a set of expert demonstration trajectories, rather than manually
designing them.


A pictorial description of our method is presented in Fig. [1]. We use the
set *MATH* to first train a policy by applying supervised
learning. This serves a good initial point for policy search using RL.
However, with sparse rewards, the search can still be difficult and the
network may forget the learned parameters in the first step if it does
not receive sufficiently useful rewards. To avoid this, we use
*MATH* to learn a function *MATH*, which
given a state, predicts sub-goals. We use this function to obtain a new
reward function, which intuitively informs the RL agent whenever it
moves from one sub-goal to another. We also learn a utility function
*MATH* to modulate the sub-goal predictions over the
states which are not well-represented in the set *MATH*. We
approximate the functions *MATH*, *MATH*, and *MATH* using
neural networks. We next describe our meaning of sub-goals followed by
an algorithm to learn them.


Sub-goal Definition 


Definition 1. Consider that the state-space *MATH* is
partitioned into sets of states as *MATH*, s.t. *MATH* and
*MATH* and *MATH* is the number of
sub-goals specified by the user. For each *MATH*, we say that the
particular action takes the agent from one sub-goal to another iff
*MATH*, *MATH* for some *MATH* and *MATH*.


We assume that there is an ordering in which groups of states appear in
the trajectories as shown in Fig. [2]. However, the states within these groups
of states may appear in any random order in the trajectories. These
groups of states are not defined a priori and our algorithm aims at
estimating these partitions. Note that such orderings are natural in
several real-world applications where a certain sub-goal can only be
reached after completing one or more previous sub-goals. We show
(empirically in the supplementary) that our assumption is soft rather
than being strict, i.e., the degree by which the trajectories deviate
from the assumption determines the granularity of the discovered
sub-goals. We may consider that states in the trajectories of
*MATH* appear in increasing order of sub-goal indices, i.e.,
achieving sub-goal *MATH* is harder than achieving sub-goal *MATH*  *MATH*.
This gives us a natural way of defining an extrinsic reward function,
which would help towards faster policy search. Also, all the
trajectories in *MATH* should start from the initial state
distribution and end at the terminal states.


Learning Sub-Goal Prediction 


We use *MATH* to partition the state-space into *MATH* sub-goals,
with *MATH* being a hyperparameter. We learn a neural network to
approximate *MATH*, which given a state
*MATH* predicts a probability mass function
(p.m.f.) over the possible sub-goal partitions *MATH*. The order in
which the sub-goals occur in the trajectories, i.e., *MATH*, acts as a
supervisory signal, which can be derived from our assumption mentioned
above.


We propose an iterative framework to learn *MATH* 
using these ordered constraints. In the first step, we learn a mapping
from states to sub-goals using equipartition labels among the sub-goals.
Then we infer the labels of the states in the trajectories and correct
them by imposing ordering constraints. We use the new labels to again
train the network and follow the same procedure until convergence. These
two steps are as follows.


Learning Step. In this step we consider that we have a set of tuples
*MATH*, which we use to learn the function *MATH*,
which can be posed as a multi-class classification problem with *MATH* 
categories. We optimize the following cross-entropy loss function,
*MATH* where *MATH* is the indicator function
and *MATH* is the number of states in the dataset *MATH*. To begin
with, we do not have any labels *MATH*, and thus we consider equipartition
of all the sub-goals in *MATH* along each trajectory. That is, given a
trajectory of states *MATH* for some *MATH*, the initial sub-goals
are, *MATH*. Using this initial labeling scheme, similar
states across trajectories may have different labels, but the network is
expected to converge at the Maximum Likelihood Estimate (MLE) of the
entire dataset. We also optimize CASL [*REF*] for stable learning
as the initial labels can be erroneous. In the next iteration of the
learning step, we use the inferred sub-goal labels, which we obtain as
follows.


Inference Step. Although the equipartition labels in Eqn.
may have similar states across different
trajectories mapped to dissimilar sub-goals, the learned network
modeling *MATH* provides maps similar states to the same sub-goal.
But, Eqn., and thus the predictions of *MATH* does not
account for the natural temporal ordering of the sub-goals. Even with
architectures such as Recurrent Neural Networks (RNN), it may be better
to impose such temporal order constraints explicitly rather than relying
on the network to learn them. We inject such order constraints using
Dynamic Time Warping (DTW).


Formally, for the *MATH* trajectory in *MATH*, we obtain the
following set: *MATH*, where *MATH* is a vector representing the p.m.f. over
the sub-goals *MATH*. However, as the predictions do not consider temporal
ordering, the constraint that sub-goal *MATH* occurs after sub-goal *MATH*,
for *MATH*, is not preserved. To impose such constraints, we use DTW
between the two sequences *MATH*,
which are the standard basis vectors in the *MATH* dimensional Euclidean
space and *MATH*. We use the *MATH* -norm of the difference between two vectors as the
similarity measure in DTW. In this process, we obtain a sub-goal
assignment for each state in the trajectories, which become the new
labels for training in the learning step.


We then invoke the learning step using the new labels (instead of Eqn.), 
followed by the inference step to obtain the
next sub-goal labels. We continue this process until the number of
sub-goal labels changed between iterations is less than a certain
threshold. This method is presented in Algorithm, where the superscript *MATH* represents the
iteration number in learning-inference alternates.


Reward Using Sub-Goals. The ordering of the sub-goals, as discussed before, provides a natural way
of designing a reward function as follows: *MATH* where the agent in state *MATH* 
takes action *MATH* and reaches state *MATH*. The augmented
reward function would become *MATH*. Considering that we have a function
of the form *MATH*, and without loss of generality that *MATH*, so that
for the initial state *MATH*, it follows from
[*REF*] that every optimal policy in *MATH*,
will also be optimal in *MATH*. However, the new reward function
may help to learn the task faster.


Out-of-Set Augmentation. In several applications, it might be the
case that the trajectories only cover a small subset of the state space,
while the agent, during the RL step, may visit states outside of the
states in *MATH*. The sub-goals estimated at these out-of-set
states may be erroneous. To alleviate this problem, we use a logical
assertion on the potential function *MATH* that the
sub-goal predictor is confident only for states which are
well-represented in *MATH*, and not elsewhere. We learn a neural
network to model a utility function *MATH*, which given a state,
predicts the degree by which it is seen in the dataset *MATH*. To
do this, we build upon Deep One-Class Classification [*REF*],
which performs well on the task of anomaly detection. The idea is
derived from Support Vector Data Description (SVDD) [*REF*],
which aims to find the smallest hypersphere enclosing the given data
points with minimum error. Data points outside the sphere are then
deemed as anomalous. We learn the parameters of *MATH* by optimizing
the following function: *MATH* where *MATH* is a
vector determined a priori [*REF*], *MATH* is modeled by a neural
network with parameters *MATH*, s.t. *MATH*. The second part is the *MATH* 
regularization loss with all the parameters of the network lumped to
*MATH*. The utility function *MATH* can be expressed as follows: *MATH*.
A lower value of *MATH* indicates that the state has
been seen in *MATH*. We modify the potential function
*MATH* and thus the extrinsic reward function, to
incorporate the utility score as follows: *MATH* where *MATH* denotes the modified potential
function. It may be noted that as the extrinsic reward function is still
a potential-based function [*REF*], the optimality conditions
between the MDP *MATH* and *MATH* still hold as discussed previously.


FIGURE 


ALGORITHM


Supervised Pre-Training. We first pre-train the policy network using
the trajectories *MATH* (details in supplementary). The
performance of the pre-trained policy network is generally quite poor
and is upper bounded by the expert performance from which the
trajectories are drawn. We then employ RL, which starts from the
pre-trained policy, to learn from the subgoal based reward function.
Unlike standard imitation learning algorithms, e.g., DAgger, which
finetune the pre-trained policy with the expert in the loop, our
algorithm only uses the initial set of expert trajectories and does not
invoke the expert otherwise.


Experiments 


FIGURE


In this section, we perform experimental evaluation of the proposed
method of learning from trajectories and compare it with other
state-of-the-art methods. We also perform ablation of different modules
of our framework.


Tasks. We perform experiments on three challenging environments as
shown in Fig. [7]. First is Ball-in-Maze Game (BiMGame) introduced
in [*REF*], where the task is to move a ball from the outermost
to the innermost ring using a set of five discrete actions - clock-wise
and anti-clockwise rotation by *MATH* along the two principal
dimensions of the board and &quot;no-op\&quot; where the current orientation of
the board is maintained. The states are images of size *MATH*.
The second environment is AntTarget which involves the Ant
[*REF*]. The task is to reach the center of a circle of
radius *MATH* m with the Ant being initialized on a *MATH* arc of the
circle. The state and action are continuous with *MATH* and *MATH* dimensions
respectively. The third environment, AntMaze, uses the same Ant, but in
a U-shaped maze used in [*REF*]. The Ant is initialized on
one end of the maze with the goal being the other end indicated as red
in Fig. [6]. Details about the network architectures we use for *MATH*,
*MATH* and *MATH* can be found in the supplementary material.


Reward. For all tasks, we use sparse terminal-only reward, i.e.,
*MATH* only after reaching the goal state and *MATH* otherwise. Standard RL
methods such as A3C [*REF*] are not able to solve these
tasks with such sparse rewards.


Trajectory Generation. We generate trajectories from A3C
[*REF*] policies trained with dense reward, which we do
not use in any other experiments. We also generate sub-optimal
trajectories for BiMGame and AntMaze. To do so for BiMGame, we use the
simulator via Model Predictive Control (MPC) as in [*REF*]
(details in the supplementary). For AntMaze, we generate sub-optimal
trajectories from an A3C policy stopped much before convergence. We
generate around *MATH* trajectories for BiMGame and AntMaze, and *MATH* 
for AntTarget. As we generate two separate sets of trajectories for
BiMGame and AntTarget, we use the sub-optimal set for all experiments,
unless otherwise mentioned.


FIGURE


Baselines. We primarily compare our method with RL methods which
utilize trajectory or expert information - AggreVaTeD [*REF*]
and value based reward shaping [*REF*], equivalent to the
*MATH* in THOR [*REF*]. For these methods, we use
*MATH* to fit a value function to the sparse terminal-only reward
of the original MDP *MATH* and use it as the expert value
function. We also compare with standard A3C, but pre-trained using
*MATH*. It may be noted that we pre-train all the methods using
the trajectory set to have a fair comparison. We report results with
mean cumulative reward and *MATH* over *MATH* independent runs.


FIGURE 


Comparison with Baselines. First, we compare our method with other
baselines in Fig [11]. Note that as out-of-set augmentation using
*MATH* can be applied for other methods which learn from trajectories,
such as value-based reward shaping, we present the results for
comparison with baselines without using *MATH*, i.e., Eqn. 
Later, we perform an ablation study with and
without using *MATH*. As may be observed, none of the baselines show
any sign of learning for the tasks, except for ValueReward, which
performs comparably with the proposed method for AntTarget only. Our
method, on the other hand, is able to learn and solve the tasks
consistently over multiple runs. The expert cumulative rewards are also
drawn as straight lines in the plots and imitation learning methods like
DAgger [*REF*] can only reach that mark. Our method is able
to surpass the expert for all the tasks. In fact, for AntMaze, even with
a rather sub-optimal expert (an average cumulative reward of only
*MATH*), our algorithm achieves about *MATH* cumulative reward at
*MATH* million steps.


The poor performance of the ValueReward and AggreVaTeD can be attributed
to the imperfect value function learned with a limited number of
trajectories. Specifically, with an increase in the trajectory length,
the variations in cumulative reward in the initial set of states are
quite high. This introduces a considerable amount of error in the
estimated value function in the initial states, which in turn traps the
agent in some local optima when such value functions are used to guide
the learning process.


Variations in Sub-Goals. The number of sub-goals *MATH* is specified
by the user, based on domain knowledge. For example, in the BiMGame, the
task has four bottle-necks, which are states to be visited to complete
the task and they can be considered as sub-goals. We perform experiments
with different sub-goals and present the plots in Fig. [15]. It
may be observed that for BiMGame and AntTarget, our method performs well
over a large variety of sub-goals. On the other hand for AntMaze, as the
length of the task is much longer than AntTarget (12m vs 5m),
*MATH* learn much faster than *MATH*, as higher number of
sub-goals provides more frequent rewards. Note that the variations in
speed of learning with number of sub-goals is also dependent on the
number of expert trajectories. If the pre-training is good, then less
frequent sub-goals might work fine, whereas if we have a small number of
expert trajectories, the RL agent may need more frequent reward (see the
supplementary material for more experiments).


FIGURE


Effect of Out-of-Set Augmentation. The set *MATH* may not
cover the entire state-space. To deal with this situation we developed
the extrinsic reward function in Eqn. using *MATH*. To evaluate its
effectiveness we execute our algorithm using Eqn. and Eqn., and show the results in Fig.
[18], with legends showing without and with *MATH* respectively. For BiMGame, we used the
optimal A3C trajectories, for this evaluation. This is because, using
MPC trajectories with Eqn. can still solve the task with similar reward
plots, since MPC trajectories visit a lot more states due to its
short-tem planning. The (optimal) A3C trajectories on the other hand,
rarely visit some states, due to its long-term planning. In this case,
using Eqn. actually traps the agents to a local optimum
(in the outermost ring), whereas using *MATH* as in Eqn., learns to solve the task consistently (Fig. [16]).


For AntTarget in Fig. [17], using *MATH* performs better than
without using *MATH* (and also surpasses Value based Reward Shaping).
This is because the trajectories only span a small sector of the circle
(Fig. [23]) while the Ant is allowed to visit states
outside of it in the RL step. Thus, *MATH* avoids incorrect sub-goal
assignments to states not well-represented in *MATH* and helps in the overall learning.


FIGURE


Effect of Sub-Optimal Expert. In general, the optimality of the
expert may have an effect on performance. The comparison of our
algorithm with optimal vs. sub-optimal expert trajectories are shown in
Fig. [21]. As may be observed, the learning curve for
both the tasks is better for the optimal expert trajectories. However,
in spite of using such sub-optimal experts, our method is able to
surpass and perform much better than the experts. We also see that our
method performs better than even the optimal expert (as it is only
optimal w.r.t. some cost function) used in AntMaze.


Visualization. We visualize the sub-goals discovered by our
algorithm and plot it on the x-y plane in Fig.
[25]. As can be seen in BiMGame, with *MATH* 
sub-goals, our method is able to discover the bottle-neck regions of the
board as different sub-goals. For AntTarget and AntMaze, the path to the
goal is more or less equally divided into sub-goals. This shows that our
method of sub-goal discovery can work for both environments with and
without bottle-neck regions. The supplementary material has more
visualizations and discussion.


Discussions


The experimental analysis we presented in the previous section contain
the following key observations:
-  Our method for sub-goal discovery works both for tasks with inherent
bottlenecks (e.g. BiMGame) and for tasks without any bottlenecks
(e.g. AntTarget and AntMaze), but with temporal orderings between
groups of states in the expert trajectories, which is the case for
many applications.
-  Experiments show, that our assumption on the temporal ordering of
groups of states in expert trajectories is soft, and determines the
granularity of the discovered sub-goals (see supplementary).
-  Discrete rewards using sub-goals performs much better than value
function based continuous rewards. Moreover, value functions learned
from long and limited number of trajectories may be erroneous,
whereas segmenting the trajectories based on temporal ordering may
still work well.
-  As the expert trajectories may not cover the entire state-space
regions the agent visits during exploration in the RL step,
augmenting the sub-goal based reward function using out-of-set
augmentation performs better compared to not using it.


Conclusion


In this paper, we presented a framework to utilize the demonstration
trajectories in an efficient manner by discovering sub-goals, which are
waypoints that need to be completed in order to achieve a certain
complex goal-oriented task. We use these sub-goals to augment the reward
function of the task, without affecting the optimality of the learned
policy. Experiments on three complex task show that unlike
state-of-the-art RL, IL or methods which combines them, our method is
able to solve the tasks consistently. We also show that our method is
able to perform much better than sub-optimal experts used to obtain the
expert trajectories and at least as good as the optimal experts. Our
future work will concentrate on extending our method for repetitive
non-goal oriented tasks.