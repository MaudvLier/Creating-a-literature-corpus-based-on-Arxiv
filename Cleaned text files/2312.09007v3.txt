LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution


Introduction 


Large Language Model (LLM) has recently attracted tremendous attention from both academia and industry. In addition to excellent language skills, LLM also exhibits a high level of reasoning and planning capability [*REF*; *REF*; *REF*; *REF*; *REF*].
Moreover, the remarkable feature of in-context learning substantially broadens the range of potential application scenarios for LLM, sparking the design of diverse general-purpose AI agents [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*; *REF*].


In particular, LLM presents new transformative opportunities for the IoT era. In this work, we propose LLMind, an AI agent framework that transforms the usage of conventional IoT devices by integrating LLM as an orchestrator for accomplishing complex tasks using them.


Compared with scripted intelligence and traditional IoT device control methods, the LLM orchestrator brings unparalleled flexibility and empowers IoT devices with advanced intelligence, enabling the coordination of diverse IoT devices to collaboratively accomplish intricate tasks. By harnessing the in-context learning capability of the LLM, the agent can achieve swift adaptation and optimal resource utilization, leading to efficient task solutions [*REF*; *REF*]. Consequently, users can effortlessly express their needs without concerns about operational details.


Integrating LLM into IoT device control presents a number of challenges before its promising potentials can be realized. First, a general LLM cannot handle specialized tasks like object detection or facial recognition, while there are already many AI models excelling in various specific domains. Thus, a challenge lies in combining the power of specialized but fragmented AI modules with LLM to handle various complex tasks. Furthermore, due to LLM&apos;s slow inference speed and high computational costs, it is crucial to investigate methods that can enhance the system&apos;s response speed and overall efficiency. Another significant challenge involves the effective control of IoT devices by the LLM-based AI agent. The LLM must accurately learn the diverse functionalities and operational characteristics of various IoT devices.
Additionally, the design of efficient interaction mechanisms that enable users to naturally and intuitively express their needs to the LLM-based AI agent presents a further challenge. Our framework effectively overcomes the aforementioned challenges.


First, taking inspiration from the functional specialization theory of the brain, our framework integrates domain-specific AI modules with the LLM, which serves as the task allocation governor. This synthesis enables the general-purpose LLM to effectively execute specific subtasks by invoking specialized AI modules to accomplish an overall complex task, enhancing system flexibility and practicality. Moreover, by incorporating new AI modules, the AI agent acquires new capabilities and continues to evolve to become even more capable.


Second, to enhance the response speed and overall efficiency of the agent, we utilize a semantic analysis NLP model to classify and locally store LLM-generated plans (control scripts) for specific user commands.
By identifying similar instructions in historical records, even with varying tones or expressions, the agent can retrieve validated solutions, thus avoiding complete replanning. This approach enables the system to accumulate experience and progressively enhance its proficiency through interactions.


Third, the agent leverages the LLM for complex task planning and the generation of the corresponding control script using a novel finite-state machine approach, enabling efficient and precise invocation of AI modules and IoT devices. This methodology significantly improves accuracy and success rates.


Last but not least, we connect our demo system to the widely used social media platform, WeChat, allowing users to conveniently interact with the LLM agent.


FIGURE


We refer to the intelligence level attained by our framework as Artificial Complex Intelligence (ACI), which serves as a bridge between Artificial Narrow Intelligence (ANI) [*REF*; *REF*] and Artificial General Intelligence (AGI) [*REF*; *REF*; *REF*], as illustrated in Fig. [1]. ACI integrates the capabilities of various ANI modules and undergoes continuous evolution towards AGI. Our framework goes beyond mere device control, providing a comprehensive blueprint for a future intelligent, integrated, and collaborative IoT device ecosystem empowered by LLM.


Overview of the framework design 


FIGURE


Our framework, depicted in Fig., consists of five components: user interface, LLM, coordinator, AI modules, and IoT devices.


The user can communicate with the AI agent via social media platforms to provide instructions and assign tasks. Once the task is comprehended, the LLM generates a control script to orchestrate IoT devices and AI modules, facilitating the completion of the assigned task.


If an error occurs during script execution, the error message is used as feedback and temporarily stored as context for subsequent trials. The successfully executed script will be locally stored and associated with the corresponding user instructions, providing a valuable learned experience for future executions.


Upon successful execution of the script or in the case of failure after multiple attempts, the agent collects the results and generates a report for the user.


Experiment configurations 


We have verified the feasibility of the framework through experiments.
In the experiments, the user sends a text message to the AI agent. It then formulates a solution by examining the user&apos;s command and the system&apos;s available resources and methods, including AI modules and connected IoT devices. Following that, the AI agent generates an executable script and responds to the user&apos;s request based on the execution outcome.


We deployed the coordinator and AI modules of the system on an edge server. The edge server is connected to the same local network as the IoT devices via WiFi and can interact with ChatGPT using OpenAI&apos;s APIs [*REF*].


The available IoT devices in the system are as follows:


1. Security cameras: The cameras are positioned within the room to capture images of the surrounding environment. These images are transmitted to the edge server via a local network. This experiment employs three cameras: two placed at the corners of the room, approximately 2 meters above the ground, and the third mounted on top of a TurtleBot mobile robot.
2. TurtleBot mobile robot [*REF*]: The TurtleBot mobile robot can receive target position coordinates through a network connection and autonomously plan paths to navigate within the room, intelligently avoiding obstacles.
3. WiFi router: The WiFi router can manage bandwidth allocation for each device and enforce restrictions on their maximum network speed.


Additionally, the system integrates the following AI modules:


1. Object detection: We employ the YOLO v8 model [*REF*] to detect various objects in the scene, including people.
2. Face recognition: We use an open-source project called Face Recognition [*REF*] to identify individuals in the scene.


Conclusion 


The integration of LLMs into IoT device control offers transformative possibilities for developing intelligent IoT ecosystems. Our framework seamlessly combines LLMs&apos; language skills, reasoning abilities, and in-context learning with specialized AI modules, enhancing the flexibility and practicality of IoT systems. The proposed approaches, including FSM-based control script generation, user interaction, and historical script retrieval mechanisms, contribute to enhancing user experiences and system efficiency, with the system becoming increasingly intelligent as it learns from interactions among LLMs, humans, AI modules, and IoT devices. This comprehensive blueprint for intelligent and collaborative IoT ecosystems sets the stage for future advancements, where LLMs and AI-driven solutions enhance device coordination and convenience in our daily lives harmoniously.