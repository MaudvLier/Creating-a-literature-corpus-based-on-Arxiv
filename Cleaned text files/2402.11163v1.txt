KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph


Introduction


Despite the remarkable performance on various NLP tasks ([Brown et al., 2020]; [Zhao et al., 2023]), large language models (LLMs) still have limited capacities in solving complex tasks ([Hu et al., 2023b]) solely based on their parametric knowledge, e.g., multi-hop and knowledge-intensive reasoning ([Lan et al., 2023]).
Knowledge graph (KG), which stores massive knowledge triples in a graph-structured format, has been broadly used to complement LLMs with external knowledge ([Pan et al., 2023]).


Due to the large volume and structured format of KG data, it is not easy for LLMs to effectively utilize the information from KG. Recent work mainly adopts retrieval-augmented ([Ye et al., 2022]) or synergy-augmented ([Jiang et al., 2023b]) methods to enhance LLMs with KG data. The former approach retrieves and serializes the task-related triples as part of the prompt for LLMs, while the latter approach designs an information interaction mechanism between KG and LLMs to iteratively find the solution to the question. In particular, synergy-augmented methods can benefit from the structured search on KG (e.g., SPARQL) and the language understanding capacity of LLMs, achieving comparable or even better performance compared with previous state-of-the-art methods ([Gu et al., 2023]).


However, there are still two major limitations on existing synergy-augmented methods. First, the information interaction mechanism between LLM and KG is often pre-defined (e.g., following a human-crafted multi-round plan), which cannot flexibly adapt to various complex tasks ([Luo et al., 2023]; [Jiang et al., 2023b]). For instance, it would become ineffective to handle the unintended requirements in the reasoning process, e.g., varied difficulties or constraints. Second, these methods ([Wang et al., 2023a]) mostly rely on stronger closed-source LLM APIs (e.g., ChatGPT and GPT-4) to understand or learn to solve complex tasks. However, the distilled plans or procedures, also limited to special task settings or capacity levels, may not be best suited for instructing these weaker models.


To address these issues, in this paper, we propose the KG-Agent, an autonomous LLM-based agent framework for complex reasoning tasks over KG. The motivations are twofold: (1) designing autonomous reasoning approaches that can actively make decisions during reasoning, without human assistance; (2) enabling relatively small models (e.g., 7B LLM) to effectively perform complex reasoning, without reliance on close-sourced LLM APIs. To achieve this, our approach makes three major technical contributions. First, we extend the LLM&apos;s capacity to manipulate structured data by curating a multifunctional toolbox, enabling LLM to perform discrete or advanced operations (e.g.,


filtering, counting, and retrieval) on KG data and intermediate results. Second, we leverage existing KG reasoning datasets for synthesizing code-based instruction data to fine-tune the LLM, where we first generate the program according to the reasoning chain on KG and then synthesize the instruction data. Third, we propose an autonomous iteration mechanism based on tool selection and memory updation that integrates the tuned LLM, multifunctional toolbox, KG-based executor, and knowledge memory, for autonomously reasoning over KG.


To verify the effectiveness, we evaluate KGAgent on both in-domain and out-of-domain tasks including KG-based question answering (KGQA) and open domain question answering (ODQA). With much fewer training data (i.e., 10K samples) for tuning a smaller LLM (i.e., LLaMA-7B), our approach can outperform competitive LLM-based baselines on in-domain datasets (e.g., using about 36% and 23% of the original training set amount while obtaining 7.5% and 2.7% relative improvement of F1 on CWQ and GrailQA respectively). On the out-of-domain datasets, the zero-shot performance of our KG-Agent is better than competitive full-data supervised fine-tuning models (e.g., 9.7% and 8.5% relative improvement of accuracy on WQ-Freebase and TQ-Wiki, respectively).


Related Work


LLM-based KG Reasoning. Benefitting from the powerful zero-shot and few-shot capability, recent studies have leveraged LLMs to perform reasoning over KG. Recent work can be roughly divided into retrieval-augmented ([Shu et al., 2022]) and synergy-augmented ([Gu et al., 2023]) two types. The retrieval-augmented method is to retrieve and serialize the triples from the KG, and then feed it to the LLM to help generate the final results (e.g., answers or SPARQL query) ([Ye et al., 2022]). Such a way loses the structured information in the original KG and may retrieve redundant knowledge, limiting LLMs&apos; understanding. To relieve these problems, the synergy-augmented methods design an information interaction mechanism between LLMs and KGs to enable LLMs to query KGs multiple times to answer the question ([Jiang et al., 2023b]). Specifically, they either first generate the full plan ([Li et al., 2023]) and then ground it on KG, or make a plan step-by-step based on the KG ([Luo] existing methods often follows a pre-defined way, which cannot flexibly adapt to various complex tasks. In contrast, our proposed KG-Agent can autonomously make decisions during reasoning over KG, without human assistance.


LLM-based Agents. Recently, LLMs have shown surprising long-horizon planning and reasoning capabilities ([Shinn et al., 2023]; [Zhong et al., 2023]), and LLM-based agents have gradually become a hot topic for autonomously solving complex interactive tasks ([Wang et al., 2023b]). A large number of agents focus on general-purpose task solving. As the representative projects, ReAct ([Yao et al., 2023]) proposes a prompting method to convert LLMs (e.g., ChatGPT) as language agents, to interact with the external environment, receive the feedback, and then generate the action for next step reasoning.
Then, AutoGPT[1] further empowers LLMs (i.e., GPT4) with long/short-term memory management and external tools like search engines to autonomously address a user request. In addition, several other agents also focus on specific domains, such as WebGPT ([Nakano et al., 2021]) for the web-browsing environment, MM-REACT ([Yang et al., 2023]) for the multi-modal scenario, and ProgPrompt ([Singh et al., 2023]) for the real-life environment. However, recent works involving language agents mostly rely on stronger closed-source LLM APIs (e.g., ChatGPT and GPT-4) to understand or learn to solve complex tasks. Our KG-Agent is the first autonomous agent framework to support complex reasoning over KG only relying on a relatively smaller 7B LLM.


Preliminary


In this section, we first formally define knowledge graph (KG), and then formalize the complex knowledge reasoning task based on KG.


Knowledge Graph (KG). A knowledge graph typically consists of a large number of fact triples, expressed as *MATH*, where E and R denote the entity set and relation set, respectively. A triple ⟨e, r, e′⟩ describes a factual knowledge that a relation r exists between the head entity e and tail entity e′. Each entity e is assigned a unique entity ID (or string value), and belongs to one entity type t such as Country and


Person. Furthermore, we introduce neighboring [et al., 2023]). Although obtaining better performance, the information interaction mechanism in relations to denote both the incoming and outgoing relations for a set of entities {e}, denoted as *MATH*.


Problem Formulation. In this work, we assume that a KG is available and contains the answer entities for the given natural language question. Our objective is to develop a LLM-based agent that can autonomously infer the answer to the question based on the given KG. As it has been shown that domain-specific interface is helpful for LLMs to manipulate the structured data ([Jiang et al., 2023b]), we further assume that a toolbox can be provided to facilitate the access to the information of KG. Formally, given a natural language question q, and a toolbox T and a KG G, we aim to develop a capable agent to deduce the final answers *MATH* for the question q by leveraging the tools in T and the knowledge information in G.


Approach


In this part, we present the proposed KG-Agent for autonomously solving complex reasoning tasks over KG. The core of our KG-Agent framework is a well-instructed LLM, which can autonomously make decisions when reasoning over KG. We first extend the LLM&apos;s capacities by designing a toolbox with supporting tools to manipulate the KG data or intermediate results (Section [4.1]).
To enhance the step-by-step reasoning capacity, we leverage existing knowlege graph question answering (KGQA) datasets to synthesize KG reasoning programs and convert them into formatted instruction tuning data (Section [4.2]). Finally, we design an effective agent framework based on the knowledge memory to support autonomous reasoning over KG (Section [4.3]). Next, we give the technical details of KG-Agent.


Toolbox for Knowledge Graph


Since LLMs struggle to accurately manipulate the structured data ([Jiang et al., 2023b]), we construct a supporting toolbox for easing the utilization of the KG information.
According to existing work ([Gu et al., 2021]; [Cao et al., 2022]), reasoning over KG (e.g., Freebase or Wikidata) typically requires three fundamental operations, namely extracting information from KG, filtering irrelevant information based on the semantics of the question, and operating on the extracted information. Therefore, we design three types of tools for LLMs reasoning over KG, i.e., extraction, semantic, and logic tools.


- Extraction tools aim to facilitate the access to information from KG. Considering the basic data types in KG, we design five tools to support the access to the relations (get_relation), the head/tail entities (get_head_entity/get_tail_entity), and entities with specific type or constraint (get_entity_by_type/ get_entity_by_constraint), w.r.t. some entity set or other input information (e.g., relation or type).
- Logic tools aim to support basic manipulation operations on the extracted KG information, including entity counting (count), entity set intersection (intersect) and union (union), condition verification (judge), and ending the reasoning process with the current entity set as the final answer(s) (end).
- Semantic tools are developed by utilizing pretrained models to implement specific functions, including relation retrieval (retrieve_relation) and entity disambiguation (disambiguate_entity). These tools extend the basic operations on KGs and can support advanced functionalities for KG reasoning.


We summarize the detailed definition and usage of the tools in Table [8] at the Appendix [B]. Note that since the format and usage for each tool have been defined in a unified way, the toolbox for KG can be flexibly extended according to the real needs.


KG-Agent Instruction Tuning


To enable the autonomous reasoning process, we construct a high-quality instruction dataset for fine-tuning a small LLM (i.e., LLaMA2-7B). For this purpose, we first leverage existing KG based question answering (KGQA) datasets to generate the KG reasoning program, and then decompose it into multiple steps. Finally, each step is formulated as the instruction data with input and output.


KG Reasoning Program Generation


Instead of distilling from close-sourced LLMs (e.g., GPT-4), we propose to leverage existing KGQA datasets to synthesize the KG reasoning program. These KGQA datasets contain the annotated SQL queries that can be executed to directly extract the answer entities for each question. In particular, the SQL query generally includes the relation chain, conditions, or constraints, which are beneficial for reasoning program synthesis. Concretely, we first ground the SQL query on the KG to obtain a query graph, then extract the reasoning chain and constraint conditions from the query graph, and finally decompose the chain into multiple code snippets as the reasoning program.


Reasoning Chain Extraction. Since the whole


FIGURE 1


KG is extremely large and contains irrelevant data, the first step is to acquire a small KG subgraph related to the question, referred to as query graph. Following previous work ([Yin et al., 2020]), we obtain the query graph from the KG via rule match. As shown in Figure [1] (b), the query graph has a tree-like structure that can be directly mapped to a logical form ([Yin et al., 2020]), and it can clearly depict the execution flow of the SQL query to obtain the answer. Second, starting from the mentioned entity in the question (i.e., Cristiano Ronaldo), we adopt breadth-first search (BFS) to visit all the nodes on the query graph. This strategy would finally produce a reasoning chain (e.g., teams→roster_team) linking the start entity to the answer entity, and the relevant constraint conditions (e.g., roster_from = &quot;2011&quot;) or numerical operation (e.g., founded must be last) can be naturally involved in this process.


Reasoning Program Generation. After extracting the reasoning chain, we next convert it into multiple interrelated triples, where each triple generally corresponds to an intermediate reasoning step.
Finally, we reformulate the triples into several function calls with the code format, which represents the tool invocation and can be executed to obtain the corresponding triples based on the KG. Given a triple ⟨e, r, e′⟩, we craft a rule-based method to synthesize the function calls that represent the information flow from e to e′. Specifically, we start from the get_relation(e) function call to obtain the current candidate relations {r} associated with e on the KG.
Then, we select one relation r and pass it to other required function calls (e.g., get_tail_entity or get_entity_by_constraint), and finally obtain new entities.
Following the order of the reasoning chain, we generate all the function calls to compose the final KG reasoning program for producing the instruction dataset. We show one example in Figure [1] (b) to intuitively illustrate the conversion process from the annotated SQL query to our required KG reasoning program.


KG Reasoning Instruction Synthesis


After obtaining the reasoning program on KG, we further utilize it for synthesizing instruction data for supervised fine-tuning (SFT). As discussed in Section [4.2.1], our instruction data is highly based on the reasoning program, which is aligned with the intermediate reasoning steps for KGQA.


Input-Output Pair Construction. The synthetic KG reasoning program consists of multiple function calls in a sequence. For each function call, we aim to construct an input-output pair as the instruction.
Specifically, the input contains the question, toolbox definition, current KG information (i.e., thE next candidate relations of the current entity set), and history reasoning program before the current step; and the output is the function call at the current step. Next, after executing the function call at the current reasoning step, the history reasoning program and current KG information in the input will be accordingly updated, and the output will be updated as the function call at the next step. By iterating the above process, for each sample in the KGQA datasets, we can obtain multiple input-output pairs derived from the corresponding reasoning program, which depict the complete reasoning trajectory on the KG. To help LLMs better understand, we further utilize a unified prompt, as shown in Figure [1] (c), to format each input-output pair and obtain the final instruction tuning data.


TABLE 1


Agent Instruction Tuning. Based on the above formatted instruction tuning data, we perform supervised fine-tuning on a small LLM (i.e., LLaMA-7B), which is much smaller than the backbone models in previous work ([Jiang et al., 2023b]). Formally, for each sample, we formulate all input-output pairs of the complete trajectory in the format of *MATH* represent the input and ground-truth response in the t-th step and n represents the total steps. For simplicity, we denote each input and output as x and y below. During the instruction tuning process, we feed the input x and output y into the decoderonly LLM and minimize the cross-entropy loss on the ground-truth response y as: *MATH* where m denotes the number of tokens in y, yk and y\&lt;k are the k-th and previous tokens in the output.


Autonomous Reasoning over KG


After instruction tuning, we further design an effective agent framework that enables KG-Agent to autonomously perform multi-step reasoning over KG for answer finding. The overall illustration of KG-Agent is shown in Figure [1] (a). It mainly contains four components, i.e., the core instruction-tuned LLM (Section [4.2]), referred to as the LLM-based planner, the multifunctional toolbox (Section [4.1]), the KG-based executor for executing the tool invocation, and the knowledge memory to record the context and currently useful information in the whole process.
Next, we introduce how KG-Agent performs autonomous reasoning over KG.


Knolwedge Memory Initialization. The knowledge memory preserves the currently useful information to support the LLM-based planner for making decisions. It mainly contains four parts of information, i.e., natural language question, toolbox definition, current KG information, and history reasoning program. The former two parts are initialized with the given question and toolbox definition, which remain unchanged during the reasoning process. The later two parts are initialized as an empty list, which will be constantly updated at each step after LLM generating the function call and executor invoking the corresponding tool.


Planner for Tool Selection. Based on the current knowledge memory, the LLM-based planner selects a tool to interact with KG at each step. Specifically, all the parts in the current knowledge memory will be formatted with corresponding prompt template to compose the input (used in Agent Instruction Tuning in Section [4.2.2]), and then the LLM will generate one function call by selecting a tool and its arguments from the input. Generally, the planner needs to invoke tools from the pre-defined toolbox to address four types of task requirements, i.e., linking the mentioned entity to KG (e.g., &quot;get_candidate_entity&quot; and &quot;disambiguate_entity&quot;), accessing the KG information (e.g., &quot;get_relation&quot; and &quot;get_head_entity&quot;), processing the intermediate results (e.g., &quot;count&quot; and &quot;intersect&quot;), or returning the final answer to end the reasoning process (e.g., &quot;end&quot;).


Executor for Memory Updation. After the planner generates the function call, the KG-based executor will execute it using a program compiler. It can cache or operate the intermediate variables, and extract new entities or relations from the KG. After execution, the knowledge memory will be accordingly updated. First, the current function call will be added to the history reasoning program. Second, if the invoked tool is to obtain the new information from the KG (e.g., &quot;get_relation&quot;), the executor will add it to the KG information for updating the knowledge memory.


TABLE 2


Iterative Autonomous KG-Agent. The KG-Agent framework autonomously iterates the above tool selection and memory updation process to perform step-by-step reasoning, where the knowledge memory is used to maintain the accessed information from KG. In this way, the multi-turn decision-making process of the agent is like walking on the KG along relations. Once reaching the answer entities, the agent will automatically stop the iterative process. Note that the whole process is agnostic to the task types (e.g., question answering) and some specific KGs. Therefore, our approach is a general framework that can be applied to a variety of complex tasks that require reasoning over any KGs.


Comparison to Previous Work


Existing methods of reasoning over KG can be categorized into two classes based on their workflow. The first line of research, such as KB-BINDER ([Li et al., 2023]), Pangu ([Gu et al., 2023]), Struct-GPT ([Jiang et al., 2023b]), and RoG ([Luo et al., 2023]), crafted a pre-defined interaction way be-


tween LLM and KG, which cannot flexibly adapt to various complex tasks. Another line of research, such as ChatBD ([Hu et al., 2023a]), conducted autonomous reasoning with chain-of-thought and memory augmented. However, it relies on the strong closed-source LLM APIs (e.g., ChatGPT) and cannot use tools to implement some specialized operations (e.g., count). Our KG-Agent is the first autonomous agent framework to support the complex interaction between LLM and KG with tool and memory augmented. Furthermore, we implement this autonomous agent by instruction tuning a smaller 7B open-source LLM compared to the backbone LLM in KB-BINDER, Struct-GPT, and ChatDB. At the same time, the agent instruction tuning data is constructed from various KGs (e.g., Wikidata and Freebase), which helps our KG-Agent to learn the general autonomous decision making capabilities over various KGs.


Experiment


Experimental Setup


We select four commonly-used KGQA datasets as in-domain datasets, i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase, and KQA Pro, which is based on Wikidata. And we select three ODQA datasets as out-of-domain datasets, i.e., WQ, NQ, and TQ.
Further, we consider three types of baseline methods, i.e., subgraph-based reasoning, LM-based seq2seq generation, and LLM-based methods for comparison on in-domain datasets, and Fine-tune based and LLM-based methods for out-of-domain datasets.
We show the details of the above datasets, baselines, evaluation protocol, and implementation in Appendix [A].


TABLE 3 


TABLE 4


Main Results


Results on In-domain Datasets. Table [2] and Table [3] show the results on in-domain datasets based on Freebase and Wikidata, respectively. First, LM-based seq2seq generation methods can achieve better F1 score compared to the subgraph-based reasoning methods on the WebQSP and KQA Pro. It indicates that the SPARQL query generated by the LM can obtain a more complete answer set, and the structured query can better support some complex operations (e.g., maximum, count) than the traditional subgraph-based reasoning methods. Second, although LLMs are powerful, directly using Davinci-003, ChatGPT, and even GPT-4 still has a large performance gap compared with the best fine-tuned methods in WebQSP, GrailQA, and KQA Pro, indicating the difficulty of answering complex questions solely by LLMs.


Finally, our KG-Agent is substantially better than all other competitive baselines in all datasets after instructing tuning on the mixed data. With the mutual augmentation between different datasets, our approach achieves 1.7%, 7.5%, and 2.7% improvements of F1 on WebQSP, CWQ, and Grailqa, respectively. Benefiting from the autonomous reasoning mechnism, our approach can perform reasoning on the two KGs and obtain consistent improvement on all datasets.


TABLE 5


Results on Out-of-domain Datasets. After instruction tuning, we directly evaluate the zero-shot performance of our KG-Agent on the out-of-domain datasets. As shown in Table [4], although fine-tuned with full data, the small pre-trained language models (e.g., T5 and BART) can not effectively answer these factual questions. Owing to the large-scale parameters, Davinci-003 and Chat-GPT performs well on NQ and TQ, which are constructed based on Wikipedia, the corpus that they may have been pre-trained on. However, they perform not well on WQ, which is constructed based on Freebase KG. In contrast, our KG-Agent only needs to learn how to interact with KG instead of memorizing the specific knowledge. Thus, it can utilize the external KG in zero-shot setting, and achieve consistent improvement compared to fine-tuned pre-trained language models.


TABLE 6


Further Analysis


Transfer to Domain-specific KG. To evaluate the transferability of our approach on other KGs, we test our KG-Agent on the MetaQA dataset which is based on a movie domain KG. Following existing work ([He et al., 2021]; [Jiang et al., 2023b]), we show the one-shot results on the test set in Table [5]. ChatGPT performs not well when directly answering these domain-specific questions, where the performance drops 45% absolutely on the MQA-3hop subset compared to the supervised fine-tuned TransferNet model. After equipping the LLM with the KG, StructGPT can greatly outperform ChatGPT with about 37% improvement. In contrast, our KG-Agent can obtain consistent performance improvement compared to the competitive supervised fine-tuning baselines on all subsets. It indicates that the agent indeed learns the general ability about reasoning on KG, which can be efficiently transferred to other KGs.


Effect of Instruction Amount. We explore how the amount of instructions affects the performance of KG-Agent and show the results in Figure [2]. With a constant sampling proportion, we scale the total amount from 2k to 64k in an exponential way and evaluate the F1 and Hist@1 scores on WebQSP and CWQ datasets. As we can see, the performance increases with more instruction tuning data, and eventually reaches a stable state, which indicates the importance of data amount. At the same time, with the data amount increasing from 16k to 64k, the KG-Agent doesn&apos;t obtain a remarkable performance improvement. We think this is relevant to the variety of our instruction tuning data, which is illustrated in existing work ([Chung et al., 2022]; [Aribandi et al., 2022]). Therefore, we will construct more diverse samples in the future to further boost the performance.


Effect of Tuning Data Proportion. Our experiment finds that only sampling 10K samples from existing datasets is enough for backbone LLM to


learn the autonomous decision making capability. Here, we conduct a further ablation study to explore the impact of sampling proportion on the agent&apos;s performance when keeping the total amount of instruction tuning data constant. Specifically, we evaluate the agent performance of WebQSP, CWQ, and GrailQA when doubling the proportion of one dataset while maintaining the other two dataset proportions. We show the results in Table [6]. We can see that as the sampling proportion of a certain dataset increases, the agent performance on it consistently improves. However, for the average performance on all three datasets, all variants are lower than our selected proportion, indicating that the proportion we chose is suitable for the LLM to balance and master more comprehensive and general abilities.


FIGURE 2


Conclusion


In this work, we proposed an autonomous agent framework to synergize LLMs and KGs to perform complex reasoning over KG, namely KG-Agent. In our approach, we first curated a toolbox for KG, consisting of three types of tools to support the typical operations when reasoning on KG.
Then, we developed an autonomous iteration mechanism based on tool selection and memory updation that integrates the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, for reasoning over KG. Next, we leveraged existing KGQA datasets to synthesize the code-based instruction tuning dataset. Finally, with only 10K tuning samples, we implemented the autonomous agent relying on the smaller 7B LLM, which mostly outperforms state-of-the-art baselines based on full-data tuning or larger LLMs. In future work, we will consider extending our framework to deal with more types of structured data, e.g., databases and tables.