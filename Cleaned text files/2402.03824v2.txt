Introduction


Over recent years, the field of artificial intelligence (AI) has
experienced a significant surge, leading to substantial breakthroughs in
areas ranging from computer vision (CV) and natural language processing
(NLP) to neuroscience. This journey through AI&apos;s development has been
marked by a series of significant triumphs interspersed with setbacks,
including the well-documented AI winter of the mid-1980s. The ambitious
goal that has propelled AI research forward from the beginning was to
create intelligence that either parallels or exceeds human abilities.
This quest for superhuman intelligence, commonly termed Artificial
General Intelligence (AGI), has been seen differently by experts across
different disciplines, yet it broadly refers to the ability of a system
to understand, learn, and apply knowledge in a wide array of tasks and
contexts, mirroring the cognitive flexibility of humans and animals.


The remarkable progress in AI over the past decade can largely be
attributed to three pivotal developments: i) advancements in deep
learning algorithms, ii) the advent of powerful new hardware, and iii)
the availability of extensive datasets for training. A prime
illustration of this advancement is the creation of Large Language
Models (LLMs) like OpenAI&apos;s GPT-4 [*REF*] and Google&apos;s Gemini
[*REF*]. The surprising abilities of these LLMs have sparked
discussions within the AI community, with some pondering whether these
models have already achieved nascent forms of AGI. Foundation models
(large networks with billions of parameters trained on massive datasets)
have found success in varied fields, ranging from predicting 3D protein
structures [*REF*] and robotic control [*REF*],
to generating images and audio
[*REF*; *REF*]. This breadth of
achievement supports the hypothesis that continued scaling and
refinement of foundation models could be a viable path toward realizing
AGI.


In our paper, we argue that despite the significant advances made by
current AI technologies, they represent only the initial steps towards
truly intelligent agents. Despite their impressive capabilities, these
large networks are static and unable to evolve with time and experience.
They leverage large datasets and cutting-edge hardware for scaling, but
they lack the ability to properly care about the truth
[*REF*], which in turn makes it impossible to dynamically adjust
their knowledge and actively search for valuable new information. The
two primary manifestations of this fundamental shortfall are i) the
difficulty in effectively aligning LLMs [*REF*], and ii)
their propensity to generate plausible but inaccurate information, a
phenomenon known as confabulation
[*REF*; *REF*]. Current strategies to
mitigate these issues, such as post-processing, fine-tuning, prompt
engineering, and incorporating human feedback, are undeniably valuable.
However, we argue that these methods address only the superficial
aspects of the problem and fall short in dealing with the core issue at
play: the inherent lack of a deeper, grounded sense of care in LLMs.


Pursuing the development of AGI, we draw upon the insights of
*REF* to advocate for designing AI agents that are bound to,
observe, interact with, and learn from the real world (including humans)
in a continuous and dynamic manner. These [eai] agents ought to prioritize their
continued existence and our bindings to them, thereby learning the
value of truth. They should also be capable of adapting to
environmental changes and evolving without human intervention.


While Large Language Models play a significant role in the development
of AI systems, they fall short of capturing the essence of what
constitutes an intelligent agent. Notably, intelligent beings, whether
humans or animals, are characterized by three fundamental components:
the mind, perception, and action capabilities [*REF*].
LLMs, or more broadly, foundation models, may be likened to an aspect of
the mind&apos;s reasoning function [*REF*]. Yet, the perceptive and
action-oriented dimensions of intelligence, along with the pivotal
ability to dynamically revise beliefs and knowledge based on
experiences, remain unaddressed. Autoregressive LLMs are not designed to
understand the causal relationships between events, but rather to
identify proximate context and correlations within sequences
[*REF*]. In contrast, a fully embodied agent should have the
ability to grasp the causality underlying events and actions within its
environment, be it digital or physical. By comprehending these causal
relationships, such an agent can make informed decisions that consider
both the anticipated outcomes and the reasons behind those outcomes.


In short, this paper argues that the necessary next step in our
pursuit for truly intelligent and general AIs is the development and
study of Embodied AI. We propose that current LLM-based foundation
models could lay the groundwork for designing these agents, but are just
one component of a truly embodied agent. This approach is akin to how
neonates come into the world equipped with inherent priors to
successfully adapt to the world [*REF*].


In the next section of this paper, we will define the concept of
embodiment and what we mean by [eai] in
Sec., analyzing the literature and various
scientific and philosophical currents. In
Sec. [3], we discuss why we believe this is a
necessary step towards [agi]. In
Sec. [4] we analyze the main components of
a truly embodied agent, and we discuss the major challenges to achieving
this ambitious goal in
Sec. Our motivation behind the need to develop
[eai] and its fundamental role in our path towards [agi] is proposed throughout. Finally,
Sec. [6] provides a short recap of our proposition.


What is embodied AI


*MISSING WORD* is s sub-field of AI, focusing on agents that interact with their
physical environment, emphasizing sensorimotor coupling and situated
intelligence. As opposed to mere passive observing,
[eai] agents act on
their environment and learn from the reaction. [eai] is deeply rooted in embodied cognition
[*REF*; *REF*], a perspective in philosophy and cognitive
science that posits a profound coupling between the mind and the body.
This idea, challenging Cartesian dualism  ---  the historically dominant
view that distinctly separates the mind from the body
[*REF*]  ---  emerged in the early 20th century.
Pioneers like *REF* [*REF*] have
significantly contributed to this paradigm by proposing that reason is
not based on abstract laws but is grounded in bodily experiences.
Embodied cognition forms a critical part of the 4E cognitive science
framework [*REF*; *REF*; *REF*], encompassing embodied,
enactive, embedded, and extended aspects of cognition. Within
[eai], the focus is
predominantly on implementing the &apos;embodied&apos; and &apos;enactive&apos; aspects,
while the &apos;embedded&apos; and &apos;extended&apos; components are more pertinent to
situating AI in a social context and as an augmentation of human
(individual or collective) cognition.


In AI, initial explorations into embodiment emerged in the 1980s, driven
by a growing recognition of the inherent limitations in disembodied
agents. These limitations were primarily attributed to the absence of
rich, high-bandwidth interactions with the environment
[*REF*; *REF*]. An early advocate for this
paradigm shift was *REF*, who built walking robots
simulating insect-like locomotion. Simultaneously, the field of computer
vision was undergoing its own transformation. Researchers and
practitioners were increasingly focusing on enabling agents to interact
with their surroundings. This emphasis on interaction led to a
concentration on the perceptual elements of embodiment, particularly
from a first-person point of view (POV) [*REF*]. This
approach aligns with the concept of visual exploration and navigation
[*REF*], where an agent acquires information
about a 3D environment through movement and sensory perception, thereby
continuously refining its model of the environment
[*REF*; *REF*]. Such exploration techniques
empower an agent to discover objects and understand their permanence. As
a result of these developments, many contemporary benchmarks in
[eai] have emerged predominantly from the domains of vision and robotics [*REF*],
reflecting the integral role these disciplines have played in advancing
the field.


That said, the broader definition of [eai] does not require vision. Sensorimotor
coupling may be implemented using any physical sense [*REF*].
In the living world, many organisms survive and thrive without vision,
using, for example, chemical or electric sensing [*REF*].
*REF* &apos;s [tame] framework further explores this idea,
suggesting that cognition emerges from the collective intelligence of
cell groups, they themselves deeply embodied within their environment
(the body they comprise). This framework challenges traditional
Cartesian dualism, embedding cognition within the physical and
biological makeup of an organism. In the [tame] perspective, cognition is not just an
attribute of higher-order organisms; it extends throughout the
ontological hierarchy of living beings, from individual cells, through
tissues and organs, to complex organisms. Each agent demonstrates
cognitive capabilities that are inherently connected to its physical
structure and the environmental interactions at its proper level. This
broadened view of cognition and embodiment goes beyond the conventional
focus on vision in robotics and computer vision. It posits that any
entity capable of perceiving, interacting with, and learning from its
environment, thereby adapting to it and influencing it, qualifies as
embodied. A technological instantiation of this concept is an
intelligent router in a telecommunication network. This device &apos;lives&apos;
in a realm dominated by electromagnetic sensing. It continuously learns
from and adapts to the network traffic, effectively mapping and managing
the flow of information. This example underscores the potential of
applying the principles of [eai] beyond the traditional domains, embracing
a more inclusive and diverse understanding of intelligence and
embodiment.


This broadening of the notion of [eai] raises the question: how close current
commercial AI tools are to embodiment? Here we examine two such tools:
Large Language Models [*REF*; *REF*] and Social
Media Content AI Recommendation Systems (SMAI)
[*REF*; *REF*; *REF*].


LLMs operate within a linguistic-symbolic domain, representing textual
information and generating new text by completing prompts. Their
foundational training is essentially static, relying on datasets
meticulously compiled and curated by teams of AI engineers. Their goal
is supervised: to generate likely tokens following a context. Their
secondary training (fine-tuning) may involve both interactions with
their symbolic environment (human users) and goals to reach (satisfy
their human users), but these interactions are presents some limits due
to both technical (e.g., catastrophic forgetting
[*REF*; *REF*]) and business (e.g., managing
individuated LLMs [*REF*; *REF*]) reasons. Looking ahead,
we anticipate advancements that might address these limitations,
potentially leading to the emergence of &quot;personal assistant&quot; LLMs. These
would represent a form of embodied agents within a symbolic realm.
However, at present, LLMs largely resemble static
[iai] [*REF*], differing significantly from the dynamic, interactive
nature characteristic of [eai].


It is intriguing that despite the growing concerns about the risks and
alignment challenges of LLMs highlighted in recent research
[*REF*], SMAIs have attracted comparatively less scrutiny
[*REF*; *REF*]. This is noteworthy considering SMAIs have
been around for a longer time and their influence on society is both
wider and more profound. We propose that their widespread acceptance and
their more integrated, less intrusive presence in our lives are due to
their closer alignment with the principles of embodiment, in contrast to
LLMs.


What do we mean by SMAIs being closer to embodiment? Firstly, SMAIs are
driven by clear objectives: to captivate our attention and maximize our
engagement with their respective platforms [*REF*; *REF*].
These goals are fundamentally linked to the business models of these
platforms, which revolve around advertising. The specifics of these
&quot;engagement&quot; objectives are typically proprietary, forming the core of
the competitive advantage of these platforms. Although these goals are
initially human-designed and not intrinsically generated
[*REF*], they are subject to evolutionary pressure and
adaptation, and thus they are tied to the existence and the survival of
the SMAI. Secondly, SMAIs learn almost entirely from the data they
collect by interacting with us. This leads to a high level of
individuation (adapting to our individual preferences [*REF*]),
and notions of exploration (offering us content not so much to satisfy
us but for the sake of learning what we like). This creates a user
experience that, when well-executed, resembles interaction with a
considerate friend, who wants our best, who connects us to things we
like, and who wants to understand us better. The flip side, however, is
the potential for these systems to morph into mechanisms that perpetuate
addictive behaviors or harmful content [*REF*]. Nevertheless,
since SMAIs connect and adapt to us in a more intuitive and deeper
manner than LLMs, we often feel a greater sense of control over our
interactions with these systems (by, for example, consciously not
clicking on content that we know we do not want to see in the long run).
This control, albeit limited, is reminiscent of persuasion more than
mechanical manipulation, aligning with how we interact with other
sentient beings rather than machines. This type of relationship with AI
systems is a fundamental aspect of *REF* &apos;s TAME
proposal. Our stance on [eai] suggests that, while systems akin to
SMAIs pose greater risks due to their seamless integration into our
social fabric, they also present more natural opportunities for
alignment with our values. This alignment process is procedural,
perspectival, and evolutionary in nature [*REF*; *REF*],
contrasting with the primarily propositional approaches being applied to
LLMs [*REF*].


We posit that the potential for more effective and naturally aligned AI
systems is, alone, a compelling reason to prioritizing
[eai] in the broader
AI research agenda.


In the forthcoming section, we further explore the pivotal role that
well-executed implementations of [eai] could play in the quest for AGI.


Why embodiement? 


In the previous section, we examined how contemporary theories of
embodiment, particularly the TAME framework [*REF*],
challenge the long-standing Cartesian dualism which posits a distinct
separation between mind and body [*REF*]. This
philosophical stance has significantly influenced the development of
current generative AI models, such as LLMs, which primarily rely on
static data and lack interaction with the physical or even the symbolic
world. It is a prevalent belief that simply scaling up such models, in
terms of data volume and computational power, could lead to AGI. We
contest this view. We propose that true understanding, not only
propositional truth but also the value of propositions that guide us
how to act, is achievable only through [eai] agents that live in the world and learn
of it by interacting with it.


The significance of embodiment in cognitive development was demonstrated
by *REF* &apos;s carousel experiment with kittens. In this study,
one kitten could actively interact with and control a carousel, while
the other could only observe it passively. Despite both kittens
receiving identical visual input, the one engaged in active interaction
exhibited normal visual development, unlike its passively observing
counterpart. This seminal experiment underscores the vital role of
embodied interaction in shaping cognitive abilities
[*REF*]. It also reinforces the observation that
all known forms of intelligence, including human intelligence, are
inherently embodied [*REF*], suggesting that embodiment
serves as a solid foundation for cognitive learning and development.
Current AI learns in a very different way from humans. We humans learn
by seeing, moving, interacting with the world and speaking with others.
We also learn by collecting sequential experiences, not by passive
observation of shuffled and randomized, even if carefully selected, data
[*REF*; *REF*]. We advocate for an approach
where insights from cognitive science and developmental psychology
inform the design of AI systems. Such systems should be designed to
learn through active interaction with their surroundings, mirroring the
embodied learning processes fundamental to human cognition.


Even advocates of static learning concede that multimodal learning is
the next milestone towards AGI
[*REF*; *REF*]. In [iai], multimodal data needs to be collected and connected painstakingly. In contrast,
[eai] agents, when equipped with multimodal sensors, will inherently collect and correlate
multi-modal data by mere co-occurrence. For instance, robots will see
(CV), communicate (NLP), reason (general intelligence), navigate and
interact with their environment (planning and RL), all simultaneously
[*REF*]. Intelligent routers will observe
requests and traffic (sensing), communicate with other routers, human
engineers, absorb news about their surroundings (NLP), reason (general
intelligence), and control the traffic (control and RL). Despite the
impressive progress in these domains, much of it has relied on the
external collection and curation of vast datasets for algorithmic
training. This approach has significant drawbacks: i) the collection and
preparation of data demands substantial investments; ii) this data can
contain biases that are hard to detect and rectify
[*REF*; *REF*; *REF*]. The issue of
biases is particularly pertinent in discussions on AI alignment
[*REF*; *REF*]. Efforts to align AI through rule-based and
procedural methods (such as RLHF [*REF*]) often
struggle, producing systems that feel mechanistic and &quot;dumb&quot;, rather
than an agent which seamlessly acts according to values compatible
with our society.


An embodied agent, designed to interact with and learn from its
environment, fundamentally changes the traditional approach to data
collection and curation in AI development. By being inherently
integrated with its physical and social contexts, such an agent bypasses
the labor-intensive processes previously required. This shift not only
simplifies the challenge of aligning AI with human values but also
enhances the agent&apos;s learning efficiency by utilizing the unique
features of its environment. As a result, the focus in AI development
transitions from data to simulators. These simulators serve a dual
purpose: they are both training grounds for [eai] and platforms for testing and refining
concepts and algorithms [*REF*]. Moreover, the process of
aligning these agents with human values becomes more intuitive as it
involves defining goals reflective of those values. This approach does
not claim to fully resolve the alignment challenge, as
[eai] systems will still necessitate oversight and guidelines to avert unwanted behaviors.
However, the alignment process becomes inherently more natural.
Adjusting and defining goals is a more straightforward task than the
extensive editing and curating of data. This methodology draws upon
our inherent, non-propositional understanding and instincts about
aligning embodied intelligences --- whether it is guiding our own actions,
nurturing children, or training pets.


Another important characteristic of [eai], stemming from the coupling between the
agent and its environment, is the agent&apos;s capacity for ongoing evolution
and adaptation. This adaptability is vital for any agent destined to
navigate a world in perpetual change. It underscores the importance of
continual learning: the process of assimilating new experiences while
retaining previously acquired knowledge [*REF*].


Moreover, *REF* have shown, both through theory and
practical application in robotics, that a close and effective
integration of control mechanisms with body dynamics significantly
enhances energy efficiency. Coupled systems lead also to the emergence
of intriguing behaviors that can be hard to explicitly program or learn
from disembodied datasets [*REF*], an observation
aligning with the principles of the TAME framework.


Embodiment is also a prerequisite for learning about affordances
[*REF*]. Learning, or more precisely realizing affordances,
according to *REF* &apos;s perspectival learning, is a fundamental
capacity of [agi], as affordances are what &quot;fill our world with meaning&quot; [*REF*],
and are thus necessary for agents that give meaning to their own world.
Affordances emerge from the dynamic interplay between an agent&apos;s
perception, objectives, abilities, and the characteristics of objects
and contexts within the environment; for example, a chair affords us to
sit, a glass to drink and a hand to grasp and pick up objects.
*REF* argue that the capacity to comprehend, utilize, and
be influenced by environmental affordances distinguishes biological
intelligence from current artificial systems. Besides affordances,
[eai] is also indispensable for investigating emergent phenomena such as qualia
[*REF*; *REF*], consciousness [*REF*], as
well as creativity, empathy [*REF*], and ethical understanding [*REF*; *REF*].


Finally, there is the important question of why an intelligent agent
would do anything in the first place [*REF*]. What drives
it to engage and acquire new knowledge without external prompts? Within
well-framed small worlds, such as a chess game, an agent&apos;s purpose is
straightforward: deciding the next move. However, when navigating large,
open worlds, the motivations guiding an agent&apos;s decisions grow
increasingly ambiguous. The concepts of active inference and the free
energy principle [*REF*; *REF*] provide a compelling
framework for understanding the behaviors of intelligent agents. This
principle posits that minimizing surprise and uncertainty is the core
objective of the agents. They achieve this through the use of internal
models to forecast outcomes, continually updating these models with
sensory input, and proactively modifying their surroundings to better
match their expectations. This concept resonates within the AI
community, particularly in the design of agents equipped with mechanisms
for intrinsic motivation[*REF*; *REF*], which incentivize agents
to explore and acquire new knowledge to reduce uncertainty.


However, what propels an intelligent agent to act, especially beyond
mere survival instincts, continues to be a matter of debate. We argue
that exploring and developing embodied agents will illuminate this
question. Thus, [eai] not only shows potential for significant breakthroughs toward achieving
AGI, but also has deep implications for our understanding of cognition in general.


Theoretical framework 


In previous sections, we have underscored the pivotal role of
[eai] in advancing toward AGI. Shifting focus, we now delve into the essential components
that, we believe, will comprise [eais]. We draw heavily on the concept of
cognitive architectures designed by cognitive scientists aiming to
model the human mind [*REF*]. Despite the promise these
architectures hold for enhancing modern machine learning methods,
progress on this has been notably limited [*REF*]. The slow
advancement is largely due to cognitive architectures being the domain
of neuroscientists and cognitive scientists, with only a select few
within the machine learning community exploring their potential for AGI.
We advocate for a synergistic strategy that marries cognitive
architectures with machine learning within the [eai] paradigm, proposing it as a viable path
toward AGI. The emergence of agent-based LLMs, such as AutoGPT
[*REF*], which pioneers the generation of autonomous agents, and
PanguAgent [*REF*], an agent-focused language model,
indicate the potential of this approach.


We identify four essential components of an [eai] system: (i) perception: the ability of
the agent to sense its environment; (ii) action: the ability to interact
with and change its environment; (iii) memory: the capacity to retain
past experiences; and (iv) learning: integrating experiences to form new
knowledge and abilities. These components are notably aligned with the
active inference framework of *REF*. In this framework, the agent
models its world through a probabilistic generative model that infers
the causes of its sensory observations (perception). This model is
hierarchical, forecasting future states in a top-down manner and
reconciling these predictions with bottom-up sensory data, with
discrepancies or errors being escalated upwards only when they cannot be
reconciled at the initial level. The agent acts to minimize the
divergence between its anticipations and reality, thus moving towards
states of reduced uncertainty (action). Concurrently, it collects and
stores new information about its environment (memory) and refines its
internal model to minimize predictive errors (learning). In the sections
that follow, we will describe in detail these four components and how
they comprise the [eai] agent.


Perception


At the heart of an embodied agent lies the ability to perceive the world
in which it exists. Perception is a process by which raw sensory data is
transformed into a structured internal representation, enabling the
agent to engage in cognitive tasks. The range of inputs that inform
perception is vast, encompassing familiar human senses such as vision,
hearing, smell, touch, and taste. It extends to any form of stimuli an
agent might encounter, be it force sensors in robotics or signal
strength indicators in wireless technology. The challenge with sensory
data is that it is often not immediately actionable. It typically
undergoes a process of transformation, a task where recent advances in
machine learning can prove invaluable. The field has seen the
development of sophisticated methods for learning feature and embedding
spaces, facilitating the conversion of raw data into meaningful
information [*REF*; *REF*]. A
particularly effective strategy has been self-supervised learning to
learn such representations. Although much of the research has
concentrated on single modalities, such as vision [*REF*],
the principles underlying these techniques are universally applicable
across different sensory inputs [*REF*; *REF*].


Action


Embodied agents navigate the world by taking actions and observing the
outcomes. Acting can be broken down into two steps: (i) choosing what
action to undertake next, like deciding to relocate to a specific spot,
and (ii) determining how to execute this action, such as plotting the
course to that location. Actions can further be categorized into
reactive and goal-directed types. Reactive actions, akin to human
reflexes, occur almost instantaneously in response to stimuli and play a
crucial role in an agent&apos;s immediate self-preservation by maintaining
stability. Goal-directed actions, on the other hand, involve strategic
planning and are motivated by high-level objectives. Reactive actions
are important for self-preservation, with model-free reinforcement
learning methods playing an important role for developing reactive
control policies in tasks like robot walking [*REF*]. On
the other hand, for an agent to achieve more complex, high-level
objectives, planning is indispensable, even if efficient planning
remains an open area of research [*REF*; *REF*]. Central
to the concept of planning is the presence of a &quot;world model&quot; within the
agent, which it can use to predict the consequences of its own actions.
Model-based RL has made significant strides in developing algorithms
that learn these world models and use them for planning
[*REF*; *REF*; *REF*].


Memory


Embodied agents learn from their experience, which are stored in memory.
Memory encompasses various dimensions, including its duration
(short-term or long-term) and its nature (procedural, declarative,
semantic, and episodic). Importantly, memory is not necessarily
represented as explicit propositional knowledge; it can be implicitly
encoded into the weights of a [nn]. To navigate cognitive tasks, agents
require diverse types of memory systems, each playing a distinct role.
Working and short-term memory offer temporary storage to support the
agent&apos;s immediate objectives. Long-term and episodic memories provide a
reservoir for information over longer time. Episodic memory captures and
stores unique, perspectival experiences, ready to be accessed when
familiar scenarios unfold. Long-term memory, conversely, is the
repository for broader propositional knowledge. LLMs, for example,
implement long-term memory using [rag] [*REF*], a technique
that reduces hallucinations using an external database. This technique
showcases how sophisticated machine learning methods can be synergized
with cognitive architectures.


Learning


A defining trait of intelligent agents is their ability to learn. Yet,
how to learn, especially in a continuous and dynamic way, remains a
subject of ongoing research and debate
[*REF*; *REF*]. While recent strides in
AI have largely been powered by training on static datasets, the concept
of continual learning, essential for adapting over time, faces
challenges. These challenges stem primarily from the inherent
limitations of deep NNs, such as catastrophic forgetting
[*REF*], and the complexities associated with learning
from non-stationary data that result from an agent&apos;s interaction with
its environment [*REF*]. The embodiment hypothesis
suggests that true intelligence is born from such interactions
[*REF*], underscoring the need for dynamic learning
methodologies. In this context, simulators emerge as a vital tool,
offering a shift away from the static learning typical of traditional
AI. Instead, they enable agents to evolve through ongoing, interactive
experiences within simulated environments [*REF*].


Challenges


[eais] agents will adopt an egocentric perspective, experiencing their environment from a
first-person viewpoint, in contrast to the allocentric perspective
prevalent in current AI systems. This shift is not only essential for
meaningful interaction with the world but also offers an advantage by
allowing the agents to focus on modeling their immediate surroundings
rather than the entirety of the world. On the other hand,
[eais] introduces several challenges, including extending current learning theories,
managing noise in perception and action effectively and safely, and
ensuring meaningful communication with humans that adheres to ethical
standards. The remainder of this section will cover these challenges,
exploring potential pathways and solutions.


New learning theory 


The principles of [eai] challenge us to reevaluate traditional
learning theories [*REF*; *REF*], bridging a gap between
supervised and reinforcement learning. Supervised learning, while
foundational in AI, assumes that the data is drawn from an unknown but
fixed distribution, collected independently of the learning process.
This theory gives rise to the classical notions of generalization, over-
and underfitting, bias and variance, and asymptotic or finite-sample
statistical consistency. This framing is obviously highly useful: even
those who are not explicitly doing theory use it transparently as their
lingua technica and cognitive scaffolding when working with algorithms
and analyzing results.


When embodied agents interact dynamically with their environment, data
collection becomes part of the data science pipeline
[*REF*; *REF*]. Classical supervised learning theory
is insufficient to analyze these cases and to guide algorithm building.
Extensions, like transfer learning [*REF*], multitask learning
[*REF*], distribution shift [*REF*], domain adaptation
[*REF*] or out-of-distribution generalization, have been proposed
to patch basic supervised learning theory, but most of these cling to
the original framing, pretending that the data is coming from outside
the learning process, encapsulating the value (business or otherwise) of
the predictive pipeline. Practically, this is obviously not the case:
the data on which we learn a predictor is often collected by the data
scientist, responsible for the quality of the pipeline
[*REF*; *REF*]. Furthermore, most of the debates around
responsible AI turn around the data, not the learning algorithm
[*REF*; *REF*]. Collecting, selecting, and curating data is
obviously part of the pipeline. The text we use to train LLMs is created
by its writers, rather than drawn from a distribution. In some cases,
when collection and model-retraining are automated, the situation may be
even worse. For example, in click-through-rate prediction
[*REF*; *REF*] or recommendation systems
[*REF*], the deployed predictor affects the data for
the next round of training, generating an often adversarial feedback. A
similar phenomenon is happening in the LLM world: as these AIs become
the go-to tools for creative and business writing, the data collected
for the next round of training will, in large part, be coming from the
previous generation of LLMs.


Reinforcement Learning [*REF*] and related paradigms (Bayesian
optimization [*REF*] or contextual bandits [*REF*]) offer
a closer fit for embodied AI, when the prediction is not the
end-product, rather part of a predictive pipeline that also includes
data collection. RL affords the data scientist to design a higher-level
objective, letting the algorithm optimize both the predictor and the
data it is trained on. Here, the mismatch between theory and practice is
different from supervised learning. The analysis in RL or bandit theory
often focuses on the convergence of the agent to a theoretical optimum,
given a fixed but often unknown environment. RL theory usually does not
offer tools to analyze the data collected during the learning process,
especially when the collection is semi-automatic (includes a human
curator in the loop). RL agents, in practice, usually do not converge
even in a stationary environment, they rather individuate, making, for
example, quite perversely, the random seed part of the algorithm
[*REF*]. This is even more pronounced in non-stationary
environments where the agent&apos;s actions alter the environment; a
situation which AGI will definitely find itself
[*REF*; *REF*].


A new learning theory for embodied AI must transcend these limitations.
It should account for the dynamic, interactive nature of data in
[eai], where the agent&apos;s actions continuously reshape its learning environment. This
theory should not just aim for optimal performance in a fixed setting
but should embrace a spectrum of behaviors suitable for evolving
environments. Moreover, it should provide diagnostics to assess the
quality and relevance of data generated through these interactions.


Noise and uncertainty


[eai] agents are tasked with navigating the real world, rife with noise and uncertainty.
These elements can drastically affect both the agent&apos;s perception of its
surroundings and the quality of its decision-making. For example,
elevated noise levels may distort the agent&apos;s interpretation of
environmental cues, leading to suboptimal decisions. This challenge is
accentuated in an egocentric perspective, where agents frequently
encounter continuous streams of fluctuating and imprecise data. Sources
of noise include the natural imprecision of sensors and actuators, which
might lack accuracy due to manufacturing inconsistencies, degradation
over time, or external disturbances. Additionally, quantization error, a
byproduct of converting analog signals into digital form
[*REF*], can further compromise data integrity.


As these agents learn and adapt to their environment, they must also
grapple with uncertainty. This uncertainty can obscure the agent&apos;s
understanding of its environment, influencing its performance. This
dilemma is especially prevalent in RL scenarios dealing with partial
observability, where decisions must be made with incomplete information,
leading to uncertainty in predicting the outcomes of its actions
[*REF*; *REF*; *REF*]. Therefore, managing
noise and uncertainty effectively is paramount for the progress of[eai].


Simulators


As we pivot towards [eai], simulators will assume a fundamental
role as a key driver of progress, similar to the role data sets play in
the training of traditional [iai] models. These simulators offer a
controlled, replicable environment where AI systems can be rigorously
trained and tested. This setup allows for learning and adapting to
diverse scenarios prior to deployment, ensuring both safety and
cost-efficiency. A notable advantage of simulators, and requirements, is
their speed and ease of parallelization, significantly accelerating
training time, making it more feasible to train sophisticated AI models
on multiple scenarios simultaneously.


Many advanced simulators have been introduced recently, yet they often
demand significant computational resources and are predominantly geared
towards robotics applications
[*REF*; *REF*; *REF*; *REF*; *REF*].
For these simulators to truly serve the needs of [eai], they must
expand their scope to a broader spectrum of environments. A major
challenge in the use of simulators is bridging the &quot;reality gap&quot;
[*REF*]: the difference between simulated conditions and
the agent&apos;s eventual real-world or virtual deployment context
[*REF*]. This gap can lead to a situation where models
that excel in simulations fail in actual application, undermining the
effectiveness of the training process. Despite numerous strategies being
put forward to mitigate the reality gap
[*REF*; *REF*; *REF*; *REF*; *REF*],
it remains an unresolved issue in the field, challenging the
applicability of simulated training environments.


Interaction with humans


A key ambition of [eai] is to seamlessly interact with and learn
from humans, enhancing AI&apos;s ability to offer personalized and impactful
solutions. By improving these interactions, [eai] will also diminish fear and mistrust
towards AI technologies, leading to broader acceptance and integration.
In this endeavor, LLMs stand out as particularly beneficial, with their
ability to comprehend and produce human-like text, facilitating
communication in natural language and making engagements with AI more
natural and accessible. The domain of Human-Robot Interaction (HRI)
offers valuable lessons for enhancing AI-human communication, as
researchers in this domain have dedicated efforts to explore innovative
methods for robots to better communicate with us
[*REF*; *REF*]. Yet, the challenge of
ensuring proper and ethical communication with AI systems persists. The
effectiveness of LLMs, for instance, hinges significantly on their
training and how well they are aligned with human intentions and values
[*REF*]. Integrating human oversight directly into the AI
development process and establishing comprehensive guidelines and
protocols for AI communication are among the proposed strategies to
address these challenges, aiming to make AI interactions more meaningful
and ethically sound.


Generalization


An important issue in AI is generalization. There have been many
attempts at developing systems capable of quickly generalizing to
settings unseen at training time [*REF*] in the same
fashion living beings do. Nonetheless this is still an open problem that
will likely afflict embodied AIs as well, as acting in the real world
exposes the agent to situations unseen at training time. For instance,
consider a service robot trained in a simulated environment. When placed
in a real household, it may encounter novel objects and behaviors not
present in its training data, leading to suboptimal or even erroneous
actions. This illustrates the critical need for AIs that can adapt and
generalize beyond their initial programming. A promising direction in
addressing this problem is the leveraging of the enormous amount of
internet data. LLMs have demonstrated remarkable zero-shot learning
capabilities with minimal fine-tuning [*REF*]. We can
envision that some form of pretraining on internet datasets can
kick-start the AI before its embodied phase, enhancing generalization
and adaptability.


Recent developments in robotics have started exploring this research
direction. *REF* used a mixed approach between I-AI and E-AI to
effectively control multiple robots in different settings. However, only
relying on internet data is insufficient. An important aspect is also
the ability to accurately identify unknown situations and avoid
overconfidence, a common shortcoming of LLMs, that often produce
plausible-sounding response that are factually incorrect
[*REF*]. The ability to assess its own uncertainty is
essential, and can prompt the AI to seek human assistance, similarly to
how infants ask for help in their early development. We believe that,
while the integration of I-AI and E-AI will prove necessary as
foundation for the development of the next generation of intelligent
systems, the active learning paradigm and precise uncertainty estimation
are vital. Active learning, where the AI actively queries for
information when uncertain, combined with reliable uncertainty
estimation, can enable an E-AI agent to manage novel situations
effectively.


Finally, we believe that to properly address the issue of
generalization, the community must first clearly define what is the
meaning of &quot;generalization&quot;. Currently, discussions around this issue
often rely on vague terms, referring to an agent&apos;s ability to adapt to
unseen settings or data. However, without a formal definition, it is
challenging to assess or improve generalization effectively.


Consider the varying degrees of generalization required in different
scenarios: transferring skills from driving a car to driving a bus
represents generalization within a similar domain, whereas adapting from
walking to swimming involves a more profound shift in the type of task.
These examples illustrate the spectrum of generalization challenges that
embodied AI might face.


To advance this field, it is imperative to develop a precise definition
of generalization and establish standardized metrics and benchmarks for
measuring an AI&apos;s generalization capabilities
[*REF*]. This necessity ties back to our discussion in
Section [5.1], highlighting the urgent need for a
new learning theory that can provide a principled approach to developing
agents that generalize well. Addressing these questions will hopefully
lead to more precise and principled approaches in the development of the
field.


Hardware limitations


A significant challenge to the broad-scale development and integration
of [eai] lies in the hardware requirements of these AI systems. Presently, AI technologies
largely depend on GPU clusters, which are, while powerful, not ideally
suited for embodied agents due to their high cost, energy consumption,
and extensive heat output. Additionally, the physical bulk and heft of
GPUs pose logistical challenges for mobile agents or those operating
within spatial limitations. Addressing these constraints necessitates
the innovation of new, energy-efficient hardware solutions that can be
embedded within the agents. Promising developments are on the horizon,
with Google&apos;s Tensor Processing Unit (TPU)
[*REF*; *REF*] and Huawei&apos;s Ascend chip
[*REF*] leading the charge. These advancements, coupled with
the potential of neuromorphic computing and the strategic synergy of
hardware-software co-design, signal a new era of hardware capability.
Moreover, the development of energy and data-efficient algorithms is
critical. Such breakthroughs in hardware and algorithm efficiency will
have a direct and profound effect on an AI&apos;s ability to understand,
decide, and interact within its environment, enabling
[eai] agents to operate more autonomously and effectively in a diverse array of settings.


Conclusion 


In this paper, we have articulated the critical role Embodied AI plays
on the path toward achieving AGI, setting it apart from prevailing AI
methodologies, notably LLMs. By integrating insights from a spectrum of
research fields, we underscored how E-AI&apos;s development benefits from
existing knowledge, with LLMs enhancing the potential for intuitive
interactions between humans and emerging AI entities. We introduced a
comprehensive theoretical framework for the development of E-AI,
grounded in the principles of cognitive science, highlighting
perception, action, memory, and learning, situating E-AI within the
context of Friston&apos;s active inference framework, thereby offering a
wide-ranging theoretical backdrop for our discussion. Despite the
outlook, the journey ahead is fraught with challenges, not least the
formulation of a novel learning theory tailored for AI and the creation
of sophisticated hardware solutions. This paper aims to serve as a
roadmap for ongoing and future research into E-AI, proposing directions
that could lead to significant advancements in the field.


Impact Statement 


While the development of Embodied AI introduces complexities and
challenges, particularly in hardware requirements, ethical
considerations, and safety protocols, the potential benefits
significantly outweigh these drawbacks. E-AI stands to evolve our
interaction with technology, imbuing AI with a deeper understanding of
and engagement with both the physical world and human society. This not
only paves the way for more natural and effective human-AI interactions
but also enhances AI&apos;s adaptability and application across a broad spectrum of fields.