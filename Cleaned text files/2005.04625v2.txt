BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps


Introduction 


Autonomous agents such as household robots need to interact with the
physical world in multiple modalities. As an example, in
vision-and-language navigation (VLN) [*REF*], the agent
moves around in a photo-realistic simulated environment [*REF*]
by following a sequence of natural language instructions. To infer its
whereabouts so as to decide its moves, the agent infuses its visual
perception, its trajectory and the instructions [*REF*; *REF*; *REF*; *REF*; *REF*].


Arguably, the ability to understand and follow the instructions is one
of the most crucial skills to acquire by VLN agents. *REF* shows
that the VLN agents trained on the originally proposed dataset
[Room2Room](i.e.[r2r] thereafter) do not
follow the instructions, despite having achieved high success rates of
reaching the navigation goals. They proposed two remedies: a new dataset
[Room4Room](or [r4r]) that doubles the path
lengths in the [r2r], and a new evaluation metric Coverage
weighted by Length Score (CLS) that measures more closely whether the
ground-truth paths are followed. They showed optimizing the fidelity of
following instructions leads to agents with desirable behavior.
Moreover, the long lengths in [r4r] are informative in
identifying agents who score higher in such fidelity measure.


In this paper, we investigate another crucial aspect of following the
instructions: can a VLN agent generalize to following longer
instructions by learning from shorter ones? This aspect has important
implication to real-world applications as collecting annotated long
sequences of instructions and training on them can be costly. Thus, it
is highly desirable to have this generalization ability. After all, it
seems that humans can achieve this effortlessly.


To this end, we have created several datasets of longer navigation
tasks, inspired by [r4r] [*REF*]. We trained VLN
agents on [r4r] and use the agents to navigate in
[Room6Room](i.e., [r6r]) and [Room8Room](i.e., [r8r]). We contrast to the
performance of the agents which are trained on those datasets directly
(&quot;in-domain&quot;). The results are shown in Fig. [1].


FIGURE


Our findings are that the agents trained on [r4r](denoted by
the purple and the pink solid lines) perform significantly worse than
the in-domain agents (denoted the light blue dashed line). Also
interestingly, when such out-of-domain agents are applied to the
dataset [r2r] with shorter navigation tasks, they also
perform significantly worse than the corresponding in-domain agent
despite [r4r] containing many navigation paths from
[r2r]. Note that the agent trained to optimize the
aforementioned fidelity measure ([rcm](fidelity)) performs
better than the agent trained to reach the goal only
([rcm](goal)), supporting the claim by *REF* that
following instructions is a more meaningful objective than merely
goal-reaching. Yet, the fidelity measure itself is not enough to enable
the agent to transfer well to longer navigation tasks.


To address these deficiencies, we propose a new approach for VLN. The
agent follows a long navigation instruction by decomposing the
instruction into shorter ones (&quot;micro-instructions&quot;, i.e.,
Baby-Steps), each of which corresponds to an intermediate
goal/task to be executed sequentially. To this end, the agent has three
components: (a) a memory buffer that summarizes the agent&apos;s experiences
so that the agent can use them to provide the context for executing the
next [Baby-Step]. (b) the agent first learns from human
experts in &quot;bite-size&quot;. Instead of trying to imitate to achieve the
ground-truth paths as a whole, the agent is given the pairs of a
[Baby-Step] and the corresponding human expert path so that
it can learn policies of actions from shorter instructions. (c) In the
second stage of learning, the agent refines the policies by
curriculum-based reinforcement learning, where the agent is given
increasingly longer navigation tasks to achieve. In particular, this
curriculum design reflects our desiderata that the agent optimized on
shorter tasks should generalize well to slightly longer tasks and then
much longer ones.


While we do not claim that our approach faithfully simulates human
learning of navigation, the design is loosely inspired by it. We name
our approach [BabyWalk] and refer to the intermediate
navigation goals in (b) as Baby-Steps. Fig. [1] shows
that [BabyWalk](the red solid line) significantly
outperforms other approaches and despite being out-of-domain, it even
reach the performance of in-domain agents on [r6r] and [r8r].


The effectiveness of [BabyWalk] also leads to an interesting
twist. As mentioned before, one of the most important observations by
*REF* is that the original VLN dataset [r2r] fails
to reveal the difference between optimizing goal-reaching (thus ignoring
the instructions) and optimizing the fidelity (thus adhering to the
instructions). Yet, leaving details to section [5], we have also shown that applying
[BabyWalk] to [r2r] can lead to equally strong
performance on generalizing from shorter instructions (i.e., [r2r]) to longer ones.


In summary, in this paper, we have demonstrated empirically that the
current VLN agents are ineffective in generalizing from learning on
shorter navigation tasks to longer ones. We propose a new approach in
addressing this important problem. We validate the approach with
extensive benchmarks, including ablation studies to identify the
effectiveness of various components in our approach.


Related Work 


Vision-and-Language Navigation (VLN)


Recent works [*REF*; *REF*; *REF*; *REF*; *REF*]
extend the early works of instruction based
navigation [*REF*; *REF*; *REF*] to
photo-realistic simulated environments. For instance,
*REF* proposed to learn a multi-modal Sequence-to-Sequence
agent (Seq2Seq) by imitating expert demonstration. *REF* 
developed a method that augments the paired instruction and
demonstration data using a learned speaker model, to teach the
navigation agent to better understand instructions.   *REF* 
further applies reinforcement learning (RL) and self-imitation learning
to improve navigation agents. *REF* [*REF*] designed
models that track the execution progress for a sequence of instructions
using soft-attention.


Different from them, we focus on transferring an agent&apos;s performances on
shorter tasks to longer ones. This leads to designs and learning schemes
that improve generalization across datasets. We use a memory buffer to
prevent mistakes in the distant past from exerting strong influence on
the present. In imitation learning stage, we solve fine-grained subtasks
(Baby-Steps) instead of asking the agent to learn the
navigation trajectory as a whole. We then use curriculum-based
reinforcement learning by asking the agent to follow increasingly longer
instructions.


Transfer and Cross-domain Adaptation


There have been a large body of works in transfer learning and
generalization across tasks and environments in both computer vision and
reinforcement learning [*REF*; *REF*; *REF*; *REF*; *REF*; *REF*].
Of particular relevance is the recent work on adapting VLN agents to
changes in visual environments [*REF*; *REF*]. To our best
knowledge, this work is the first to focus on adapting to a simple
aspect of language variability --- the length of the instructions.


Curriculum Learning


Since proposed in [*REF*], curriculum learning was
successfully used in a range of tasks: training robots for goal
reaching [*REF*], visual question answering [*REF*], image generation [*REF*]. To
our best knowledge, this work is the first to apply the idea to learning in VLN.


Notation and the Setup of VLN 


In the VLN task, the agent receives a natural language instruction
*MATH* composed of a sequence of sentences. We model the agent
with an Markov Decision Process (MDP) which is defined as a tuple of a
state space *MATH*, an action space *MATH*, an initial
state *MATH*, a stationary transition dynamics *MATH*, a reward function
*MATH*, and the discount factor *MATH* for weighting future rewards. The agent acts according
to a policy *MATH*. The state and action spaces are defined the same as in [*REF*]
(cf. § [4.4] for details).


For each *MATH*, the sequence of the pairs *MATH* is called a trajectory
*MATH* where *MATH* denotes the length of the sequence
or the size of a set. We use *MATH* to denote an action
taken by the agent according to its policy. Hence, *MATH* 
denotes the agent&apos;s trajectory, while *MATH* (or *MATH*)
denotes the human expert&apos;s trajectory (or action). The agent is given
training examples of *MATH* to optimize its policy
to maximize its expected rewards.


In our work, we introduce additional notations in the following. We will
segment a (long) instruction *MATH* into multiple shorter
sequences of sentences *MATH*, to which we refer
as Baby-Steps. Each *MATH* is interpreted as a
micro-instruction that corresponds to a trajectory by the agent
*MATH* and is aligned with a part of the human
expert&apos;s trajectory, denoted as *MATH*. While the alignment
is not available in existing datasets for VLN, we will describe how to
obtain them in a later section (§ [4.3]).
Throughout the paper, we also freely interexchange the term &quot;following
the *MATH* th micro-instruction&quot;, &quot;executing the
[Baby-Step] *MATH* &quot;, or &quot;complete the *MATH* th subtask&quot;.


We use *MATH* to denote the (discrete) time steps the agent takes actions. Additionally, when the
agent follows *MATH*, for convenience, we sometimes use
*MATH* to index the time steps, instead of the &quot;global time&quot; *MATH*.


Approach 


We describe in detail the 3 key elements in the design of our navigation
agent: (i) a memory buffer for storing and recalling past experiences to
provide contexts for the current navigation instruction (§ [4.1]);
(ii) an imitation-learning stage of navigating with short instructions
to accomplish a single [Baby-Step](§ [4.2.1]); (iii) a curriculum-based reinforcement
learning phase where the agent learns with increasingly longer
instructions (i.e. multiple Baby-Steps) (§ [4.2.2]). We describe new benchmarks created for
learning and evaluation and key implementation details in § [4.3] and § [4.4]
(with more details in the Appendix).


FIGURE


The [BabyWalk] Agent 


The basic operating model of our navigation agent [BabyWalk]
is to follow a &quot;micro instruction&quot; *MATH* (i.e., a short
sequence of instructions, to which we also refer as
[Baby-Step]), conditioning on the context *MATH* and to output a trajectory
*MATH*. A schematic diagram is shown in Fig. [2]. Of
particularly different from previous approaches is the introduction of a
novel memory module. We assume the Baby-Steps are given in
the training and inference time -- § [4.3]
explains how to obtain them if not given a prior (Readers can directly
move to that section and return to this part afterwards). The left of
the Fig. gives an example of those micro-instructions.


Context


The context is a summary of the past experiences of the agent, namely
the previous *MATH* mini-instructions and trajectories:
*MATH* where the function *MATH* is implemented with a multi-layer
perceptron. The summary function *MATH* is explained
in below.


Summary


To map variable-length sequences (such as the trajectory and the
instructions) to a single vector, we can use various mechanisms such as
LSTM. We reported an ablation study on this in
§ [5.3]. In the following, we describe the &quot;forgetting&quot; one that weighs more
heavily towards the most recent experiences and performs the best
empirically. *MATH* where the weights are normalized to 1 and inverse
proportional to how far *MATH* is from *MATH*,
*MATH* is a hyper-parameter (we set to *MATH*) and *MATH* is a
monotonically nondecreasing function and we simply choose the identity
function.


Note that, we summarize over representations of &quot;micro-instructions&quot;
(*MATH*) and experiences of executing those
micro-instructions *MATH*. The two encoders *MATH* 
and *MATH* are described in § [4.4].
They are essentially the summaries of &quot;low-level&quot; details, i.e.,
representations of a sequence of words, or a sequence of states and
actions. While existing work often directly summarizes all the low-level
details, we have found that the current form of &quot;hierarchical&quot;
summarizing (i.e., first summarizing each [Baby-Step],
then summarizing all previous Baby-Steps) performs better.


Policy


The agent takes actions, conditioning on the context
*MATH*, and the current instruction
*MATH*: *MATH* where the policy is implemented with a LSTM with the
same cross-modal attention between visual states and languages as
in [*REF*].


Learning of the [BabyWalk] Agent 


FIGURE


The agent learns in two phases. In the first one, imitation learning is
used where the agent learns to execute Baby-Steps
accurately. In the second one, the agent learns to execute successively
longer tasks from a designed curriculum.


Imitation Learning 


Baby-Steps are shorter navigation tasks. With the *MATH* th
instruction *MATH*, the agent is asked to follow the
instruction so that its trajectory matches the human expert&apos;s
*MATH*. To assist the learning, the context is computed from
the human expert trajectory up to the *MATH* th
[Baby-Step] (i.e., in eq. , *MATH* s are replaced with
*MATH* s). We maximize the objective
*MATH*. We emphasize here each [Baby-Step] is treated independently
of the others in this learning regime. Each time a
[Baby-Step] is to be executed, we &quot;preset&quot; the agent in the
human expert&apos;s context and the last visited state. We follow existing
literature [*REF*; *REF*] and use
student-forcing based imitation learning, which uses agent&apos;s predicted
action instead of the expert action for the trajectory rollout.


Curriculum Reinforcement Learning 


We want the agent to be able to execute multiple consecutive
Baby-Steps and optimize its performance on following
longer navigation instructions (instead of the cross-entropy losses from
the imitation learning). However, there is a discrepancy between our
goal of training the agent to cope with the uncertainty in a long
instruction and the imitation learning agent&apos;s ability in accomplishing
shorter tasks given the human annotated history. Thus it is
challenging to directly optimize the agent with a typical RL learning
procedure, even the imitation learning might have provided a good
initialization for the policy, see our ablation study in § [5.3].


Inspired by the curriculum learning strategy [*REF*], we
design an incremental learning process that the agent is presented with
a curriculum of increasingly longer navigation tasks.
Fig. illustrates this idea with two &quot;lectures&quot;.
Given a long navigation instruction *MATH* with *MATH* 
Baby-Steps, for the *MATH* th lecture, the agent is given all
the human expert&apos;s trajectory up to but not including the
*MATH* th [Baby-Step], as well as the history context *MATH*. The agent is then asked to
execute the *MATH* th micro-instructions from *MATH* to *MATH* using
reinforcement learning to produce its trajectory that optimizes a task
related metric, for instance the fidelity metric measuring how faithful
the agent follows the instructions.


As we increase *MATH* from 1 to *MATH*, the agent faces the challenge
of navigating longer and longer tasks with reinforcement learning.
However, the agent only needs to improve its skills from its prior
exposure to shorter ones. Our ablation studies show this is indeed a
highly effective strategy.


New Datasets for Evaluation &amp; Learning 


TABLE


FIGURE


To our best knowledge, this is the first work studying how well VLN
agents generalize to long navigation tasks. To this end, we create the
following datasets in the same style as in [*REF*].


[Room6Room] and [Room8Room]


We concatenate the trajectories in the training as well as the
validation unseen split of the [Room2Room] dataset for 3
times and 4 times respectively, thus extending the lengths of navigation
tasks to 6 rooms and 8 rooms. To join, the end of the former trajectory
must be within 0.5 meter with the beginning of the later trajectory.
Table [1] and Fig. [3] contrast the different datasets in the of instructions, 
the average length (in words) of instructions and how
the distributions vary.


Table [1] summarizes the descriptive statistics of Baby-Steps across
all datasets used in this paper. The datasets and the
segmentation/alignments are made publically available.


Key Implementation Details 


In the following, we describe key information for research
reproducibility, while the complete details are in the Appendix.


States and Actions


We follow [*REF*] to set up the states as the visual
features (i.e. ResNet-152 features [*REF*]) from the
agent-centric panoramic views in 12 headings *MATH* 3 elevations with
30 degree intervals. Likewise, we use the same panoramic action space.


Identifying Baby-Steps


Our learning approach requires an agent to follow micro-instructions
(i.e., the Baby-Steps). Existing
datasets [*REF*; *REF*; *REF*] do not
provide fine-grained segmentations of long instructions. Therefore, we
use a template matching approach to aggregate consecutive sentences into
Baby-Steps. First, we extract the noun phrase using POS
tagging. Then, we employs heuristic rules to chunk a long instruction
into shorter segments according to punctuation and landmark phrase
(i.e., words for concrete objects). We document the details in the
Appendix.


Aligning Baby-Steps with Expert Trajectory


Without extra annotation, we propose a method to approximately chunk
original expert trajectories into sub-trajectories that align with the
Baby-Steps. This is important for imitation learning at
the micro-instruction level (§ [4.2.1]). Specifically, we learn a multi-label
visual landmark classifier to identify concrete objects from the states
along expert trajectories by using the landmark phrases extracted from
the their instructions as weak supervision. For each
trajectory-instruction pair, we then extract the visual landmarks of
every state as well as the landmark phrases in [Baby-Step]
instructions. Next, we perform a dynamic programming procedure to
segment the expert trajectories by aligning the visual landmarks and
landmark phrases, using the confidence scores of the multi-label visual
landmark classifier to form the function.


Encoders and Embeddings


The encoder *MATH* for the (micro)instructions is a LSTM. The
encoder for the trajectory *MATH* contains two separate
Bi-LSTMs, one for the state *MATH* and the other for the
action *MATH*. The outputs of the two Bi-LSTMs are then
concatenated to form the embedding function *MATH*. The details of
the neural network architectures (i.e. configurations as well as an
illustrative figure), optimization hyper-parameters, etc. are
included in the Appendix.


Learning Policy with Reinforcement Learning


In the second phase of learning, [BabyWalk] uses RL to learn
a policy that maximizes the fidelity-oriented rewards (CLS) proposed
by  *REF*. We use policy gradient as the optimizer [*REF*]. Meanwhile, we set the maximum number of
lectures in curriculum RL to be 4, which is studied in Section [5.3].


TABLE


Experiments 


We describe the experimental setup (§ [5.1]),followed by the main results in
§ [5.2] where we show the proposed [BabyWalk] agent attains
competitive results on both the in-domain dataset but also generalizing
to out-of-the-domain datasets with varying lengths of navigation tasks.
We report results from various ablation studies in
§ [5.3]. While we primarily focus on the [Room4Room] dataset, we
re-analyze the original [Room2Room] dataset in
§ [5.4] and were surprised to find out the agents trained on it can generalize.


Experimental Setups. 


Datasets


We conduct empirical studies on the existing datasets
[Room2Room] and [Room4Room][*REF*; *REF*], and the two
newly created benchmark datasets [Room6Room] and
[Room8Room], described in § [4.3].
Table [1] and Fig. [3] contrast their differences.


Evaluation Metrics


We adopt the following metrics: Success Rate ([sr]) that
measures the average rate of the agent stopping within a specified
distance near the goal location [*REF*], Coverage
weighted by Length Score ([cls]) [*REF*] that
measures the fidelity of the agent&apos;s path to the reference, weighted by
the length score, and the newly proposed Success rate weighted
normalized Dynamic Time Warping ([sdtw]) that measures in
more fine-grained details, the spatio-temporal similarity of the paths
by the agent and the human expert, weighted by the success
rate [*REF*]. Both [cls] and [sdtw] measure explicitly the agent&apos;s ability to follow
instructions and in particular, it was shown that [sdtw]
corresponds to human preferences the most. We report results in other
metrics in the Appendix.


Agents to Compare to


Whenever possible, for all agents we compare to, we either re-run,
reimplement or adapt publicly available codes from their corresponding
authors with their provided instructions to ensure a fair comparison. We
also &quot;sanity check&quot; by ensuring the results from our implementation and
adaptation replicate and are comparable to the reported ones in the
literature.


We compare our [BabyWalk] to the following: (1) the
[seq2seq] agent [*REF*], being adapted to the
panoramic state and action space used in this work; (2) the Speaker
Follower ([sf]) agent [*REF*]; (3) the
Reinforced Cross-Modal Agent ([rcm]) [*REF*]
that refines the [sf] agent using reinforcement learning
with either goal-oriented reward ([rcm(goal)]) or
fidelity-oriented reward ([rcm(fidelity)]); (4) the
Regretful Agent ([regretful]) [*REF*] that uses a
progress monitor that records visited path and a regret module that
performs backtracking; (5) the Frontier Aware Search with Backtracking
agent ([fast]) [*REF*] that incorporates global
and local knowledge to compare partial trajectories in different
lengths.


The last 3 agents are reported having state-of-the art results on the
benchmark datasets. Except the [seq2seq] agent, all other
agents depend on an additional pre-training stage with data
augmentation [*REF*], which improves cross-board. Thus, we
train two [BabyWalk] agents: one with and the other without
the data augmentation.


Main results 


In-domain Generalization


This is the standard evaluation scenario where a trained agent is
assessed on the unseen split from the same dataset as the training data.
The leftmost columns in Table reports the results where the training
data is from [r4r]. The [BabyWalk] agents outperform all other agents when evaluated on [cls] and
[sdtw].


When evaluated on [sr], [fast] performs the best
and the [BabyWalk] agents do not stand out. This is
expected: agents which are trained to reach goal do not necessarily lead
to better instruction-following. Note that [rcm(fidelity)]
performs well in path-following.


FIGURE


Out-of-domain Generalization


While our primary goal is to train agents to generalize well to longer
navigation tasks, we are also curious how the agents perform on shorter
navigation tasks too. The right columns in Table report the comparison. The
[BabyWalk] agents outperform all other agents in all metrics
except [sr]. In particular, on [sdtw], the
generalization to [r6r] and [r8r] is especially
encouraging, resulting almost twice those of the second-best agent
[fast]. Moreover, recalling from Fig. [1],
[BabyWalk]&apos;s generalization to [r6r] and
[r8r] attain even better performance than the
[rcm] agents that are trained in-domain.


Fig. [4] provides additional evidence on the
success of [BabyWalk], where we have contrasted to its
performance to other agents&apos; on following instructions in different
lengths across all datasets. Clearly, the [BabyWalk] agent
is able to improve very noticeably on longer instructions.


Qualitative Results


Fig. contrasts visually several agents in
executing two (long) navigation tasks. [BabyWalk]&apos;s
trajectories are similar to what human experts provide, while other
agents&apos; are not.


FIGURE


Memory Buffer is Beneficial


Table illustrates the importance of having a memory
buffer to summarize the agent&apos;s past experiences. Without the memory
([null]), generalization to longer tasks is significantly
worse. Using LSTM to summarize is worse than using forgetting to
summarize (eqs.). Meanwhile, ablating *MATH* of the
forgetting mechanism concludes that *MATH* is the optimal to our
hyperparameter search. Note that when *MATH*, this mechanism
degenerates to taking average of the memory buffer, and leads to
inferior results.


Curriculum-based RL (CRL) is Important


Table establishes the value of CRL. While
imitation learning ([il]) provides a good warm-up for
[sr], significant improvement on other two metrics come from
the subsequent RL ([il+rl]). Furthermore, CRL (with 4
&quot;lectures&quot;) provides clear improvements over direct RL on the entire
instruction (i.e., learning to execute all Baby-Steps
at once). Each lecture improves over the previous one, especially in
terms of the [sdtw] metric.


TABLE


Revisiting [Room2Room] 


TABLE


Our experimental study has been focusing on using [r4r] as
the training dataset as it was established that as opposed to
[r2r], [r4r] distinguishes well an agent who
just learns to reach the goal from an agent who learns to follow
instructions.


Given the encouraging results of generalizing to longer tasks, a natural
question to ask, how well can an agent trained on [r2r]
generalize?


Results in Table [2] are interesting. Shown in the top panel, the
difference in the averaged performance of generalizing to
[r6r] and [r8r] is not significant. The agent trained on [r4r] has a small win on [r6r]
presumably because [r4r] is closer to [r6r] than
[r2r] does. But for even longer tasks in [r8r],
the win is similar.


In the bottom panel, however, it seems that
[r2r] *MATH* [r4r] is stronger (incurring
less loss in performance when compared to the in-domain setting
[r4r] *MATH* [r4r]) than the reverse
direction (i.e., comparing [r4r] *MATH* 
[r2r] to the in-domain [r2r] *MATH* 
[r2r]). This might have been caused by the noisier
segmentation of long instructions into Baby-Steps in
[r4r]. (While [r4r] is composed of two
navigation paths in [r2r], the segmentation algorithm is not
aware of the &quot;natural&quot; boundaries between the two paths.)


Discussion 


There are a few future directions to pursue. First, despite the
significant improvement, the gap between short and long tasks is still
large and needs to be further reduced. Secondly, richer and more
complicated variations between the learning setting and the real
physical world need to be tackled. For instance, developing agents that
are robust to variations in both visual appearance and instruction
descriptions is an important next step.