The Ecosystem Path to AGI


Animal cognition 


All organisms in nature are subject to natural selection and, where
applicable, also sexual selection [*REF*]. These forces
operate on physical as well as cognitive properties of the organisms.
One factor that contributes to the selection pressure on animal
intelligence is other organisms. In fact, organisms coevolve with each
other and are part of each other&apos;s environment. Intelligence may be an
advantage in the &quot;arms race&quot; between predators and prey, in the
competition for scarce resources, in collaborations with mutual
benefits, and in sexual selection processes. Another factor that
contributes to the selection pressure on intelligence is the terrain. In
fact, animals must continuously handle challenges imposed by the terrain
and adapt their behavior to local conditions when migrating, foraging,
approaching prey, escaping predators, mating, and parenting.


To survive and reproduce, animals need to deal with a continuous stream
of challenges in their lives. They must find food, avoid predators,
navigate, and mate. Solving these challenges requires efficient
information processing, in particular perception, decision-making, and
action. Nervous systems are present in almost all taxa of the animal
kingdom and play a key role in animal information processing. They are
typically far from monolithic. For example, in humans, the complex
nervous system that controls digestion is essentially separated from the
brain and may operate even in a brain-dead person. The human brain
itself is highly modular, with its anatomically distinct lobes and
regions, such as the Brain stem which controls reflexes ---positive,
e.g., the knee reflex, and negative, e.g., the diving reflex; the
Prefrontal cortex, which maps sensory signals to actions; and the
Insula, which combines internal signals, such as blood sugar level and
external signals, such as smells, into signals linked to happiness and
reward [*REF*].


Reflexes are critical to many animals [*REF*]. For
example, a newborn lamb might not have the time to find its way to its
mother&apos;s udder through random trial and error. Instead, its early life
hinges on an instinct that draws it to the smell of milk and a positive
suck reflex that causes it to drink. The lamb might also benefit
critically from negative reflexes that prevent it from eating lethal
objects, inhaling liquid, or jumping from high cliffs. From an
evolutionary perspective, one of the great advantages of nervous systems
is that they enable learning, i.e., physical modification, and thereby
efficient adaptation to the dangers and resources of the local
environment. A prominent example of learning is reinforcement learning
(RL), which is used across the animal kingdom
[*REF*; *REF*]. Many animals combine
untrainable reflex circuits with circuits that are trainable with RL.
For instance, humans have hundreds of hardwired reflexes, e.g. the knee
reflex, but we can also learn via RL. This enables us to combine the
benefits of reflexes with the benefits of RL.


Models of animal cognition 


There is a great number of computational models of animal cognition. In
the &quot;reflex tradition&quot;, animals are modeled as reflex agents, without
the ability to learn. Such models might be adequate for animals with
hardwired nervous systems that remain essentially unchanged during their
lifetime. An example could be the famous nematode C. elegans
[*REF*]. Many biologically inspired algorithms, such as
cellular automata, swarm algorithms, and ant algorithms, belong to this
tradition and so does the subfield that studies populations of reflex
agents powered by evolutionary algorithms.


In the &quot;RL tradition&quot;, animals are modeled as RL agents. This field
includes physical animal robots that get rewarded for fast crawling,
running, swimming, or flying. It also includes homeostatic agents that
get rewarded for keeping a set of homeostatic variables --- like energy,
water, and oxygen --- close to their target values, or sweet spots
[*REF*; *REF*]. RL algorithms
have been shown to have a great potential for AI. For instance, they
outperform humans at several video games and strategic board games
[*REF*]. There are also several agent-based ecosystem
simulators that are based on RL
[*REF*; *REF*; *REF*].


In analytic approaches to ecosystem modeling, organisms are typically
modeled with numbers representing population size or biomass and the
interaction dynamics is modeled with systems of differential equations.
There is no model of individual animals, no model of animal cognition,
and no model of the terrain. Examples include the well-known
Lotka-Volterra predator-prey dynamics [*REF*]. The Ecopath
(with Ecosim and Ecospace) simulator for marine ecosystems
[*REF*] divides maps of ecosystem into geographical
cells, where each cell contains populations, for example, given as
tonnes of phytoplankton, zooplankton, planktivores, and pescivores.


AI via models of animal cognition 


Nervous systems provided the original inspiration for the neural network
model and its applications to supervised learning [*REF*]
and RL [*REF*]. Some researchers try to copy animal
brains in wetware or reproduce their connectome in software
[*REF*], others build computational models of the brain,
sometimes called cognitive architectures [*REF*]. Rather than
aiming directly for a model of the brain, one may aim for a model of the
process that led to the development of the brain. Since natural general
intelligence emerged as the result of an evolutionary process that took
place in an ecosystem, a natural strategy for creating AGI is to
construct an ecosystem simulator that exploits the natural selection
pressure on intelligence. This strategy is in line with Wilson&apos;s &quot;animat
path to AI&quot; [*REF*] and it enables a gradual approach to AI
that starts with relatively simple ecosystems.


Section [1] of this paper presents the theoretical framework of the ecosystem simulator
Ecotwin, which combines RL and reflexes. Section
[2] presents simulation results, in which well-known patterns from
population dynamics and ethology emerge. Section
[3], finally, draws some conclusions.


Ecosystem simulator 


In this section, we present the theoretical framework of the ecosystem
simulator Ecotwin. Ecotwin uses the game engine Unity with ML-Agents
[*REF*], which provides a graphical user interface, a physics
engine, and several RL algorithms. More details about Ecotwin, including
its open source code, can be found at WEBSITE.


Ecosystems 


We model space with a set *MATH* and
time with the numbers *MATH*. The building blocks of our
ecosystems are called objects. For instance, we could introduce cat
objects, dandelion objects, and rock objects. Each object is assigned a
set of object properties.


DEFINITION


Note that the conformation of an object describes its shape and position
in *MATH*. The physical and chemical properties of objects may be
defined as aggregations of the properties of the points inside their
conformations, e.g., the average temperature of a rock object. It is
sometimes convenient to introduce additional properties. For example, to
talk about cat objects, one may want to introduce properties such as Eye
color, Paw size, Blood pressure, and Blood sugar.


DEFINITION 


DEFINITION


The genome has two roles: (1) It encodes the organism at the start of
its &quot;life&quot;, in particular, it could encode its initial conformation
(body shape), nervous system, and hyperparameters; (2) It is the input
to the reproduction process, in which one or two genomes give rise to a
new genome.


This definition of organism encompasses pure reflex agents (with an
empty policy network), pure RL-agents (with an empty reflex network),
and agents that combine reflexes with RL. By leaving the nervous system
empty, we can also model agents lacking nervous systems altogether, such
as bacteria and plants. It is often convenient to represent the reflex
and happiness networks as hand-coded functions. If desired, these
functions can easily be converted into neural networks, by using
supervised learning. This step might be useful when applying mutations
that add noise to connection weights.


DEFINITION


Several examples of ecosystems will be given in Section [2].
The ecosystem is updated at each tick, using the physics mechanisms of
Unity and the update rules of the organisms.


Decision-making 


This is how the nervous system is used for making decisions at time *MATH*:
1. For each sensor, read off its physical or chemical property at its
current position. This produces a vector *MATH* that encodes
the sensory input at *MATH*.
2. Give the input *MATH* to the Reflex network, which then
outputs some vector *MATH*, with component values -1, 0, or 1.
The intended interpretation of these values are, respectively, to
block, accept, and force the corresponding action.
3. Also give the input *MATH* to the Policy network, which then
selects an action *MATH*. Represent this action as a one-hot vector
*MATH*, where 1 is in the position of *MATH*.
4. Finally, compute the vector *MATH*. Here,
*MATH* is defined as *MATH*, where
*MATH* is 1 if *MATH* and 0 otherwise. This produces a multi-hot
vector that constitutes the decision at *MATH*. Thus, the decision
might be no action, one action, or many actions.


Now that we have seen how nervous systems are used for decision-making,
let us consider a concrete example of a nervous system of a lamb model.
The sensors, actions, and reflex network are shown in Fig.
[1]. The happiness network maps Glucose and
Oxygen to a happiness value, so that ingestion and breathing are
encouraged. An additional input could be Sheep_smell, so that the animal
is encouraged to approach sheep: a hardwired social instinct. The policy
network is trained using RL and updated at each time step. Thus, the
animal might learn to breathe when Oxygen is low and move toward the
strongest smell of milk when Glucose is low.


FIGURE


Learning 


The reflex and happiness networks develop through evolution only, and
not through learning. In contrast, the policy network is updated at each
time step via RL. The reward signal at time *MATH* is defined as
*MATH*, where *MATH* is computed by the
happiness network. In the simulations discussed in this paper, we use
the standard RL algorithm PPO [*REF*] together with the
animals&apos; individual hyperparameters.


Results 


We will present several results that were obtained using Ecotwin
simulations. Videos of the simulations are available at
[www.ecotwin.se](www.ecotwin.se). Ecotwin can run on ecosystems
populated with models of real organisms, imaginary organisms, or robots.
In this section, we will consider simple models of real organisms. The
properties and the mechanisms of our model organisms will be taken from
the corresponding real organisms. In order to find a reasonable starting
point, we initialize the policy networks randomly and then pre-train
them with RL in Ecotwin, before turning on reproduction and starting the
simulations.


Emergence of predator-prey dynamics 


Previous work in predator-prey dynamics has shown that a two-species
predator-prey system, with agents trained through RL, exhibited
Lotka-Volterra cycles under certain choices of parameters
[*REF*]. We wanted to explore whether a three-species
system would also exhibit such cycles.


Our study concerns a three species predator-prey ecosystem with grass,
deer, and wolves, as illustrated in Fig.
[2] (Left). Deer and wolves have vision, which is modeled via Unity&apos;s ray casts, and gives
the direction and distance to the closest visible objects of each type,
within a certain radius [*REF*]. Moreover, the deer can
smell grass and wolves, while the wolves can smell deer. At each time
step, each animal decides whether it should stand still, walk, or run
forward. It also decides whether it should rotate left, right, or not at
all. The happiness of the deer is determined by their Energy and Wolf
smell sensors, whereas happiness of the wolves is determined by their
Energy and Deer smell sensors. The deer obtains energy by eating grass
and wolves by eating deer. The consumption of energy depends on the
velocity of the animal. A simplified reproduction mechanism was used,
where each animal had a probability of giving birth to a new agent,
which was then spawned at a random unoccupied location in the ecosystem.


We ran two simulations with Ecotwin, starting with the same ecosystem.
The result is shown in Fig. [2] (Right). As expected, given the dependence on
randomness, the two simulations are different. In both simulations we
see Lotka-Volterra cycles, with an increase in grass, followed by an
increase in deer, followed by an increase in wolves, and similar
decreases. More details about the study can be found in [*REF*].


FIGURE 


Emergence of diel vertical migration 


In this study, we consider a marine ecosystem with krill, copepods and
phytoplankton, illustrated in Fig.[3] (Left). 
A diel light cycle was simulated, with the sun going up and
down. We also simulated decreasing light intensity at greater depths.
Three behaviors observed in real Copepods [*REF*] were
studied: (1) Diel Vertical Migration (DVM): a cyclic behavior in which
Copepods migrate to the surface at night (enabling them to graze
phytoplankton that are near the surface where the light is) and go down
to greater and darker depths in the daytime (where the vision of
predators such as krill is less efficient). (2) Quick escape reactions
that enable Copepods to escape from predators. (3) Chemotaxis causing
Copepods to avoid the smell of predators.


The Copepod models have a simple form of vision, by which they can
perceive a rough direction and distance to near-by phytoplankton.
Moreover, they can perceive light intensity, their own energy level, and
whether they are touching food, krill, the environment&apos;s boundaries, or
another Copepod. Their happiness value, yielding their reward, depends
on energy and light intensity.


The Copepods and krill were pre-trained with RL for 1 million time
steps. Then an Ecotwin simulation was run, starting with the ecosystem
shown in Fig. [3] (Left). The behavioral patterns (1)-(3) were
observed in the simulations. In particular, a clear DVM pattern was
observed, as shown in Fig. [3] (Right). More information about the study can
be found in [*REF*].


FIGURE


Emergence of critical reflexes 


This study concerns the interplay between RL and reflexes. We consider a
terrestrial ecosystem with goats and grass, illustrated in Fig.
[4] (Left). There are three types of grass in the environment: green, yellow
and red. The green grass is good for the goats to eat, the yellow grass
is bad, but not deadly, whereas the red grass is deadly. The goats
reproduce sexually and have a genome that encodes their policy networks
and reflex networks. Thus, they may pass on genomes, including genes
that code for reflexes, to their offspring. The goats have four
different genes that we call red, yellow, green, and blue, for
convenience. These genes correspond to a reflex which prevents a goat
from eating red, yellow and green food objects, respectively. The blue
gene does not affect a goat&apos;s reflexes.


An Ecotwin simulation was run with these organisms and the result is
shown in Fig. [4] (Right). We can see several patterns: (1) The
red gene eventually dominates the population. This is expected, as a
reflex to avoid lethal food gives them a clear advantage. (2) The
domination of the red gene suggests that the combination of reflexes and
RL is more effective than RL alone when there are lethal dangers in the
environment. Avoiding lethal food cannot be learned during a goat&apos;s
life. (3) The yellow gene does not have a clear advantage over the blue
gene. This suggests that the combination of reflexes and RL gives no
advantage, that cannot be learned, over pure RL-agents when the dangers
are not lethal. (4) The goats with the green gene keep dying out and
then reappearing because of the mutation in the inheritance mechanisms.
(5) A goat&apos;s genome may include multiple genes. This explains why the
superior red gene does not dominate the population completely.


FIGURE


More details about the study can be found in [*REF*].


Conclusion 


We have presented the open source ecosystem simulator Ecotwin. It was
run on three ecosystems, on which it reproduced certain population
dynamics and behavioral patterns that can be observed in real
ecosystems. Agent-based ecosystem simulators can be used for predicting
the consequences of human interventions, e.g., via fishing, forestry, or
urbanization. They might also be used as &quot;general AI gyms&quot;, in which
populations of agents coevolve in a fully automatic process, while
taking advantage of the built-in selection pressure on intelligence.
This research path toward general AI seems to be worthy of further
exploration.
