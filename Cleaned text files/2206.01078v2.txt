Deep Transformer Q-Networks for Partially Observable Reinforcement Learning


Introduction


In recent years, deep neural networks have become the computational
backbone of reinforcement learning, achieving strong performance across
a wide array of difficult tasks including games [*REF*; *REF*] and
robotics [*REF*; *REF*]. In particular, Deep
Q-Networks (DQN) [*REF*] revolutionized the field of deep RL by
achieving super-human performance on Atari 2600 games in the Atari
Learning Environment [*REF*]. Since then, several
advancements have been proposed to improve DQN [*REF*], and
deep RL has been shown to excel in continuous control tasks as
well [*REF*; *REF*].


However, most Deep RL methods assume the agent is operating within a
fully observable environment; that is, one in which the agent has access
to the environment&apos;s full state information. But this assumption does
not hold for many realistic domains due to components such as noisy
sensors, occluded images, or additional unknown agents. These domains
are partially observable, and pose a much bigger challenge for RL
compared to the standard fully observable setting. Indeed, naïve methods
often fail to learn in partially observable environments without
additional architectural or training
support [*REF*; *REF*; *REF*].


To solve partially observable domains, RL agents may need to remember
(some or possibly all) previous observations [*REF*].
As a result, RL methods typically add some sort of memory component,
allowing them to store or refer back to recent observations in order to
make more informed decisions. The current state-of-the-art approaches
integrate recurrent neural networks, like LSTMs [*REF*] or
GRUs [*REF*], in conjunction with fully observable Deep RL
architectures to process an agent&apos;s history [*REF*]. But
recurrent neural networks (RNNs) can be fragile and difficult to train,
often requiring complicated &quot;warm-up&quot; strategies to initialize its
hidden state at the start of each training batch [*REF*].
Conversely, the Transformer has been shown to model sequences much
better than RNNs and is ubiquitous in natural language processing
(NLP) [*REF*] and increasingly common in computer vision [*REF*].


Therefore, we propose Deep Transformer Q-Network (DTQN), a novel
architecture using self-attention to solve partially observable RL
domains. DTQN leverages a transformer decoder architecture with learned
positional encodings to represent an agent&apos;s history and accurately
predict Q-values at each timestep. Rather than a standard approach that
trains on a single next step for a given history, we propose a training
regime called intermediate Q-value prediction, which allows us to train
DTQN on the Q-values generated for each timestep in the agent&apos;s
observation history and provide more robust learning. DTQN encodes an
agent&apos;s history more effectively than recurrent methods, which we show
empirically across several challenging partially observable
environments. We evaluate and analyze several architectural components,
including: gated skip connections [*REF*], positional encodings, identity map
reordering [*REF*], and intermediate value prediction [*REF*]. Our results provide strong evidence that
our approach can successfully represent agents&apos; histories in partially
observable domains. We visualize attention weights showing DTQN learns
an understanding of the domains as it works to solve tasks.


Background


When an environment does not emit its full state to the agent, the
problem can be modeled as a Partially Observable Markov Decision Process
(POMDP) [*REF*]. A POMDP is formally described as the 6-tuple *MATH*.
*MATH*, *MATH*, and *MATH* represent the environment&apos;s
set of states, actions, and observations, respectively. *MATH* is
the state transition function *MATH*,
denoting the probability of transitioning from state *MATH* to state *MATH* 
given action *MATH*. *MATH* describes the reward function
*MATH*; that is, the resultant scalar reward emitted by the environment for an agent that
was in some state *MATH* and took some action
*MATH*. And *MATH* is the observation function
*MATH*, the probability of observing *MATH* 
when action *MATH* is taken resulting in state *MATH*. At each time step,
t, the agent is in the environment&apos;s state *MATH*, takes
action *MATH*, manipulates the environment&apos;s state to some
*MATH* based on the transition probability *MATH* and receives a reward,
*MATH*. The goal of the agent is to maximize *MATH*, its expected discounted
return for some discount factor *MATH*  [*REF*].


Because agents in POMDPs do not have access to the environment&apos;s full
state information, they must rely on the observations *MATH* 
which relate to the state via the observation function, *MATH*. In general,
agents acting in partially observable space cannot simply use
observations as a proxy for state, since several states may be aliased
into the same observation. Instead, they often consider some form of
their full history of information, *MATH*. Because the
history grows indefinitely as the agent proceeds in a trajectory,
various ways of encoding the history exist. Previous work has truncated
the history to make it a fixed length [*REF*] or used an
agent&apos;s belief, which represents the estimate of the current state
[*REF*]. Since the deep learning revolution, others
have used forms of recurrency, such as LSTMs and GRUs, to encode the
history [*REF*; *REF*].


Deep recurrent Q-networks


Q-Learning [*REF* -learning] aims to learn a function *MATH* which represents the
value of each state-action pair in an MDP. Given a state *MATH*, action
*MATH*, reward *MATH*, next state *MATH*, and learning rate *MATH*, the
*MATH* -function is updated with the equation *MATH*.
In more challenging domains, however, the state-action space of the
environment is often too large to be able to learn an exact *MATH* -value
for each state-action pair. Instead of learning a tabular Q-function,
DQN [*REF*] learns an approximate *MATH* -function featuring strong
generalization capabilities over similar states and actions. DQN is
trained to minimize the Mean Squared Bellman Error *MATH* 
where transition tuples of states, actions, rewards, and future states
*MATH* are sampled uniformly from a replay buffer, *MATH*, of past
experiences while training. The target *MATH* invokes DQN&apos;s target
network (parameterized by *MATH*), which lags behind the main network
(parameterized by *MATH*) to produce more stable updates.


However, in partially observable domains, DQN may not learn a good
policy by simply replacing the network&apos;s input from states to
observations (i.e., an agent can often perform better by remembering
some history). To address this challenge, Deep Recurrent Q-Networks
(DRQN) [*REF*] incorporated histories into the
*MATH* -function by way of a long short-term memory (LSTM)
layer [*REF*]. In DRQN&apos;s training procedure, the sampled
states are replaced with histories *MATH* from timestep *MATH* to step
*MATH*, sampled randomly within each episode. The hidden state of the
LSTM is zeroed at the start of each update.


Transformer decoders


The transformer architecture [*REF*], originally
introduced for natural language processing, stacks blocks of attention
layers [*REF*] and is typically used to model sequential
data. Intuitively, the transformer&apos;s attention module receives as input
a sequence of tokens (e.g., a sequence of observations in an episode)
and the model learns to place stronger weights or more attention on
the most important tokens. For more details about the attention module
in transformers, refer to Appendix [8.3].


While the original transformer architecture formed an encoder-decoder
structure, recent works often use either the encoder [*REF*]
or the decoder [*REF*]. The key difference between the
two is that the decoder applies a causal masking to the attention layer;
that is, the *MATH* token cannot attend to tokens which come later in the
sequence. In general, the transformer decoder has been shown to perform
better on generative tasks like next token prediction, while the
transformer encoder is able to learn richer representations and excels
on tasks such as language understanding.


DTQN utilizes the transformer decoder structure. Given a tensor of shape
*MATH*, where *MATH* is the batch size, *MATH* is the context length, and
*MATH* is the model&apos;s dimensionality size, the transformer decoder layer
returns a tensor of the same shape, enabling us to stack layers on top
of each other. The last transformer layer&apos;s output can then be projected
to the desired shape, or sent as input to another network. To ensure the
raw inputs are of the correct shape, we often prepend a feature
extraction step, such as a lookup embedding for text or integers, a
multilayer perceptron for vectors, or convolutional neural network for
images.


FIGURE


Related work


Since its inception, several works have built upon DRQN. For example,
DRQN was shown to beat human test subjects on the challenging 3D VizDoom
video game environment [*REF*] when augmented with game feature
information [*REF*]. Action-based DRQN
(ADRQN) [*REF*] conditioned the network on the agent&apos;s full
history rather than just its observation history. Deep Distributed
Recurrent Q-Networks (DDRQN) [*REF*] extends DRQN into
the multi-agent reinforcement learning setting. Like ADRQN, DDRQN
conditioned on actions, but also shared weights between each agent, all
while forgoing each agent&apos;s replay buffer to sample from.


The concept of attention is also well-studied in the reinforcement
learning setting. The most closely related work to ours using attention
in deep Q-learning is Deep Attention Recurrent Q-Network
(DARQN) [*REF*], which used attention to aid an LSTM&apos;s
representation of the agent&apos;s history. Similarly, visual attention has
been added to DRQN-like architectures in an effort towards creating more
interpretable reinforcement learning algorithms [*REF*].
Unlike our work, which uses self-attention such that agent&apos;s history
forms the queries, keys, and values, these works use the recurrent
network&apos;s last output state to form the queries, and the environment&apos;s
most recent observation forms the keys and values. In the multi-agent
setting, Multi-Actor-Attention-Critic [*REF*] created an
attention module in which each agent&apos;s query is their own observation,
and the keys and values are formed by the other agents&apos; observations.
Finally, the Simple Neural AttentIve Learner (SNAIL) [*REF*]
used attention to develop a meta-learning agent capable of transferring
its skills to similar but different environments.


The use of transformers in reinforcement learning has become more
popular within the last few years. In the offline reinforcement learning
setting, Decision Transformer [*REF*] and Trajectory
Transformer [*REF*] concurrently proposed the idea of using
transformer decoders for sequence modeling, surpassing the current
offline RL state of the art. Online Decision
Transformer [*REF*] extended Decision
Transformer [*REF*] by treating the offline training as a
pre-training step, and fine-tuned the transformer in the online setting
for even better performance. FlexiBiT [*REF*] trained a
transformer encoder to learn a variety of inference tasks, such as
behavioural cloning, forward and backward modeling, and inferring an
agent&apos;s history given its current state. Contrary to our work, which is
trained completely online using reinforcement learning, these works are
specialized to take advantage of an offline RL dataset, and train their
agents in a supervised way [*REF*]. Other
methods use transformers to learn from scratch in the online RL setting,
like GTrXL [*REF*], which modifies the ordering of
components within the transformer block, and introduces a new gating
mechanism to replace the residual skip connections. We compare the
effects of these modifications to our architecture. Lightweight
transformers have shown strong performance in text adventure
games [*REF*], and the transformer encoder was applied to video
games [*REF*]. In contrast to these works, we utilize
a multi-layer transformer decoder architecture. Vision transformers have
been used in conjuction with DQN to stabilize Q-learning with data
augmentation, replacing the standard convolutional neural
networks [*REF*]. The self-attention block in their
work attends to features within a single observation whereas ours
attends throughout the agent&apos;s history.


ALGORITHM 


Deep transformer Q-network architecture


Transformers seem like a natural fit to represent histories in POMDPs,
but there are several open questions regarding how to use them best in
deep RL. In particular, it is unclear what form of transformer to use,
how to integrate it into deep RL methods and how they should be trained.
We chose to build DTQN using a transformer decoder structure
incorporating learned position encodings, and train on the Q-values
generated for each timestep in the agent&apos;s observation history. DTQN
takes as input the agent&apos;s previous *MATH* observations,
*MATH*, linearly projects each
observation into the dimensionality of the model, and adds positional
encodings to add information about the absolute temporal location of
each observation. The embedded history is then passed through *MATH* 
transformer layers, and finally projected to the action space of the
environment (see Figure [1] and Algorithm). DTQN outputs a set of Q-values relating to
each observation in the input.


While we only use the Q-values from the most recent observation during
execution, we train the network using all generated Q-values, even those
relating to the observations at the beginning of the subhistory using
the loss function in Algorithm. This training regime challenges the network
to predict the Q-values in situations where it has little to no context,
and produces a more robust agent. The remainder of this section expands
on each contribution of the DTQN architecture.


Observation embeddings and positional encodings


Before the observation history is passed to DTQN&apos;s transformer layers,
each observation in the agent&apos;s most recent *MATH* observations,
*MATH*, is linearly projected to the dimensionality of the
transformer via a learned observation embedding (see
Figure [1]). After embedding, we add a learned
positional encoding to each observation based on its position in the
observation history. This result, which we call *MATH* in
Algorithm, is the input to the first transformer layer in DTQN.


Position encodings are common practice in transformers, especially for
NLP tasks, where they are well studied [*REF*]. However, the
importance of position is less clear in the reinforcement learning
setting. In some control tasks, the temporal position of an observation
may not have any effect on its importance or meaning to solve the task.
For instance, the importance of the priest observation in the classic
HeavenHell domain [*REF*] is not dependent on when the
observation occurs in the episode. On the other hand, domains with more
dynamic state transitions may benefit greatly from the positional
information. For this reason, we choose to learn our positional
encodings as it gives the agent the most flexibility in terms of how it
chooses to use them. We ablate this choice by comparing our learned
positional encodings to sinusoidal positional encodings (used in the
original transformer [*REF*]) as well as not using any
positional encodings in section [5.4].


Transformer decoder structure


Like the original GPT architecture [*REF*], each
transformer layer in DTQN features two submodules: masked multi-headed
self-attention and a position-wise feedforward network. As described in
Algorithm, first we project the output of the previous
layer, *MATH* to the queries, *MATH*, keys, *MATH*, and values, *MATH*,
through the weight matrices *MATH*, *MATH*, and *MATH*, respectively. After
each submodule, that submodule&apos;s input and output are combined (see the
&quot;Combine&quot; step in Figure [1]) followed by a LayerNorm [*REF*].
Finally, after the last transformer layer, we project the final
embedding (*MATH* in Algorithm ) to the action space of our environment to
represent the Q-value for each action.


DTQN uses a residual skip connection [*REF*] to combine the two
streams, matching the original transformer, in favor of other choices of
combination layers such as the GRU gating combination
layer [*REF*]. Another contested decision is the
position of LayerNorm with respect to each submodule; the original
transformer [*REF*] and original GPT [*REF*] apply LayerNorm after the combine step
whereas other works have moved the LayerNorm to immediately before the
submodule [*REF*; *REF*; *REF*].
DTQN applies the LayerNorm after the combine step, a choice we found to
be simple while also demonstrating strong empirical performance. We
ablate our choices of network with the aforementioned variants in
section [5.3].


FIGURE


Intermediate Q-value prediction


DTQN outputs a set of Q-values for each timestep in the agent&apos;s
observation history. During evaluation, DTQN selects the action with the
highest Q-value from the last timestep in its history. It would
therefore be straightforward to train DTQN using just the last
timestep&apos;s Q-values, since those have the most context to work with and
are the most informed to select the optimal action. This regime,
however, is very wasteful, as only a fraction of the generated Q-values
actually get used for training. Instead, we train DTQN using all
generated Q-values. Originally used in the NLP setting where each
position was tasked with predicting the next character and formed an
auxiliary loss [*REF*], we adapt this training regime to the
reinforcement learning setting, as shown in
Algorithm. Note that the for loop depicted in
Algorithm can be done in one forward pass of the
network because of the causally-masked self-attention mechanism.


We ablate training based on all Q-values with training only on the last
timestep&apos;s Q-values in section [5.5], and show the performance gains in
Figure [2].


Experiments


Our experimental evaluation is designed to compare DTQN not only to
previous Q-network baselines, but also to ablate our own method with
other architectural choices. We evaluate these methods on a range of
challenging domains featuring partial observations and requiring memory
to solve them. We baseline DTQN against Deep Recurrent Q-Networks
(DRQN) [*REF*] to show the transformer is a more effective
history representation module than RNNs, Deep Q-Networks
(DQN) [*REF*] to demonstrate the need for memory to solve the
task consistently, and against an attention baseline (called &quot;ATTN&quot; in
Figure [2]) to show the benefits of our architectural
choices. ATTN, like the transformer, has observation and position
embeddings, attention and feedforward network modules, but does not have
LayerNorm or skip connections, and does not stack multiple blocks.


FIGURE


Domains


We conduct our experiments on 4 different environment sets designed to
challenge DTQN in different ways: classic POMDPs, gym
(GV) [*REF*], car flag [*REF*], and
memory cards. Hallway [*REF*] and
HeavenHell [*REF*] are classic navigation POMDPs
requiring the agent to take and remember several information gathering
steps before it can consistently achieve its goal. Gym offers
procedurally generated gridworlds containing difficult partially
observable tasks. The agent&apos;s field of view is restricted such that it
can only see the cells in a *MATH* grid in front of it (see
Figure), which introduces state aliasing and forces
the agent to gather localizing information before it can successfully
solve the task. We evaluate our agents in gridverse environments
&quot;Memory&quot; and &quot;Memory Four Rooms&quot;, which require the agent to first find
the colored information beacons, and then go to the flag whose color
matches the beacon. The colors of the flags and beacons are initialized
randomly and, in Memory-Four-Rooms, the locations of the flag and
beacons are also initialized randomly, increasing the environment&apos;s
difficulty. Example screenshots of the gridverse domains are shown in
Figure. Car Flag features a car on a 1D line, where
the car must first drive to an oracle flag to learn which direction the
finish line is. Memory cards is a novel domain designed to test how much
information an agent can memorize. Based on the popular children&apos;s
memory card game, 5 pairs of cards are hidden to the agent, with one
card revealed at each timestep and the agent must guess the position of
that card&apos;s pair. We chose this set of domains to be representative of
challenging partially observable problems. For more information
regarding the domains, please see Appendix [9].


Baseline comparison 


Our evaluation against baselines is shown in
Figure [2]. Each learning curve shows the success
rate of the agent in the environment, where the line is the mean across
5 random seeds, and the shaded region represents standard error. We
search across hyperparameters of interest, and select the best
performing hyperparameter set, prioritizing consistency across domains.
For specific hyperparameters and training details, refer to
Appendix [8].


The results in Figure [2] show DTQN outperforms the baselines in
terms of learning speed and final performance on nearly all domains.
ATTN learns quickly, but rarely reaches optimal performance, and often
becomes unstable. DRQN, featuring an LSTM as its memory module, often
performs well in our set of domains, but learns slower than DTQN, and is
in general less stable. Sometimes, especially in the gridverse memory
domains, DRQN&apos;s performance plummets shortly after it begins to learn.
It then struggles to regain its initial performance gains, taking as
many as one million timesteps in the Gridverse memory 7x7 domain to
improve its success rate. DQN, designed for MDPs and without any form of
memory in its architecture, fails to achieve higher than 50% success
rate on any of our domains, exemplifying the difficulty of the domains
and the importance of using memory to solve them. These results
highlight the effectiveness of DTQN in solving a range of POMDPs.


GRU-gates and identity map reordering 


In this section, we compare DTQN with different forms of the &quot;Combine&quot;
step (see Figure [1]) as well as different positions of
LayerNorm. DTQN&apos;s combine step is a residual skip connection, and the
LayerNorm occurs after both the attention and the feedforward
submodules. In contrast, GTrXL [*REF*] introduced
GRU-like gating in the combine step, and identity map reordering, which
moves the LayerNorms directly in front of the masked multi self
attention and feedforward sections. We compare our DTQN with residual
skip connection to a version of DTQN which uses GRU-like gating, a
version of DTQN which uses identity map reordering, and a version which
uses both GRU-like gating and identity map reordering. When both GRU
gates and the identity map reordering is used, the architecture
resembles GTrXL. However, we do not use the
TransformerXL [*REF*] as in GTrXL, therefore we are not
comparing to an exact replica.


The results for this ablation are shown in
Table [2]. DTQN performs competitively with the
ablated versions. The variant with only identity map reordering performs
significantly worse than the other versions, and the version with both
identity map reordering and GRU-like gating performs worst on hallway.
Both DTQN and the GRU-like gating variant perform competitively on all
three domains we tested. Although we do not use the TransformerXL in our
experiments, we would expect to see the same relative performance across
if we replaced our transformer decoder with the TransformerXL. A
comparison of different transformer backbones, such as Big
Bird [*REF*], sparse transformers [*REF*], or
the TransformerXL would be interesting future study.


Positional encodings 


DTQN uses learned positional encodings to allow the network to adapt to
different domains. Partially observable domains will exhibit a broad
range of temporal sensitivity, and we want to provide DTQN the
flexibility to learn encodings to match its domain. In this section, we
compare the use of learned positional encodings with the sinusoidal
encodings in the original transformer [*REF*] as well as
no positional encodings. The results for this comparison are shown in
Table [2]. In the memory cards domain, the variant of
DTQN without positional encodings performs significantly worse than both
our learned encodings as well as the sinusoidal encodings. However, in
the gridverse memory task and hallway, the three styles of positional
encodings perform comparably. We analyze the resulting learned
positional encodings from our trained DTQN agents across various domains
in Appendix [12].


TABLE 


Intermediate Q-value prediction 


DTQN predicts and trains on the Q-values generated for each timestep in
the agent&apos;s observation history. During evaluation, however, we only
consider the last timestep&apos;s Q-values when determining which action to
take. We could, therefore, train in the same way, only training with the
last timesteps&apos; Q-values. We compare these two training regimes, and the
results for this are shown in Table [2]. Our results show the variant trained
without intermediate Q-values suffers a significant performance
decrease. In the memory cards case, DTQN excels and solves the task with
nearly 90% success rate, but the variant without intermediate Q-value
prediction can barely solve the task 10% of the time. By training on all
generated Q-values, we produce a more robust and effective agent.


FIGURE


Discussion 


DTQN outperforms or is competitive with our baselines in terms of
learning speed and final performance on all our domains. One additional
advantage of transformers is the ability to visualize self-attention
weights as a form of interpreting the model. Intuitively, the
self-attention mechanism allows the agent to prioritize observations in
its history which provide it with the most information useful in solving
its task. The causal masking ensures the agent cannot attend to
observations in its future, resembling how the agent will need to
perform during execution. While the use of attention weights as a tool
for explainability is still being studied [*REF*; *REF*], it does allow us
to observe which observations the agent finds most valuable in its
history. In Figure, we visualize a trained DTQN agent&apos;s
attention weights from a trajectory in gridverse memory 7x7. Crucially,
the observation including the green beacon (circle with X, magnified on
right) is strongly attended to by all future observations, indicating
the DTQN agent has correctly learned which observations are important in
solving the task. When the agent sees the green flag (magnified on
left), it attends to the observation with green beacon to ensure it
selects the correct flag. We provide additional attention visualizations
in Appendix [11].


Conclusion


In this work we introduce Deep Transformer Q-Networks, a novel
architecture for solving challenging partially observable domains with
reinforcement learning. DTQN incorporates the transformer decoder, which
excels in generating Q-values for each timestep of the agent&apos;s
observation history. We train the model on all generated Q-values,
enabling an efficient training regime and faster learning. DTQN also
utilizes learned positional encodings, empowering the model to learn
domain-specific encodings which match the temporal dependencies of the
environment. We explore and ablate several architectural structures, and
find our choices either outperform or are at least competitive with all
tested variants. Finally, we provide a modular code implementation
of DTQN that is easy to extend and modify, which we hope the research
community will be able to use as we expect our approach to serve as the
basis and benchmark for future transformer-based methods in partially
observable reinforcement learning.