Introduction 


Recurrent neural networks (RNNs) have shown impressive results in
modeling generation tasks that have a sequential structured output form,
such as machine translation [*REF*; *REF*], caption
generation [*REF*; *REF*], and natural language
generation [*REF*; *REF*]. These discriminative models are
trained to learn only a conditional output distribution over strings and
despite the sophisticated architectures and conditioning mechanisms used
to ensure salience, they are not able to model the underlying actions
needed to generate natural dialogues. As a consequence, these
sequence-to-sequence models are limited in their ability to exhibit the
intrinsic variability and stochasticity of natural dialogue. For example
both goal-oriented dialogue systems [*REF*; *REF*] and sequence-to-sequence learning
chatbots [*REF*; *REF*; *REF*] struggle to generate
diverse yet causal responses [*REF*; *REF*]. In addition,
there is often insufficient training data for goal-oriented dialogues
which results in over-fitting which prevents deterministic models from
learning effective and scalable interactions. In this paper, we propose
a latent variable model -- Latent Intention Dialogue Model (LIDM) -- for
learning the complex distribution of communicative intentions in
goal-oriented dialogues. Here, the latent variable representing dialogue
intention can be considered as the autonomous decision-making center of
a dialogue agent for composing appropriate machine responses.


Recent advances in neural variational inference
[*REF*; *REF*] have sparked a series of latent variable
models applied to NLP
[*REF*; *REF*; *REF*; *REF*]. For models
with continuous latent variables, the reparameterisation
trick [*REF*] is commonly used to build an unbiased and
low-variance gradient estimator for updating the models. However, since
a continuous latent space is hard to interpret, the major benefits of
these models are the stochasticity and the regularisation brought by the
latent variable. In contrast, models with discrete latent variables are
able to not only produce interpretable latent distributions but also
provide a principled framework for semi-supervised
learning [*REF*]. This is critical for NLP tasks, especially
where additional supervision and external knowledge can be utilized for
bootstrapping [*REF*; *REF*; *REF*].
However, variational inference with discrete latent variables is
relatively difficult due to the problem of high variance during
sampling. Hence we introduce baselines, as in the
REINFORCE [*REF*] algorithm, to mitigate the high variance
problem, and carry out efficient neural variational
inference [*REF*] for the latent variable model.


In the LIDM, the latent intention is inferred from user input
utterances. Based on the dialogue context, the agent draws a sample as
the intention which then guides the natural language response
generation. Firstly, in the framework of neural variational
inference [*REF*], we construct an inference network to
approximate the posterior distribution over the latent intention. Then,
by sampling the intentions for each response, we are able to directly
learn a basic intention distribution on a human-human dialogue corpus by
optimising the variational lower bound. To further reduce the variance,
we utilize a labeled subset of the corpus in which the labels of
intentions are automatically generated by clustering. Then, the latent
intention distribution can be learned in a semi-supervised fashion,
where the learning signals are either from the direct supervision
(labeled set) or the variational lower bound (unlabeled set).


From the perspective of reinforcement learning, the latent intention
distribution can be interpreted as the intrinsic policy that reflects
human decision-making under a particular conversational scenario. Based
on the initial policy (latent intention distribution) learnt from the
semi-supervised variational inference framework, the model can refine
its strategy easily against alternative objectives using policy
gradient-based reinforcement learning. This is somewhat analagous to the
training process used in AlphaGo [*REF*] for the game of
Go. Based on LIDM, we show that different learning paradigms can be
brought together under the same framework to bootstrap the development
of a dialogue agent [*REF*; *REF*].


In summary, the contribution of this paper is two-fold: firstly, we show
that the neural variational inference framework is able to discover
discrete, interpretable intentions from data to form the decision-making
basis of a dialogue agent; secondly, the agent is capable of revising
its conversational strategy based on an external reward within the same
framework. This is important because it provides a stepping stone
towards building an autonomous dialogue agent that can continuously
improve itself through interaction with users. The experimental results
demonstrate the effectiveness of our latent intention model which
achieves state-of-the-art performance on both automatic corpus-based
evaluation and human evaluation.


Latent Intention Dialogue Model for Goal-oriented Dialogue 


Goal-oriented dialogue [*REF*] aims at building models that can
help users to complete certain tasks via natural language interaction.
Given a user input utterance *MATH* at turn *MATH* and a knowledge base
(KB), the model needs to parse the input into actionable commands *MATH* 
and access the KB to search for useful information in order to answer
the query. Based on the search result, the model needs to summarise its
findings and reply with an appropriate response *MATH* in natural language.


FIGURE


Model 


The LIDM is based on the end-to-end system architecture described
in [*REF*]. It comprises three components: (1) Representation
Construction; (2) Policy Network; and (3) Generator, as shown in
Figure . To capture the user&apos;s intent and match it against
the system&apos;s knowledge, a dialogue state vector *MATH* 
is derived from the user input *MATH* and the knowledge base KB:
*MATH* is the distributed utterance representation,
which is formed by encoding the user utterance *MATH* with a
bidirectional LSTM [*REF*] and concatenating the final stage
hidden states together, *MATH*.
The belief vector *MATH*, which is a concatenation of a
set of probability distributions over domain specific slot-value pairs,
is extracted by a set of pre-trained RNN-CNN belief
trackers [*REF*; *REF*], in which *MATH* and *MATH* are
processed by two different CNNs as shown in
Figure , *MATH*, where *MATH* is the preceding machine response and
*MATH* is the preceding belief vector. They are
included to model the current turn of the discourse and the long-term
dialogue context, respectively. Based on the belief vector, a query *MATH* 
is formed by taking the union of the maximum values of each slot. *MATH* is
then used to search the internal KB and return a vector
*MATH* representing the degree of matching in the KB.
This is produced by counting all the matching venues and re-structuring
it into a six-bin one-hot vector. Among the three vectors that comprise
the dialogue state *MATH*, *MATH* is
completely trainable from data, *MATH* is pre-trained
using a separate objective function, and *MATH* is
produced by a discrete database accessing operation. For more details
about the belief trackers and database operation refer to Wen et al .


Conditioning on the state *MATH*, the policy network
parameterises the latent intention *MATH* by a single layer MLP,
*MATH*, where *MATH*, *MATH*,
*MATH*, *MATH* are model
parameters. Since *MATH* is a
discrete conditional probability distribution based on dialogue state,
we can also interpret the policy network here as a latent dialogue
management component in the traditional POMDP-based
framework [*REF*; *REF*]. A latent intention *MATH* (or an
action in the reinforcement learning literature) can then be sampled
from the conditional distribution, *MATH*. This sampled
intention (or action) *MATH* and the state vector
*MATH* can then be combined into a control vector
*MATH*, which is used to govern the generation of the
system response based on a conditional LSTM language model,
*MATH*, *MATH*, where *MATH* and *MATH* are
parameters, *MATH* is the 1-hot representation of
*MATH*, *MATH* is the last output token (i.e. a word, a
FORMULA slot name or a FORMULA slot value), and *MATH* is
the decoder&apos;s last hidden state. Note in Equation FORMULA the
degree of information flow from the state vector is controlled by a
sigmoid gate whose input signal is the sampled intention *MATH*.
This prevents the decoder from over-fitting to the deterministic state
information and forces it to take the sampled stochastic intention into
account. The LIDM can then be formally written down in its parameterised
form with parameter set *MATH*, *MATH*.


Inference 


To carry out inference for the LIDM, we introduce an inference network
*MATH* to approximate the posterior
distribution *MATH* so that we can
optimise the variational lower bound of the joint probability in a
neural variational inference framework [*REF*]. We can then derive
the variational lower bound as, *MATH*, where *MATH* is a shorthand for
*MATH*. Note that we use a modified
version of the lower bound here by incorporating a trade-off factor
*MATH*  [*REF*]. The inference network
*MATH* is then constructed by *MATH*, *MATH*, *MATH*,
where *MATH* is the joint representation, and both
*MATH* and *MATH* are modeled by a
bidirectional LSTM network. Although both *MATH* and
*MATH* are modelled as
parameterised multinomial distributions, the approximation
*MATH* only functions during
inference by producing samples to compute the stochastic gradients,
while *MATH* is the generative
distribution that generates the required samples for composing the machine response.


Based on the samples *MATH*, we use
different strategies to alternately optimise the parameters *MATH* and
*MATH* against the variational lower bound
(Equation FORMULA). To do this, we further divide *MATH* into two
sets *MATH*. Parameters *MATH* on the
decoder side are directly updated by back-propagating the gradients,
*MATH*. Parameters *MATH* in the generative network,
however, are updated by minimising the KL divergence, *MATH*, where the entropy derivative
*MATH* and therefore can be ignored. Finally, for the parameters *MATH* in the
inference network, we firstly define the learning signal
*MATH*, *MATH*. Then the parameters *MATH* are updated by,
*MATH*. However, this gradient estimator has a large variance
because the learning signal *MATH* 
relies on samples from the proposal distribution
*MATH*. To reduce the variance during
inference, we follow the REINFORCE
algorithm [*REF*; *REF*] and introduce two baselines *MATH* 
and *MATH*, the centered learning signal and input
dependent baseline respectively to help reduce the variance. *MATH* is a
learnable constant and *MATH*. During
training, the two baselines are updated by minimising the distance,
*MATH* and the gradient w.r.t. *MATH* can be rewritten as *MATH*. 


Semi-Supervision 


Despite the steps described above for reducing the variance, there
remain two major difficulties in learning latent intentions in a
completely unsupervised manner: (1) the high variance of the inference
network prevents it from generating sensible intention samples in the
early stages of training, and (2) the overly strong discriminative power
of the LSTM language model is prone to the disconnection phenomenon
between the LSTM decoder and the rest of the components whereby the
decoder learns to ignore the samples and focuses solely on optimising
the language model. To ensure more stable training and prevent
disconnection, a semi-supervised learning technique is introduced.


Inferring the latent intentions underlying utterances is similar to an
unsupervised clustering task. Standard clustering algorithms can
therefore be used to pre-process the corpus and generate automatic
labels *MATH* for part of the training examples
*MATH*. Then when the
model is trained on the unlabeled examples
*MATH*, we optimise it against
the modified variational lower bound given in
Equation FORMULA *MATH*. However, when the model is updated based on examples
from the labeled set *MATH*, we treat the
labeled intention *MATH* as an observed variable and train the
model by maximising the joint log-likelihood, *MATH*. 
The final joint objective function can then be written as
*MATH*, where *MATH* controls the trade-off between the supervised and unsupervised examples.


Reinforcement Learning 


One of the main purposes of learning interpretable, discrete latent
intention inside a dialogue system is to be able to control and refine
the model&apos;s behaviour with operational experience. The learnt generative
network *MATH* encodes the policy
discovered from the underlying data distribution but this is not
necessarily optimal for any specific task. Since
*MATH* is a parameterised policy
network itself, any policy gradient-based reinforcement learning
algorithm [*REF*; *REF*] can be used to fine-tune the
initial policy against other objective functions that we are more interested in.


Based on the initial policy *MATH*,
we revisit the training dialogues and update parameters based on the
following strategy: when encountering unlabeled examples *MATH* at
turn *MATH* the system samples an action from the learnt policy
*MATH* and receives a
reward *MATH*. Conditioning on these, we can directly fine-tune a
subset of the model parameters *MATH* by the policy gradient method,
*MATH*, where *MATH* 
is the MLP that parameterises the policy network (Equation FORMULA).
However, when a labeled example *MATH* is encountered we force
the model to take the labeled action *MATH* and update
the parameters by Equation FORMULA as well. Unlike Li et al  where the whole model is
refined end-to-end using RL, updating only *MATH* effectively allows
us to refine only the decision-making of the system and avoid the problem of over-fitting.


Experiments 


Dataset &amp; Setup for Goal-oriented Dialogue 


We explored the properties of the LIDM model using the CamRest676
corpus collected by Wen et al , in which the task of the system is
to assist users to find a restaurant in the Cambridge, UK area. The
corpus was collected based on a modified Wizard of Oz [*REF*] online
data collection. Workers were recruited on Amazon Mechanical Turk and
asked to complete a task by carrying out a conversation, alternating
roles between a user and a wizard. There are three informable slots
(food, pricerange, area) that users can use to constrain the
search and six requestable slots (address, phone, postcode plus
the three informable slots) that the user can ask a value for once a
restaurant has been offered. There are 676 dialogues in the dataset
(including both finished and unfinished dialogues) and approximately
2750 conversational turns in total. The database contains 99 unique restaurants.


To make a direct comparison with prior work we follow the same
experimental setup as in Wen et al . The corpus was partitioned into
training, validation, and test sets in the ratio 3:1:1. The LSTM hidden
layer sizes were set to 50, and the vocabulary size is around 500 after
pre-processing, to remove rare words and words that can be
delexicalised FORMULA. All the system components were trained jointly
by fixing the pre-trained belief trackers and the discrete database
operator with the model&apos;s latent intention size *MATH* set to 50, 70, and
100, respectively. The trade-off constants *MATH* and *MATH* were
both set to 0.1. To produce self-labeled response clusters for
semi-supervised learning of the intentions, we firstly removed function
words from all the responses and clustered them according to their
content words. We then assigned the responses in the *MATH* -th frequent
cluster to the *MATH* -th latent dimension as its supervised set. This
results in about 35% (*MATH*) to 43% (*MATH*) labeled responses across
the whole dataset. An example of the resulting seed set is shown in
Table. During inference we carried out stochastic
estimation by taking one sample for estimating the stochastic gradients.
The model is trained by Adam [*REF*] and tuned (early stopping,
hyper-parameters) on the held-out validation set. We alternately
optimised the generative model and the inference network by fixing the
parameters of one while updating the parameters of the other.


During reinforcement fine-tuning, we generated a sentence *MATH* from the
model to replace the ground truth *MATH* at each turn and define an
immediate reward as whether *MATH* can improve the dialogue
success [*REF*] relative to *MATH*, plus the sentence BLEU
score [*REF*], *MATH*, *MATH* was set to 0.5. We
fine-tuned the model parameters using RL for only 3 epochs. During
testing, we greedily selected the most probable intention and applied
beam search with the beamwidth set to 10 when decoding the response. The
decoding criterion was the average log-probability of tokens in the
response. We then evaluated our model on task success rate [*REF*]
and BLEU score [*REF*] as in Wen et al  in which the model
is used to predict each system response in the held-out test set.


Experiments on Goal-oriented Dialogue 


TABLE


In order to assess the human perceived performance, we evaluated the
three models (1) NDM, (2) LIDM, and (3) LIDM+RL by recruiting paid
subjects on Amazon Mechanical Turk. Each judge was asked to follow a
task and carried out a conversation with the machine. At the end of each
conversation the judges were asked to rate and compare the model&apos;s
performance. We assessed the subjective success rate, the perceived
comprehension ability and the naturalness of responses on a scale of 1
to 5. For each model, we collected 200 dialogues and averaged the
scores. During human evaluation, we sampled from the top-5 intentions of
the LIDM models and decoded a response based on the sample. The result
is shown in Table 1. One interesting fact to note is that
although the LIDM did not perform well on the corpus-based task success
metric, the human judges rated its subjective success almost
indistinguishably from the others. This discrepancy between the two
experiments arises mainly from a flaw in the corpus-based success metric
in that it favors greedy policies because the user side behaviours are
fixed rather than interactional. Despite the fact that LIDMs are
considered only marginally better than NDM on subjective success, the
LIDMs do outperform NDM on both comprehension and naturalness scores.
This is because the proposed LIDM models can better capture multiple
modes in the communicative intention and thereby respond more naturally
by sampling from the latent intention variable.


TABLE


TABLE


TABLE


Three example conversations are shown between a human judge and a
machine, one from LIDM in
Table and two from LIDM+RL in
Table, respectively. The results are displayed one
exchange per block. Each induced latent intention is shown by a tuple
(index, probability) followed by a decoded response, and the sample
dialogues were produced by following the responses highlighted in bold.
As can be seen, the LIDM shown in
Table  clearly has multiple modes in the distribution
over the learned intention latent variable, and what it represents can
be easily interpreted by the response generated. However, some
intentions (such as intent 0) can result in very different responses
under different dialogue states even though they were supervised by a
small response set as shown in
Table . This is mainly because of the variance introduced
during variational inference. Finally, when comparing Table  and
Table , we can observe the difference between the two
dialogue strategies: the LIDM, by inferring its policy from the
supervised dataset, reflects the diverse set of modes in the underlying
distribution; whereas the LIDM+RL, which refined its strategy using RL,
exhibits a much greedier behavior in achieving task success (e.g. in
Table  in block 2 &amp; 4 the LIDM+RL agent provides the
address and phone number even before the user asks). This is also
supported by the human evaluation in
Table  where LIDM+RL has much shorter dialogues on
average compared to the other two models.


Discussion 


Learning an end-to-end dialogue system is appealing but challenging
because of the credit assignment problem. Discrete latent variable
dialogue models such as LIDM are attractive because the latent variable
can serve as an interface for decomposing the learning of language and
the internal dialogue decision-making. This decomposition can
effectively help us resolve the credit assignment problem where
different learning signals can be applied to different sub-modules to
update the parameters. In variational inference for discrete latent
variables, the latent distribution is basically updated by the reward
from the variational lower bound. While in reinforcement learning, the
latent distribution (i.e. policy network) is updated by the rewards from
dialogue success and sentence BLEU score. Hence, the latent variable
bridges the different learning paradigms such as Bayesian learning and
reinforcement learning and brings them together under the same
framework. This framework provides a more robust neural network-based
approach than previous approaches because it does not depend solely on
sequence-to-sequence learning but instead explicitly models the hidden
dialogue intentions underlying the user&apos;s utterances and allows the
agent to directly learn a dialogue policy through interaction.


Related work 


Modeling chat-based dialogues [*REF*; *REF*] as a
sequence-to-sequence learning [*REF*] problem is a common theme
in the deep learning community. Vinyals and Le  has demonstrated a
seq2seq-based model trained on a huge amount of conversation corpora
which learns interesting replies conditioned on different user queries.
However, due to an inability to model dialogue context, these models
generally suffer from the generic response
problem [*REF*; *REF*]. Several approaches have been
proposed to mitigate this issue, such as modeling the
persona [*REF*], reinforcement learning [*REF*], and
introducing continuous latent variables [*REF*; *REF*].
While in our case, we not only make use of the latent variable to inject
stochasticity for generating natural and diverse machine responses but
also model the hidden dialogue intentions explicitly. This combines the
merits of reinforcement learning and generative models.


At the other end of the spectrum, goal-oriented dialogue systems
typically adopt the POMDP framework [*REF*] and break down the
development of the dialogue systems into a pipeline of modules: natural
language understanding [*REF*], dialogue
management [*REF*], and natural language generation [*REF*].
These system modules communicate through a dialogue act
formalism [*REF*], which in effect constitute a fixed set of
handcrafted intentions. This limits the ability of such systems to scale
to more complex tasks. In contrast, the LIDM directly infers all
underlying dialogue intentions from data and can handle intention
distributions with long tails by measuring similarities against the
existing ones during variational inference. Modeling of end-to-end
goal-oriented dialogue systems has also been studied
recently [*REF*; *REF*; *REF*], however, these models
are typically deterministic and rely on decoder supervision signals to
fine-tune a large set of model parameters.


Much research has focused on combining different learning paradigms and
signals to bootstrap performance. For example, semi-supervised
learning [*REF*] has been applied in the sample-based neural
variational inference framework as a way to reduce sample variance. In
practice, this relies on a discrete latent
variable [*REF*; *REF*] as the vehicle for the
supervision labels. As in reinforcement learning, which has been a very
common learning paradigm in dialogue
systems [*REF*; *REF*; *REF*], the policy is also
parameterised by a discrete set of actions. As a consequence, the LIDM,
which parameterises the intention space via a discrete latent variable,
can automatically enjoy the benefit of bootstrapping from signals coming
from different learning paradigms. In addition, self-supervised
learning [*REF*] (or distant, weak supervision) as a simple way
to generate automatic labels by heuristics is popular in many NLP tasks
and has been applied to memory networks [*REF*] and neural dialogue
systems [*REF*] recently. Since there is no additional effort
required in labeling, it can also be viewed as a method for
bootstrapping.


Conclusion 


In this paper, we have proposed a framework for learning dialogue
intentions via discrete latent variable models and introduced the Latent
Intention Dialogue Model (LIDM) for goal-oriented dialogue modeling. We
have shown that the LIDM can discover an effective initial policy from
the underlying data distribution and is capable of revising its strategy
based on an external reward using reinforcement learning. We believe
this is a promising step forward for building autonomous dialogue agents
since the learnt discrete latent variable interface enables the agent to
perform learning using several differing paradigms. The experiments
showed that the proposed LIDM is able to communicate with human subjects
and outperforms previous published results.