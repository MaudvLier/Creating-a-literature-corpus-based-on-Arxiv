Learning Optimal Behavior Through Reasoning and Experiences


Introduction


There is a deep, interdisciplinary interest in understanding and modeling cognitive limitations in decision making. Across both economics and cognitive sciences, the literature recognizes two broad ways in which people learn about and deduce what is the best course of action in a given situation. The first is cognition and reasoning: through introspective, abstract deliberations, humans can get a better grasp of what is their optimal action in the situation at hand. The second is accumulated experiences: by observing realized outcomes of past decisions, agents update their views on the respective benefits of taking those actions in these circumstances. These two sources of information are conceptually distinct and are both limited: experiences are observed only along the realized path of situations the agent actually faced in the past, and while abstract thinking could help the agent deduce outcomes in counterfactual actions and situations, such deliberations are cognitively costly.


This paper develops a novel framework of constrained-optimal behavior under cognitive frictions that studies jointly learning through reasoning and accumulated experiences. To do so, the paper draws on conceptual, methodological and empirical insights from both economics and the cognitive sciences literature on reinforcement learning (RL) ([*REF*], [*REF*]).


In our framework, agents are uncertain about optimal behavior in the sense of facing subjective uncertainty over the optimal policy and value functions that characterize their decision problem. In particular, we follow the RL approach of letting agents learn about the action value function *MATH*, which gives the expected discounted sum of utility when taking action *MATH* in state *MATH* and following a given policy *MATH* thereafter. While the standard approach in economics is to endow the agents with perfect knowledge about the optimal policy function *MATH* and therefore the action value function *MATH*, in our model agents perceive *MATH* as uncertain ex-ante, and gradually learn about it over time.


Learning can occur through cognition, which is costly, but beneficial in reducing agents&apos; uncertainty about the best course of action. Agents trade off that benefit and cost of engaging cognitive resources, giving rise to a state- and history-dependent choice of reasoning and thus exhibit constrained-optimal, or &quot;resource-rational&quot; behavior. Agents also update beliefs about optimal behavior based on the experienced flow utility each period. Critically, the effective precision of both reasoning and experiences in informing behavior is endogenous, as a function of the agent&apos;s beginning of period prior beliefs and uncertainty which evolve dynamically and endogenously.


Elements of framework. There are four key specific features of our learning framework.


First, we model agents&apos; beliefs over the unknown function *MATH* as a Gaussian Process (GP) distribution. On the one hand, the GP distribution is methodologically appealing as a prior over the space of functions, as it is very flexible, and also tractable in recursively characterizing the conditional moments of the unknown function. On the other hand, cognitive sciences increasingly emphasizes a Bayesian approach and in particular the appeal of GP distributions, both for conceptual and descriptive reasons.


Second, given an estimate of the action value function *MATH*, agents take constrained optimal actions trading off exploitation and experimentation incentives. On the one hand, the agent has incentives to choose the action with the highest estimated value *MATH* at the current state *MATH* (exploitation).
On the other hand, the agent recognizes that *MATH* is just an uncertain estimate so there is a benefit to exploration and learning more about the unknown *MATH*. We model this desire for experimentation following the concept of maximum entropy reinforcement learning, which essentially the entropy of the action distribution to be bigger than some minimum threshold, thus ensuring randomization. In contrast to the standard approach that sets this minimum threshold as an exogenous time-invariant parameter, we let the entropy lower bound be proportional to the remaining subjective uncertainty over *MATH*. This captures the intuition that experimentation is valuable only to the extent that the *MATH* -function is uncertain. Putting it all together, the resulting optimal action policy function takes the form of the softmax function widely used in statistics and machine learning (see eg.
[*REF*]).


Turning to the learning modes and the dynamic evolution of the estimates of the action value function, our third modeling feature is learning from experienced utility. Because it is derived from actual observed outcomes, this learning follows the so-called &quot;model-free&quot; or &quot;Q-learning&quot; dynamic programming solution techniques used in machine learning and cognitive sciences (e.g. [*REF*]). In particular, at the beginning of the period, the agent observes the realization of the new state *MATH* and also the realized utility *MATH* that she experienced based on last period&apos;s choice of action *MATH*. Knowing the flow utility yesterday and the realized state today implies an experience-based informative signal that updates beliefs about *MATH*. The key intuition behind this update, is that the flow utility *MATH* reveals the &quot;temporal difference&quot; in the Q-function, i.e. *MATH*.


Finally, but importantly, within the same objective function that determines optimal actions, we model the benefit and cost of abstract reasoning. By reasoning we mean the internal deliberation process through which the agent produces information and learns about *MATH* generally. Intuitively, this is akin to an economist trying to solve for the value function globally. As such, our terminology of reasoning is similar to the notion model-based learning in the RL literature ([*REF*]). The benefit of reasoning works through the resulting reduction of uncertainty over *MATH*. We allow this reduction to impact the objective function in two ways. One is direct utility cost of higher uncertainty (like a cognitive dissonance cost, eg. [*REF*]).


The second emerges indirectly and endogenously, through an interaction between actions and reasoning. In particular, since reasoning decreases current conditional uncertainty, it also weakens the incentive to experiment, by formally lowering the threshold on the action distribution entropy. Intuitively, the agent values the reduction in uncertainty from reasoning because it allows her to select an action closer to the one with the currently highest value estimate, not having to worry about experimentation. In turn, the cost of reasoning is also in utility terms, quantified as the reduction in entropy achieved by acquiring these deliberation signals, as in information theory (e.g.
[*REF*]), capturing effort as in the cognitive control literature (e.g. [*REF*], [*REF*]).


We show how both reasoning and experience signals can be incorporated in revising beliefs over the unknown *MATH* -function using formal non-parametric Bayesian updating formulas. Put together, these two sources of information update agents&apos; conditional mean and uncertainty over the whole function *MATH*, providing a recursive structure to the GP distribution entering next period.


Key contributions. Our framework connects to several literature strands.


In terms of the economics literature, we use insights from the RL literature to integrate learning about optimal behavior from experiences into models of resource-rationality that emphasize a cost-benefit tradeoff of reasoning. This tradeoff is shared with various such approaches in economics (including [*REF*], [*REF*], [*REF*], [*REF*], [*REF*] and others) and cognitive sciences (e.g.
[*REF*], [*REF*], [*REF*]). In turn, we connect to a growing interest and evidence in economics on experience-based learning, as in [*REF*], [*REF*]. We differ from this latter approach by studying experiences jointly with reasoning, as well as by modeling experiences as informative not about the state law of motion, but instead about the perceived value of taking specific actions, as in RL.


Our framework innovates within RL modeling itself as well, by building a dynamic cognitive model with several key features emerging endogenously, as follows. 


First, we obtain and characterize an endogenous arbitration between learning from reasoning and experiences. We use the terminology of &quot;arbitration&quot; as in RL, meaning the characterization of the different weights put on the two modes of learning (&quot;model-based&quot; vs &quot;model-free&quot; learning, in the language of RL). Our framework produces an internally consistent, endogenous arbitration in the form of the state- and history-dependent Bayesian updating weights our agents put on their reasoning and experience signals. The property that arbitration is based on uncertainty is shared with cognitive and neuroscience work arguing that the brain puts more weight on the learning mode that it deems more reliable (as in [*REF*], [*REF*]). Our key contribution here is to incorporate everything in a unified Bayesian framework with GP priors, which allows (i) prior uncertainty to account for correlation between *MATH* -values at different state/action pairs, (ii) non-parametric cognitive learning, and (iii) delivers a tractable and parsimonious unified conditional belief process, as opposed to having to keep track and arbitrarily tie together two separate model-based and model-free estimates.


Second, the framework is characterized by an endogenous decision to engage reasoning. The model lets agents treat cognition as an accessible, but costly resource. Thus, it is not just that, given some model-based and model-free estimates, there is endogenous arbitration, but the intensity of the model-based learning and the associated precision of its reasoning signal is time-varying, conditional on states and choices. This implication of adjusting cognition in a state-dependent way is desirable for both empirical and conceptual reasons. Empirically, it is consistent with evidence and a general desired approach (see eg. [*REF*]) in the neuroscience and RL literatures emphasizing the cognitive cost of mental simulations versus their benefit. The latter is here modulated through reduction of uncertainty, where the value of reducing that uncertainty is (partly) state-dependent. In addition, when the agent values the reduction in uncertainty due to a direct, cognitive dissonance utility cost, this latter mechanism can capture evidence that active deliberation is indeed engaged only when there is sufficient &quot;conflict&quot; or uncertainty in prior beliefs (see eg. [*REF*]).


Third, the framework describes jointly the evolution of beliefs and the constrained optimal policy function which agents follow in selecting their actions. We endogenously derive a constrained optimal action policy that takes the form of a softmax function, which is otherwise exogenously postulated in RL. A key input in that function is the &apos;temperature&apos; parameter, which the existing literate treats as an exogenous parameter -- the lower the temperature, the more the action policy leans away from &quot;exploration&quot; and exploits the action with the current highest estimated payoff. In our model, the temperature parameter is endogenous and is state- and history-dependent, since it depends on the conditional subjective uncertainty the agent still perceives in his estimate of *MATH*. As such, this &quot;temperature&quot; of the softmax action policy endogenously evolves as the agent learns. The study of exogenous adjustments of this parameter along the sample path has been of large interest in RL, machine learning and constrained optimization literatures. In that literature, a robustly successful time-varying temperature (i.e. maximizing reward along simulated paths) is found to be decreasing through time, since later along the path there is typically less reason to explore ([*REF*]). Our framework delivers endogenously that result, as uncertainty over *MATH* estimates naturally declines over time, as well as adds novel state-dependencies in that logic.


Overall, the framework thus results in a dual-learning system, with two components akin to the &quot;model-based&quot; and the &quot;model-free&quot; modes of learning discussed in the RL literature. Crucially, the paper also extends this literature in providing a unified, formal way of modulating these two modes of learning via subjective uncertainty, which evolves through information accumulation. Thus, the paper extends both (i) the economics literature, by providing a conceptually new bounded rationality model deeply rooted in cognitive science insights and empirical results, and (ii) the RL literature, by bringing in constrained-optimal maximization approaches and tools typical to information economics.


Framework 


We aim to model the process of economic agents figuring out optimal behavior using both their abstract thinking abilities, i.e. cognition, and their accumulated experience with past actions.


A generic recursive problem


To fix ideas, consider a generic recursive problem with discrete time indexed by *MATH*, where an agent chooses an action *MATH* (potentially a vector of actions) and is perfectly aware of all payoff-relevant details of the environment. The problem can be expressed as a Bellman equation *MATH* *MATH* *MATH* where *MATH* collects all the relevant state variables, both exogenous and endogenous. The state follows the known law of motion *MATH* in (DP).


The primitives of the environment are the per-period utility function *MATH*, the time discount factor *MATH*, the budget *MATH* defining the set of currently feasible actions *MATH* and the law of motion of the state *MATH*. The value function *MATH* encodes the continuation value attached to starting in any particular realization of *MATH*, when following the optimal policy function *MATH*. The latter is the optimally selected mapping from any given state *MATH* to probabilities over feasible actions *MATH* from equation (DP).


Such dynamic problems are at the core of modern economics. And as economists we understand that since dynamic problems require evaluating whole infinite paths of actions, or in recursive terms, an action plan with all possible future contingencies, characterizing *MATH* is generally a challenging functional problem (see e.g., [*REF*] or [*REF*]). This is often not fully tractable even to highly trained economists, and state-of-the-art approximate solution techniques require a lot of sophistication and effort to implement.


At the same time, in standard economic models is taken as given that the agents themselves always know the optimal policy *MATH* and value *MATH* functions. The difficulty economists face in actually computing the optimal objects *MATH* and *MATH* is simply abstracted away.
We aim to address this apparent paradox, by developing a framework which puts the economic agents on a similar footing as economists, by requiring agents to invest cognitive effort in figuring out *MATH*, and thus *MATH* and modeling their gradual learning process.


In particular our starting point is that in real life people would typically have two sources of information about optimal behavior. The first source is experiences: by observing the per-period utility outcomes of different actions taken at various states in the past, the agent learns about *MATH*. The second source is cognition and reasoning: a unique human characteristic is the ability to think abstractly about the problem at hand. Through such internal deliberations, agents can learn about the implied value of taking different courses of action -- for example, this could take the form of mentally simulating paths forward of possible behavior and comparing expected utility. These two sources of information are conceptually distinct and are both limited: experiences are realized only along the actual path taken by the agent, while abstract thinking is a scarce cognitive resource.


To model learning from both mental simulation and actual experiences, we connect to the large reinforcement learning (RL) literature ([*REF*], [*REF*]). There an agent interacting with a dynamic and stochastic environment learns an optimal control policy for a sequential decision problem, typically a Markov Decision Process.


Using the notation of equation (DP), the key object we focus on, as in RL, is the action-value function *MATH*. This function gives the expected utility when taking action *MATH* in state *MATH* and following a given policy *MATH* thereafter (not necessarily the optimal *MATH* ).
The agent wants to know the optimal *MATH*, defined as *MATH* *MATH* *MATH* where *MATH* satisfies the Bellman equation (DP). The functions *MATH* and *MATH* are implicitly related, as *MATH*, but as we detail later, it will be useful to work with *MATH*.


Human learning with Gaussian Processes


We model agents&apos; beliefs over the unknown function *MATH* as Gaussian Process (GP) distributions. In particular, at the beginning of time agents have the initial prior *MATH* *MATH* *MATH* *MATH* The defining feature of a Gaussian Process distribution is that for any two action-state pairs *MATH* and *MATH*, the values *MATH* and *MATH* have a joint-Normal distribution with mean and variance-covariance given by *MATH* and *MATH*.


Why learning with GP? There is a variety of motivating insights and arguments to using GP distributions and in particular so in the context of modeling human cognition.


On the one hand, GP distributions are methodologically appealing due to their flexibility and tractability. In particular, GP distributions extend the familiar Kalman filter to learning about functions and the law of motion for the conditional mean and variance can be easily characterized recursively. As a result, the conditional beliefs in each period also follow a GP distribution, with properly updated mean and variance functions, as detailed below. In this formulation the resulting variance-covariance function *MATH* encodes the agent&apos;s prior view of how likely correlated are the function&apos;s values at different points (here pairs *MATH* and *MATH* ), and therefore is inducing a measure of proximity, or similarity, between those points.


On the other hand, the cognitive sciences literature increasingly emphasizes a Bayesian approach and in particular the appeal of GP distributions. Indeed, at a broad level, cognitive sciences emphasize that cognition is fundamentally related to forming uncertain conjectures from partial or noisy information, and thus a probabilistic framework is particularly well suited both conceptually and in terms of accounting for data on observed behavior (eg. [*REF*], [*REF*], [*REF*]).


The GP distribution in particular has been increasingly used in the cognitive sciences literature, building on connections to statistics and machine learning (on the latter see eg. [*REF*]). The reason is in the recent experimental and neuroscience evidence that the human brain&apos;s learning process is well described by GP (see for example [*REF*] for a survey and [*REF*], [*REF*], [*REF*] for evidence using bandit-like tasks).


Even more specific to RL, the Bayesian approach and the GP distribution have also found applications in this RL literature (see eg. [*REF*]). As surveyed in [*REF*], this approach can provide a tractable and coherent way to model the exploitation-exploration tradeoff, fundamental to learning from experiences, as a function of subjective uncertainty over *MATH* estimates, as well as offering a formal way to incorporate prior beliefs into the action-selection algorithms.


In building our framework, we nevertheless emphasize that these motivating insights from cognitive science have not been connected in a single framework that interacts learning from abstract thinking and from experiences, and have also not been previously applied to economic models.


Learning from experienced outcomes


We start by detailing how we formalize the learning through experienced outcomes and its connection to the concept of &quot;model-free&quot; learning in machine learning. We will consider this type of learning as free of cognitive costs, as it accumulates by the agent simply experiencing utility flows of the specific actions she has taken in the past, and not through abstractly thinking about contingencies and counterfactuals.


Consider a typical period *MATH*. The agent enters the period with some prior beliefs, conditional on the prior information set *MATH*. As anticipated earlier, given the time zero prior in equation, the conditional *MATH* beliefs also have a GP distribution: *MATH* *MATH* *MATH* where *MATH* follow a recursive formulation detailed below.


At the beginning of the period, the agent observes the realized utility *MATH* that she experienced based on last period&apos;s choice of action *MATH*. In addition, the time *MATH* shock also realizes and the agent observes the realization of the new state variable *MATH*.
Knowing the flow utility yesterday and the realized state today implies an experience-based informative signal that updates beliefs about *MATH*. This update, along the realized path of state action pairs *MATH*, follows the so called &quot;model-free&quot; or &quot;Q-learning&quot; dynamic programming solution techniques used in machine learning and cognitive sciences (e.g. [*REF*]).


The key intuition behind this update, is that the flow utility *MATH* reveals the &quot;temporal difference&quot; in the Q-function, that is from the Bellman equation we can express


*MATH* *MATH* *MATH*.

Given beliefs *MATH*, one can then compute the deviation from the above equation that any specific beliefs *MATH* imply, and adjust them accordingly. To do this directly from the above equation, this requires computing the expectation *MATH* which integrates over all possible states *MATH* that could have realized at time *MATH*. This integration is computationally and conceptually a complex step, and thus, typically machine learning applications actually use the more robust approach which utilizes instead the approximation *MATH* *MATH* *MATH* where the temporal difference on the right-hand side is not between the Q-function at *MATH* and the average across all possible states *MATH*, but only the difference with the value of the Q-function in the actually realized state *MATH*.


This approach is &quot;robust&quot;, in the sense that it does not need to compute the expectation *MATH* and also does not need to assume full knowledge of the transition probabilities *MATH*. The agent is simply sitting at time *MATH* and observing the actual realization of *MATH*, and then only looks back at the old state-action pair *MATH* and the realized utility *MATH*.
Still, using this approximation to update estimates of the *MATH* -function is asymptotically consistent, in that if agents visit all possible states in the support of *MATH* with positive probability, then the update based on the approximation will eventually converge to the true *MATH* as *MATH*.


Also worth stressing is that this &quot;model-free&quot; update is very simple to do, as it just compares the agent&apos;s realized utility yesterday and her current expectation of the *MATH* -function at the realized state today. As such, it is straightforward to model this experiential updating as &quot;cognitively free&quot; -- as something that just comes by default to the agent.


Formally, the agent perceives the following experience signal *MATH* *MATH* *MATH*.

And lastly, since our framework keeps track of beliefs about *MATH*, but not explicitly of beliefs over *MATH*, for tractability it is convenient to approximate the latter by setting *MATH*. Thus, we assume that the agent perceives the following structure of the experience signal *MATH* *MATH* *MATH*.

This expression for *MATH* can be readily used to compute a formal Bayesian update based on this experiential information.
Specifically, *MATH* *MATH* *MATH* where *MATH* is not quite the end-of-period *MATH* beliefs, as beliefs will potentially be further updated via the abstract reasoning we describe below.
Moreover, the signal to noise ratio *MATH* can be derived in a straightforward say by evaluating *MATH* *MATH*. 


The signal also reduced the conditional uncertainty facing the agent, as *MATH* *MATH* *MATH*.


Learning through abstract reasoning


After updating beliefs for free, each period *MATH* the agent decides how intensely to engage in cognitively costly abstract reasoning, which produces further information about *MATH*. And then, after updating beliefs based on all sources for time *MATH* information, the agent optimally chooses an action policy *MATH*. To understand these choices, consider first the reasoning process.


Reasoning


By abstract reasoning we mean the internal deliberation process through which the agent thinks abstractly about his decision problem, and learns about *MATH* generally. We will remain agnostic about the specific mode of deliberation the agent engages in, but for example they can be trying to do value function iteration or just thinking forward through the decision tree of their problem. This kind of reasoning is deliberate and abstract, as it tries to deduce new information about *MATH* over and above the simple experience of the flow utility given past choices. It is a potentially powerful source of information for the agent, but is mentally costly.


Formally, each period the agent can generate a vector of reasoning signals *MATH* as unrestricted linear functions of *MATH* *MATH* *MATH* *MATH* where *MATH* and *MATH* is a *MATH* matrix. The agent optimally chooses the structure of the matrix *MATH* and the noise variance matrix *MATH*, subject to a cost-benefit tradeoff introduced below in subsection [choice].


The vector of signals *MATH* then updates beliefs over *MATH* at all pairs *MATH* : *MATH* *MATH* *MATH* where the signal-to-noise ratio *MATH* is given by *MATH* *MATH* *MATH* which can be tractably computed from *MATH* described above.


Importantly,the reasoning signals further reduce uncertainty, so that *MATH* *MATH* *MATH* denote the posterior variance function over the vector.


Interpretation. Our terminology of reasoning is similar to the notion of planning or model-based learning in the computational and RL literature ([*REF*]). Reasoning can in practice work through various specific processes. Consider for example a specific tool used in the model-based RL -- the simulation of a limited number of future paths of actions and resulting states and utility flows in the future (see for example [*REF*]). In our framework, this simulation is captured by the noisy signals *MATH* that update the prior beliefs of the agent. In a related way, by producing new information internally to the agent, without being observed by an outsider, the notion of reasoning here also resembles in economics that of &apos;fact-free learning&apos; in [*REF*] and [*REF*].


However, our proposed framework purposefully abstracts from the specifics of the mental method behind *MATH*. Instead it aims to capture a key tradeoff that many different solution methods share, namely that taking more computational steps in any given method (e.g.
simulate more and further paths forward), leads to (i) higher accuracy of the resulting solution, but (ii) is cognitively costlier, as we describe next.


Overall, by incorporating both signals *MATH* and *MATH* in the conditional beliefs of our agent we are integrating, in an endogenous way, both &quot;model-free&quot; and &quot;model-based&quot; learning, which are the two main types of learning discussed in RL.


This joint integration of both types of learning is present in both some recent treatments of RL (see [*REF*]) and also in classical RL algorithms such as Dyna-Q ([*REF*; *REF*]). Nevertheless, the RL literature usually resorts to ad-hoc assumptions on how the two types of learning are mixed together, while our framework proposes an endogenous arbitration between the two types of signals based on formal Bayesian updating notions.


Furthermore, the specific formulation of belief updating for both signals in equations resembles temporal difference solution techniques, used extensively in RL ([*REF*]), and in particular their Bayesian, GP-based version, connecting to the approach in [*REF*; *REF*].


Joint choice over reasoning and actions at time *MATH*. 


Equations describe the updated beliefs, taking as given the structure for the signal in equation. Critically, given the current state *MATH* and prior beliefs in equation, we let the agent jointly choose her optimal reasoning structure (i.e. *MATH* and *MATH* ) and the action policy *MATH* that he will follow this period.


Objective function. We let the agent maximize the following joint objective function *MATH* *MATH* *MATH* where recall that *MATH* is the variance conditional on both the beginning-of-period information set *MATH* and the experience signal *MATH*, while *MATH* is the end-of-period posterior variance defined in equation *MATH* *MATH* denotes the diagonal entries of the posterior variance *MATH*.


The objective function in [OBJ] is subject to two types of constraints on the action distribution. First, is that of feasibility: *MATH* for actions that do not satisfy the budget constraint *MATH*, and that the action distribution probabilities sum to one: *MATH* *MATH* Second, the action distribution *MATH* is subject to an entropy constraint: *MATH* *MATH* *MATH*.

Interpretation. The objective function in [OBJ] and the entropy constraint in [OBJ] allow our framework to capture jointly a variety of forces of interest emphasized in the RL and cognitive science literature, as follows.


The first term in the objective of equation [OBJ] reflects the agent&apos;s benefit of exploitation, or &quot;greediness&quot; in the RL language.
This force incentivizes the agent to choose the action *MATH* with the current highest estimated value *MATH*, where the latter is given in equation. The second term captures a possible cognitive dissonance cost ([*REF*], [*REF*]). Essentially, when the primitive disutility parameter *MATH*, there is a disutility cost of uncertainty over the values associated to each action. This cost generates a benefit of reasoning that goes purely through the reduction of that dissonance and the disutility of facing uncertainty about the Q-function. This mechanism is qualitatively similar to the one used in [*REF*], and is distinct from the novel exploitation-experimentation tradeoff described below.


The third term in the objective function measures the cognitive cost of reasoning as proportional to the information content of signals *MATH*, with information flow quantified as the reduction in entropy achieved by acquiring these signals, following a standard information theory approach, e.g. [*REF*]. The reasoning cost captures the fact that increasing the reasoning intensity (e.g. mentally simulating more paths forward) requires higher cognitive effort. Like in the cognitive control literature (e.g. [*REF*], [*REF*], [*REF*]), the constant marginal cost *MATH* can be interpreted as the opportunity cost of cognitive capacity, which can be otherwise employed on other, outside-the-model tasks.


In turn, the interpretation and aim of the constraint in [entropyactions] is to capture a desire for experimentation. As in typical bandit problems, as long as the value function *MATH* is uncertain, there is a desire to explore the function in other parts of the state space. Following the RL literature, we parsimoniously model this desire to experiment using the idea of &quot;entropy regularization &quot;, or maximum entropy RL, (eg.
[*REF*], [*REF*], [*REF*]). This regularization requires the entropy of the action distribution (the LHS of equation [entropyactions]) to be bigger than some minimum threshold, thus ensuring randomization. With this constraint, the chosen action distribution *MATH* is not degenerate, but always puts some probability of exploring any feasible action, thus capturing the exploration incentive inherent to dynamic learning problems.
Importantly, in our framework this entropy constraint builds in that experimentation is valuable only to the extent to which the *MATH* -function is uncertain -- indeed, if *MATH* was known, there would be no point to experiment as there is nothing further to learn. Hence, when the experimentation parameter *MATH*, in the RHS of equation the entropy lower bound is set proportional to the remaining subjective uncertainty over *MATH*.


Optimal policy action


Let *MATH* denote the Lagrange Multiplier on the constraint in equation. Given the set of feasible actions in *MATH*, the optimal policy action is *MATH* *MATH* MATH* where *MATH* is the degenerate greedy policy *MATH* *MATH* *MATH*.

Softmax. The optimal policy in [pi] takes the form of the softmax function widely used in statistics and machine learning. This function takes the expected rewards of following any given action and transforms them into action probabilities (eg. [*REF*] ). However, in that literature *MATH*, known as the &apos;temperature&apos; parameter, is typically exogenous. The lower is *MATH*, the more the expected rewards affect the probability of taking actions. In our model, *MATH* is endogenous and state and time-dependent. Specifically, it will be high for states *MATH* where the agent faces a higher uncertainty (and thus *MATH* is high).


In economics, a similar softmax function describes the multinomial logic model of choices (eg. [*REF*]), typically micro-founded from a random utility model where valuations of actions are subject to additively separable and independently distributed shocks drawn from the extreme-value distribution (eg. [*REF*]). Instead, the optimal policy *MATH* proposed here does not rely on such random utility shocks.


A complementary micro-foundation for logit choice in the literature is based on rational inattention ([*REF*]). In that work (eg. [*REF*] in economics, or the policy compression work of [*REF*] in cognitive sciences), the entropy of the actions distribution, i.e. LHS of equation, speaks to an information flow constraint across actions, given signals about the rewards associated to those actions. Here instead the entropy on actions distribution speaks to the entropy regularization logic in RL, per above.


To see that comparison, consider setting *MATH* in constraint [entropyactions]. In that case, the constraint would not be binding and the optimal action becomes *MATH*. Critically, even without a role for the entropy on actions distributions, when *MATH* there is still an entropy informational cost on reasoning signals in objective. So, when *MATH* reasoning is here still imperfect, but given imperfect posterior Bayesian estimates *MATH*, the agent would take the greedy perceived action with probability one. The extreme case of *MATH* would thus have no role for exploration, a fundamental feature to dynamic learning problems.


At another extreme, consider the limit of costless reasoning, i.e.
*MATH* In that case, *MATH* as cognitive effort is chosen to be infinitely high, and agents learn *MATH* perfectly. As there is no remaining uncertainty, the entropy regularization constraint would not bind, *MATH* and the optimal action follows the greedy policy. The objective in equation would then recover exactly the problem solved under full rationality (see equations) since *MATH*. 


Optimal reasoning signal structure


The optimal reasoning choice consists of the agent deciding on the matrix *MATH* and signal noise variance matrix *MATH*.
To solve this problem, we use insights from rate-distortion theory.


Let *MATH* denote the eigenvalues of posterior variance *MATH*. Using the standard property that a matrix determinant equals the product of its eigenvalues, *MATH*, it follows that optimal reasoning can be expressed as choosing the resulting eigenvalues *MATH* optimally.


For notational convenience, it is useful to sort the eigenvalues in descending order so *MATH* *MATH* *MATH* By standard properties of variance matrices *MATH* is positive definite, and thus the (sorted) eigenvalues of the posterior variance *MATH* must be weakly smaller than those of the &quot;prior&quot; variance *MATH*, hence *MATH* (where *MATH* denote the sorted eigenvalues of *MATH* ). Taking this constraint into account and optimizing the objective function over *MATH*, the optimal reasoning choice implies *MATH* *MATH* *MATH*.

Hence, the agent has an optimal target level for uncertainty and wants to only reduce eigenvalues that currently bigger than that threshold *MATH* -- a property known as reverse &apos;water filling&apos; in rate-distortion theory (see [*REF*], chapter 13). Thus, we can show that the optimal signal structure is for *MATH* to be the matrix of eigenvectors of *MATH*, and *MATH* be a diagonal matrix with entries *MATH* *MATH* *MATH*.

Put together, equations uncover a tight intuitive interaction between actions and reasoning intensity. For example, take states *MATH* where uncertainty is high and thus the endogenous *MATH* is larger. By [pi], a tighter constraint induces the agent to explore more and thus deviate from the action with the currently highest perceived *MATH*. On the other hand, reasoning lowers uncertainty and thus relaxes this constraint, lowering *MATH* and leads to a policy function *MATH* that selects actions *MATH* with a higher expected payoff *MATH*.


Looping back to the choice of how much cognitive effort to invest in reasoning, from [optlambda] we see that when *MATH* is larger, the agent employs more cognition to achieve a greater reduction in uncertainty (the optimal target for *MATH* is lower).
Intuitively, she values that reduction in uncertainty precisely because it allows her to select an action closer to the currently perceived greedy policy, not having to worry about experimentation.


Overall, the framework is well suited to apply to a range of concrete economic environments, some of which we actively investigate in our own work in progress. For example, a standard consumption-savings problem in a simple [*REF*] framework is a transparent and widely-studied setting in economics. Indeed, the consumption-saving decision is a fundamental mechanism in a number of different economic settings, and has been used as a laboratory for other recent bounded rationality and behavioral papers, such as for example [*REF*] and [*REF*]. Due to its core elements, the bounded rationality framework proposed here appears promising in delivering endogenously empirically documented properties for consumption that are puzzling for standard fully-rational models: (a) experience effects of past income shocks on future consumption choices, (b) high sensitivity of consumption to income shocks; (c) large heterogeneity in consumption responses, through stochastic signals and choice.