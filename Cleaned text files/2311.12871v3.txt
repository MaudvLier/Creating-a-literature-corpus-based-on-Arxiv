An Embodied Generalist Agent in 3D World


Introduction


Building one generalist model that can handle comprehensive tasks like humans has been a long-existing pursuit in artificial intelligence and neuroscience ([Lake et al. 2015; 2017; Zhu et al. 2020; Mountcastle 1979; Schmid- huber 2018; Huang et al. 2022a]). Recent advances in LLMs ([Brown et al. 2020]) and &quot;foundation models&quot; ([Bom- masani et al. 2021]) emerge as a promising paradigm in building such generalist models in natural language processing ([OpenAI 2022; 2023]), computer vision ([Kirillov et al. 2023; Alayrac et al. 2022]), and robotics ([Brohan et al. 2022; 2023; Reed et al. 2022; Driess et al. 2023; Li et al. 2023c]). The keys to the success of this paradigm lie in large-scale internet-level datasets from numerous tasks and domains, as well as scalable Transformer architectures ([Vaswani et al. 2017]) that can absorb generalizable and task-agnostic knowledge from the data. Nonetheless, existing generalist models primarily thrive within 2D domains, lacking comprehension of the 3D physical environment that envelops human-level intelligence. This limitation stands as an obstacle that prevents current models from solving real-world tasks and approaching general intelligence. Therefore, we ask a fundamental question: how to equip the generalist agent with a comprehensive understanding of and the ability to interact with the real 3D world?


The development of such generalist agents encounters three primary challenges: the lack of suitable datasets, unified models, and effective learning strategies. Despite substantial progress in scaling up image-text models ([Tsimpoukelli et al. 2021; Alayrac et al. 2022]) and the curation of corresponding datasets ([Radford et al. 2021; Schuhmann et al. 2022]), advancement in 3D scene-level understanding has significantly lagged behind. This is largely attributed to the limited scale and manual labeling of 3D datasets ([Dai et al. 2017; Wald et al. 2019; Chen et al. 2020]), given the higher cost associated with collecting 3D data compared to 2D data. Furthermore, large-scale unified pretraining and efficient finetuning are under-explored by previous 3D VL models, which are often designed with strong priors ([Zhao et al. 2021; Chen et al. 2022]). Notably, recent works ([Zhu et al. 2023c; Hong et al. 2023]) utilize multi-modal Transformer together with synthetic data to enhance the model&apos;s capability in grounded 3D scene understanding. Nevertheless, they fall short in embodied tasks, e.g., acting within 3D environments. Additionally, there are significant yet rarely explored problems, e.g., the potential of VLA learning and efficient adaptation of LLMs for 3D tasks.


In this work, we introduce the generalist agent LEO, which is generically embodied, multi-modal, and general-purpose. It can take egocentric 2D images, 3D point clouds, and texts as task input and handle comprehensive tasks within the 3D environment. As shown in Fig.
[1], LEO exhibits the capability of perceiving, grounding, reasoning, planning, and acting with a unified task interface, model architecture, and objective. LEO perceives through an egocentric 2D image encoder for the embodied view and an object-centric 3D point cloud encoder for the third-person global view. Such perception modules can be flexibly adapted to various embodied environments and enhance 3D reasoning. The encoded visual tokens are interleaved with text tokens to form a unified multi-modal task sequence, which further serves as the input to a decoder-only LLM. Equipped with a vocabulary containing both text and action tokens, the LLM can generate responses to various tasks simultaneously. Consequently, all the tasks are formulated as sequence prediction, thereby accommodating a unified training objective.


Following prior experiences ([Liu et al. 2023b]), we adopt a two-stage learning scheme, i.e., 3D VL alignment and 3D VLA instruction tuning. We accordingly collect largescale comprehensive datasets LEO-align and LEO-instruct, which comprise diverse object-level and scene-level tasks. Notably, we meticulously design an LLM-assisted pipeline to generate high-quality 3D VL data, wherein we propose to prompt LLMs ([OpenAI 2022]) with scene graphs and Object-centric Chain-of-Thought (O-CoT) method. To further enhance quality control, we devise a series of refinement procedures via regular expression matching and scene graph retrieval. We demonstrate our approach largely enriches the data scale and diversity, meanwhile mitigating hallucination in LLM-generated data.


We quantitatively evaluate and ablate LEO on diverse 3D tasks, including 3D captioning ([Chen et al. 2021]), 3D question answering ([Azuma et al. 2022]), situated question answering ([Ma et al. 2023]), embodied navigation ([Ramrakhya et al. 2022]), and robotic manipulation ([Shridhar et al. 2021]). The results indicate (i) through task-agnostic instruction tuning with a unified model, LEO achieves state-of-the-art performances on most tasks, particularly surpassing previous task-specific models; (ii) LEO shows proficiency in scene-grounded dialogue and planning, capable of generating flexible and coherent responses; (iii) LEO achieves comparable performances to state-of-the-art task-specific models on navigation and manipulation tasks, and exhibits remarkable generalization ability; (iv) LEO&apos;s strong performances stem from both data and model aspects, including the alignment stage, data diversity, generalist-style instruction tuning, and object-centric representation; (v) LEO manifests the scaling law that echoes prior findings ([Kaplan et al. 2020; Reed et al. 2022; OpenAI 2023]).
We also present qualitative results to illustrate LEO&apos;s versatility and proficiency in grounded 3D scene understanding.


In summary, our main contributions are as follows: (i) we propose LEO, the first embodied generalist agent capable of following human instructions to perceive, ground, reason, plan, and act in the 3D world; (ii) we propose a simple yet effective framework that connects object-centric 3D representation and LLM to efficiently bridge the gap between vision, language, and embodied action; (iii) we collect large-scale comprehensive datasets for our two-stage generalist training scheme, and particularly propose an LLM-assisted pipeline for the generation of high-quality 3D VL data; (iv) we conduct extensive experiments to demonstrate LEO&apos;s proficiency across various tasks, and present in-depth analyses to reveal valuable insights; (v) we release the data, code, and model weights to endow the future research in embodied generalist agents.


Model


The leading design principles of LEO are two-fold: 1) It should handle the multi-modal input of egocentric 2D, global 3D, and textual instruction, and the output of textual response as well as embodied action commands in a unified architecture; 2) It should leverage pre-trained large language models (LLMs) as a powerful prior for the downstream tasks. We therefore convert all data of different modalities into a sequence of tokens, illustrated below:


You are *MATH*, tokens in SentencePiece. After tokenization, all tokens are ordered into the format in ([1]).


Token Embedding &amp; LLM


We apply several token embedding functions to process the tokens in the sequence before sending them to the LLM. The LLM will then align these tokens of different modalities, and produce the response. Most of the responses are text and can be decoded directly. For responses that include embodied actions, we will map the reserved SentencePiece text tokens back to action commands.


Text &amp; 2D token embedding. For text tokens (including embodied actions that have been mapped to the reserved text tokens), an embedding look-up table is used to map them into vectors. While the egocentric 2D image is encoded by a pretrained OpenCLIP ConvNext ([Liu et al. 2022]) for obtaining image token embeddings. We apply MLP adapters to match the dimensions of all token embeddings.


Object-centric 3D token embedding. Each 3D object token (i.e., the point cloud of a 3D object) is first encoded by a pretrained point cloud encoder (e.g., PointNet++ ([Qi et al. 2017])). We then adopt the Spatial Transformer introduced in ([Chen et al. 2022]) to further process the point cloud embedding of all objects into object-centric 3D token embeddings. In a nutshell, Spatial Transformer biases the standard attention score with relative position and size for capturing 3D relations between objects. Due to space limit, the readers are referred to ([Chen et al. 2022]) and instruction *MATH*. 


Pretrained LLM. We choose Vicuna-7B ([Chiang et al.], Appendix [D.2] for more details.


With this representation, we formulate the learning of LEO as GPT-style autoregressive language modeling ([Brown et al. 2020]) given the prefix (from system message to instruction), i.e. prefix language modeling ([Raffel et al. 2020]). Therefore, a pretrained LLM can be used to process such sequences. Next, we will detail the tokenization of multimodal data, model architecture, training loss, and inference settings. An overview of our model can be found in Fig. [1].


2.1. Tokenization


We follow prior practices in 2D VLM ([Liu et al. 2023b; Alayrac et al. 2022]) and 3D VLM ([Zhu et al. 2023c]) to tokenize the multi-modal data in LEO. We use SentencePiece tokenizer ([Kudo &amp; Richardson 2018]) to encode text to process the token sequence. In order to tackle the challenging alignment and grounding problem of multimodal tokens (2D, 3D, text, embodied action) while preserving the LLM pretrained knowledge, we employ LoRA ([Hu et al. 2022]) to introduce additional tunable parameters to the frozen pretrained LLM.


Training &amp; Inference


We formulate the learning objective of LEO following ([Brown et al. 2020; Raffel et al. 2020]) in a prefix language modeling fashion. For a batch B of token sequence s, we optimize LEO via: *MATH* with 32k subwords; 2D image tokens for egocentric 2D images; and object-centric 3D tokens extracted over Mask3D-based ([Schult et al. 2022]) object proposals for 3D point cloud inputs. For embodied action commands, continuous actions (e.g. in manipulation) are discretized (details in Appendix [D.3]) to join the discrete actions (e.g. navigation) and form a unified discrete action space.
We follow ([Brohan et al. 2023]) to map these discrete actions to the least used where sprefix denotes the prefix tokens (from system message to instruction) in ([1]). During training, we freeze the pretrained 3D point cloud encoder and the LLM and fine-tune the 2D image encoder, the Spatial Transformer, and the LoRA parameters. In total, LEO has 7B parameters and 142M of them will be tuned.
During inference, we use beam search to generate textual responses. For tasks that require action commands, we map the textual outputs to action commands as discussed in Sec. [2.1]. More details on the model and training can be found in Appendix [D].


TABLE 2


Datasets


Since LEO is a generalist agent that receives multi-modal inputs and follows instructions, we adopt the two-stage training proposed by ([Liu et al. 2023b]) and split the data into two sets: (i) LEO-align (Sec.
[3.1]) that focuses on 3D vision-language (VL) alignment to bridge the gap between 3D scene representation and natural language; and (ii) LEO-instruct (Sec. [3.2]) that targets at 3D VLA instruction tuning to endow LEO with various capabilities. The statistics and examples of these datasets can be found in Tab. [1] and Appendix [C], respectively. Due to the data scarcity, we adopt LLMs to facilitate the data generation process and outline the details in Sec. [3.3].


LEO-align: 3D Vision-Language Alignment


In LEO-align, we focus on 3D VL alignment. Similar to BLIP-2 ([Li et al. 2023d]), we train LEO to generate captions given various 3D inputs. Specifically, we collect three types of 3D captioning data: 1) object-level captions, where we align 3D individual objects with their descriptions ([Luo et al. 2023]); 2) object-in-the-scene captions, where the goal is to generate the referring expressions of objects in a 3D scene context ([Achlioptas et al. 2020; Zhu et al. 2023c]); and 3) scene-level captions, which focuses on depicting global 3D scene using natural language. Due to the space limit, we defer details including data source and components to Appendix [B.1].


LEO-instruct: Instruction Following in 3D world


In LEO-instruct, LEO will be tuned to follow instructions and accomplish various 3D VLA tasks. We curate a comprehensive set of tasks that covers a broad spectrum from grounded scene understanding and reasoning ([Chen et al. 2021; Ma et al. 2023]), to dialogue, planning, and embodied acting ([Savva et al. 2019; Shridhar et al. 2021]). Specifically, we introduce 1) 3D captioning and question answering -- given 3D scene input, the agent needs to generate a natural language response to describe the scene or answer questions; 2) 3D dialogue and task planning, where the agent is expected to generate flexible and coherent responses to complex instructions with respect to the given 3D scene, and 3) navigation and manipulation, which require the agent to accomplish a variety of embodied acting tasks in the 3D scene. We defer details to Appendix [B.2].


LLM-assisted 3D-language Data Generation


As mentioned above, at the core of producing a large proportion of LEO-align and LEO-instruct is the assistance of LLMs. We now detail the key techniques of prompting LLMs (i.e., ChatGPT) to generate 3D-text paired data. An overview can be found in Fig.
[2].


Scene-graph-based prompting. Our data generation pipeline starts with 3D scene graphs from 3DSSG ([Wu et al. 2021]), which provide scene contexts for prompting. Compared to counterparts that utilize object boxes ([Yin et al. 2023; Hong et al. 2023; Wang et al. 2023e]), it offers both rich object attributes and accurate spatial relation information among objects, allowing LLMs to generate data with high-quality 3D details (comparisons in Appendix [B.8]). Next, we manually design some examples as seed tasks ([Liu et al. 2023b]), including scene and object captioning, QA, dialogue, and planning, and ask LLM to produce more tasks as well as the responses. Details for designing the seed tasks can be found in Appendix [B.3].


Object-centric CoT. To further combat the hallucination of LLMs ([Bang et al. 2023]) in open-ended generation as in our pipeline, we propose the object-centric chain of thought (O-CoT) prompting that requires the LLM to explicitly provide the label and ID of object candidates as thoughts during text generation. We also utilize subgraph sampling to further enhance the diversity of 3D scene graphs (see details in Appendix [B.7]). We provide examples of O-CoT in Fig. [2].


FIGURE 2


Refinement procedures. Upon the scene graph and O-CoT prompting, we introduce refinement as an additional safeguard to the quality and reliability of our generated data. Specifically, we send raw LLM responses to several human-defined filters based on the 3D scene graphs: negative responses (e.g., lacking the necessary information to answer) will be removed; unnatural narratives will be rewritten, etc. Further, we detect text that involves logical reasoning (e.g., counting) or hallucination, and manually fix the wrong responses according to the ground truth provided by scene graphs. We provide illustrative examples in Fig. [2] and Appendix [B.6], and quantitative analysis on the impact of data refinement procedures in Appendix [I.1].


Assess the quality of generated data. In addition to data examples, we propose to assess the quality of generated data quantitatively. We focus on the LLM-produced question-answer pairs about objects (questions starting with How many/Is there and ending with in the room/bedroom/kitchen/living room/bathroom). We first divide these pairs into three categories: counting, existence, and non-existence, which examines the number of certain objects/whether an object exists/whether an object does not exist in the scene, respectively. We manually check if the answers in these pairs are correct, and report the overall accuracy. Results in Tab. [2] demonstrate that our proposed scene-graph-based prompting, O-CoT prompting and refinement bring consistent improvement to data quality and the complete data generation pipeline outperforms a recent counterpart (3DLLM). We also demonstrate how we help address the grammatical errors compared to counterparts in Appendix [B.9].
Finally, we provide the data distribution in Appendix [B.10] to illustrate the diversity of our generated data.


Capabilities and Analyses


We demonstrate LEO&apos;s capabilities by a comprehensive evaluation on the full spectrum of embodied 3D tasks encompassing perceiving, grounding, reasoning, planning, and acting. In Sec. [4.1], we present quantitative comparisons between LEO and state-of-the-art models on various 3D VL tasks, underscoring LEO&apos;s proficiency in 3D VL understanding and reasoning. In Sec. [4.2], we highlight LEO&apos;s strength in scene-grounded dialogue and task planning. In Sec. [4.3], we extend LEO to embodied acting tasks wherein LEO exhibits remarkable versatility. In Sec.
[4.4], we conduct ablative studies to reveal more insights into LEO, including data and model aspects. In Sec. [4.5], we probe the scaling effect and manifest the potential for further development.


TABLE 4


3D Vision-Language Understanding and Reasoning


Overview. Understanding and reasoning about object attributes, object relations, and other facets of 3D scenes from an agent&apos;s egocentric perspective is a fundamental capability of an embodied generalist agent in the 3D world. We investigate how well can LEO perform 3D VL understanding and embodied reasoning tasks, especially when being compared against task-specific models and existing generalist agents. Specifically, we consider three renowned 3D tasks: 3D captioning on Scan2Cap ([Chen et al. 2021]), 3D QA on ScanQA ([Azuma et al. 2022]), and 3D embodied reasoning on SQA3D ([Ma et al. 2023]). Our evaluation metrics include conventional scores (e.g., CIDEr, BLEU, METEOR, ROUGE) and other metrics adapted for open-ended generation, e.g., sentence similarity ([Reimers &amp; Gurevych 2019]) and refined exact-match accuracy (see details in Appendix [H.1]). Following 3D-VisTA ([Zhu et al. 2023c]), we use object proposals from Mask3D ([Schult et al. 2022]) instead of ground-truth object segments for evaluation.


Baselines. For quantitative comparisons, we include both task-specific approaches and generalist models: 1) state-of-the-art specialists in 3D dense captioning ([Chen et al. 2021; Cai et al. 2022; Chen et al. 2023]); 2) state-of-the-art specialists in 3D QA ([Azuma et al. 2022; Ma et al. 2023]); 3) task-specific fine-tuned generalist models like 3D-VisTA ([Zhu et al. 2023c]) and 3D-LLM ([Hong et al. 2023]). To the best of our knowledge, LEO is the first model that, in stark contrast to prior models, can directly handle the aforementioned 3D VL tasks in a unified architecture without task-specific fine-tuning. This lends greater credence to LEO&apos;s comparative superiority.


Results &amp; analysis. As shown in Tab. [4], LEO surpasses both state-of-the-art single-task and task-specific fine-tuned models significantly on 3D dense captioning and 3D QA tasks. In contrast to the specialist models that utilize task-specific heads, our LLM-based approach not only affords the flexibility of generating open-ended responses but also exhibits excellent quantitative results. On the other hand, considering the complicated feature aggregation in 3D-LLM, we believe that object-centric 3D representation is a simple yet effective option to connect 3D scenes with LLM while harnessing the inherent knowledge of LLM.


Scene-grounded Dialogue and Planning


Overview. Upon the 3D VL understanding and reasoning, we anticipate LEO to support more sophisticated interaction with humans, e.g., responding to complex multi-round user instructions in the 3D world. To verify these capabilities, we conduct qualitative studies on 3D dialogue and planning tasks, with unseen scenarios from the held-out test sets of LEO-instruct. We defer the quantitative results of dialogue and planning to our ablation study in Sec. [4.4]. Quantitative comparison with other approaches is infeasible given the absence of comparable benchmarks.


Results &amp; analysis. As shown in Fig. [A.1], LEO is capable of generating high-quality responses, which encompass two features: 1) Precisely grounded to the 3D scenes. The task plan proposed by LEO involves concrete objects related to the 3D scene, as well as plausible actions regarding these objects. 2) Rich informative spatial relations. The entities in LEO&apos;s responses often accompany detailed depictions. Such information helps identify specific objects in complex 3D scenes and affords considerable assistance to humans.


Embodied Action in 3D World


Overview. To probe LEO&apos;s capacity of bridging vision-language-acting in the 3D world, we select two canonical embodied AI tasks: embodied navigation (ObjNav) on AI Habitat ([Ramrakhya et al. 2022]) and robotic manipulation on CLIPort ([Shridhar et al. 2021]).
Specifically, for CLI-Port robotic manipulation, we evaluate LEO on the three tasks listed in Tab. [5] including their unseen counterparts, and report the success scores. For ObjNav, we evaluate LEO on the original MP3D ObjNav validation split. Additionally, we test generalization to the validation split of the newly introduced HM3D ObjNav task ([Ramakrishnan et al. 2021]). We report the success rate and SPL metrics following [Ramrakhya et al.] ([2022]). We consider both Habitat-web ([Ramrakhya et al. 2022]) (fully supervised) and ZSON ([Majumdar et al. 2022]) (zero-shot) as baselines.


TABLE 7 


Results &amp; analysis. We present the results of CLIPort manipulation and object navigation in Tabs. [5] and [6]. Our findings are as follows: 1) In robotic manipulation, LEO is comparable to state-of-the-art performances and even better on some challenging unseen tasks. In particular, LEO directly produces motor commands without inductive bias (e.g., heatmap) that benefit previous methods, showcasing LEO&apos;s considerable capacity for learning embodied actions.


2) In ObjNav, LEO achieves a success rate that is comparable to the baselines and has a better SPL on MP3D-val, suggesting that LEO can leverage the object-centric 3D scene input (potentially offering a coarse global map) and take a shorter path to the target.
Furthermore, results on HM3D-val confirm LEO&apos;s zero-shot generalization to novel scenes. Notably, all baselines are equipped with recurrent modules while LEO only incorporates truncated past actions, which could account for a lower success rate (see discussion in Appendix [H.2]). 3) Overall, the two-stage learning scheme endows LEO with semantic-level generalization (novel objects, etc.) in both manipulation and navigation tasks. We demonstrate the efficacy of tackling embodied acting tasks with a general framework from 3D VL.


Additional results. We further investigate the perception modules, data regime, and generalization to unseen objects in ObjNav task. See the results in Appendix [I.4].


More Insights into LEO


Overview. In this section, we aim to offer deeper insights into LEO&apos;s characteristics, mainly from the data perspective (model perspective is deferred to Appendix [G.2]). Specifically, we evaluate LEO when trained with different data configurations, including exact match, sentence similarity, and human rating. We regard LEO instruction-tuned without embodied acting tasks (w/o Act) as the default setting.


Following [Achlioptas et al.] ([2020]), we use ground-truth object segments in these analyses. We present additional analyses on data in Appendix [I.2] and model in Appendix [I.3].


Alignment stage. In contrast to complete two-stage training (w/o Act), we direct instruction-tune a model without alignment stage (w/o Align). The results in Tab. [7] show the consistent impact of alignment. In particular, the benefit of alignment is significant on Scan2Cap since it concerns detailed scene understanding and captioning, which is a primary focus of alignment training.


Specialist vs. generalist. We train a specialist on ScanNet scenes (ScanNet). As shown in Tab. [7], ScanNet performs slightly worse than w/o Act even on ScanNet tasks, and particularly struggles at generalization across scenes (3RQA) and tasks (3RDialog and 3RPlan). This demonstrates the advantage of generalist-style instruction tuning with broad coverage of scenes and tasks.


VL vs. VLA. We compare w/o Act and VLA, which differ in whether embodied acting tasks are included for training. The results in Tab. [7] show that incorporating embodied acting tasks could lead to performance drops on 3D VL tasks. This may stem from 1) the gap between language generation and embodied action prediction, and 2) the imbalanced data scale of embodied acting tasks.
In contrast to the finding that VL data benefits embodied acting tasks in VLA co-training ([Brohan et al. 2023]), our observation implies that embodied acting tasks may harm VL capabilities in turn. How to continually bridge the gap between VL and embodied acting tasks is an important direction for further exploration.


Dialogue and planning data. In contrast to the default model (w/ Dialg in Tab. [8]), we train LEO without dialogue and planning data (w/o Dialg). We design an evaluation set with three types of questions (Answerable, Unanswerable, and NLP) and evaluate with TrueSkill ([Graepel et al. 2007]) according to human preference (see details in Appendix [G.3]). The results in Tab.
[8] confirm more hallucinations (less preferred by users on &quot;Unanswerable&quot;) and worse NLP skills for w/o Dialg. This is probably because 1) the diverse conversations in our dialogue data can help cultivate flexible responses to complex instructions, and 2) our planning data can offer scene-grounded commonsense knowledge and also encourage detailed coherent text.


Data balancing. We find imbalanced data could induce hallucination in LEO, e.g., it tends to respond with &quot;Yes&quot; when asked &quot;Is there something in this room?&quot;. To address this, we augment the 3RScanQA data with more negative samples where non-existent objects are queried. We also design an evaluation set with different types (Yes and No) of object-existence questions (see details in Appendix [G.4]). Results in Tab. [9] demonstrate that we can effectively mitigate the hallucination problem by balancing the tuning data. Moreover, the benefit of augmenting 3RScan data can transfer to ScanNet scenes in a zero-shot manner.


Scaling Law Analysis


Settings. We study the scaling effect ([Kaplan et al. 2020; Reed et al. 2022]) of data and model in LEO by tracking the instruction-tuning loss on the test set with the growth of data scale.
In addition to the default Vicuna-7B, we incorporate two LLMs at different scales: OPT-1.3B ([Zhang et al. 2022]) and Vicuna-13B ([Chiang et al. 2023]). For Vicuna-7B, we also probe the influence of alignment (Scratch vs. Aligned).


Results &amp; analysis. From the test loss curves in Fig.
[3], we have the following findings: 1) The instruction tuning of LEO conforms to the scaling law ([Kaplan et al. 2020; Reed et al. 2022]). We observe that all curves decrease log-linearly with the data scale. 2) Scaling up LLM leads to consistent improvements. Aligned Vicuna-7B shows significantly lower losses than Aligned OPT-1.3B. In contrast, despite the consistent improvements, the gap between Aligned Vicuna-7B and Vicuna-13B appears less significant, suggesting potential saturation if we continue to scale up the LLM. This indicates the scalability of LEO and the necessity of scaling up data to match the model capacity. 3) Alignment leads to consistent improvements. Aligned Vicuna-7B shows consistently lower losses than Scratch Vicuna-7B, which corresponds to the inferior performances of w/o Align in Tab. [7].


Related Work


Generalist agents. The AI community has witnessed the rising generalist models in both vision ([Lu et al. 2023; Wang et al. 2023b; Kirillov et al. 2023]) and language ([OpenAI 2022; 2023]) domains. A generalist agent requires additional embodiment knowledge to interact with the environment and complete embodied acting tasks.
Existing efforts towards generalist agents include: grounded reasoning and task planning in the real world ([Ahn et al. 2022; Huang et al. 2022b]), skill generalization in open-world environment ([Fan et al. 2022; Cai et al. 2023a; Wang et al. 2023d; Cai et al. 2023b; Gong et al. 2023b]), general robotic manipulation ([Brohan et al. 2022; Jiang et al. 2023; Gong et al. 2023a]), and unified vision-language-action (VLA) models such as Gato ([Reed et al. 2022]), PaLM-E ([Driess et al. 2023]), EmbodiedGPT ([Mu et al. 2023]), and RT-2 ([Brohan et al. 2023]). LEO belongs to the VLA model, however, its goal is to build a generalist agent that can understand the real 3D world beyond 2D images, which is absent in existing works.


Multi-modal instruction tuning. Pre-trained LLMs demonstrated practical for solving vision-language tasks ([Tsimpoukelli et al. 2021; Alayrac et al. 2022; Guo et al. 2023; Li et al. 2023d; Zhao et al. 2023]). Meanwhile, the instruction-tuning paradigm exhibited strong zero-shot generalization in NLP tasks ([Wei et al. 2022; Sanh et al. 2022; Ouyang et al. 2022; Chung et al. 2022]). The two streams merged into instruction-tuned LVLMs ([Liu et al. 2023b; Zhu et al. 2023b; Ye et al. 2023; Gao et al. 2023; Li et al. 2023b; Gong et al. 2023c; Dai et al. 2023]). Despite the burst, these models are confined to 2D visual modalities, e.g., image or video. Concurrent works ([Yin et al. 2023; Hong et al. 2023; Wang et al. 2023e; Xu et al. 2023]) extend to 3D vision tasks, but these models either lack the acting capability or unified efficient architecture.


Grounded 3D scene understanding. One key obstacle to building LEO is grounding the 3D world with natural languages. There exist diverse methods of grounded scene understanding, e.g., spatial relation modeling ([Zhao et al. 2021; Chen et al. 2022; Zhu et al. 2023c]) and fine-grained open-scene understanding ([Peng et al. 2023b; Kerr et al. 2023]). However, due to data scarcity, how to utilize LLMs to ground the 3D scene is rarely explored. Recently, 3D-LLM ([Hong et al. 2023]) leverages multi-view images and Chat-3D ([Wang et al. 2023e]) uses object-centric point clouds to enable the LLMs with 3D grounding. In this work, we devise both 2D and 3D encoders for grounding various visual representations and employ LoRA ([Hu et al. 2022]) to efficiently fine-tune the LLMs.


3D data prompting from LLMs. LLMs exhibit extraordinary capabilities of text generation and serve as a source for collecting diverse instruction-following data ([Wang et al. 2023c; Taori et al. 2023; Peng et al. 2023a]). However, the lack of access to visual modalities makes it troublesome to collect visual instruction-tuning data. To address this issue, existing methods provide bounding boxes ([Liu et al. 2023b]) and add dense captions ([Li et al. 2023a; Liu et al. 2023a]) as image descriptions or directly use off-the-shelf large vision-language models (LVLM) ([Zhu et al. 2023a; Luo et al. 2023]) to help collect such data.
Unlike concurrent attempts ([Yin et al. 2023; Hong et al. 2023; Wang et al. 2023e]) in collecting 3D instruction-tuning data, our approach features a scene-graph-based prompting and refinement method to prompt and correct the data.


Conclusions


The proposed agent LEO extends the current generalist ability of LLMs from text towards the 3D world and embodied tasks. It is a crucial initial step towards building embodied generalist agents. Nonetheless, there are also limitations, e.g., generalization to novel scenes, and a notable gap between VL learning and embodied action control. In light of this work, we identify several promising directions that hold the potential for substantial advancement: (1) enhancing the 3D VL understanding capability by leveraging larger-scale VL data from richer 3D domains; (2) continually bridging the gap between 3D VL and embodied action, as our experiments reveal the efficacy of their joint learning; (3) investigating the issues of safety and alignment in the context of embodied generalist agents, particularly given that our scaling law analysis suggests significant enhancements through scaling on data and model.