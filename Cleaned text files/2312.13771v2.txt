AppAgent: Multimodal Agents as Smartphone Users


Introduction


The emergence of large language models (LLMs), such as ChatGPT ([OpenAI 2021]) and GPT-4 ([OpenAI 2023]), marks a significant milestone in the field of artificial intelligence and natural language processing. These advanced models represent a fundamental change in how machines understand and generate human language, exhibiting a level of sophistication and versatility previously unattainable. One of the most exciting developments in this field is the capability of LLMs to function not just as language processors, but as agents capable of performing complex tasks. This evolution is evident in initiatives such as AutoGPT ([Yang et al. 2023a]) and MetaGPT ([Hong et al. 2023]), which showcase the practical applications of LLMs in tasks requiring advanced cognitive functions like reasoning, planning, and collaboration. The significance of these developments cannot be overstated, as they extend the utility of LLMs beyond simple language tasks, revolutionizing various aspects of technology and daily life.


However, a key limitation of these LLM-based agents has been their reliance solely on text-based information. This restriction has historically curtailed their perception and interaction with their environment. The introduction of models equipped with vision capabilities, such as the latest iteration of GPT-4, marks a pivotal breakthrough. By integrating the ability to process and interpret visual information, these models can now understand aspects of their surroundings that are difficult or impossible to convey through text alone. This extended capability enables LLMs to interpret context, recognize patterns, and respond to visual cues, thus providing a more holistic and interactive experience with the world.


In our work, we focus on building a multimodal agent leveraging the vision capabilities of multimodal large language models to undertake tasks previously unachievable by text-only agents. In particular, we explore an interesting but challenging application that builds an agent to operate any smartphone application (App) in the mobile operating system. Our approach differs significantly from existing intelligent phone assistants like Siri, which operate through system back-end access and function calls. Instead, our agent interacts with smartphone apps in a human-like manner, using low-level operations such as tapping and swiping on the graphical user interface (GUI). The proposed agent offers multiple advantages. Firstly, it eliminates the need for system back-end access, making our agent universally applicable across various applications. Additionally, this approach enhances security and privacy, as the agent does not require deep system integration.
Furthermore, by operating on the GUI level, our agent can adapt to changes in app interfaces and updates, ensuring long-term applicability and flexibility.


However, creating a multimodal agent capable of operating diverse smartphone apps presents significant challenges. Existing research indicates that adapting current models for embodied tasks necessitates extensive training data, and collecting a large dataset of app demonstrations for training is a formidable task. Moreover, different apps have unique GUIs with varying icon meanings and operational logic, and it remains uncertain whether these adapted models can effectively generalize to unseen apps.


In this paper, we introduce a multimodal agent framework aimed at operating any smartphone app like human users. The learning of our framework involves an exploration phase where the agent interacts autonomously with apps through a set of pre-defined actions and learns from their outcomes. These interactions are documented, which assists the agent in navigating and operating the apps. This learning process can be accelerated by observing a few human demonstrations. Following this exploratory phase, the agent can operate the app by consulting the constructed document based on its current state, eliminating the need to adapt the parameters of the LLMs or collect extensive training data for each app.


To validate its effectiveness, we tested our agent on 50 tasks across 10 different apps, ranging from social media and messaging to email, maps, shopping, and even complex image editing apps. Both quantitative results and user studies underscore the advantages of our design, particularly its adaptability, user-friendliness, and efficient learning and operating capabilities across a wide range of applications. This underlines the potential of our agent as a versatile and effective tool in the realm of smartphone app operation.


In summary, this paper makes the following contributions:


- We open-source a multimodal agent framework, focusing on operating smartphone applications with our developed action space.
- We propose an innovative exploration strategy, which enables the agent to learn to use novel apps.
- Through extensive experiments across multiple apps, we validate the advantages of our framework, demonstrating its potential in the realm of AI-assisted smartphone app operation.


Related Work


Large language models


The development of ChatGPT ([OpenAI 2021]) and GPT-4 ([OpenAI 2023]) represents a crucial advancement in natural language processing. Unlike earlier large language models (LLMs), these new models ([Touvron et al. 2023ab]; [Zeng et al. 2022]; [Taori et al. 2023]; [Zheng et al. 2023]) enable multi-round conversations and have the impressive ability to follow complex instructions. The integration of vision capabilities in GPT-4V ([Yang et al. 2023b]) is a further milestone, enabling the language model to process and interpret visual data. This addition has broadened the scope of potential AI applications, allowing GPT-4 to undertake diverse tasks such as problem-solving, logical reasoning, tool usage, API calls, and coding. Recent studies ([Yang et al. 2023c]; [Yan et al. 2023]) have shown that GPT-4V can understand various types of images, including simple user interfaces (UIs) in popular smartphone apps. However, challenges arise when the apps are new and their UIs are less typical, which highlights a major problem that our work aims to address. Among open-source efforts from the industry and research community, the LLaMA series ([Touvron et al. 2023ab]) are the most popular equivalents and have been fine-tuned to acquire conversational abilities, employing a decoderonly architecture similar to ChatGPT ([Taori et al. 2023]; [Zheng et al. 2023]). Building upon LLaMA, many multimodal LLMs, such as LLaVA ([Liu et al. 2023ba]), ChartLlama ([Han et al. 2023]), and StableLLaVA ([Li et al. 2023]), also demonstrate vision understanding capabilities akin to those of GPT-4V. Nevertheless, a performance gap persists between these open-source models and GPT-4V, suggesting potential areas for further development.


LLMs as agents


The use of LLMs as agents for executing complex tasks has gained increasing attention. Initiatives like AutoGPT ([Yang et al. 2023a]), HuggingGPT ([Shen et al. 2023]), and MetaGPT ([Hong et al. 2023]) illustrate this trend, and many projects demonstrate impressive capabilities, moving beyond basic language tasks to engaging in activities requiring higher cognitive functions, such as software development ([Qian et al. 2023]; [Chen et al. 2021]) and gaming ([FAIR et al. 2022]; [Park et al. 2023]; [Xu et al. 2023]). In this context, Yao et al. ([Yao et al. 2023]) introduce an innovative approach that synergizes reasoning and acting in LLMs, significantly enhancing their decisionmaking and interactive capabilities. LLM-based agents are designed to utilize the advanced language and reasoning skills of LLMs to interact with and manipulate their environment ([Liu et al. 2023c]; [Gur et al. 2023]; [Xie et al. 2023]). This includes performing tasks that require understanding context, making decisions, and learning from interactions ([Xi et al. 2023]; [Hu and Shu 2023]). Such agents are pivotal in applications where human-like cognitive abilities are essential.


The emergence of multimodal LLM agents ([Wang et al. 2023]; [Furuta et al. 2023]; [Brohan et al. 2022 2023]; [Reed et al. 2022]), capable of processing various inputs including text, images, audio, and video, has further broadened the scope of LLM applications. This versatility is particularly beneficial for LLM-based agents, enabling them to interact more effectively with their environment and complete more complex tasks, be it completing household tasks in a physical world ([Ahn et al. 2022]), generating 3D assets via procedural tool use ([Sun et al. 2023]), or mastering over 600 tasks across different domains at the same time ([Reed et al. 2022]). Our research contributes to this area by focusing on an agent designed to operate smartphone applications. This agent&apos;s ability to interpret screenshots from the operating system demonstrates its flexibility and adaptability, making it a valuable tool in a wide range of applications.


FIGURE 


Method


This section details the methodology behind our innovative multimodal agent framework. This framework enables an agent to interact with smartphone applications in a manner akin to human behavior. We first describe the experimental environment and action space, which are foundational elements of our system. Next, we discuss the exploration phase, where the agent learns app functionalities either through autonomous interactions or by observing human demonstrations. Finally, we outline the deployment phase, explaining how the agent applies its acquired knowledge to execute high-level tasks.


Environment and Action Space


Experimental Environment: Our experimental environment is built on a command-line interface (CLI), allowing the agent to interact with smartphone apps. We chose the Android operating system for our experiments. The agent receives two key inputs: a real-time screenshot showing the app&apos;s interface and an XML file detailing the interactive elements. To enhance the agent&apos;s ability to identify and interact with these elements seamlessly, we assign each element a unique identifier. These identifiers are derived either from the resource ID in the XML file (if provided) or are constructed by combining the class name, size, and content of the element. These elements are overlaid as semi-transparent numbers on the screenshot.


This helps the agent to interact accurately without needing to specify exact positions on the screen and enhances the agent&apos;s precision in controlling the phone.


Action Space: Our agent&apos;s action space mirrors common human interactions with smartphones: taps and swipes. We designed four basic functions:


- Tap(element: int): This function simulates a tap on the UI element numbered on the screen. For example, tap(5) would tap the element labeled &apos;5&apos;.
- Long_press(element: int): This function emulates a long press (for 1 second) on a UI element.
- Swipe (element: int, direction: str, dist: str): It allows the agent to swipe on an element in a specified direction (up, down, left, right) and distance (short, medium, long). For instance, swipe(21, &quot;up&quot;, &quot;medium&quot;) would swipe up on element &apos;21&apos; for a medium distance.
- Text(text: str): To bypass inefficient virtual keyboard typing, this function inputs text directly into an input field when a virtual keyboard is visible. For example, text(&quot;Hello, world!&quot;) inputs the string &quot;Hello, world!\&quot;.
- Back(): A system-level function that helps the agent return to the previous UI page, especially useful for exiting irrelevant pages.
- Exit(): A specialized function is employed to conclude processes, typically invoked upon successful task completion.


These predefined actions are designed to simplify the agent&apos;s interactions, particularly by eliminating the need for precise screen coordinates, which can pose challenges for language models in accurately predicting.


Exploration Phase


Exploring by autonomous interactions. The Exploration Phase is central to our framework. Here, the agent learns about the functionalities and features of smartphone apps through trial and error. In this phase, the agent is assigned a task and starts interacting autonomously with the UI elements. It uses different actions and observes the resulting changes in the app interface to understand how it works. The agent, driven by a large language model, attempts to figure out the functions of UI elements and the effects of specific actions by analyzing screenshots before and after each action. This information is compiled into a document that records the effects of actions applied to different elements. When a UI element is acted upon multiple times, the agent will update the document based on past documents and current observations to improve quality. To make exploration more efficient, the agent stops further exploring UI elements if the current UI page seems unrelated to the main tasks of the app, like advertisement pages. In such cases, it uses the Android system&apos;s Back() function to return to the previous UI page. Compared with random exploration, such as Depth-First Search and Breadth-First Search, this goal-oriented exploration approach ensures that the agent focuses on elements crucial for the effective operation of the app. The agent also utilizes the LLM&apos;s existing knowledge about user interfaces to improve exploration efficiency. The exploration stops when the agent completes the assigned task.


Exploring by watching demos. An alternative and often more effective exploration method involves the agent observing human demonstrations. These demonstrations provide the agent with examples of efficient app usage, especially for understanding complex functionalities that might be challenging to discover through autonomous interactions. In this method, a human user operates the apps while the agent observes, recording only the elements and actions employed by the human.


This strategy narrows down the exploration space and prevents the agent from engaging with irrelevant app pages, making it a more streamlined and efficient approach compared to autonomous interactions.


Deployment Phase


Following the exploration phase, the agent is well-equipped to execute complex tasks based on its accrued experience. The agent adheres to a step-by-step approach when given a task, with each step encompassing access to a screenshot of the current UI and a dynamically generated document detailing the functions of UI elements and the actions&apos; effects on the current UI page. The prompts also provide detailed explanations of all available actions. In each step, the agent is first tasked with providing its observations of the current UI, followed by articulating its thought process concerning the task and current observations. Subsequently, the agent proceeds to execute actions by invoking available functions. After each action, the agent summarizes the interaction history and the actions taken during the current step. This information is incorporated into the next prompt, which provides the agent with a form of memory. This meticulous approach enhances the reliability and interpretability of the agent&apos;s actions, thereby facilitating more informed decision-making. The deployment phase stops when the agent determines that the task has been accomplished, at which point it can exit the process by taking the Exit() action.


Experiments


In this section, we will present our evaluation of the multimodal agent framework through a combination of quantitative and qualitative experiments. Our primary goal is to assess the agent&apos;s performance and its ability to operate a diverse set of smartphone applications effectively.


Experimental Setup


To comprehensively evaluate our method, we construct a benchmark that includes 10 popular applications, each serving various purposes. These applications include Google Maps, Twitter, Telegram, YouTube, Spotify, Yelp, Gmail, TEMU, Clock, and Lightroom. We have intentionally chosen this diverse set of apps to test the agent&apos;s adaptability across various functions and interfaces. In particular, to gain a more comprehensive insight into the vision capabilities of our agent, we conducted an in-depth case study using Adobe Lightroom, an image-editing application. This specific case study allowed us to evaluate the agent&apos;s proficiency in handling visual tasks and its ability to interpret and manipulate images within the app. For the exploration phase, we capped the maximum number of steps at 40. During testing, we limited the maximum number of steps to 10. For these experiments, we utilized the state-of-the-art multimodal large language model, GPT-4. GPT-4 is equipped to process interleaved image-and-text inputs effectively. This unique capability enables our agent to interpret and interact with both visual and textual information seamlessly within the applications.


FIGURE 3


Design and Analysis


Baselines. To comprehensively evaluate our multimodal agent framework, we considered various design choices and their impact on performance. We conducted experiments using different configurations to provide valuable insights into the agent&apos;s behavior. We started with GPT-4 without any reference documents during testing and examined its performance both with the raw action API and our simplified action space. Next, we explored different ways to generate guiding documents for the agent. These included documents generated through autonomous exploration, watching human demonstrations, and the manually crafted document as an oracle benchmark.


To effectively compare the performance of different methods, we employed three key metrics: Successful Rate (SR): This metric measures the average rate at which the agent successfully completes tasks within an app. If the agent fails to finish the task in 10 steps, it is considered a failure. Reward: To provide a more fine-grained measurement, we developed a reward model to assess performance. For each task within an app, we scored different UI pages. The closer the UI page was to the objective, the higher the score received. This means that even if the agent failed to complete the task, it would still receive credit based on its final state.


TABLE 1


TABLE 2


Average Steps: We also reported the average number of steps required to successfully finish tasks across the selected applications.


Results. The comparison of our experimental results is presented in Table [1]. We report the average performance of 45 tasks on 9 of the 10 previously described apps. Notably, we excluded Lightroom from this evaluation, as assessing task completion in this application presented inherent ambiguities. As demonstrated, our simplified action space significantly improves the performance of the GPT-4 baseline. Our observations indicate that LLM struggles with producing accurate xy coordinates, while our simplified action space eliminates this challenging requirement. Additionally, documents generated through autonomous exploration and observing human demonstrations proved to be highly effective. Their results consistently outperformed the GPT-4 baseline and are comparable to the results of human-written documents, which highlights the efficacy of our design in enhancing the agent&apos;s performance across a diverse set of applications.


Qualitative results. In Fig. [3], we provide examples showcasing the agent&apos;s execution process for various tasks.
This qualitative analysis serves to demonstrate the agent&apos;s capacity to accurately perceive, reason, and act in response to given tasks.
For a more comprehensive understanding of our agent&apos;s capabilities, please refer to our project page, which includes additional demonstration videos.


Case Study


To gain deeper insights into the vision capabilities of our agent, we conducted an extensive case study using Adobe Lightroom, an image-editing application. This specific case study allowed us to evaluate the agent&apos;s proficiency in handling visual tasks, which was previously impossible for text-only agent models. Lightroom, as an imageediting app with various editing tools, demands a wide range of operations, such as selecting appropriate tools and manipulating image parameters. This case study provides a robust evaluation of the agent&apos;s overall capabilities. Additionally, the open-ended nature of image editing tasks allows us to assess the agent&apos;s problem-solving abilities. We prepared five images with visual issues, such as low contrast and overexposure. Various variants of our model, as previously illustrated, were used to edit these images. A user study was conducted to rank the editing results produced by different methods. We also reported the average number of tools used for image editing, providing an additional reference to the editing process&apos;s complexity. All models were assigned the task of &quot;fix this image until it looks good to you&quot; without specifying the image&apos;s problems. The comparison of the results is presented in Table [2]. As we can see, our agent model with documents yields consistently better results than the GPT-4 baseline, which emphasizes the influence of documents in our design. The generated documents by watching the demonstration produced comparable results with the results of manually crafted documents, which suggests the effectiveness of the exploration phase. We also find that with a document, the agent tends to use various tools to improve the image quality, while the GPT-4 baseline uses fewer tools.


Conclusion


In this paper, we have introduced a novel multimodal agent framework that leverages the vision capabilities of large language models to operate smartphone applications in a human-like manner. Our approach eliminates the need for system backend access and offers security, adaptability, and flexibility advantages. Our exploration-based learning strategy allows the agent to quickly adapt to new applications with unfamiliar user interfaces, making it a versatile tool for various tasks. Our extensive experiments across various apps highlight our agent&apos;s ability to handle diverse high-level tasks and underscore its adaptability and learning efficiency.