Mimicking Evolution with Reinforcement Learning


Introduction


Evolution is the only process we know of today that has given rise to
general intelligence (as demonstrated in animals, and specifically in
humans). This fact has been inspiring artificial intelligence (AI)
researchers to mimic biological evolution in computers for a long time.
With better tools and more efficient computational methods to mimic
evolution, we can increase the pace of research discoveries. In fact,
having these tools would not only have a major impact in AI research but
it would also speed up research across a multitude of other fields. The
study of different types of replicators (the entities which are subject
to evolutionary pressure, such as genes, ideas, business plans and
behaviours) and their specific mechanisms for replicating makes up the
core foundation for different research fields (such as biological
evolution, mimetics [*REF*], economics [*REF*; *REF*; *REF*]
and sociology [*REF*; *REF*] correspondingly). The way
a population of replicators evolves in all of these fields is based on
the following universal truth: replicators that are better at surviving
and self-replicating increase in numbers faster than their less capable
peers. When these replicators are competing for a common
limited-resource, the less capable ones eventually disappear leaving the
more capable peers to dictate the world&apos;s future. In this work we
propose a reward function that allows reinforcement learning (RL)
algorithms to maximise the evolutionary fitness of the agents being
trained. This reward function is suitable for any open-ended
evolutionary environment, that we define as: a never-ending environment
where adaptable replicators are competing for a common limited-resource
to survive and replicate. In these environments, as in Nature, the
fitness function (or any goal function) is not defined anywhere but
simply emerges from the survival and reproduction of replicators.


In the remaining part of this introduction we 1) motivate a promising
research methodology for progress in artificial general intelligence
(AGI) based on convergent evolution; 2) describe how evolution changes
what we learn; 3) introduce our contribution and describe how maximising
a reward function can lead to the maximisation of evolutionary fitness.


A promising methodology for progress in AGI based on Convergent Evolution 


Convergent Evolution occurs when different species independently evolve
similar solutions to solve similar problems. For example, the
independent evolution of eyes has occurred at least fifty times across
different species [*REF*; *REF*] (most famously, the
anatomy of an octopus&apos;s eye is incredibly similar to the human eye
despite our common ancestor having lived more than 750 million years ago
and had practically no ability to see much beyond detecting the absence
or presence of light). Additionally, there is now compelling evidence
that complex cognitive abilities such as love, friendship and grief have
been independently evolved in social species such as elephants,
dolphins, whales and humans to solve the problems that occur when
individuals interact frequently with members of the same
species [*REF*]. We argue that to evolve AI and have it converge to
similar cognitive abilities as the ones found in nature, we need to
subject artificial agents to the same problems life finds in nature. In
nature, the degree of adaptability of living beings together with the
level of freedom provided by their natural environment enables the
growth of a wide variety of complex beings. A modified DNA can alter
what an animal can sense and do by changing the animal&apos;s body (its
sensors and actuators). It can alter how an animal learns by modifying
its learning system and its brain architecture. Lastly, it can also
alter what knowledge the animal is born with by changing its instincts
(its innate behaviours). Achieving the level of freedom of the natural
environment and the level of adaptability that the DNA provides in a
simulation is not feasible. We need to reduce the adaptability degree of
our agents and the degree of freedom of their environment but still be
able to evolve complex cognitive abilities similar to the ones found in
nature. With that end, we propose the following methodology:
1. Design simplified bio-inspired agent(s) to live in a simplified
bio-inspired environment.
2. Evolve these agents.
3. Compare the artificial behaviours obtained with the natural
behaviours observed in the natural environment that served as
inspiration.
4. If there is a mismatch, formulate a hypothesis that might explain
it. Did the mismatch occur due to an oversimplification of the
agent&apos;s body (sensors and actuators)? Was it due to its brain
architecture or its reward system? Repeat from step 1 to test the
new hypothesis.


The general idea of using first principles to build generative models,
collect data from simulations and compare it to data from the real
world, was proposed by Axelrod in 1997 [*REF*] and
baptized as the third way of doing science as in differing from the
deductive and inductive ways of doing science.


Past work in artificial life [*REF*], evolutionary
computation [*REF*] and co-evolution [*REF*] contribute towards different
steps of this methodology. Our contribution is towards the 2^nd^ step,
as our proposed reward function allows RL algorithms to maximise an
agent&apos;s evolutionary fitness.


Evolving what to learn 


In nature, there are two different mechanisms driving the development of
the brain (figure [1]). Evolution acts slowly, across generations,
and amongst other things, it defines what agents learn by changing their
internal reward function. Learning acts fast, across one&apos;s lifetime. It
quickly updates agents&apos; policy to maximise pleasure and minimise pain.
Combining evolution and learning has long history in AI research. The
evolutionary reinforcement learning algorithm, introduced in
1991 [*REF*], makes the evolutionary process determine
the initial weights of two neural networks: an action and an evaluation
network. During an agent&apos;s lifetime, learning adapts the action network
guided by the output of its innate and fixed (during its lifetime)
evaluation network. NEAT+Q [*REF*] uses an
evolutionary algorithm, NEAT [*REF*], to evolve
topologies of NN and their initial weights so that they can better learn
using RL. In NEAT-Q the reward function remains fixed. However,
evolutionary algorithms have also been used to evolve potential-based
shaping rewards and meta-parameters for RL [*REF*].


We say that a reward function is aligned with evolution when the
maximisation of the reward leads to the maximisation of the agent&apos;s
fitness. Evolution slowly aligns the reward function. However, when
agents are evolved together, the behavior of some agents effect the
fitness of others. As such, the fitness landscape of an individual is
changing over time, as the populations evolve. Therefore, the typical
internal rewards function that individuals optimize, also keep evolving
over time: there is no clear fixed point to which evolution of the
reward function would converge. More formally, we say that the
environment from the perspective of agent *MATH* is defined by the state
transition distribution; *MATH*. Where
*MATH* is the concatenation of the policies of all agents
except agent *MATH*; *MATH*. Policy adaptation occurs by: *MATH*,
where *MATH* is the sampled history of the adaptations of policy
*MATH* which resulted from agent *MATH* learning *MATH* to
maximise its reward function *MATH* by interacting *MATH* with
the environment. If agent *MATH* is the only agent learning then
*MATH* is static and so is the environment *MATH*. In this case,
the optimally aligned reward function is given by:
*MATH* Where *MATH* is the fitness function. The notion of an optimal
reward function for a given fitness function was introduced by
Singh [*REF*; *REF*], here we adapted his
original formulation. In the general case, all agents are learning, and
therefore, the environment is non-static, the fitness for *MATH* is
changing and so is the optimally aligned reward *MATH*.
However, in this paper, we show that in simulation it is possible to
define a fixed reward function which is always aligned, although not
guaranteed to be optimally aligned, with the essence of fitness: the
ability of the individual to survive and reproduce its replicators
(genes).


The implementation of the described framework (figure [1]) results
in a very computational expensive algorithm as it requires two loops 1)
learning (the inner loop) where agents maximise their innate reward
functions across their lifetimes and 2) evolution (the outer loop) where
natural selection and mutation defines the reward functions for the next
generation (amongst other things, such as NN topologies and initial
weights). Our work allows learning to single-handedly drive the search
for policies with increasingly evolutionary fitness by ensuring the
alignment of the reward function with the fitness function. We can do
this because our reward is extrinsic to the agent, it&apos;s given by God,
and therefore, only possible within a simulation.


FIGURE


Learning to maximise evolutionary fitness 


The distinction between an agent and a replicator is key to
understanding this paper. Formally, evolution is a change in replicator
frequencies in a population (of agents) over time. The replicator is the
unit of evolution, and an agent carries one or more replicators. For
example, in biology, evolutionary pressure occurs at the gene level, not
at the level of the individual. In this case, the replicator is the
gene, and the agent is the animal, plant, bacteria, and so on. Richard
Dawkins has famously described our bodies as throwaway survival machines
built for replicating immortal genes [*REF*]. His description
illustrates well the gene-centered view of
evolution [*REF*; *REF*], a view that has been able
to explain multiple phenomena such as intragenomic conflict and altruism
that are difficult to explain with organism-centered or group-centered
viewpoints [*REF*; *REF*; *REF*].
The world would be a very different place if evolution was
organism-centered, it is therefore crucial, to evolve biological-like
cognition, that we exert evolutionary pressure on the replicators and
not on the individuals.


Evolution acts on the replicator level, but RL acts on the agent level.
RL can be aligned with the evolutionary process by noting what evolution
does to the agents through its selection of replicators: evolution
generates agents with increasing capabilities to maximise the survival
and reproduction success of the replicators they carry.


From the replicator&apos;s perspective, the evolutionary process is a
constant competition for resources. However, from the agent&apos;s
perspective, the evolutionary process is a mix between a cooperative
exercise with agents that carry some of its replicators (its family) and
a competition with unrelated agents. Evolution pressures agents to
engage in various degrees of collaboration depending on the degree of
kinship between them and the agents they interact with (i.e. depending
on the amount of overlap between the replicators they carry). This
pressure for cooperation amongst relatives was named kin selection [*REF*].


Cooperation in multi-agent reinforcement learning (MARL) is usually
approached in a binary way [*REF*; *REF*; *REF*]. Agents
are grouped into teams and agents within the same team fully cooperate
amongst each other whilst agents from different teams don&apos;t cooperate at
all (cooperation is either one or zero); we define this scenario as the
binary cooperative setting. In this paper, we extend the concept of team
to the concept of family to move from binary cooperation into continuous
degrees of cooperation. We then use this family concept to propose a new
RL reward function that is aligned with the fitness landscape of an
open-ended evolutionary environment.


Related work


Arms-race with non-gradient methods


To achieve increasingly complex behaviours, agents have to face an
increasingly complex task. This happens when they compete against
adaptable entities. Their task gets harder every time their competitors
learn something useful - an arms race is created and it drives the
continued emergence of ever new innovative and sophisticated
capabilities necessary to out-compete adversaries. Evolutionary
Algorithms (EA) have been successfully used to co-evolve multiple
competing entities (some classical examples are: Sims,
1994 [*REF*] and Reynolds, 1994 [*REF*]).
However, in sequential decision problems EA algorithms discard most of
the information by not looking at the whole state-action trajectories
the agents go through their lifetime. This theoretical disadvantage
limits their potential efficiency to tackle sequential problems when
compared with RL. Empirically, EA algorithms usually have a higher
variance when compared with gradient methods [*REF*; *REF*; *REF*].


Arms-race with gradient methods


With regards to gradient methods (deep learning methods in specific),
impressive results have been recently achieved by training NN, through
back-propagation, to compete against each other in simulated games
(OpenFive [*REF*], AlphaZero [*REF*],
GAN [*REF*]). More closely aligned with our proposed
methodology, OpenAI has recently developed Neural
MMO [*REF*], a simulated environment that captures some
important properties of Life on Earth. In Neural MMO artificial agents,
represented by NN, need to forage for food and water to survive in a
never-ending simulation. Currently, they have presented results for
agents which use RL to maximise their survival, instead of maximising
the survival and reproduction success of a replicator as it happens in
nature. We build on top of this work and add three important
contributions: we introduce replicators (genes), give our agents the
ability to reproduce, and design an RL algorithm that increases agents
capability to maximise their genes survival and replication success.
These are key properties of life on Earth that we must have in
simulation environments if we hope to have them evolve similar solutions
to the ones evolved by nature (in other words, these are key properties
to achieve convergent evolution).


Cooperative MARL


Cooperative MARL is an active research area within RL that has been
experiencing fast progress [*REF*; *REF*; *REF*].
The setting is usually described as a binary cooperation exercise where
agents are grouped into teams and all team members receive the same
reward. The teams may have a fixed number of members or change
dynamically [*REF*; *REF*; *REF*; *REF*].
The most straightforward solution would be to train independent learners
to maximise their team&apos;s reward. However, independent learners would
face a non-stationary learning problem, since from their perspective the
learning process of other agents is changing the dynamics of the
environment. The MADDPG [*REF*] algorithm tackles this problem
by using a multi-agent policy gradient method with a centralised critic
and decentralised actors so that training takes into account all the
states and actions of the entire team but during execution each agent
can act independently. More relevant to our work, factored value
functions[*REF*] such as Value Decomposition Networks
(VDN) [*REF*], Q-Mix [*REF*] and
QTRAN [*REF*] use different methods to decompose the team&apos;s
central action-value function into the decentralised action-value
functions of each team member. We build on top of VDN (which is further
explained in section [3.0.0.4]) to extend the concept of team to the concept of
family and introduce continuous degrees of cooperation.


Background 


Reinforcement Learning


We recall the single agent fully-observable RL
setting [*REF*], where the environment is typically
formulated as a Markov decision process (MDP). At every time step,
*MATH*, the agent observes the environment&apos;s state
*MATH*, and uses it to select an action
*MATH*. As a consequence, the agent receives a reward
*MATH* and the environment transitions to
the state *MATH*. The tuple (*MATH*) is sampled from the
static probability distribution *MATH* 
whilst the actions *MATH* are sampled from the parametric policy function
*MATH*: *MATH*. The goal of the agent is to find the optimal policy parameters
*MATH* that maximise the expected return
*MATH*, where *MATH* is the discount factor. In the more general framework, the state is only
partially observable, meaning that the agent can not directly observe
the state but instead it observes *MATH* which is
typically given by a function of the state. In this situation, the
environment is modelled by a partial observable Markov decision process
(POMDP) and the policy usually incorporates past history *MATH*.


Q-Learning and Deep Q-Networks


The action-value function *MATH* gives the estimated return when the
agent has the state history *MATH*, executes action *MATH* and follows the
policy *MATH* on the future time steps. It can be recursively defined by
*MATH*. Q-learning and Deep Q-Networks (DQN) [*REF*] are popular
methods for obtaining the optimal action value function *MATH*. Once we
have *MATH*, the optimal policy is also available as
*MATH*. In DQN, the action-value function
is approximated by a deep NN with parameters *MATH*. *MATH* is
found by minimising the loss function: *MATH*.
Where *MATH* is the *MATH* -greedy policy which takes action
*MATH* with probability *MATH*, and
takes a random action with probability *MATH*.


Multi-Agent Reinforcement Learning


In this work, we consider the MARL setting where the underlying
environment is modelled by a partially observable Markov
game [*REF*]. In this setting, the environment is populated
by multiple agents which have individual observations and rewards and
act according to individual policies. Their goal is to maximise their
own expected return.


Binary Cooperative MARL and VDN 


Our work builds on VDN [*REF*], which was designed to
address the binary cooperative MARL setting. In this setting, the agents
are grouped into teams and all the agents within a team receive the same
reward. VDN&apos;s main assumption is that the joint action-value function of
the whole team of cooperative agents can be additively decomposed into
the action-value functions across the members of the team.
*MATH*. Where *MATH* is the set of agents belonging to the team, and
*MATH* is the value function of agent *MATH* which
depends solely on its partial observation of the environment and its
action at time *MATH*. *MATH* are trained by back-propagating
gradients from the Q-learning rule through the summation.
*MATH*. Where *MATH* are the parameters of *MATH*, *MATH* is its
gradient and *MATH* is the reward for the team *MATH* 
at the time instant *MATH*. Note that even though the training process is
centralised, the learned agents can be deployed independently, since
each agent acting greedily with respect to its own *MATH* will
also maximise its team value function *MATH*.


Evolution via Evolutionary Reward


In this section, we propose a reward function that enables RL algorithms
to search for policies with increasingly evolutionary success. We call
this reward the evolutionary reward because it is always aligned with
the fitness function (see section [1.2] for
a definition of this alignment), and therefore, we don&apos;t need to go
through the expensive process of aligning the agents&apos; reward functions
through evolution. We also propose a specific RL algorithm that is
particularly suited to maximise the evolutionary reward in open-ended
evolutionary environments (however other RL algorithms could also be
used).


Evolutionary reward 


The evolutionary reward of an agent is proportional to the number of
copies its replicators have in the world&apos;s population. Maximising this
reward leads to the maximisation of the survival and reproduction
success of the replicators an agent carries. We start by defining the
kinship function between a pair of agents *MATH* and *MATH*, who carry *MATH* 
replicators represented by the integer vectors *MATH* and *MATH* 
(we chose to use *MATH* for genome, which in biology is the set of
genes (replicators) an agent carries): *MATH*. Where *MATH* is the Kronecker delta
which is one if *MATH* and zero otherwise. When agent *MATH* is
alive at time *MATH*, it receives the reward: *MATH*.
Where *MATH* is the set of agents alive at the instant
*MATH*. Note that since agent *MATH* is alive at *MATH*, *MATH* 
includes agent *MATH*. *MATH* is the last time step that agent *MATH* is
alive and so, at this instant, the agent receives its final reward which
is proportional to the discounted sum of the number of times its genes
will be present on other agents after its death: *MATH*.


With this reward function, the agents are incentivised to maximise the
survival and replication success of the replicators they carry. In the
agent-centered view, the agents are incentivised to survive and
replicate, but also to help their family (kin) survive and replicate;
and to make sure that when they die their family is in a good position
to carry on surviving and replicating.


The discount factor, *MATH*, needs to be in the interval *MATH* to
ensure the final reward remains bounded. Due to the exponential
discounting we can compute the final reward up to an error of *MATH* 
by summing over a finite period of time denoted by the effective horizon
(*MATH*). To see how to compute the *MATH* for a given environment and
*MATH* see the appendix [10.1]. We can now use Q-learning to train agents with this
evolutionary reward. However, in the next section we introduce a more
practical algorithm that allows us to estimate the final reward without
having to simulate the environment forward for *MATH* iterations.


Evolutionary Value-Decomposition Networks


We propose Evolutionary Value-Decomposition Networks (E-VDN) as an
extension of VDN (explained in detail in section
[3.0.0.4]) from the binary cooperative setting with static teams to the continuous
cooperative setting with dynamic families. E-VDN helps us reduce the
variance of the value estimation and allows us to estimate the final
evolutionary reward without having to simulate the environment forward
for *MATH* iterations.


Within a team, each agent fully cooperates with all the other members of
the team, and it does not cooperate at all with any agent outside of the
team. Moreover, if *MATH* and *MATH* are members of the same team and *MATH* is a
member of *MATH* &apos;s team then *MATH* and *MATH* are also in the same team. Within
a family, the degrees of cooperation amongst its members depends on
their kinship degree (which can be any real number from 0 to 1). Also,
if *MATH* and *MATH* are members of the same family and *MATH* is part of *MATH* &apos;s
family, *MATH* is not necessarily part of *MATH* &apos;s family.


Each agent *MATH* sees the members of its family from an unique
perspective, based on the kinship degree it shares with them. In E-VDN,
each agent *MATH* has a joint action-value function, *MATH*. E-VDN assumes
*MATH* can be composed by averaging the action-value functions across the
members of *MATH* &apos;s family weighted by their kinship with agent *MATH* (this
extends VDN&apos;s assumption, given by equation, to the continuous cooperative setting):
*MATH*. Where *MATH* is a normalisation coefficient defined as *MATH*. Composing *MATH* 
with an average, instead of a sum, is necessary as E-VDN allows the
number of value functions contributing to the composition to vary as the
family gets bigger or smaller (agents born and die). This averaging
allows us to incorporate the local observations of each family member
and reduce variance in the value estimation.


More importantly, E-VDN allows us to deal with the difficulty of
estimating the terminal reward equation in a particularly convenient way. As is
clear from its definition equation, the terminal reward is the expected sum
(over time) of kinship that agent *MATH* has with other agents *MATH* after
its death. The key idea is to note that this value (*MATH*) can
be approximated by the Q-value of other agents *MATH* that are close to
(have high kinship with) agent *MATH*: *MATH*. The final reward is zero if,
and only if, at the time of its death the agent has no surviving family.


Each *MATH* is trained by back-propagating gradients from the
Q-learning rule: *MATH*. Where *MATH* is the concatenation of all the parameters
*MATH*, used in each *MATH*, contributing to the
estimation of *MATH*; i.e. *MATH*.
Note that *MATH* are neural networks with parameters
*MATH* and *MATH* is simply the average stated in equation.


The learning targets *MATH* are given by: *MATH*. *MATH* is the evolutionary reward
and *MATH* is the estimate of the final evolutionary reward. We don&apos;t use a replay buffer in our
training (which is commonly used in DQN) due to the non-stationary of
multi-agent environments (more about this in the appendix [10.2]).


Since the joint action-value *MATH* increases monotonically with
increasing *MATH*, an agent acting greedily with respect to its
action-value function will also act greedily in respect to its family
action-value function: *MATH*.


Experimental Setup


We want to test two hypotheses: 1) E-VDN is particularly well suited to
make agents climb the fitness landscape in open-ended evolutionary
environments; 2) E-VDN is able to increase the evolutionary fitness of
agents in non-binary cooperative environments. To test these hypotheses,
we introduce a binary and non-binary cooperative setting: the asexual
(binary) and the sexual (non-binary) environments. In this section, we
give a quick overview of these two multi-agent environments, as well as
details of the network architectures and the training regime. For a more
complete description of the environments, you can refer to the appendix [9].


The Asexual Environment


The asexual environment is a 2-dimensional grid world, which is
initialised with five agents carrying five unique genomes (figure
[2]). At each time step, each agent may move one step and produce an attack to another
agent in an adjacent tile. When an agent moves to a tile with food it
collects all the food available in it. If an agent chooses to produce an
attack, it decreases its victim&apos;s health by one point, if the victim&apos;s
health reaches zero it dies and 50% of its collected food is captured by
the attacker. The food is used to survive (one unit of food must be
consumed every time step to remain alive), and to reproduce. When agents
are within their fertile age and they have stored enough food, they
reproduce themselves asexually and give birth to an agent carrying an
exact copy of their genome. Each genome has only a single gene and there
are no mutations. These rules make the cooperation between agents
binary, agents either fully-cooperate (they have the exact same genome)
or they don&apos;t cooperate at all (their genome has no overlap). Note that
despite this environment being in the binary cooperative setting, the
VDN algorithm can not be directly applied to it since the \&quot;team\&quot; sizes
increase and decrease as agents born and die, moreover, VDN would have
to estimate the agent&apos;s final reward by simulating the environment
forward for the effective horizon. E-VDN solves these problems by
decomposing the team action-value function with an average instead of a
sum, and estimating the final reward of a dying agent by using the
value-functions of the remaining family members.


FIGURE


The Sexual Environment


The sexual environment has the same rules as the asexual environment
with the difference that the agents now have 32 genes in their genome
and they reproduce sexually. When two fertile agents are adjacent, they
give birth to an agent who&apos;s genome is composed by two halves of the
genes of each parent, selected randomly. There are no genders, any agent
can reproduce with any other agent. These rules give rise to different
levels of collaboration (33 levels of cooperation to be exact, from 0 to
1 in steps of *MATH*).


Policy


Each agent observes a 5x5 square crop of the surrounding state (figure
[2]). The agent sees six features for every visible tile; i.e. the input is a 5x5x6 tensor.
This includes two features corresponding to tile properties (food
available and whether it is occupied or not) and four features
corresponding to the occupying agents&apos; properties (age, food stored,
kinship and health). Besides these local inputs, each agent also
observes its absolute position, family size and the total number of
agents in the world. We intend to remove these extra inputs in future
work as we provide agents with memory (we&apos;re currently providing our
policy with *MATH* instead of *MATH*). The NN has ten outputs (five
movement actions with no attack and five movement actions with an
attack). In this work, we used two different feed forward architectures:
one is simply a fully connected NN with three hidden layers and 244 288
parameters in total, the other architecture is composed by convolutional
and dense layers and it is much smaller containing only 23 616
parameters. The smaller NN was used to compare our algorithm with an
evolutionary algorithm which doesn&apos;t scale well to larger networks.


Training details


In the asexual environment, we train five different policies (with the
same architecture but different weights) simultaneously. At each
training episode, we sample five policies with replacement and assign
each one to one of the five unique genomes. We do this, to force each
policy to interact with all other policies (including itself),
increasing their robustness in survival and reproduction. During the
test episodes, no sampling occurs, each policy is simply assigned to
each unique genome. The training episodes had a length between 450 and
550 (note that the reward is computed as if there was no episode end),
and the test episodes had a length of 500 steps.


In the sexual environment, due to the large number of unique genomes, it
is unfeasible to assign a unique policy to each unique genome. To keep
things simple, we chose to use only one policy in this environment. In
this work, the genome does not directly encode the policy, however, we
think it would be interesting to do that in future work. For example in
tabular RL the policy could be a table of integers (the actions to take
at every discrete state) and the genome could be that same table of
integers, EvER would then evolve the agents&apos; genomes and policies.


Evolution Strategies


In the asexual environment, we compare the E-VDN algorithm with a
popular ES algorithm. ES algorithms optimise an agent&apos;s policy by
sampling policy weights from a multivariate Gaussian distribution,
evaluating those weights on the environment, giving them a fitness score
and updating the Gaussian distribution parameters so that the next
samples are more likely to achieve a higher fitness. There are a few
different methods on how to update the distribution parameters, we chose
to use CMA-ES [*REF*] because it has been successful in
optimising NN for a wide range of sequential decision
problems [*REF*; *REF*; *REF*].
However, note that CMA-ES was not designed for multi-agent settings
where the fitness function landscape changes as the other agents learn.
Nevertheless, we used five independent multivariate Gaussians
distributions each one associated with a unique gene and each one being
updated by the CMA-ES algorithm. In the beginning, when the agents can
not survive for long, the fitness function is given by the total sum of
family members along time, once the agents learn how to survive and
reproduce we change the fitness function to be the number of family
members at the end of an episode with 500 steps. Since the CMA-ES
algorithm computation time grows quadratic with the number of
parameters, O(*MATH*), we had to use the smaller NN for this comparison.
This algorithm was not applied to the sexual environment due to the
large number of unique genomes available in this environment. The
algorithm was implemented using an available python
library [*REF*].


Evaluation Metrics


In our simple environments, fitter policies can use the environment
resources more efficiently and increase their population size to larger
numbers. Therefore, to evaluate the performance of the algorithms in
generating increasingly fitter species we track the average population
size along training time.


Results


Training agents with E-VDN generates quite an interesting evolutionary
history. Throughout the asexual environment history, we found four
distinct eras where agents engage in significantly distinct behaviour
patterns (1st row of fig. [3]). In the first era (the blue line - barely
visible in the figure), the agents learned how to survive, and through
their encounters with the other founding agents, they have learnt that
it was always (evolutionary) advantageous to attack other agents. In the
second era (orange line), the agents&apos; food-gathering skills increased to
a point where they started to reproduce. In this era, the birth-rate and
population numbers increased fast. However, with the extra births,
intra-family encounters became more frequent, and intra-family violence
rose to its all-time maximum driving the average life span down. This
intra-family violence quickly decreased in the third era (green line),
as agents started to recognize their kin. Kin detection allowed for
selective kindness and selective violence, which took the average life
span to its all-time maximum. Finally, in the fourth era (red line),
agents learned how to sacrifice their lives for the future of their
family. Old infertile agents started allowing the younger generation to
eat them without retaliation. Through this cannibalism, the families had
found a system for wealth inheritance. A smart allocation of the
family&apos;s food resources in the fitter generation led to an increase in
the population size with the cost of a shorter life span. This behaviour
emerges because the final reward incentivises agents to plan for the success
of their genes even after their death. This behaviour is further
investigated in the appendix [11.1]. These results show that optimising open-ended
evolutionary environments with E-VDN does indeed generate increasingly
complex behaviours.


The 2nd row of figure [3], shows the macro-statistics obtained by
training the smaller NN with CMA-ES and E-VDN. From the figure, we
observe that E-VDN is able to produce a larger population of agents with
a longer life-span and a higher birth rate. A small population means
that many resources are left unused by the current population, this
creates an opportunity for a new and more efficient species to collect
the unused resources and multiply its numbers. These opportunities are
present in the CMA-ES environment, however the algorithm could not find
them, which suggests that E-VDN is better at finding the way up the
fitness landscape than CMA-ES. [Video 1](WEBSITE),
shows that each family trained with CMA-ES creates a swarm formation in
a line that moves around the world diagonally. When there is only one
surviving family, this simple strategy allows agents to only step into
tiles that have reached their maximum food capacity. However, this is
far from an evolutionarily stable strategy [*REF*] (ESS;
i.e. a strategy that is not easily driven to extinction by a competing
strategy), as we verify when we place the best two families trained with
CMA-ES on the same environment as the best two E-VDN families and
observe the CMA-ES families being consistently driven quickly to
extinction by their competition (fig.[4]a).


FIGURES


In the sexual environment, figures
[4]b,c and d show that E-VDN has been able to
train a policy that consistently improves the survival success of the
founding genes. Note, that this environment is much harder than the
previous one. To replicate, agents need to be adjacent to other agents.
In the beginning, all agents are unrelated making it dangerous to get
adjacent to another agent as it often leads into attacks, but it is also
dangerous to get too far away from them since with a limited vision it
is hard to find a fertile mate once they lose sight of each other.
[Video 2](WEBSITE) shows a simulation of the
evolved policy being run on the sexual environment, it seems that agents
found a way to find mates by moving to a certain region of the map (the
breeding ground) once they are fertile.


Conclusion &amp; Future Work


This paper has introduced an evolutionary reward function that when
maximised also maximises the evolutionary fitness of the agent. This
allows RL to be used as a tool for research of open-ended evolutionary
systems. To implement this reward function, we extended the concept of
team to the concept of family and introduce continuous degrees of
cooperation. Future work will be split into three independent
contributions: 1) Encode the agents&apos; policy directly in their genome
(e.g. for discrete state-actions spaces a table of integers can both
represent the policy and the genome); 2) Explore a different reward
function that makes agents maximise the expected geometric growth rate
of their replicators. We call this the Kelly reward
*MATH* (*MATH* is defined in eq.), inspired by the Kelly Criterion [*REF*]
(KC), an investment strategy that guarantees higher wealth, after a
series of bets, when compared with any other betting strategy in the
long run (i.e. when the number of bets tends to infinity). In fact, the
field of economics (which includes investments) is often seen from the
evolutionary perspective [*REF*]. More wealth allows
investors to apply their investment policy at a larger scale,
multiplying its replication ability. The same can be said about genes,
more agents carrying those genes allows the genes&apos; policy to be applied
at a larger scale, multiplying its replication ability. Moreover,
investors can enter an absorption state from where there is no escape
(bankruptcy), and the same happens with genes (extinction). The KC
results from maximising the expected geometric growth rate of wealth, as
opposed to the usual goal of maximising the expected wealth which, in
some cases, may lead to an investment strategy that tends to give
bankruptcy with probability of one as the number of bets grows to
infinite (see the St. Petersburg paradox for an example); 3) Following
our proposed methodology for progress in AI (section [1.1]), we
will research the minimum set of requirements to emerge natural
cognitive abilities in artificial agents such as identity awareness and
recognition, friendship and hierarchical status.