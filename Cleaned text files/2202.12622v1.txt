Behavioristic AI by neoRL nodes


Thorndike&apos;s law-an-effect in functionalist psychology have been
reported as an important inspiration for Reinforcement Learning (RL) in
AI [*REF*]. Thorndike considered the reinforcement of
randomly encountered reflexes as a plausible explanation for simple
behavior and acquired reflexes. The law-of-effect represents an
essential first step toward


FIGURE


the study of behavior becoming a natural science, quickly replaced by
behaviorism for explaining advanced policies and human behavior.
Edward C. Tolman (1866-1959) further proposed that behavior is separate
from learning, attempting to explain observations where an animal could
express different behavior as a function of varying motivation
[*REF*]. Combining Tolman&apos;s latent learning with operant
conditioning from E. C. Skinner, considering policies as being
operant toward an objective, the neoRL framework allows for
purposive behaviorism for Euclidean navigation [*REF*]. By
expressing latent learning as a set of operant reflexes in the
environment, i.e., with a set of general value functions
(GVF) [*REF*] trained by mutually exclusive conditionals as
reward signals, agent purpose becomes an expression of the parameter
configurations where operant value functions are extracted. Considering
a set of conditionals inspired by the 2014 Nobel Price in neuroscience,
the discovery of place cells and other mechanisms behind state
representation for neural navigation, Leikanger (2019) demonstrated how
operant neoRL sub-agents could apply for autonomous navigation. Latent
learning by a set of operant GVFs combined with elements-of-interest,
projections associated with reward in the considered Euclidean space,
allows for autonomous navigation governed by the purpose of attaining
reward. See my thesis [*REF*] for more on the
theory behind NRES-oriented RL (neoRL) agents, scheduled to be presented
in a public online PhD defence only days before RLDM.


A neoRL learning module can be considered as a behavioral node in a
purposive network; early results for neoRL navigation explored the
effect of considering multiple state spaces in parallel for the neoRL
agent. In some ways analogous to the Hybrid Reward
Architecture [*REF*], the neoRL navigation agent combines
several learners that establish GVFs toward separate
concerns [*REF*]. From applying the superposition
principle in the value domain, the neoRL agent is capable of combining
value function from many learners in one state
space [*REF*], across multiple representations of the same
state space [*REF*], or across information represented
in orthogonal Euclidean spaces -- thus capable of fully decomposing the
state space to simpler considerations [*REF*]. The
behavioral node consists of three parts; first, the latently learned
cognitive map formed by GVF on operant desires toward NRES cells.
Operant desires are trained by off-policy GVF, expressing latent
learning on how to accomplish different conditionals in this environment
representation. Second, the neoRL agent is governed by purpose -- mental
projections of parameter configurations associated with reward are
expressed as elements-of-interest in the NRES map.


FIGURE


These free-ranging representations of desire in the considered space are
mapped to NRES nodes, activating GVFs corresponding to the associated
valence -- the element&apos;s expectancy of reward upon achievement. Note
that the same neoRL node, containing latent knowledge in one NRES
representation, can be harvested by different sets of
elements-of-interest. Third, the value function can be extracted by
elements-of-interest from the digital analogy to Tolman&apos;s cognitive map
-- resulting in an actionable Q-vector output of the neoRL node. When
mutually exclusive NRES receptive fields are used for latent learning,
the GVF components become operant toward that NRES cell -- Operant
Value-function Components (OVC). The singular (orthogonal) value
component can be combined with others to form the full value function of
the agent, further implying that multiple modalities can be learned in
parallel and combined to one agent value function [*REF*].
Figure shows the aggregation of the value function for the neoRL agent in
[*REF*]. The location and valence of elements-of-interest can be considered as inputs to the neoRL behavioral
node, and the superposition of weighted OVC establishes an output of the
neoRL node.


The purposive AI expressed by the neoRL agent in figure can well be
considered as a single-layered behavioral network with a single output
state-action value function. Figure presents a
functional representation of the multi-map navigational agent from
figure. The individual neoRL sub-nodes *MATH* are trained by latent learning, and
purposive state-action value functions *MATH* can be
extracted for the digital analogy to a cognitive map by purposive
elements *MATH*. The network would have an input, the full set of
elements of interest *MATH*; the purposive network would have a
latent state, formed by latent learning expressed as off-policy operant
desires; the neoRL network would have an actionable state-action
Q-vector as output. When further assuming a Euclidean significance for
the action set, as with *MATH* in [*REF*], the state-action value could be interpreted
a Euclidean desire-vector; *MATH* where *MATH* 
represents the vector sum. The valence of the desire vector *MATH* 
should express the combined valence across *MATH*.
*MATH* where *MATH* signifies the coordinate and
*MATH* represents the valence of purposive element *MATH*. A functional
schematic of the neoRL module is illustrated in
figure; a single output-desire can be formed from
any number of input elements *MATH*. Different sets of purposive
elements-of-interest *MATH* can establish different output desire
*MATH* and actionable state-action values *MATH* from the same
cognitive map. Earlier networks of the neoRL node could be seen as a
behavioral analog to a one-layered
perceptron [*REF*]. The enclosed theory allows for
multi-layered neoRL networks governed by autonomous desire.


Experiments


A comprehensive environment for research on autonomous navigation is the
PLE implementation [*REF*] of Karpathy&apos;s WaterWorld
environment; An agent controls acceleration of the self in the four
Cardinal directions *MATH*, i.e., \[up, down, right, left\].
Three objects move around in the Euclidean plane according to predefined
mechanics. Encountering an object replaces it with a new object with a
random color, location, and speed vector. Green objects are desirable
with a positive reward *MATH*, and red objects are
repulsive with a negative valence of *MATH*. Capturing the last green
object resets all remaining (red) objects by the same reset mechanisms.
The only reward comes from encountering objects, making the accumulation
of *MATH* an objective measure for navigational capabilities and
its time course an indication of real-time learning capabilities.


Rewards are only received sparsely, with discrete *MATH* or *MATH* steps
after encountering objects in the WaterWorld environment; measuring the
immediate proficiency of the agent by accumulated *MATH* can be a
challenge.


FIGURE


Capturing the transient time course of agent skill can be done by
averaging independent runts; the enclosed experiments average *MATH* 
separate runs to measure transient navigational proficiency, i.e.,
across *MATH* separate agents. No pre-training or other precursors are
available for the agents, making all navigation happen live as the agent
gathers experience in the environment for the first time. Curves are
presented with minutes along the x-axis, signifying the wall-clock time
since the beginning of each run.


We shall explore four aspects in the WaterWorld environment; first, we
challenge the basic principle of propagating purpose by the layout
illustrated in figure [6]a. The first
neoRL node, *MATH*, forms a purposive desire based on all reported
objects from WaterWorld; a single desire vector *MATH* with
accompanying valence *MATH* propagates to the compatible neoRL node
*MATH* as autonomous desire *MATH*. Experiment [b]
explores the effect of extracting separate desires from the same learned
cognitive map. A purposive desire vector from the neoRL node comes from
extracting latent knowledge from considered coordinates; the agent
extracts two purposive vectors from *MATH*, one from desirable
objects *MATH* and a separate from aversive objects
*MATH*. The output from multiple neoRL nodes can be combined for
the policy-forming value function in the neoRL framework; experiment
[c] explores the effect of aggregating value function from
multiple depths of the neoRL net. The output desire from a neoRL node
*MATH* can be applied to any compatible neoRL node *MATH*, including
itself: experiment [d] explores recurrent connections for neoRL
nodes. Collaborative experience is explored with and without recursive
desires, as illustrated in figure [3] and [4]. All results are reported in figure [5].


FIGURE


Discussion


The neoRL agent navigates continuous space by projections of desire,
vectors of purpose associated with an agent&apos;s expectancy or reward. When
actions have a Euclidean significance, purposive Q-vectors can form
autonomous projections of desire, experience-based inferences that can
establish input to deeper neoRL nodes. Experiments demonstrate how
deeper or recurrent desires are crucial for navigational proficiency,
suggesting purposive neoRL networks as a plausible approach to
autonomous navigation.


This work explores four principles of purposive neoRL networks. First,
experience-based desire vectors can shape purposive navigation in
Euclidean space. The neoRL network from illustration [1]
improves navigational proficiency over time; however, considering all
objectives under one, i.e., forming a single desire vector based on all
red and green objects, becomes too simple for proficient navigation.
Second, a single neoRL node can generate different *MATH* desires
by considering different sets of objectives *MATH*. Experiment
b demonstrates the effect of separating desires according to
valence, resulting in the increased performance by the neoRL
navigational agent. The simplicity and clarity expressed by separation
of desires, as illustrated in figure [2],
facilitates explainability of the trained solution -- a crucial element
if traditional AI is to be applied for desire classification. Third,
the neoRL agent can base agent value function on any neoRL node in the
network. Agents extracting purpose from multiple depths of the neoRL
network, as illustrated in [3], becomes better navigators than more
superficial agents. Fourth, desire vectors *MATH* from one neoRL
node can form objectives for any compatible neoRL node -- including
itself. Experiment [d] explores recursive desires, where the output of
*MATH* -- desire vector *MATH* -- establish an additional
purpose for neoRL node *MATH*. The proficiency of the recursive
agent from [4] is reported as curve [D], showing how
recursive desires drastically improve the agent&apos;s navigational
proficiency. Results can be examined in figure [5] and in
real-time video demonstrations at WEBSITE.


Note that no comparison has been made with alternative approaches for
control; this work is only concerned with uncovering the basic
principles of purposive neoRL nets for behavioral AI. Still, any attempt
on finding RL or AI solutions capable of allocentric Euclidean
navigation in real-time has failed. Further work could involve finding
and comparing alternative approaches for real-time autonomous navigation
in the WaterWorld environment. Likewise, this work involves no search
for optimal parameters for neoRL navigation. Experiment [c] explores
collaborative experience with 1:1 weight ratio between *MATH* and
*MATH*. and experiment [d] only explores a unitary feedback loop
*MATH*. It is left for further work to explore network architecture
theory or find data-driven methods for parameter adaptation. We have
barely scratched the surface of autonomous navigation by purposive
networks, proposing neoRL networks as a plausible new approach toward
navigational autonomy.