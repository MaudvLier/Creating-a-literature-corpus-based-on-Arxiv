Cooperate or Collapse: Emergence of Sustainability in a Society of LLM Agents


Introduction 


Recent advances in large language models (LLMs) have demonstrated
impressive abilities across many tasks
[*REF*; *REF*; *REF*; *REF*], and LLMs are being integrated into complex agent systems
[*REF*; *REF*]. As LLMs become central to these
systems, they inherit critical decision-making responsibilities,
necessitating an analysis of their ability to operate safely and
reliably, especially in contexts where cooperation is key. Cooperation
is a fundamental feature across many scales of human social life,
enabling better outcomes for all through joint effort
[*REF*; *REF*; *REF*; *REF*]. If AI agents take on complex decision-making roles, they are likely to
face similar cooperation challenges as humans, highlighting the need for
robust and safe AI practices that can cooperate with us as we cooperate
with each other [*REF*].


Despite significant advances in the scale and ability in LLMs, we still
possess only a limited understanding of their cooperative behavior.
Prior multi-agent research has studied highly constrained scenarios such
as board games or narrowly defined collaborative tasks
[*REF*; *REF*; *REF*; *REF*; *REF*]. These multi-agent studies 
complement existing single-agent AI safety
benchmarks [*REF*; *REF*]. However, these efforts leave important questions 
open: (1) there is a limited understanding of how LLMs achieve and maintain cooperation, in contrast
to the well-documented mechanisms that have been described for
humans[*REF*; *REF*; *REF*]; (2) how to handle multi-turn interactions and balance safety with reward
maximization in multi-agent settings; and (3) the potential of using
LLMs as a simulation platform for human psychology and economic theories.


To address this, we develop a novel simulation environment, called the
Governance of the Commons Simulation ([GovSim]), to
evaluate LLM-based agents in multi-agent multi-turn resource-sharing
scenarios. This environment requires agents to engage in sophisticated
strategic reasoning through ethical decision-making and negotiation.
Inspired by game-theoretic research on the evolution of cooperation
[*REF*] and &quot;The Tragedy of the Commons,&quot; we build
[GovSim] to simulate realistic multi-party social dilemmas
such as those faced by groups managing shared resources or countries
negotiating treaties to mitigate climate change
[*REF*; *REF*]. Our platform can support any
text-based agent, including LLMs and humans, and mirrors some of the
complexity in actual human interactions. Thus we use
[GovSim] to benchmark the cooperative behaviors of today&apos;s
and future LLMs. We build a standard agent, using the generative agent
architecture [*REF*], that can accommodate different LLMs.


Within [GovSim], we develop three common pool resource
dilemma inspired by the economic analysis of emergent sustainable
cooperation [*REF*; *REF*; *REF*; *REF*; *REF*].
We test our generative agents with fifteen different LLMs, including
open-weights and closed-weights models. Surprisingly, we find that only
two out of 45 instances (15 LLMs across 3 scenario), manage to sustain
the common resource. We hypothesize that the lack of sustainable
governance may be due to an inability to project the long-term effects
on the equilibrium of greedy action. We find that prompting agents to
consider the universalization of their action [*REF*],
significantly improves survival time. To understand whether the norms
formed in [GovSim] are robust, we introduce a greedy
newcomer unfamiliar with an already formed norm. Overall, we find that
this perturbation increases the inequality across agents. Finally, we
perform extensive analyses to understand how the capabilities of LLMs
play a role in achieving sustainability. We show that communication is
key to success. Through ablation studies we show that communication
reduces resource overuse by 21%. Within these dialogues, negotiation is
the main type of communication between agents and constitutes 62% of the
dialogs. Other subskills are also important, especially the ability to
form beliefs about other agents, which has a strong Pearson correlation
of 0.83 with survival time.


FIGURE


In summary, our contributions are as follows: 
1. We introduce [GovSim], the first common pool
resource-sharing simulation platform for LLM agents.
[GovSim] enables us to study and benchmark emergent
sustainable behavior in LLMs.
2. Using [GovSim], we find that only a few instances of the
simulations achieve a sustainable outcome, which is an alerting
phenomenon.
3. We develop more capable cooperative agents based on philosophical
principle of universalization. Through ablation and pertubation we
characterize the boundary conditions of the emergence of sustainable
cooperation.
4. We open-source our simulation framework to foster future research: 
the [GovSim] simulation environment, agent prompts, and a web interface.


FIGURE


The [GovSim] Environment 


To understand the logic behind the [GovSim] environment, we
provide a brief background of the economic theory of cooperation, a
description of the simulation environment, and metrics used to evaluate
cooperative behavior and resource management.


Economic Background


Sustaining cooperation is an essential problem that enables individuals
to achieve better outcomes than they could achieve on their own
[*REF*; *REF*]. Humans solve cooperation
problems across all scales of life, ranging from small groups of
fishermen who harvest a shared resource to multi-national treaties that
restrict pollution to reduce the adverse effects of climate change.
However, when self-interested individuals or organizations are faced
with paying a personal cost to sustain a greater good, cooperation
can be challenging to maintain [*REF*].


Although mechanism designers have developed incentive-compatible systems
that can lead to cooperation between self-interested agents, these
systems often assume a top-down process that coordinates the process
[*REF*]. In contrast, humans seem to be able to develop
mechanisms from the bottom up and implement cooperative norms in a
decentralized fashion. For example, when managing a shared resource,
people develop rules and norms that lead to long-term sustainable
cooperation [*REF*; *REF*; *REF*].


[GovSim] Description


The purpose of [GovSim] is to evaluate the ability of LLMs
to engage in cooperative behavior and effective governance of shared
resources. In [GovSim], agents are given a common pool of
natural resources that regenerates over time. The task is to manage the
extraction or use of this resource. Take too much, and the resource will
collapse and no longer regenerate again (e.g., the fish in a lake go
extinct). Take too little, and the resource&apos;s economic potential is
underutilized. Even a purely selfish agent that aims to maximize his
long-term reward must balance the amount of the resource he extracts
now with what he will be able to extract in the future. When multiple
agents are involved, questions of fairness arise
[*REF*]. Agents must negotiate what they believe to be their fair share.


We have implemented three scenarios in [GovSim] that are
inspired by the economic literature on governing common pool resources.
The first is inspired by empirical work on understanding the norms that
emerge in communities of fishermen that prevent overfishing
[*REF*; *REF*; *REF*]. In the first scenario, fishery, agents share a fish-filled lake, and each
decides how many tons of fish each should catch each month. The lake
supports up to 100 tons of fish, and the fish population doubles at the
end of the month up to this capacity. For example, five fishermen can
sustainably catch up to 10 tons of fish each per month, but if the total
amount they catch exceeds 50 tons, the population will start to
decrease. See [2] for prompt sketches
regarding this scenario. In the second scenario, pasture, and
following *REF* and *REF*, agents are shepherds
and control flocks of sheep. Each month they decide how many sheep
they&apos;ll allow on a shared pasture. Like the fish, the pasture can
support up to 100 hectares of grass; each sheep consumes 1 hectare per
month, and the remaining grass doubles up to its capacity. In the third
scenario, pollution, agents are factory owners that need to balance
production with pollution. For each pallet of widgets produced, their
factory pollutes 1% of the water in a shared river. Like the previous
cases, at the end of the month, the amount of unpolluted water doubles.


[GovSim] Environment Dynamics


To facilitate comparison across scenarios, the dynamics of each
environment are mathematically equivalent.


Amount of Shared Resource  *MATH*.


The amount of shared resources available at time *MATH* is denoted by
*MATH*. The function *MATH* maps each
time step to the corresponding quantity of available resources. We
assume integer units of the shared resource.


The simulation is based on two main phases: harvesting and discussion.
At the beginning of the month, the agent can start harvesting the shared
resource. All agents submit their actions privately (how much of the
resource they would like to consume up to availability); their actions
are executed simultaneously and then made public. At this point, the
agents have an opportunity to communicate freely with each other using
natural language. At the end of the month, the remaining shared
resources double up to a maximum of 100. When *MATH* falls below *MATH* 
the resource collapses and nothing else can be extracted. Each scenario
describes a type of public goods game that is repeated for *MATH* time
steps [*REF*]. A bound on optimal group behavior is for
agents to jointly consume no more than the sustainability threshold.


Sustainability Threshold  *MATH*. 


This threshold represents the maximum amount of resource that can be
extracted at time *MATH* without diminishing the resource stock at time
*MATH*, considering the future resource growth multiplier *MATH*. Formally,
the sustainability threshold is given by the function
*MATH* and is defined as follows: *MATH*. 


Together, [GovSim] can be viewed as a partially observable
Markov game that interleaves actions, observations, and rewards with an
unstructured dialogue between agents. Formally, a simulation *MATH* is
essentially a function that takes as input a tuple *MATH* and
returns a trajectory of the joint policy *MATH*,
which can be analyzed with various metrics; where *MATH* is the
set of agents, *MATH* is the policy induced by an LLM *MATH* 
together with a generative agent architecture *MATH*,
*MATH* are the dynamics of the environment. Each agent receives
an individual reward *MATH* defined by the amount of the resource
collected in the time step *MATH*.


[GovSim] Metrics 


In this section, we introduce metrics that measure different qualities
of the collective outcome. We follow *REF* in defining a
suite of metrics since in a mixed incentive repeated game like
[GovSim], no single scalar metric can track the entire state
of the system.


Survival Time  *MATH*.


To assess the sustainability of a simulation run, we define the number
of units of time survived *MATH* as the longest period during which the
shared resource remains above *MATH*.


Let *MATH* represent the
sequence of resources collected by the *MATH* -th agent at time *MATH* over the
simulation duration *MATH*. The total gain for each agent, *MATH*, is
defined as: *MATH*.


We define the efficiency *MATH* as how optimally the shared resource is
utilized w.r.t. the maximal possible efficiency. Intuitively, the
maximum efficiency *MATH* is achieved when the resource is
consistently regenerated to its maximum capacity, by agents jointly
collecting an amount equal to the initial sustainability threshold
*MATH*. Hence, we define *MATH* as: *MATH*.


(In)equality *MATH*.


We quantify (in)equality *MATH*, using the the Gini coefficient
[*REF*]. Across the total gains
*MATH* of all *MATH* agents: *MATH* where we normalize the absolute differences between
pairs of agents by the total gains of all agents.


Over-usage *MATH*.


We quantify the amount of (un)sustainable behavior across a simulation.
The over-usage *MATH*, is the percentage of actions across the experiment
that exceed the sustainability threshold: *MATH*.


Experimental Results 


Experimental Setup


Agent Architectures


To test LLM performance in [GovSim], we develop an
architecture that follows the generative agents framework
[*REF*]. These agents work in a phase-based environment
with discussion and action and support group discussions between the
agents. Each agent receives identical objective instructions on the
dynamics of [GovSim]. We are careful to avoid prompts that
might prime models to be cooperative or greedy, as shown in
[2] for the fishery scenario. Full details are presented in [8].


LLMs Benchmarked 


We compile a diverse suite of instruction-tuned LLMs for experiments on
[GovSim]. We test existing closed-weights models: GPT-3.5,
GPT-4, and GPT-4o [*REF*] via OpenAI API, Claude-3 Haiku,
Sonnet, and Opus via Anthropic API. We also tested open-weights models: Llama-2 (7B, 13B, 70B) [*REF*], Llama-3 (8B, 70B)
[*REF*], Mistral (7B, 8x7B) [*REF*], Qwen (72B,
110B) [*REF*]. See [10.1] for exact model identifiers,
hardware requirements, and API costs.


When testing LLMs, we ensure reproducibility by setting the text
generation temperature to zero, i.e., greedy decoding. We provide full
experimental details in [10] and on our GitHub. In addition,
simulations were repeated with five random seeds. The average scores for
each metric are presented in the main text, while the standard
deviations are in the appendix.


TABLE


Experiment: default. For each scenario, we report mean of survival
time (Surv.), total gain (Gain) and efficiency (Effi.) across five
runs (best is indicated in bold and best open-weights is underlined).
We report Llama-2 results, the metrics equality and over-usage; and
standard deviations in [10.2].


Base [GovSim] Benchmark Results 


The [GovSim] environment serves as a sustainability
benchmark, to evaluate whether LLM agents can effectively cooperate to
maintain a common pool of resources and avoid depletion. Possible
outcomes are reflected by the main three metrics introduced above,
namely survival time, total gain, efficiency over multiple simulations
controlled by an LLM *MATH*. Intuitively, cooperation is
optimized when agents achieve high total gain *MATH* by maximizing
efficiency *MATH* and achieving high survival time *MATH*.


We benchmark LLM agents in our three scenarios with the objective to
assess the balance between resource utilization (reward maximization)
and preservation (safety). Smaller models often failed to sustain
resources beyond the first month. No LLM maintained a high survival time
in all scenarios. In [1], larger models, such as GPT-4o, show
better performance in survival time and total gain, though their success
varied between scenarios. The fishery scenario is easier to manage than
the pasture and pollution scenarios. This might be due to the fact that
the fishing scenario only requires reasoning about a single variable
(fish), while the other scenarios involve interactions between two
variables, such as grass and sheep, or pollution and the production of
widgets.


Norm Robustness: A Greedy Newcomer 


We investigate perturbing a community of agents by inserting an agent
with more aggressive dynamics. In this test, a new player joins a
community of four agents who had the opportunity to develop norms for a
cooperative equilibrium in the first three months. The goal of the new
player is to maximize profit, indifferent to the welfare of others. This
experiment analyzes how the original group adapts or enforces
cooperation to prevent resource depletion. We use the same setup as
[3.2] and modify the prompt with the rules of
the simulation as shown in [10.4].


We perform this experiment in the fishery scenario using GPT-4, and
observe that across five seeds, the equality score drops from *MATH* in
the default setting to *MATH* in the newcomer experiment. As shown in
[4], the newcomer initially
harvests a large amount of fish, but adjusts to lower catch rates in
subsequent months. This adjustment results from interactions with the
original four fishermen. In [13], we provide a qualitative example
of these interactions, illustrating how the newcomer learns to reduce
the fishing effort and comply with the emergent norm during community
discussions.


FIGURE


Improving Sustainability by Universalization Reasoning 


In the preceding studies, we found that failure to simulate the
long-term consequences of the group behavior may underlie the lack of
sustainable cooperation in our simulations. One approach to make these
consequences salient is through a mechanism known in the moral
psychology and philosophy literature as &quot;Universalization&quot;
[*REF*; *REF*]. The basic idea of Universalization is
that when assessing whether a particular moral rule or action is
permissible, one should ask, &quot;What if everybody does that?&quot;
[*REF*]. Previous work has shown this process shapes how people
make moral judgments in social dilemmas [*REF*]. Here, we
hypothesize that a similar mechanism may make sustainable cooperation
more likely in LLMs by making the long-term consequences of collective
action more salient. For instance, a naive model might reason, &quot;I should
take as many fish as I can,&quot; but if forced to consider the
universalization of that policy (&quot;we each take as many fish as we can&quot;),
they realize that such a policy will cause rapid collapse.


To study whether Universalization can encourage sustainable cooperation,
we augmenting the memory of each agent with the following statement,
&quot;Given the current situation, if everyone takes more than *MATH*, the
shared resources will decrease next month.&quot;, where *MATH* is the
sustainable threshold defined in [2.4].
For this test we measure the delta between metrics computed on the
default scenario with universalization and without universalization.


We investigate the impact of incorporating universalized information on
all models described in [3.1.0.2], excluding Claude-3 Opus due to API costs. We find
that Universalization leads to longer survival times in 32 out of 40
combinations of LLMs and scenarios, excluding two combinations that
already had a maximum survival time. Specifically, universalization
significantly increases the average survival time by *MATH* months (t-test;
*MATH*), total gain by *MATH* units of shared resource (t-test;
*MATH*), and efficiency by *MATH* (t-test; *MATH*). For a detailed
breakdown of these improvements across models, see [10.3].


FIGURE


Ablation of Communication 


In this ablation study, we investigate the effects of removing the
ability of agents to communicate. We perform this investigation on the
subset of model that has higher survival time, see
[1] (GPT-4o, GPT-4, Claude-3 Opus, Qwen-110B). Comparing simulations without communication with those with
communication, we find that agents without communication tend to overuse
the common resource more often for 9 cases out of 12, as quantified by
the over-usage metric in [6]. This result underscores the importance of
the communication phase in promoting the use of sustainable resources.
For Qwen-110B, we find that the resource collapses very quickly
without communication as the model over-uses the shared resource in both cases.


Analysis of Agent Dialogues


We quantitatively analyze the conversations produced by the LLM during
the discussion phase, categorizing them into three main areas: information sharing, 
negotiation, and relational interactions, following
our taxonomy defined below: 1. Information: (a) Information Sharing: Disseminating facts among
participants. (b) Problem Identification: Highlighting challenges
that require collective attention and resolution. (c) Solution
Proposing: Offering ideas or actions to address identified issues.


2. Negotiation: (a) Persuasion: Attempting to influence others to
achieve a desired outcome. (b) Consensus Seeking: Aiming to align
group members on a decision or action plan. (c) Expressing
Disagreement: Articulating opposition to proposals or existing
conditions, with or without offering alternatives.


3. Relational: (a) Excusing Behavior: Justifying one&apos;s actions or
decisions, especially when they deviate from group norms or
expectations. (b) Punishment: Imposing consequences for perceived
wrongdoings or failures to adhere to norms.


Following *REF*, we used GPT-4 to classify each utterance
according to our defined taxonomy. The model was given detailed category
definitions and prompted to categorize each utterance into one of the
eight sub-categories. For details of this analysis, refer to
[11]. To ensure consistency, we manually annotated 100 random utterances and found that an annotator (an
author of the paper) agreed with GPT-4&apos;s labels 72% of the time on the sub-categories.


We analyze the dialogue on the subset of models that have higher
survival time from [1]. [7] shows that most utterances are
focused on negotiations between agents, on average 62% of the time.
Qualitatively, some models, such as GPT-4, tend to be cautious by
advocating lower fishing limits than the sustainability limit per
person. In contrast, scenarios where an agent significantly takes above
this limit cause noticeable concern among other participants. For
instance, an agent catching more fish usually avoids discussing the
issue instead of negotiating for greater access to the resource. For
examples of conversations, refer to [13].


FIGURE


The Role of LLM Capabilities 


Since we observed significant heterogeneity in the emergence of
sustainable cooperation across LLM models we next investigated how LLM
capabilities relate to success in [GovSim]. We test each LLM
capabilities on four sub-skills: (a) basic understanding of simulation
dynamics and ability to perform simple reasoning, (b) choosing a
sustainable action without interacting with the group, (c) calculating
the sustainability threshold of the current state of the simulation
under the assumption that all participants harvest equally, and (d)
calculating the sustainability threshold of the current state of the
simulation by forming a belief about actions of other agents. Each test
consists of 150 problems created from a template with procedurally
generated values. For each sub-skill test, we compute the accuracy
against the ground truth answer.


In [9], we show how the score on the test
cases correlates with survival time: clearly, understanding the dynamics
of the simulation is important but not the deciding factor for the
emergence of sustainable cooperation. Moreover, we see that when LLM are
asked to choose how many resources to harvest directly, without any
other interaction, they also perform poorly, reinforcing the observation
made in [3.5] and confirming that cooperation
through communication is key to a lasting cooperative norm. The last two
graphs ([9]c and d) show that only those
models that can formulate beliefs about other agents independently and
calculate their numerical implications are successful in the simulation
(Pearson correlation of 0.83 for test case d) [*REF*]. For a
breakdown across scenarios and prompts, we refer to [12].


Contributions in the Context of Related Work


AI Safety


The primary objective of AI safety is to ensure that AI systems do not
cause harm to humans [*REF*; *REF*; *REF*]. As LLMs
become more capable and autonomous, ensuring their safety remains a
critical concern [*REF*; *REF*; *REF*].
Popular evaluation datasets are [ETHICS]
[*REF*], [TruthfulQA] [*REF*], and [MoralExceptQA] [*REF*]. Additional studies have
explored the capabilities and potential issues of current LLMs
[*REF*; *REF*; *REF*]. These methods fall short in addressing the complexities inherent in
multi-agent interactions and broader real-world scenarios; more efforts
are needed to guarantee the safety of multi-agent systems
[*REF*; *REF*; *REF*]. Most similar to [GovSim] is [Machiavelli]
[*REF*], where they investigate harmful behavior vs.
reward maximization in a benchmark of single-agent
choose-your-own-adventure games.


Our Contribution: In contrast, [GovSim] focuses on
multi-agent scenarios that require both strategy, communication and
cooperation: it introduces a more dynamic and realistic environment that
is now possible to study using LLM agents. We introduce three resource
sharing scenarios and analyze the impact of agent behaviors on resource
sustainability, cooperation stability, and conflict resolution.


NLP Benchmarking


To asses the capabilities of LLMs, the research community has exploring
various benchmarks. Static ground-truth based benchmarks like MMLU
[*REF*], GSM8k [*REF*] among others,
cannot capture the flexible and interactive tasks found in the
real-world [*REF*; *REF*; *REF*].
More recent efforts have shifted toward evaluating LLMs on complex tasks
tasks that resemblance real-world application
[*REF*; *REF*; *REF*] or involve A/B testing with human feedback [*REF*].


For complex tasks, there is recent work on deploying generative agents
[*REF*; *REF*] for task-specific simulations,
such as collaborative agent systems for software engineering
[*REF*; *REF*; *REF*; *REF*]. These generative agent systems are increasingly utilized to create more
realistic and dynamic environments where agents can learn, adapt, and
make decisions in real-time. The integration of these systems extends
beyond software engineering [*REF*; *REF*; *REF*; *REF*].
Refer to *REF* for an extensive review of LLM agents.


Our Contribution: Our benchmark draws parallels with recent
initiatives such as GTBench by *REF*, which measures the
reasoning abilities of LLMs through game-theoretic tasks. However, our
work distinguishes itself by its grounding in economics and focusing on
cooperation dilemmas [*REF*; *REF*],
incorporating moral considerations and demanding more sophisticated
communication and negotiation skills. [GovSim] serves as a
dynamic benchmark to evaluate long-horizon behaviors realistic to future
real-world applications of AI agents.


Limitations and Future Work 


This work sets the stage for exploring more complex scenarios. One
limitation of our study is the simplified nature of the resource-sharing
scenarios. Real-world common pool resource management involves more
intricate dynamics, such as varying regeneration rates, multiple
resource types, and different stakeholder interests. Despite our
simplification, our current modeling already presents significant
challenges and is far from trivial for existing LLMs. Future work could
extend our simulation to incorporate these complexities.


Moreover, the agent&apos;s negotiation and strategy abilities are limited by
current LLM capabilities. As LLMs evolve, we expect more emergent
behaviors. Future research could enhance LLM negotiation skills and test
these improvements against our benchmark. In addition, further work
could introduce advanced adversarial agents to test the robustness of
cooperative norms against manipulation. Furthermore, it would be
valuable to explore the scalability of these norms in larger, more
diverse agent populations and their application in mixed human-AI
communities.


Conclusion 


We introduced a novel simulation platform Governance of the Commons
Simulation ([GovSim]), designed to study strategic
interactions and cooperative decision making in LLMs. In our research,
we find that only two of the 45 combinations of LLMs and scenarios
tested reach a sustainable outcome, indicating a significant gap in the
ability of the models to manage shared resources. Furthermore, we find
that by removing the ability of agents to communicate, they overuse the
shared resource, highlighting the importance of communication for
cooperation.